```{r, include=FALSE, purl=F}

knitr::opts_chunk$set(echo = TRUE,include = T, results = "hide",collapse=TRUE)
```

## Demoskript

**ANCOVA**
Experiment zur Fruchtproduktion (“Fruit”) von Ipomopsis sp. (“Fruit”) in Abhängigkeit Ungrazedvon der Beweidung (Grazing mit 2 Levels: Grazed, Ungrazed) und korrigiert für die Pflanzengrösse vor der Beweidung (hier ausgedrückt als Durchmesser an der Spitze des Wurzelstock: “Root”)

```{r}
summary(compensation)
attach(compensation)

plot(Fruit~Root)
plot(Fruit~Grazing)

tapply(Fruit,Grazing,mean)

aoc.1<-lm(Fruit~Root*Grazing)
summary.aov(aoc.1)

aoc.2<-lm(Fruit~Grazing*Root)
summary.aov(aoc.2)

aoc.3<-lm(Fruit~Grazing+Root)
summary.lm(aoc.3)

# Plotten der Ergebnisse
plot(Fruit~Root,pch=21,bg=(1+as.numeric(Grazing)))
legend(locator(1),c("grazed","ungrazed"),col=c(2,3),pch=16) # Position von Legende von Hand setzen

#legend(4.5,110,c("grazed","ungrazed"),col=c(2,3),pch=16) # Position von Legende in Code definieren
#legend("topleft",c("grazed","ungrazed"),col=c(2,3),pch=16) # Alternative position von Legende in Code definieren


abline(-127.829,23.56,col="red")
abline(-127.892+36.103,23.56,col="green")
```


**Polynomische Regression**

```{r}
e<-c(20,19,25,10,8,15,13,18,11,14,25,39,38,28,24)
f<-c(12,15,10,7,2,10,12,11,13,10,9,2,4,7,13)

summary(lm(f~e))

par(mfrow=c(1,1))
plot(f~e,xlim=c(0,40),ylim=c(0,30))
abline(lm(f~e))

par(mfrow=c(2,2))
plot(lm(f~e))
plot(lm(f~e+I(e^2)))

summary(lm(f~e+I(e^2)))
```

**Multiple lineare Regression basierend auf Logan, Beispiel 9A** 

```{r}
loyn <- read.table("loyn.csv", header=T,sep=",")
loyn
library(car)

summary(loyn)

lm.1 <- lm (ABUND ~ YR.ISOL + ALT + GRAZE, data=loyn)
summary(lm.1)

aov(lm.1)
par(mfrow=c(2,2))
plot(lm.1)
influence.measures(lm.1)

cor <- cor(loyn[,2:7])
print(cor, digits=2)

cor[abs(cor)<0.6] <- 0
cor
print(cor, digits=3)

vif(lm.1)
```

**Simulation Overfitting**

```{r}
test <- data.frame("x"=c(1,2,3,4,5,6),"y"=c(34,21,70,47,23,45))
attach(test)

plot(x,y)
lm0 <- lm(y~1)
lm1 <- lm(y~x)
lm2 <- lm(y~x+I(x^2))
lm3 <- lm(y~x+I(x^2)+I(x^3))
lm4 <- lm(y~x+I(x^2)+I(x^3)+I(x^4))
lm5 <- lm(y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5))
lm6 <- lm(y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)+I(x^6))
summary(lm0)
summary(lm1)
summary(lm2)
summary(lm3)
summary(lm4)
summary(lm5)

xv<-seq(from=0,to=10,by=0.1)

plot(x,y,cex=2,col="black",lwd=3)
yv<-predict(lm1,list(x=xv))
lines(xv,yv,col="red",lwd=3)
yv<-predict(lm2,list(x=xv))
lines(xv,yv,col="blue",lwd=3)
yv<-predict(lm3,list(x=xv))
lines(xv,yv,col="green",lwd=3)
yv<-predict(lm4,list(x=xv))
lines(xv,yv,col="orange",lwd=3)
yv<-predict(lm5,list(x=xv))
lines(xv,yv,col="black",lwd=3)
```

**Modellvereinfachung (mit Loyn-Datensatz)**
```{r}
lm.1 <- lm(ABUND ~ YR.ISOL + ALT + GRAZE, data=loyn)
summary(lm.1)
lm.2 <- update(lm.1,~.-YR.ISOL)

summary(lm.2)

anova(lm.1,lm.2)
```

**Hierarchical partitioning**
```{r}
if(!require(hier.part)){install.packages("hier.part")}
library(hier.part)

loyn.preds <-with(loyn, data.frame(YR.ISOL,ALT,GRAZE))
par(mfrow=c(1,1))
hier.part(loyn$ABUND,loyn.preds,gof="Rsqu")
```

**Partial regressions**

```{r}
avPlots(lm.1,ask=F)
```

**Multimodel inference**

```{r}
if(!require(MuMIn)){install.packages("MuMIn")}
library(MuMIn)

global.model <- lm (ABUND ~ YR.ISOL + ALT + GRAZE, data=loyn)
options(na.action="na.fail")
allmodels <- dredge(global.model)
allmodels
importance(allmodels)

avgmodel<-model.avg(get.models(dredge(global.model,rank="AICc"),subset=TRUE))
summary(avgmodel)
```
