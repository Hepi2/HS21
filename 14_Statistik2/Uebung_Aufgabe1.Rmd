
```{r, message=F}
library(tidyverse)
library(ggfortify) # um autoplot() für lm/aov nutzen zu können

decay <- read_csv("14_Statistik2/data/decay.csv")

```

##### Lösungsweg 1: $log_{10}$-Transformation

We start by fitting a straight line through the scatterplot, using fitline with a linear model:

```{r}
ggplot(decay, aes(time, amount)) +
  geom_point() +
  geom_smooth(method = "lm")

```

This draws attention to the pronounced curvature in the data. Most of the residuals at low values of t ime are positive, most of the residuals for intermediate values of t ime are negative, and most of the residuals at high values of t ime are positive. This is clearly not a good model for these data. There is a very important point here. If, instead of looking at the fit of the model to the data using plot , we had simply done the statistics, then we might easily have come to the opposite conclusion. Here is a summary of the linear model applied to these data:

```{r}
summary(lm(amount~time, decay))
```


The model explains more than 76% of the variation in the response (a very high value of $r$-squared) and the $p$ value is vanishingly small. The moral is that $p$ values and $r$-squared are *not* good measures of model adequacy.

Because the data relate to a decay process, it might be that an exponential function $y = ae^{-bx}$ describes the data better. If we can linearize this equation, then we can estimate the parameter values using a linear model. Let us try taking logs of both sides

$$y = ae^{-bx}$$
$$log(y) = log(a) - bx$$


If we replace log(y) by Y and log(a) by A, you can see that we have a linear model:
$$Y = A - bx$$

The intercept of this linear model is A and the slope is $-b$. To fit the model we have the *untransformed values* of time on the x axis and the $log$ of amount on the y axis:

```{r}
ggplot(decay, aes(time, log(amount))) +
  geom_point() +
  geom_smooth(method = "lm")

```


The fit to the model is greatly improved. There is a new issue, however, in that the variance appears to increase with time and, as you will recall, non-constant variance is a potentially serious problem. Let us estimate the parameter values of this exponential model and then check its assumptions using plot (model ).

```{r}
model <- lm(log(amount)~time, decay)
summary(model)
```

The slope of the straight line is -0.068528 and its standard error is 0.005743. The value of $r~2$ is even higher following transformation (83%) and the $p$ value is even lower. The intercept of 4.547386 with its standard error of 0.100295 is for A, not the value we need for our exponential equation, but a is the antilog of A. When we back-transform, the standard errors become asymmetric up and down. It may take a moment for you to see why this is the case. Let us add one standard error to the intercept and subtract one standard error from it to get upper and lower intervals.

```{r}
upper <- 4.547386 + 0.100295
lower <- 4.547386 - 0.100295
```

Now we return to the original scale of measurement by taking antilogs using exp:

```{r}
exp(upper)
exp(lower)
```

so the intercept on the original axis is between 85.38 and 104.34, but the best estimate for the intercept is
```{r}
exp(4.547386)
```

which means that the interval above the intercept is 9.957 but the interval below it is 9.007. Beginners often find it disconcerting that the two unreliability measures are different sizes.

Now we check the assumptions of the model using `autoplot(model)`:


```{r}
autoplot(model)
```

The good news is that the normality of errors assumption looks good (the top right plot is reasonably straight). As we guessed by looking at the transformed data, however, the variance does show strong signs of non-constancy (the top left and bottom left plots). The bottom right plot shows that data points 30 and 31 have high leverage and point number 28 has a large residual. We shall see how deal with these issues later, but for the moment, we want to plot the curved line through the scatterplot on the original scale of measurement.


```{r}

fun.1 <- function(x){94.38536 * exp(-0.068528 * x)}


ggplot(decay, aes(time, amount)) +
  geom_point() +
  stat_function(fun = fun.1)
```

As you can see, our model is a good description of the data for intermediate values of time, but the model is poor at predicting amount for time = 0 and for time > 28. Clearly, more work is required to understand what is going on at the extremes, but exponential decay describes the central part of the data reasonably well.



##### Lösungsweg 2: Polynomial Regression

The relationship between $y$ and $x$often turns out not to be a straight line. But Occam’s razor requires that we fit a linear model unless a non-linear relationship is significantly better at describing the data. So this begs the question: how do we assess the significance of departures from linearity? One of the simplest ways is to use polynomial regression

$$y = a + bx + cx^2 + dx^3+ ...$$

The idea of polynomial regression is straightforward. As before, we have just one continuous explanatory variable, x, but we can fit higher powers of $x$, such as $x^2$ and $x^3$, to the model in addition to $x$ to describe curvature in the relationship between $y$ and $x$. It is useful to experiment with the kinds of graphs that can be generated with very simple models. Even if we restrict ourselves to the inclusion of a quadratic term, $x^2$, there are many curves we can describe, depending upon the signs of the linear and quadratic terms:

```{r}
df <- data.frame(x = 0:10)

ggplot(df) +
  lims(x= c(0,10), y = c(4,14)) +
  stat_function(fun = function(x)4+2*x-0.1*x^2, col = "blue") +
  stat_function(fun = function(x)4+2*x-0.2*x^2, col = "green") +
  stat_function(fun = function(x)12-4*x+0.3*x^2, col = "red") +
  stat_function(fun = function(x)4+0.5*x+0.1*x^2, col = "yellow")
```

The first function (blue) describes a curve with positive but declining slope, with no hint of a hump ($y = 4 + 2x - 0.1x^2$). The second (green) has a curve with a clear maximum ($y = 4+2x-0.2x^2$), and the third (red) has a curve with a clear minimum ($y= 12-4x+0.35x^2$). The last curve (yellow) shows a positive association between $y$ and $x$ with the slope increasing as $x$ increases ($y=4+0.5x+0.1x^2$). So you can see that a simple quadratic model with just three parameters (an intercept, a slope for $x$, and a slope for $x^2$) is capable of describing a wide range of functional relationships between $y$ and $x$. It is very important to understand that the quadratic model describes the relationship between $y$ and $x$; it does not pretend to explain the mechanistic (or causal) relationship between $y$ and $x$. 

We can use the decay data as an example of model comparison. How much better than a linear model with two parameters (call it `model2`) is a quadratic with three parameters (`model3`)? The function I stands for "as is" and allows you to use arithmetic operators like caret (^ for calculating powers) in a model formula where the same symbol would otherwise mean something different (in a model formula, caret means the order of interaction terms to be fitted).
```{r}



model2 <- lm(amount~time, decay)
model3 <- lm(amount~time+I(time^2), decay)
summary(model3)



```
