--- 
title: "Research Methods"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
#output: bookdown::gitbook
documentclass: book
bibliography: [00_Admin/book.bib, 00_Admin/packages.bib]
biblio-style: apalike
link-citations: yes
description: "Begleitmaterial zum Modul 'Research Methods' "
---

# Einleitung


Das Modul „Research Methods“ vermittelt vertiefte Methodenkompetenzen für praxisorientiertes und angewandtes wissenschaftliches Arbeiten im Fachbereich „Umwelt und Natürliche Ressourcen“ auf MSc-Niveau. Die Studierenden erarbeiten sich vertiefte Methodenkompetenzen für die analytische Betrachtung der Zusammenhänge im Gesamtsystem „Umwelt und Natürliche Ressourcen“. Die Studierenden erlernen die methodischen Kompetenzen, auf denen die nachfolgenden Module im MSc Programm UNR aufbauen. Das Modul vermittelt einerseits allgemeine, fächerübergreifende methodische Kompetenzen (z.B. Wissenschaftstheorie, computer-gestützte Datenverar-beitung und Statistik).

Auf dieser Plattform (RStudio Connect) werden die Unterlagen für die R-Übungsteile bereitgestellt. Es werden sukzessive sowohl Demo-Files, Aufgabenstellungen und Lösungen veröffentlicht.




```{r, include=F, message=F}
# Set Root Directory / Working directory to Project folder for all Files (if M-K)
knitr::opts_knit$set(root.dir = getwd())
```


```{r, include=F, message=F}


grepl_loop <- function(vector,remove){
  for(remove_i in remove){
    vector <- vector[!grepl(remove_i,vector)]
  }
  return(vector)
}




# Allow duplicate Labels so that calling purl() does not create an error
# https://stackoverflow.com/q/36868287/4139249
options(knitr.duplicate.label = 'allow')

# purl all Rmd Documents (with some exceptions) and store them in a Subfolder /RFiles
# Document cannot be knitted if the folder "RFiles" does not exist!
library(stringr)

keywords <- c("ResearchMethods","_Rcode","99_","index","Archive","Admin","main","Abstract")


rmds <- list.files(pattern = ".Rmd",recursive = T)

rmds <- rmds[grepl("Prepro|InfoVis|RaumAn",rmds)]

rmds <- grepl_loop(rmds,keywords)


# 2019-08-15 rata: folgender Teil ist auskommentiert, um die Komplexität des ganzen zu verkleinern
for (file in rmds){
  file_r <- gsub("Rmd","R",file)                          # change fileextension from .rmd to r
  file_r <- str_split_fixed(file_r,"/",Inf)               # split path at /
  file_r <- append(file_r, "RFiles",length(file_r)-1)     # append Foldername "RFiles" in 2nd last pos
  file_r <- paste(file_r,collapse = "/")                  # collapse vector to string
  if(file.exists(file_r)){
    file.remove(file_r)
  }
  knitr::purl(file,documentation = 0,output = file_r)
}

```




```{r include=FALSE, message=F}
# automatically create a bib database for R packages
# 2019-08-15 rata: folgender Teil ist auskommentiert, um die Komplexität des ganzen zu verkleinern
# knitr::write_bib(c(
#   .packages(), 'bookdown', 'knitr','forcats','carData', 'rmarkdown','tidyverse','plotly','car','ggfortify','boot','pander','scales','multicomp','ggExtra','lubridate','dplyr','purrr','readr','tidyr','tibble','ggplot2','webshot','bindrcpp','GGally','hier.part','gtools','MuMIn','nlme','lme4','languageR','lmerTest','rms','SparseM','Hmisc','Formula','survival','lattice','Matrix'
# ), '00_Admin/packages.bib')

```
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:index.Rmd-->

# PrePro1 (14.10.2019)

Die Datenkunde 2.0 gibt den Studierenden das Wissen und die Fertigkeiten an die Hand, selbst erhobene und bezogene Daten für Ihre eigenen Analysen vorzubereiten und anzureichern (preprocessing). Die Einheit vermittelt zentrale Datenverarbeitungskompetenzen und thematisiert bekannte Problemzonen der umweltwissenschaftlichen Datenverarbeitung – immer mit einer „hands-on“ Perspektive auf die begleitenden R-Übungen. Die Studierenden lernen die Eigenschaften ihrer Datensätze in der Fachsprache korrekt zu beschreiben. Sie lernen ausserdem Metadaten zu verstehen und die Implikationen derselben für ihre eigenen Analyseprojekte kritisch zu beurteilen. Zentrale Konzepte der Lerneinheit sind Skalenniveaus, Datentypen, Zeitdaten und Typumwandlungen.


```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:09_PrePro1/Abstract.Rmd-->


```{r, include=FALSE, purl=F}

knitr::opts_chunk$set(echo = TRUE,include = T, collapse=TRUE)
```

## Demo: Datentypen, Tabellen

[R-Code als Download](09_PrePro1/RFiles/Demo_Datentypen.R)

### Datentypen 


#### Numerics

Unter die Kategorie `numeric` fallen in R zwei Datentypen:

- `double`: Gleitkommazahl (z.B. 10.3, 7.3)
- `integer`: Ganzzahl (z.B. 10, 7)

##### Doubles

Folgendermassen wird eine Gleitkommazahl einer Variabel zuweisen:

```{r}
x <- 10.3

x

typeof(x)
```



Statt `<-`kann auch `=` verwendet werden. Dies funktioniert aber nicht in allen Situationen, und ist zudem leicht mit `==` zu verwechseln.

```{r}
y = 7.3

y
```



Ohne explizite Zuweisung nimmt R immer den Datentyp `double`an:

```{r}
z <- 42
typeof(z)
is.integer(z)
is.numeric(z)
is.double(z)

```

#### Ganzzahl / Integer 


Erst wenn man eine Zahl explizit als `integer` definiert (mit `as.integer()` oder `L`), wird sie auch als solches abgespeichert.

```{r}
a <- as.integer(z)
is.numeric(a)
is.integer(a)

c <- 8L
is.numeric(c)
is.integer(c)
```




```{r}
typeof(a)

is.numeric(a)
is.integer(a)
```



Mit `c()` können eine Reihe von Werten in einer Variabel zugewiesen werden (als `vector`). Es gibt zudem auch `character vectors`. 

```{r}
vector <- c(10,20,33,42,54,66,77)
vector
vector[5]
vector[2:4]

vector2 <- vector[2:4]
```



Eine Ganzzahl kann explizit mit `as.integer()` definiert werden.

```{r}
a <- as.integer(7)
b <- as.integer(3.14)
a
b
typeof(a)
typeof(b)
is.integer(a)
is.integer(b)

```

Eine Zeichenkette kann als Zahl eingelesen werden.

```{r}
c <- as.integer("3.14")
c
typeof(c)
```


#### Logische Abfragen 

Wird auch auch als boolesch (Eng. **boolean**) bezeichnet.

```{r}
e <- 3
f <- 6
g <- e > f
e
f
g
typeof(g)

```

#### Logische Operationen


```{r}
sonnig <- TRUE
trocken <- FALSE

sonnig & !trocken
```

Oft braucht man auch das Gegenteil / die Negation eines Wertes. Dies wird mittels `!` erreicht

```{r}
u <- TRUE
v <- !u 
v
```



#### Zeichenketten

Zeichenketten (Eng. **character**) stellen Text dar

```{r}
s <- as.character(3.14)
s
typeof(s)
```



Zeichenketten verbinden / zusammenfügen (Eng. **concatenate**)

```{r}
fname <- "Hans"
lname <- "Muster"
paste(fname,lname)

fname2 <- "hans"
fname == fname2
```


#### `Factors`

Mit `Factors` wird in R eine Sammlung von Zeichenketten bezeichnet, die sich wiederholen, z.B. Wochentage (es gibt nur 7 unterschiedliche Werte für "Wochentage").

```{r}
wochentage <- c("Montag","Dienstag","Mittwoch","Donnerstag","Freitag","Samstag","Sonntag",
                "Montag","Dienstag","Mittwoch","Donnerstag","Freitag","Samstag","Sonntag")

typeof(wochentage)

wochentage_fac <- as.factor(wochentage)

wochentage
wochentage_fac


```

Wie man oben sieht, unterscheiden sich `character vectors` und `factors` v.a. dadurch, dass letztere über sogenannte `levels` verfügt. Diese `levels` entsprechen den Eindeutigen (`unique`) Werten.

```{r}
levels(wochentage_fac)

unique(wochentage)
```



#### Zeit/Datum

Um in R mit Datum/Zeit Datentypen umzugehen, müssen sie als `POSIXct` eingelesen werden (es gibt alternativ noch `POSIXlt`, aber diese ignorieren wir mal). Anders als Beispielsweise bei Excel, sollten in R Datum und Uhrzeit immer in **einer Spalte** gespeichert werden.

```{r}
datum <- "2017-10-01 13:45:10"

as.POSIXct(datum)

```

Wenn das die Zeichenkette in dem obigen Format (Jahr-Monat-Tag Stunde:Minute:Sekunde) daher kommt, braucht `as.POSIXct`keine weiteren Informationen. Sollte das Format von dem aber Abweichen, muss man der Funktion das genaue Schema jedoch mitteilen. Der Syntax dafür kann via `?strptime` nachgeschlagen werden.

```{r}
datum <- "01.10.2017 13:45"

as.POSIXct(datum,format = "%d.%m.%Y %H:%M")

```

Beachtet, dass in den den obigen Beispiel R automatisch eine Zeitzone angenommen hat (`CEST`). R geht davon aus, dass die Zeitzone der **System Timezone** (`Sys.timezone()`) entspricht.


### Data Frames und Conveniance Variabeln

Eine `data.frame` ist die gängigste Art, Tabellarische Daten zu speichern. 

```{r}
df <- data.frame(
  Stadt = c("Zürich","Genf","Basel","Bern","Lausanne"),
  Einwohner = c(396027,194565,175131,140634,135629),
  Ankunft = c("1.1.2017 10:00","1.1.2017 14:00",
              "1.1.2017 13:00","1.1.2017 18:00","1.1.2017 21:00")
)

str(df)

```

In der obigen `data.frame` wurde die Spalte `Einwohner` als Fliesskommazahl abgespeichert. Dies ist zwar nicht tragisch, aber da wir wissen das es sich hier sicher um Ganzzahlen handelt, können wir das korrigieren. Wichtiger ist aber, dass wir die Ankunftszeit (Spalte`Ankunft`) von  einem `Factor` in ein Zeitformat (`POSIXct`) umwandeln. 


```{r}
df$Einwohner <- as.integer(df$Einwohner)

df$Einwohner

df$Ankunft <- as.POSIXct(df$Ankunft, format = "%d.%m.%Y %H:%M")

df$Ankunft
```


Diese Rohdaten können nun helfen, um Hilfsvariablen (**convenience variables**) zu erstellen. Z.B. können wir die Städte einteilen in gross, mittel und klein. 

```{r}
df$Groesse[df$Einwohner > 300000] <- "gross"
df$Groesse[df$Einwohner <= 300000 & df$Einwohner > 150000] <- "mittel"
df$Groesse[df$Einwohner <= 150000] <- "klein"

```



Oder aber, die Ankunftszeit kann von der Spalte `Ankunft`abgeleitet werden. Dazu brauchen wir aber das Package `lubridate`

```{r, message = F}
library(lubridate)
```


```{r}
df$Ankunft_stunde <- hour(df$Ankunft)
```

### Quellen

```{r code=readLines('00_Admin/get_chapter_references.R'), echo=F, eval=T,purl = F, results="asis"}
```




```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:09_PrePro1/Demo_Datentypen.Rmd-->


```{r, include=FALSE, purl = F}
knitr::opts_chunk$set(echo = F, include = T, collapse=TRUE, warning = F)
```


## Übung A

R ist ohne Zusatzpackete nicht mehr denkbar. Die allermeisten Packages werden auf [CRAN](https://cran.r-project.org/) gehostet und können leicht mittels `install.packages()` installiert werden. Eine sehr wichtige Sammlung von Packages wird von RStudio entwickelt. Unter dem Namen [Tidyverse](https://www.tidyverse.org/) werden eine Reihe von Packages angeboten, den R-Alltag enorm erleichtert. Wir werden später näher auf das "Tidy"-Universum eingehen, an dieser Stelle können wir die Sammlung einfach mal installieren.

```
install.packages("tidyverse")
```

Um ein `package` in R verwenden zu können, gibt es zwei Möglichkeiten: 

- entweder man lädt es zu Beginn der R-session mittles `library()`. 
- oder man ruft eine `function` mit vorangestelltem Packetname sowie zwei Doppelpunkten auf. `dplyr::filter()` ruft die Funktion `filter()` des Packets `dplyr` auf. 

Letztere Notation ist vor allem dann sinnvoll, wenn sich zwei unterschiedliche Funktionen mit dem gleichen namen in verschiedenen pacakges existieren. `filter()` existiert als Funktion einersits im package `dplyr` sowie in  `stats`. Dieses Phänomen nennt man "masking". 


Zu beginn laden wir die nötigen Pakete:


```{r,message = F}
library(tidyverse)
# Im Unterschied zu `install.packages()` werden bei `library()` keine Anführungs- 
# und Schlusszeichen gesetzt.


library(lubridate)
# Im Unterschied zu install.packages("tidyverse") wird bei library(tidyverse) 
# das package lubridate nicht berücksichtigt
```


### Aufgabe 1

Erstelle eine `data.frame` mit nachstehenden Daten.

Tipps:

- Eine leere `data.frame` zu erstellen ist schwieriger als wenn erstellen und befüllen der `data.frame` in einem Schritt erfolgt
- R ist dafür gedacht, Spalte für Spalte zu arbeiten ([warum?](http://www.noamross.net/blog/2014/4/16/vectorization-in-r--why.html)), nicht Reihe für Reihe. Versuche dich an dieses Schema zu halten.

```{r}

# Lösung Aufgabe 1

df <- data.frame(
  Tierart = c("Fuchs","Bär","Hase","Elch"),
  Anzahl = c(2,5,1,3),
  Gewicht = c(4.4, 40.3,1.1,120),
  Geschlecht = c("m","f","m","m"),
  Beschreibung = c("Rötlich","Braun, gross", "klein, mit langen Ohren","Lange Beine, Schaufelgeweih")
  )

```


```{r, echo = F, purl=F}
knitr::kable(df)
```



### Aufgabe 2

Was für Datentypen wurden (in Aufgabe 1) von R automatisch angenommen? Sind diese sinnvoll? 

Tipp: Nutze dazu `str()`

```{r}
# Lösung Aufgabe 2

str(df)

# Anzahl wurde als `double` interpretiert, ist aber eigentlich ein `integer`. 
# Mit data.frame() wurde Beschreibung wurde als `factor` interpretiert, ist 
# aber eigentlich `character`
```


```{r}


typeof(df$Anzahl)

df$Anzahl <- as.integer(df$Anzahl)
df$Beschreibung <- as.character(df$Beschreibung)

```


### Aufgabe 3


Nutze die Spalte `Gewicht` um die Tiere in 3 Gewichtskategorien einzuteilen: 

- leicht: < 5kg
- mittel: 5 - 100 kg
- schwer: > 100kg


```{r}

# Lösung Aufgabe 3

df$Gewichtsklasse[df$Gewicht > 100] <- "schwer"
df$Gewichtsklasse[df$Gewicht <= 100 & df$Gewicht > 5] <- "mittel"
df$Gewichtsklasse[df$Gewicht <= 5] <- "leicht"

```


```{r, purl=F}
knitr::kable(df)
```




### Aufgabe 4

Importiere den Datensatz [order_52252_data.txt](09_PrePro1/data/order_52252_data.txt). Es handelt sich dabei um die stündlich gemittelten Temperaturdaten an verschiedenen Standorten in der Schweiz im Zeitraum 2000 - 2005. Wir empfehlen `read_table()`^[@wickham2017, Kapitel 8 bzw. http://r4ds.had.co.nz/data-import.html)] anstelle von `read.table()`.

```{r, message = F}
# Lösung Aufgabe 4

wetter <- readr::read_table("09_PrePro1/data/order_52252_data.txt")
```




```{r, purl=F}
knitr::kable(head(wetter,10))
```


### Aufgabe 5

Schau dir die Rückmeldung von `read_table()`an. Sind die Daten korrekt interpretiert worden?


```{r}
# Lösung Aufgabe 5
# Die Spalte 'time' wurde als 'integer' interpretiert. Dabei handelt es
# sich offensichtlich um Zeitangaben.
```



### Aufgabe 6

Die Spalte `time` ist eine Datum/Zeitangabe im Format JJJJMMTTHH (siehe [meta.txt](09_PrePro1/data/meta.txt)). Damit R dies als Datum-/Zeitangabe erkennt, müssen wir die Spalte in einem R-Format (`POSIXct`) einlesen und dabei R mitteilen, wie sie aktuell formatiert ist. Lies die Spalte mit `as.POSIXct()` (oder `parse_datetime`) ein und spezifiziere sowohl `format` wie auch `tz`. 

Tipps: 

- Wenn keine Zeitzone festgelegt wird, trifft `as.POSIXct()` eine Annahme (basierend auf `Sys.timezone()`). In unserem Fall handelt es sich aber um Werte in UTC (siehe [meta.txt](09_PrePro1/data/meta.txt))
- `as.POSIXct`erwartet `character`: Wenn du eine Fehlermeldung hast die `'origin' must be supplied` (o.ä) heisst, hast du der Funktion vermutlich einen `Numeric` übergeben.

```{r}
# Lösung Aufgabe 6

# mit readr
parse_datetime(as.character(wetter$time[1:10]), format = "%Y%m%d%H")


# mit as.POSIXct()
wetter$time <- as.POSIXct(as.character(wetter$time), format = "%Y%m%d%H",tz = "UTC")

```


```{r, purl=F}
knitr::kable(head(wetter,10))
```




### Aufgabe 7


Erstelle zwei neue Spalten mit Wochentag (Montag, Dienstag, etc) und Kalenderwoche. Verwende dazu die neu erstellte `POSIXct`-Spalte


```{r}

# Lösung Aufgabe 7

wetter$wochentag <- wday(wetter$time,label = T)
wetter$kw <- week(wetter$time)

```


```{r, purl=F}
knitr::kable(head(wetter,10))
```





### Aufgabe 8


Erstelle eine neue Spalte basierend auf die Temperaturwerte mit der Einteilung "kalt" (Unter Null Grad) und "warm" (über Null Grad)

```{r}

# Lösung Aufgabe 8

wetter$temp_kat[wetter$tre200h0>0] <- "warm"
wetter$temp_kat[wetter$tre200h0<=0] <- "kalt"
```


```{r, purl=F}
knitr::kable(head(wetter,10))
```


```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:09_PrePro1/Uebung_A.Rmd-->

## Übung A Lösung

[R-Script als Download](09_PrePro1/RFiles/Uebung_A.R)

```{r code=readLines('09_PrePro1/RFiles/Uebung_A.R'), echo=T, eval=F}
```

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:09_PrePro1/Uebung_A_loesung.Rmd-->


```{r, include=FALSE, purl = F}

knitr::opts_chunk$set(echo = F, include = T, collapse=TRUE)
# knitr::opts_knit$set(root.dir = "09_PrePro1") 

```



## Übung B 




```{r, message = F}
library(tidyverse)
```



Fahre mit dem Datensatz `wetter` aus Übung A fort. 
```{r, purl=F}
wetter <- read_table("09_PrePro1/data/order_52252_data.txt",
                  col_types = list(
                    col_factor(levels = NULL),    
                    col_datetime(format = "%Y%m%d%H"),
                    col_double()
                    )
                  )
```


### Aufgabe 1

Nutze `plot()` um die Temparaturkurve zu visualisieren. Verwende aber vorher `filter()` um dich auf eine Station (z.B. "`ABO`") zu beschränken (es handelt sich sonst um zuviele Datenpunkte).



```{r}
# Lösung Aufgabe 1

wetter_fil <- dplyr::filter(wetter, stn == "ABO")

plot(wetter_fil$time,wetter_fil$tre200h0, type = "l")

```


Nun schauen wir uns das plotten mit `ggplot2` an. Ein simpler Plot wie der in der vorherigen Aufgabe ist in `ggplot2` zugegebenermassen *etwas* komplizierter. `ggplot2` wird aber rasch einfacher, wenn die Grafiken komplexer werden. Wir empfehlen deshalb stark, `ggplot2` zu verwenden.

Schau dir ein paar online Tutorials zu `ggplot2` an (siehe ^[@wickham2017, Kapitel 1 bzw. [http://r4ds.had.co.nz/data-visualisation.html](http://r4ds.had.co.nz/data-visualisation.html) oder hier ein sehr schönes Video: [Learn R: An Introduction to ggplot2](https://youtu.be/YxKr2a-Y1WE?t=1m40s)]) 
und reproduziere den obigen Plot mit `ggplot2`


```{r}

p <- ggplot(wetter_fil, aes(time,tre200h0)) +
  geom_line()

p
```



### Aufgabe 2

Spiele mit Hilfe der erwähnten Tutorials mit dem Plot etwas rum. Versuche die x-/y-Achsen zu beschriften sowie einen Titel hinzu zu fügen.

```{r}
# Lösung Aufgabe 2
p <- p +
  labs(x = "Datum", y = "Temperatur", title = "Stündlich gemittelte Temperaturwerte")

p
```


### Aufgabe 3

Reduziere den x-Achsenausschnitt auf einen kleineren Zeitraum, beispielsweise einn beliebigen Monat. Verwende dazu `lims()` zusammen mit `as.POSIXct()` oder mache ein Subset von deinem Datensatz mit einer convenience-Variabel und `filter()`.

```{r}
# Lösung Aufgabe 3

limits <- as.POSIXct(c("2002-01-01 00:00:00","2002-02-01 00:00:00"),tz = "UTC")

p +
  lims(x = limits)
```



```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:09_PrePro1/Uebung_B.Rmd-->

## Übung B Lösung

[R-Code als Download](09_PrePro1/Uebung_B.R)

```{r code=readLines('09_PrePro1/RFiles/Uebung_B.R'), echo=T, eval=F}
```

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:09_PrePro1/Uebung_B_loesung.Rmd-->

# PrePro2 (15.10.2019)

Die Lerneinheit vermittelt zentralste Fertigkeiten zur Vorverarbeitung von strukturierten Daten in der umweltwissenschaftlichen Forschung: Datensätze verbinden (Joins) und umformen („reshape“, „split-apply-combine“). Im Anwendungskontext haben Daten selten von Anfang an diejenige Struktur, welche für die statistische Auswertung oder für die Informationsvisualisierung erforderlich wäre. In dieser Lerneinheit lernen die Studierenden die für diese oft zeitraubenden Preprocessing-Schritte notwendigen Konzepte und R-Werkzeuge kennen und kompetent anzuwenden.
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:10_PrePro2/Abstract.Rmd-->

```{r, include=F, purl=F}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,include = T, collapse=TRUE)
```

## Ergänzungen zu PrePro 1

### Integer mit "L"

In `R` kann eine Zahl mit dem Suffix "L" explizit als Integer spezifiziert werden. 

```{r, parse = F}
typeof(42)
typeof(42L)
```


Warum dazu der Buchstabe "L" verwendet wird ist nirgends offiziell Dokumentiert (zumindest haben wir nichts gefunden). Die gängigste Meinung, die auch [von renommierten R-Profis vertreten wird](https://hypatia.math.ethz.ch/pipermail/r-devel/2017-June/074467.html
) ist, dass damit `Long integer` abgekürzt wird.



### Arbeiten mit RStudio "Project"

Wir empfehlen die Verwendung von "Projects" innerhalb von RStudio. RStudio legt für jedes Projekt dann einen Ordner an, in welches die Projekt-Datei abgelegt wird (Dateiendung .Rproj). Sollen innerhalb des Projekts dann R-Skripts geladen oder erzeugt werden, werden diese dann auch im angelegten Ordner abgelegt. Mehr zu RStudio Projects findet ihr  [hier](https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects).


Das Verwenden von Projects bringt verschiedene Vorteile, wie zum Beispiel:

- Festlegen der Working Directory ohne die Verwendung des expliziten Pfades (`setwd()`). Das ist sinnvoll, da sich dieser Pfad ändern kann (Zusammenarbeit mit anderen Usern, Ausführung des Scripts zu einem späteren Zeitpunkt) 
- Automatisches Zwischenspeichern geöffneter Scripts und Wiederherstellung der geöffneten Scripts bei der nächsten Session
- Festlegen verschiedener projektspezifischer Optionen
- Verwendung von Versionsverwaltungssystemen (Github oder SVN)



### Arbeiten mit `factors`

Wie bereits angedeutet, ist das Arbeiten mit `factors` etwas gewöhnungsbedürftig. Wir gehen hier auf ein paar Stolpersteine ein.

```{r}
zahlen <- factor(c("null","eins","zwei","drei"))

zahlen
```

Offensichtlich sollten diese `factors` geordnet sein, R weiss davon aber nichts. Eine Ordnung kann man mit dem Befehl `ordered = T` festlegen. 

Beachtet: `ordered = T` kann nur bei der Funktion `factor()` spezifiziert werden, nicht bei `as.factor()`. Ansonsten sind `factor()` und `as.factor()` sehr ähnlich.


```{r}
zahlen <- factor(zahlen,ordered = T)

zahlen
```

Beachtet das "<"-Zeichen zwischen den Levels. Die Zahlen werden nicht in der korrekten Reihenfolge, sondern Alphabetisch geordnet. Die richtige Reihenfolge kann man mit `levels = ` festlegen.

```{r}
zahlen <- factor(zahlen,ordered = T,levels = c("null","eins","zwei","drei","vier"))

zahlen
```

Wie auch schon erwähnt werden `factors` als `character` Vektor dargestellt, aber als Integers gespeichert. Das führt zu einem scheinbaren Wiederspruch wenn man den Datentyp auf unterschiedliche Weise abfragt.
```{r}
typeof(zahlen)

is.integer(zahlen)
```


Mit `typeof()` wird eben diese Form der Speicherung abgefragt und deshalb mit `integer` beantwortet. Da es sich aber nicht um einen eigentlichen Integer Vektor handelt, wird die Frage `is.integer()` mit `FALSE` beantwortet. Das ist etwas verwirrend, beruht aber darauf, dass die beiden Funktionen die Frage von unterschiedlichen Perspektiven beantworten. In diesem Fall schafft `class()` Klarheit:

```{r}
class(zahlen)
```


Wirklich verwirrend wird es, wenn `factors` in numeric umgewandelt werden sollen.

```{r}
zahlen
as.integer(zahlen)
```

Das die Übersetzung der auf Deutsch ausgeschriebenen Nummern in nummerische Zahlen nicht funktionieren würde, war ja klar. Weniger klar ist es jedoch, wenn die `factors` bereits aus nummerischen Zahlen bestehen.

```{r}
zahlen2 <- factor(c("3","2","1","0"))

as.integer(zahlen2)

```

In diesem Fall müssen die `factors` erstmals in `character` umgewandelt werden.

```{r}
zahlen2 <- factor(c("3","2","1","0"))

as.integer(as.character(zahlen2))
```




### Heikle Annahmen - bessere Alternativen

Aus oben beschriebenen Grund ist es auch problematisch, dass `data.frame()` sowie alle `read.*` Funktionen (`read.table`, `read.csv` etc) immer davon ausgehen, dass `strings` als `factors` interpretiert werden sollten. Es gibt in Base R einige Funktionen, welche Annahmen treffen die problematisch sein können. Ein weiteres Beispiel ist die Annahme der Zeitzone und Verwendung von Sommerzeit bei `as.POSIXct()`.

Oft gibt es dafür im Tidyverse alternative Funktionen, in denen diese Probleme besser gelöst sind. Wir empfehlen, wenn immer Möglich die Tidyverse-Alternativen zu verwenden. Beispiele:

- `data_frame()` statt `data.frame()` 
- `read_*` statt `read.*`
- `parse_datetime` statt `as.POSIXct()`


Beim Import von Daten kann es sinnvoll sein, die Datentypen der Spalten bereits _im Importbefehl_ zu spezifizieren. So vermeidet man die anschliessende  Typumwandlung und die damit verbundenen Fehlerquellen. Zudem wird der Importprozess beschleunigt, da R keine Zeit daran verschwenden muss die Datentypen (aufgrund der ersten 1000 Zeilen) zu erraten.

```{r,message=FALSE, eval = FALSE}

library(tidyverse)

df1 <- read_table("09_PrePro1/data/order_52252_data.txt",
                  col_types = list(
                    col_character(),                  # Macht aus der 1.Spalte ein character
                    col_datetime(format = "%Y%m%d%H"),# Macht aus der 2.Spalte ein POSIXct
                    col_double()                      # Macht aus der 3.Spalte ein double
                    )
                  )


df1 <- read_table("09_PrePro1/data/order_52252_data.txt",
                  col_types = list(
                    col_factor(levels = NULL),        # Macht aus der 1.Spalte ein factor
                    col_datetime(format = "%Y%m%d%H"),# Macht aus der 2.Spalte ein POSIXct
                    col_double()                      # Macht aus der 3.Spalte ein double
                    )
                  )



```


```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:10_PrePro2/Nachtrag.Rmd-->


```{r, include=F, purl=F}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,include = TRUE, collapse=TRUE)
```



## Demo: `tidyverse`

[Demoscript als Download](10_PrePro2/RFiles/Demo_Tidyverse.R)


Hier möchten wir euch mit einer Sammlung von Tools vertraut machen, die spezifisch für das Daten prozessieren in Data Science entwickelt wurden. Der Prozess und das Modell ist hier^[http://r4ds.had.co.nz/introduction.html#] schön beschrieben.
Die Sammlung von Tools wird unter dem Namen [tidyverse](https://www.tidyverse.org/) vertrieben, welches wir ja schon zu Beginn der ersten Übung installiert und geladen haben. Die Tools erleichtern den Umgang mit Daten ungeheuer und haben sich mittlerweile zu einem "must have" im Umgang mit Daten in R entwickelt. 

Wir können Euch nicht sämtliche Möglichkeiten von tidyverse zeigen. Wir fokussieren uns deshalb auf einzelne Komponenten^[`dplyr, ggplot2, tidyr, stringr, magrittr, lubridate`] und zeigen ein paar Funktionalitäten, die wir oft verwenden und Euch ggf. noch nicht bekannt sind. Wer sich vertieft mit dem Thema auseinandersetzen möchte, der sollte sich unbedingt das Buch @wickham2017 beschaffen. Eine umfangreiche, aber nicht ganz vollständige Version gibt es online^[http://r4ds.had.co.nz/], das vollständige eBook kann über die Bibliothek bezogen werden^[https://ebookcentral.proquest.com/lib/zhaw/detail.action?docID=4770093].



### Split-Apply-Combine

#### Packete laden

```{r,message=F}
library(tidyverse)
```

Mit `library(tidyverse)` werden nicht alle Packete geladen, die mit `install.packages(tidyverse)` intalliert wurden ([warum?](https://community.rstudio.com/t/which-packages-get-loaded/298)). Unter anderem muss `lubridate` noch separat geladen werden:

```{r, message=F}
library(lubridate) 
```



#### Daten Laden

Wir laden die Wetterdaten von der letzten Übung.

```{r}

wetter <- read_table("09_PrePro1/data/order_52252_data.txt",
                  col_types = list(
                    col_factor(levels = NULL),    
                    col_datetime(format = "%Y%m%d%H"),
                    col_double()
                    )
                  )

```


#### Kennwerte berechnen
Wir möchten den Mittelwert aller gemessenen Temperaturwerte berechnen. Dazu könnten wir folgenden Befehl verwenden:

```{r}
mean(wetter$tre200h0, na.rm = TRUE) 
```

Die Option `na.rm = T` bedeutet, dass NA Werte von der Berechnung ausgeschlossen werden sollen. 

Mit der selben Herangehensweise können diverse Werte berechnet werden (z.B. das Maximum (`max()`), Minimum (`min()`), Median (`median()`) u.v.m.). 

Diese Herangehensweise funktioniert nur dann gut, wenn wir die Kennwerte über *alle* Beobachtungen (Zeilen) für eine Variable (Spalte) berechnen wollen. Sobald wir die Beobachtungen gruppieren wollen, wird es schwierig. Zum Beispiel, wenn wir die durchschnittliche Temperatur *pro Jahr* berechnen wollen.


#### Convenience Variablen

Um diese Aufgabe zu lösen, muss zuerst das "Jahr" berechne werden (das Jahr ist die *convenience variabel*).   Hierfür brauchen wir die Funktion `year()` (von `lubridate`). 

Nun kann kann die **convenience Variable** "Jahr" erstellt werden. Ohne `dpylr` wird eine neue Spalte wird folgendermassen hinzugefügt. 
```{r}
wetter$year <- year(wetter$time)
```


Mit `dplyr` (siehe ^[@wickham2017, Kapitel 10 / http://r4ds.had.co.nz/transform.html]) sieht der gleiche Befehl folgendermassen aus:
```{r}
wetter <- mutate(wetter,year = year(time))
```

Der grosse Vorteil von `dplyr` ist an dieser Stelle noch nicht ersichtlich. Dieser wird aber später klar.


#### Kennwerte nach Gruppen berechnen

Jetzt kann man die `data.frame` mithilfe der Spalte "Jahr" filtern. 
```{r}
mean(wetter$tre200h0[wetter$year == 2000], na.rm = TRUE)
```

Dies müssen wir pro Jahr wiederholen, was natürlich sehr umständlich ist, v.a. wenn man eine Vielzahl an Gruppen hat (z.B. Kalenderwochen statt Jahre). Deshalb nutzen wir das package `dplyr`. Damit geht die Aufgabe (Temperaturmittel pro Jahr berechnen) folgendermassen:


```{r}
summarise(group_by(wetter,year),temp_mittel = mean(tre200h0, na.rm = TRUE))
```


#### Verketten vs. verschachteln

Auf Deutsch übersetzt heisst die obige Operation folgendermassen: 

1) nimm den Datensatz `wetter`
2) Bilde Gruppen pro Jahr  (`group_by(wetter,year)`) 
3) Berechne das Temperaturmittel (`mean(tre200h0)`)

Diese Übersetzung `R`-> Deutsch unterscheidet sich vor allem darin, dass die Operation auf Deutsch *verkettet* ausgesprochen wird (Operation 1->2->3) während der Computer *verschachtelt* liest 3(2(1)). Um `R` näher an die gesprochene Sprache zu bringen, kann man den `%>%`-Operator verwenden  (siehe ^[@wickham2017, Kapitel 14 / http://r4ds.had.co.nz/pipes.html]). 
```{r, eval = F}

summarise(group_by(wetter,year),temp_mittel = mean(tre200h0))

# wird zu:

wetter %>%                                #1) nimm den Datensatz "wetter"
  group_by(year) %>%                      #2) Bilde Gruppen pro Jahr
  summarise(temp_mittel = mean(tre200h0)) #3) berechne das Temperaturmittel 

```


Dieses Verketten mittels `%>%` macht den Code einiges schreib- und leserfreundlicher, und wir werden ihn in den nachfolgenden Übungen verwenden. Dabei handelt es sich um das package `magrittr`, welches mit `tidyverse` mitgeliefert wird. 

Zu `dplyr` und `magrittr`gibt es etliche Tutorials online (siehe^[@wickham2017, Kapitel 10 / http://r4ds.had.co.nz/transform.html, oder [Hands-on dplyr tutorial..](https://youtu.be/jWjqLW-u3hc)]), deshalb werden wir diese Tools nicht in allen Details erläutern. Nur noch folgenden wichtigen Unterschied zu zwei wichtigen Funktionen in `dpylr`: `mutate()` und `summarise()`.

- `summarise()` fasst einen Datensatz zusammen. Dabei reduziert sich die Anzahl Beobachtungen (Zeilen) auf die Anzahl Gruppen (z.B. eine zusammengefasste Beobachtung (Zeile) pro Jahr). Zudem reduziert sich die Anzahl Variablen (Spalten) auf diejenigen, die in der "summarise" Funktion spezifiziert wurde (z.B. `temp_mittel`).
- mit `mutate` wird ein `data.frame` vom Umfang her belassen, es werden lediglich *zusätzliche* Variablen (Spalten) hinzugefügt (siehe Beispiel unten).

```{r, eval=T}
# Maximal und minimal Temperatur pro Kalenderwoche
wetter %>%                              #1) nimm den Datensatz "wetter"
  filter(stn == "ABO") %>%              #2) filter auf Station namnes "ABO"
  mutate(kw = week(time)) %>%       #3) erstelle eine neue Spalte "kw"
  group_by(kw) %>%                      #4) Nutze die neue Spalte um Guppen zu bilden
  summarise(
    temp_max = max(tre200h0, na.rm = TRUE),#5) Berechne das Maximum 
    temp_min = min(tre200h0, na.rm = TRUE) #6) Berechne das Minimum
    )   
```


#### Resultate plotten

Mit diesen Tools können wir nun auch eine neue Grafik plotten, ähnlich wie in der Übung 1. Dafür müssen wir die ganzen Operationen aber zuerst in einer Variabel speichern (bis jetzt hat R zwar alles schön berechnet, aber uns nur auf die Konsole ausgegeben).

```{r, warning = F}

wetter_sry <- wetter %>%                              
  mutate(
    kw = week(time)
    ) %>%
  filter(stn == "ABO") %>%
  group_by(kw) %>%                      
  summarise(
    temp_max = max(tre200h0),               
    temp_min = min(tre200h0),
    temp_mean = mean(tre200h0)
    )  
```

Dieses Mal plotten wir nur mit `ggplot2` (siehe ^[@wickham2017, Kapitel 1 / http://r4ds.had.co.nz/data-visualisation.html oder hier ein sehr schönes Video: [Learn R: An Introduction to ggplot2](https://youtu.be/YxKr2a-Y1WE?t=1m40s)]) 
 
```{r}

ggplot() +
  geom_line(data = wetter_sry, aes(kw,temp_max), colour = "yellow") +
  geom_line(data = wetter_sry, aes(kw,temp_mean), colour = "pink") +
  geom_line(data = wetter_sry, aes(kw,temp_min), colour = "black") +
  labs(y = "temp")

```


Das sieht schon mal gut aus. Nur, wir mussten pro Linie einen eigene Zeile schreiben (`geom_line()`) und dieser eine Farbe zuweisen. Bei drei Werten ist das ja ok, aber wie sieht es denn aus wenn es Hunderte sind? Da hat ggplot natürlich eine Lösung, dafür müssen aber alle Werte in *einer* Spalte daher kommen. Das ist ein häufiges Problem: Wir haben eine *breite* Tabelle (viele Spalten), bräuchten aber eine *lange* Tabelle (viele Zeilen).


### Reshaping data

#### Breit -> lang

Da kommt `tidyverse` wieder ins Spiel. Die Umformung von Tabellen *breit*->*lang* erfolgt mittels `tidyr`(siehe ^[http://r4ds.had.co.nz/tidy-data.html#gathering]). Auch dieses package funktioniert wunderbar mit piping (`%>%`). 

```{r}
wetter_sry_long <- wetter_sry %>%
  gather(Key, Value, c(temp_max,temp_min,temp_mean))

```

Im Befehl `gather()` braucht es drei Werte:

- beliebiger Name der neuen Variablen (Spalte) für die *Schlüssel*: "temp_mean", "temp_min"... (ich verwenden den Namen: `Key`)
- beliebiger Name der neuen Variablen (Spalte) für die effektiven *Werte*: 5°C, 10°C (ich verwenden den Namen: `Value`)
- Name der (bestehenden) Variablen (Spalten), die zusammen gefasst werden sollten: `(hier: temp_max,temp_min,temp_mean`)


Die ersten 6 Zeilen von `wetter_sry`:
```{r, echo = F, purl = F}
kable(head(wetter_sry,6))
```

Die ersten 6 Zeilen von `wetter_sry_long`:
```{r, echo = F, purl=F}
kable(head(arrange(wetter_sry_long,kw),6))

```

Beachte: `wetter_sry_long` umfasst 159 Beobachtungen (Zeilen), das sind 3 mal soviel wie `wetter_sry`, da wir ja drei Spalten zusammengefasst haben.
```{r}
nrow(wetter_sry)
nrow(wetter_sry_long)
```


Statt die Variablen (Spalten) zu benennen, die zusammengefasst werden sollten, wäre es in unserem Fall einfacher, die Variablen (Spalten) zu benennen die *nicht* zusammengefasst werden sollen (`kw`):

```{r,}
wetter_sry_long <- wetter_sry %>%
  gather(Key, Value, -kw)
```

Nun können wir den obigen Plot viel einfacher erstellen:

```{r}
ggplot(wetter_sry_long, aes(kw,Value, colour = Key)) +
  geom_line()
```

Beachtet, dass wir gegenüber dem letzten Plot `colour` nun *innerhalb* von `aes()` festlegen und nicht mit einem expliziten Farbwert, sondern mit dem Verweis auf die Spalte `key`.


#### Lang -> breit

Um unsere *lange* Tabelle wieder zurück in eine *breite* zu überführen, brauchen wir lediglich einen Befehl (`spread`):

```{r}
wetter_sry_long %>%
  spread(Key,Value)
```


### Quellen

```{r code=readLines('00_Admin/get_chapter_references.R'), echo=F, eval=T,purl = F, results="asis"}
```

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:10_PrePro2/Demo_Tidyverse.Rmd-->

```{r include=FALSE, purl=F}
knitr::opts_chunk$set(echo = TRUE, include = F, collapse=TRUE)
# knitr::opts_knit$set(root.dir = "10_PrePro2") 

select <- dplyr::select


```

## Übung A


```{r,message=F}
library(tidyverse)
library(lubridate)
library(stringr)
```


### Aufgabe 1

Lade die Wetterdaten aus der letzten Übung.

```{r}
# Lösung Aufgabe 1

wetter <- read_table("09_PrePro1/data/order_52252_data.txt",
                  col_types = list(
                    col_factor(levels = NULL),    
                    col_datetime(format = "%Y%m%d%H"),
                    col_double()
                    )
                  )
```


### Aufgabe 2

Bereinige den Datensatz. Entferne z.B. alle Zeilen, bei dem der Stationsnahme oder Temperaturwerte fehlen 

```{r}
# Lösung Aufgabe 2

wetter <- wetter %>%
  filter(!is.na(stn)) %>%
  filter(!is.na(tre200h0))

```



### Aufgabe 3

Überführe die **lange** Tabelle über in eine breite. Dabei sollte jede Station eine eigene Spalte enthalten (`key`), gefüllt mit den Temperaturwerten (`value`).  Speichere diese Tabelle in einer neuen Variabel.

```{r}

# Lösung Aufgabe 3

wetter_spread <- spread(wetter, stn,tre200h0)


```



### Aufgabe 4



Importiere die Datei [order_52252_legend.csv](09_PrePro1/data/order_52252_legend.csv) (z.B. mit `read_delim`).

Hinweis: Wenn Umlaute und Sonderzeichen nicht korrekt dargestellt werden (z.B. in Gen*è*ve), hat das vermutlich mit der [Zeichencodierung](https://de.wikipedia.org/wiki/Zeichenkodierung) zu tun. Das File ist aktuell in 'ANSI' Codiert, welche für gewisse Betriebssysteme / R-Versionen ein Problem darstellt. Um das Problem zu umgehen muss man das File mit einem Editor öffnen (Windows 'Editor' oder 'Notepad++', Mac: 'TextEdit') und mit einer neuen Codierung (z.B 'UTF-8') abspeichern. Danach kann die Codierung spezifitiert werden (bei `read_delim(): mit `locale = locale(encoding = "UTF-8")`)

```{r}

# Lösung Aufgabe 4

wetter_legende <- read_delim("09_PrePro1/data/order_52252_legend.csv",delim = ";", locale = locale(encoding = "UTF-8"))

```



### Aufgabe 5


Die x-/y-Koordinaten sind aktuell in einer Spalte erfasst. Um mit den Koordinaten sinnvoll arbeiten zu können, brauchen wir die Koordinaten getrennt. Trenne die `x` und `y` Koordinaten aus der Spalte `Koordinaten` (Tipp: nutze dafür `tidyr::separate()`).

```{r}

# Lösung Aufgabe 5

# Variante mit str_split_fixed()
koordinaten <- str_split_fixed(wetter_legende$Koordinaten, "/", 2)

colnames(koordinaten) <- c("x","y")

wetter_legende <- cbind(wetter_legende,koordinaten)



# Variante mit tidyr::separate
# ich lösche die Spalten wieder, damit ich die tidyr lösung zeigen kann
wetter_legende$x <- NULL 
wetter_legende$y <- NULL

wetter_legende <- wetter_legende %>%
  separate(Koordinaten,c("x","y"),"/")

```


### Aufgabe 6

Nun wollen wir den Datensatz `wetter`mit den Informationen aus `wetter_legende`anreichern. Uns interessiert aber nur das Stationskürzel, der Name, die x/y Koordinaten sowie die Meereshöhe. Lösche die nicht benötigten Spalten (oder selektiere die benötigten Spalten).

Tipp: Nutze `select()` von `dplyr`

```{r, message=F}

# Lösung Aufgabe 6

wetter_legende <- dplyr::select(wetter_legende, stn, Name, x,y,Meereshoehe)
```


### Aufgabe 7

Nun ist der Datensatz `wetter_legende`genügend vorbereitet. Jetzt kann er mit dem Datensatz `wetter` verbunden werden. Überlege dir, welcher Join dafür sinnvoll ist und mit welchem Attribut wir "joinen" können.

Nutze die Join-Möglichkeiten von `dplyr` (Hilfe via `?dplyr::join`)  um die Datensätze `wetter` und `wetter_legende`zu verbinden.

```{r}

# Lösung Aufgabe 7

wetter <- left_join(wetter,wetter_legende,by = "stn")

# Jointyp: Left-Join auf 'wetter', da uns nur die Stationen im Datensatz 'wetter' interessieren.
# Attribut: "stn"
```

### Aufgabe 8

Berechne die Durchschnittstemperatur pro Station. Nutze dabei `dplyr::summarise()` und wenn möglich `%>%`. Speichere das Resultat in einer neuen Variabel.


```{r,warning=F}

# Lösung Aufgabe 8

wetter_sry <- wetter %>%
  group_by(stn) %>%
  summarise(temp_mean = mean(tre200h0))
```

### Aufgabe 9

Nun wollen wir das Resultat aus Aufgabe 7 nutzen, um die Durchschnittstemperatur der Meereshöhe gegenüber zu stellen. Dummerweise ging das Attribut `Meereshoehe` bei der `summarise()` Operation verloren (da bei `summarise()` alle Spalten weg fallen, die **nicht** in `group_by()` definiert wurden). Um die Spalte `Meereshoehe` beizubehalten, muss sie also unter `group_by()` aufgelistet werden. 

Wiederhole Übung 7 und siehe zu, dass die Meereshöhe beibehalten wird. Stelle danach in einem Scatterplot (wenn möglich mit `ggplot()`) die Meereshöhe der Durchschnittstemperatur gegenüber.

```{r}
# Lösung Aufgabe 9

wetter_sry <- wetter %>%
  group_by(stn,Meereshoehe) %>%
  summarise(temp_mean = mean(tre200h0))

# Achtung: wenn mehrere Argumente in group_by() definiert werden führt das 
# üblicherweise zu Untergruppen. In unserem Fall hat jede Station nur EINE 
# Meereshöhe, deshalb wird die Zahl der Gruppen nicht erhöht.
```


```{r}

ggplot(wetter_sry, aes(temp_mean,Meereshoehe)) +
  geom_point()
```


```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:10_PrePro2/Uebung_A.Rmd-->

## Übung A: Lösung

[R-Code als Download](10_PrePro2/RFiles/Uebung_A.R)


```{r code=readLines('10_PrePro2/RFiles/Uebung_A.R'), echo=T, eval=F, include = T}
```

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:10_PrePro2/Uebung_A_loesung.Rmd-->

```{r, include=FALSE, purl = F}
knitr::opts_chunk$set(collapse=TRUE)
# knitr::opts_knit$set(root.dir = "10_PrePro2")

```

## Übung B


```{r,message=F}
library(tidyverse)
library(lubridate)
library(stringr)
```



### Aufgabe 1

Gegeben sind die Daten von drei Sensoren ([sensor1.csv](10_PrePro2/data/sensor1.csv), [sensor2.csv](10_PrePro2/data/sensor2.csv), [sensor3.csv](10_PrePro2/data/sensor3.csv)). Lade die Datensätze runter und lese sie ein.



```{r, message=F}
# Lösung Aufgabe 1

sensor1 <- read_delim("10_PrePro2/data/sensor1.csv",";")
sensor2 <- read_delim("10_PrePro2/data/sensor2.csv",";")
sensor3 <- read_delim("10_PrePro2/data/sensor3.csv",";")

```

### Aufgabe 2



Füge die drei Tabellen zu **einer** zusammen. Dazu kannst du entweder die  Spalten (Variablen) mittels `join()` oder die Zeilen (Beobachtungen) mittels `rbind()` zusammen "kleben".  Überführe zudem die Spalte `Datetime` in ein `POSIXct`-Format. Das ursprüngliche Format lautet:`DDMMYYYY_HHMM`


```{r}

# Lösung Aufgabe 2 (Var 1: Spalten [Variabeln] zusammen 'kleben')
sensor_all <- sensor1 %>%
  rename(sensor1 = Temp) %>%              # Spalte "Temp" in "sensor1" umbenennen
  full_join(sensor2,by = "Datetime") %>%    
  rename(sensor2 = Temp) %>%
  full_join(sensor3, by = "Datetime") %>%
  rename(sensor3 = Temp) %>%
  mutate(Datetime = as.POSIXct(Datetime,format = "%d%m%Y_%H%M"))
```




```{r}

# Lösung Aufgabe 2 (Var 2: Zeilen [Beobachtungen] zusammen 'kleben)

sensor1$sensor <- "sensor1"
sensor2$sensor <- "sensor2"
sensor3$sensor <- "sensor3"

sensor_all <- rbind(sensor1,sensor2,sensor3)

sensor_all <- sensor_all %>%
  mutate(
    Datetime = as.POSIXct(Datetime,format = "%d%m%Y_%H%M")
  ) %>%
  spread(sensor, Temp)

```


```{r,  echo = F, eval = T, purl=F}
# Die neue Tabelle sollte folgendermassen aussehen:
knitr::kable(sensor_all)
```



### Aufgabe 3

Importiere die Datei [sensor_1_fail.csv](10_PrePro2/data/sensor_fail.csv) in `R`.


```{r, message = F}

# Lösung Aufgabe 3

sensor_fail <- read_delim("10_PrePro2/data/sensor_fail.csv", delim = ";")

```


```{r,  echo = F, eval = T, purl=F}
knitr::kable(sensor_fail)
```


`sensor_fail.csv` hat eine Variabel `SensorStatus`: `1` bedeutet der Sensor misst, `0` bedeutet der Sensor miss nicht. Fälschlicherweise wurde auch dann der Messwert `Temp = 0` erfasst, wenn `Sensorstatus = 0`. Richtig wäre hier `NA` (not available). Korrigiere den Datensatz entsprechend.


```{r}

# Lösungsweg 1
sensor_fail$Datetime <- as.POSIXct(sensor_fail$Datetime,format = "%d%m%Y_%H%M")

sensor_fail$`Hum_%`[sensor_fail$SensorStatus == 0] <- NA
sensor_fail$Temp[sensor_fail$SensorStatus == 0] <- NA
```


```{r, message = F}

# Lösungsweg 2

sensor_fail <- read_delim("10_PrePro2/data/sensor_fail.csv", delim = ";")


sensor_fail_corr <- sensor_fail %>%
  mutate(
    Datetime = as.POSIXct(Datetime,format = "%d%m%Y_%H%M")
  ) %>%
  rename(Humidity = `Hum_%`) %>%         # Weil R "%" in Headers nicht mag
  gather(key,val, c(Temp, Humidity)) %>%
  mutate(
    val = ifelse(SensorStatus == 0,NA,val)
  ) %>%
  spread(key,val)
  
```


### Aufgabe 4


Warum spielt das es eine Rolle, ob `0` oder `NA` erfasst wird? Vergleiche dazu die Mittlere Temperatur / Feuchtigkeit vor und nach der Korrektur. 

```{r}

# Lösung Aufgabe 4

# Mittelwerte der unkorrigierten Sensordaten (`NA` als `0`)
mean(sensor_fail$Temp)
mean(sensor_fail$`Hum_%`)

```



```{r}
# Mittelwerte der korrigierten Sensordaten (`NA` als `NA`). Hier müssen wir die Option 
# `na.rm = T` (Remove NA = T) wählen, denn `mean()` (und ähnliche Funktionen) retourieren 
# immer `NA`, sobald ein **einzelner** Wert in der Reihe `NA`ist.
mean(sensor_fail_corr$Temp, na.rm = T)
mean(sensor_fail_corr$Humidity, na.rm = T)

```



```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:10_PrePro2/Uebung_B.Rmd-->

## Übung B: Lösung

[R-Code als Download](10_PrePro2/RFiles/Uebung_B.R)


```{r code=readLines('10_PrePro2/RFiles/Uebung_B.R'), echo=T, eval=F, include = T}
```

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:10_PrePro2/Uebung_B_loesung.Rmd-->

# InfoVis1 (21.10.2019)

Die konventionelle schliessende Statistik arbeitet in der Regel konfirmatorisch, sprich aus der bestehenden Theorie heraus werden Hypothesen formuliert, welche sodann durch Experimente geprüft und akzeptiert oder verworfen werden. Die Explorative Datenanalyse (EDA) nimmt dazu eine antagonistische Analyseperspektive ein und will in den Daten zunächst Zusammenhänge aufdecken, welche dann wiederum zur Formulierung von prüfbaren Hypothesen führen kann. Die Einheit stellt dazu den klassischen 5-stufigen EDA-Prozess nach Tukey (1980!) vor. Abschliessend wird dann noch die Brücke geschlagen zur modernen Umsetzung der EDA in Form von Visual Analytics.
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:11_InfoVis1/Abstract.Rmd-->

```{r, include=F, purl=F}

knitr::opts_chunk$set(echo = T,include = T,message = F, collapse=TRUE)
# knitr::opts_knit$set(root.dir = "11_InfoVis1") 

```




## EDA Beispiel Vorlesung

[Demoscript als Download](11_InfoVis1/RFiles/Demo_EDA.R)


```{r,message=F}


library(tidyverse)
library(scales)

# create some data about age and height of people
people <- data.frame(
  ID = c(1:30),
  
  age = c(5.0, 7.0, 6.5 ,9.0, 8.0, 5.0, 8.6, 7.5, 9.0, 6.0,
          63.5 ,65.7, 57.6, 98.6, 76.5, 78.0, 93.4, 77.5, 256.6, 512.3,
          15.5, 18.6, 18.5, 22.8, 28.5, 39.5, 55.9, 50.3, 31.9, 41.3),
  
  height = c(0.85, 0.93, 1.1, 1.25, 1.33, 1.17, 1.32, 0.82, 0.89, 1.13,
             1.62, 1.87, 1.67, 1.76, 1.56, 1.71, 1.65, 1.55, 1.87, 1.69,
             1.49, 1.68, 1.41, 1.55, 1.84, 1.69, 0.85, 1.65, 1.94, 1.80),
  
  weight = c(45.5, 54.3, 76.5, 60.4, 43.4, 36.4, 50.3, 27.8, 34.7, 47.6,
             84.3, 90.4, 76.5, 55.6, 54.3, 83.2, 80.7, 55.6, 87.6, 69.5,
             48.0, 55.6, 47.6, 60.5, 54.3, 59.5, 34.5, 55.4, 100.4, 110.3)
)



# build a scatterplot for a first inspection
ggplot(people, aes(x=age, y=height)) + 
  geom_point() + 
  scale_y_continuous(limits=c(0.75, 2.0)) +
  theme(axis.text=element_text(size=12), axis.title=element_text(size=20,face="bold"))

# Go to help page: http://docs.ggplot2.org/current/ -> Search for icon of fit-line
# http://docs.ggplot2.org/current/geom_smooth.html

# build a scatterplot for a first inspection, with regression line
ggplot(people, aes(x=age, y=height)) + 
  geom_point() + 
  scale_y_continuous(limits=c(0, 2.0)) +
  geom_smooth(method="loess", fill='lightblue', size=0.5, alpha=0.5) +
  theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face="bold"))

?stem

# stem and leaf plot
stem(people$height)
stem(people$height, scale=2)

# explore the two variables with box-whiskerplots
summary(people$age)
boxplot(people$age)

boxplot(people$age)

summary(people$height)
boxplot(people$height)

boxplot(people$height)


# explore data with a histgram
ggplot(people, aes(x=age)) + 
  geom_histogram(stat="bin", fill='green', binwidth=20) + 
  theme_bw() + labs(x = '\nage', y = 'count\n') +
  theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face="bold")) 

density(x = people$height)


# re-expression: use log or sqrt axes
#
# Find here guideline about scaling axes 
# http://www.cookbook-r.com/Graphs/Axes_(ggplot2)/
# http://docs.ggplot2.org/0.9.3.1/scale_continuous.html


# logarithmic axis: respond to skewness in the data, e.g. log10 
ggplot(people, aes(x=age, y=height)) + 
  geom_point() + 
  scale_y_continuous(limits=c(0, 2.0)) +
  geom_smooth(method="lm", fill='lightblue', size=0.5, alpha=0.5) +
  theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face="bold")) +
  scale_x_log10()

# logarithmic axis: show multiplicative factors, e.g. log2
ggplot(people, aes(x=age, y=height)) + 
  geom_point() + 
  scale_y_continuous(limits=c(0, 2.0)) +
  geom_smooth(method="lm", fill='lightblue', size=0.5, alpha=0.5) +
  theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face="bold")) +
  scale_x_continuous(trans = log2_trans(),
                   breaks = trans_breaks("log2", function(x) 2^x),
                   labels = trans_format("log2", math_format(2^.x)))


# outliers: Remove very small and very old people
peopleTemp <- subset(people, ID != 27) # Diese Person war zu klein.
peopleClean <- subset(peopleTemp, age < 100) # Fehler in der Erhebung des Alters

# re-explore cleaned data with a histgram
ggplot(peopleClean, aes(x=age)) + 
  geom_histogram(stat="bin", fill='#6baed6', binwidth=10) + 
  theme_bw() + labs(x = '\nage', y = 'count\n') +
  theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face="bold"))

ggplot(peopleClean, aes(x=age, y=height)) + 
  geom_point() + 
  scale_y_continuous(limits=c(0, 2.0)) +
  geom_smooth(method="lm", fill='lightblue', size=0.5, alpha=0.5) +
  theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face="bold"))

# with custom binwidth
ggplot(peopleClean, aes(x=age)) + 
  geom_histogram(stat="bin", fill='#6baed6', binwidth=10) + 
  theme_bw() + labs(x = '\nAlter', y = 'Anzahl\n')



# quadratic axis
ggplot(peopleClean, aes(x=age, y=height)) + 
  geom_point() + scale_y_continuous(limits=c(0, 2.0)) +
  geom_smooth(method="lm", fill='lightblue', size=0.5, alpha=0.5) +
  theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face="bold")) +
  scale_x_sqrt()




# subset "teenies": No trend
kids <- subset(peopleClean, age < 15)

ggplot(kids, aes(x=age, y=height)) + 
  geom_point() + 
  scale_y_continuous(limits=c(0, 2.0)) +
  geom_smooth(method="lm", fill='lightblue', size=0.5, alpha=0.5) +
  theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face="bold"))


# subset "teenies": No trend
oldies <- subset(peopleClean, age > 55)

ggplot(oldies, aes(x=age, y=height)) + 
  geom_point() + 
  scale_y_continuous(limits=c(0, 2.0)) +
  geom_smooth(method="lm", fill='lightblue', size=0.5, alpha=0.5) +
  theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face="bold"))


# Onwards towards multidimensional data

# Finally, make a scatterplot matrix
pairs(peopleClean[,2:4], panel=panel.smooth)

pairs(peopleClean[,2:4], panel=panel.smooth)

# Or as a bubble chart
peopleClean$radius <- sqrt( peopleClean$weight/ pi )
symbols(peopleClean$age, peopleClean$height, circles=peopleClean$radius)

symbols(peopleClean$age, peopleClean$height, circles=peopleClean$radius)

```

### Quellen

```{r code=readLines('00_Admin/get_chapter_references.R'), echo=F, eval=T,purl = F, results="asis"}
```
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:11_InfoVis1/Demo_EDA.Rmd-->


```{r, include=F, purl=F}

knitr::opts_chunk$set(echo = T,include = T,message = F, collapse=TRUE) # 
# knitr::opts_knit$set(root.dir = "11_InfoVis1") 

```

```{r, message = F}
library(tidyverse)
library(lubridate)

```

## Demo: `ggplot2`

[Demoscript als Download](11_InfoVis1/RFiles/Demo_ggplot.R)

Als erstes laden wir den Wetterdatensatz von der Übung Prepro1 ein.

```{r}
wetter <- read_table("09_PrePro1/data/order_52252_data.txt",
                  col_types = list(
                    col_factor(levels = NULL),    
                    col_datetime(format = "%Y%m%d%H"),
                    col_double()
                    )
                  )
```


```{r, echo = F, eval = T, purl=F}

knitr::kable(head(wetter))

```

Der Datensatz hat `r nrow(wetter)` Zeilen. Bevor wir mit plotten beginnen, müssen wir den Datensatz etwas filtern da die Plots ansonsten zu schwerfällig werden. Wir filtern deshalb auf Januar 2000.

```{r}
wetter_fil <- wetter %>%
  mutate(
    year = year(time),
    month = month(time)
    ) %>%
  filter(year == 2000 & month == 1)
```


Ein ggplot wird durch den Befehl `ggplot()` initiiert. Hier wird einerseits der Datensatz festgelegt, auf dem der Plot beruht (`data = `), sowie die Variablen innerhalb des Datensatzes, die Einfluss auf den Plot ausüben (`mapping = aes()`). 

Weiter braucht es *mindestens* ein "Layer" der beschreibt, wie die Daten dargestellt werden sollen (z.B. `geom_point()`).

Anders als bei "Piping" (`%>%`) wird ein Layer mit `+` hinzugefügt.

```{r}
# Datensatz: "wetter_fil" | Beeinflussende Variabeln: "time" und "tre200h0"
ggplot(data = wetter_fil, mapping = aes(time,tre200h0)) +
  # Layer: "geom_point" entspricht Punkten in einem Scatterplot 
  geom_point()                                                 

```


Da ggplot die Eingaben in der Reihenfolge `data = ` und dann `mapping = `erwartet, können wir diese Spezifizierungen auch weglassen.

```{r, eval=F}

ggplot(wetter_fil, aes(time,tre200h0)) +
  geom_point()

```

Nun wollen wir die unterschiedlichen Stationen unterschiedlich einfärben. Da wir Variablen definieren wollen, welche Einfluss auf die Grafik haben sollen, gehört diese Information in `aes()`.

```{r}
ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) +
  geom_point()
```

Wir können noch einen Layer mit Linien hinzufügen:

```{r}
ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) +
  geom_point() +
  geom_line()

```

Weiter können wir die Achsen beschriften und einen Titel hinzufügen. Zudem lasse ich die Punkte (`geom_point()`) nun weg, da mir diese nicht gefallen.

```{r}
ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) +
  geom_line() +
  labs(x = "Woche",
       y = "Temperatur in Grad C°", 
       title = "Temperaturdaten Schweiz",
       subtitle = "Januar 2000")
```

Man kann auch Einfluss auf die x-/y-Achsen nehmen. Dabei muss man zuerst festlegen, was für ein Achsentyp der Plot hat (vorher hat `ggplot` eine Annahme auf der Basis der Daten getroffen). 




Bei unserer y-Achse handelt es sich um numerische Daten, `ggplot` nennt diese: `scale_y_continuous()`. Unter [ggplot2.tidyverse.org](http://ggplot2.tidyverse.org/reference/#section-scales) findet man noch andere x/y-Achsentypen (`scale_x_irgenwas` bzw. `scale_y_irgendwas`).

```{r}
ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) +
  geom_line() +
  labs(x = "Woche",
       y = "Temperatur in Grad C°", 
       title = "Temperaturdaten Schweiz",
       subtitle = "Januar 2000") +    
  scale_y_continuous(limits = c(-30,30))    # y-Achsenabschnitt bestimmen

```


Das gleiche Spiel kann man für die y-Achse betreiben. Bei unserer y-Achse handelt es sich ja um unsere `POSIXct` Daten. `ggplot` nennt diese: `scale_x_datetime()`. 
```{r}
ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) +
  geom_line() +
  labs(x = "Woche",
       y = "Temperatur in Grad C°", 
       title = "Temperaturdaten Schweiz",
       subtitle = "Januar 2000") +    
  scale_y_continuous(limits = c(-30,30)) +
  scale_x_datetime(date_breaks = "1 week", 
                   date_minor_breaks = "1 day", 
                   date_labels = "KW%W")

```


Mit `theme` verändert man das allgmeine Layout der Plots. Beispielsweise kann man mit `theme_classic()` `ggplot`-Grafiken etwas weniger "Poppig" erscheinen lassen: so sind sie besser für Bachelor- / Masterarbeiten sowie Publikationen geeignet. `theme_classic()` kann man indiviudell pro Plot anwenden, oder für die aktuelle Session global setzen (s.u.)

Individuell pro Plot:
```{r}
ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) +
  geom_line() +
  labs(x = "Woche",
       y = "Temperatur in Grad C°", 
       title = "Temperaturdaten Schweiz",
       subtitle = "Januar 2000") +    
  scale_y_continuous(limits = c(-30,30)) +
  scale_x_datetime(date_breaks = "1 week", 
                   date_minor_breaks = "1 day", 
                   date_labels = "KW%W") +
  theme_classic()
```

Global (für alle nachfolgenden Plots der aktuellen Session):

```{r}
theme_set(theme_classic())
```


Sehr praktisch sind auch die Funktionen für "Small multiples". Dies erreicht man mit `facet_wrap()` (oder `facet_grid()`, mehr dazu später). Man muss mit einem Tilde-Symbol "`~`" nur festlegen, welche *Variable* für das Aufteilen des Plots in kleinere Subplots verantwortlich sein soll. 

```{r, fig.width=8,fig.height=10}
ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) +
  geom_line() +
  labs(x = "Woche",
       y = "Temperatur in Grad C°", 
       title = "Temperaturdaten Schweiz",
       subtitle = "Januar 2000") +    
  scale_y_continuous(limits = c(-30,30)) +
  scale_x_datetime(date_breaks = "2 weeks", 
                   date_minor_breaks = "1 day", 
                   date_labels = "KW%W") +
  facet_wrap(~stn)

```


Auch `facet_wrap` kann man auf seine Bedürfnisse anpassen. Da wir 24 Stationen haben möchte ich lieber 3 pro Zeile, damit es schön aufgeht. Dies erreiche ich mit `ncol = 3`.

Zudem brauchen wir die Legende nicht mehr, da der Stationsnamen über jedem Facet steht. Ich setze deshalb `theme(legend.position="none")` 

```{r, fig.width=8,fig.height=10}
ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) +
  geom_line() +
  labs(x = "Woche",
       y = "Temperatur in Grad C°", 
       title = "Temperaturdaten Schweiz",
       subtitle = "Januar 2000") +  
  scale_y_continuous(limits = c(-30,30)) +
  scale_x_datetime(date_breaks = "1 week", date_minor_breaks = "1 day", date_labels = "KW%W") +
  facet_wrap(~stn,ncol = 3) +
  theme(legend.position="none")
```


Genau wie `data.frames` und andere Objekte, kann man einen ganzen Plot auch in einer Variabel speichern. Dies kann nützlich sein um einen Plot zu exportieren (als png, jpg usw.) oder sukzessive erweitern wie in diesem Beispiel.

```{r, message = F}
p <- ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) +
  geom_line() +
  labs(x = "Woche",
       y = "Temperatur in Grad C°", 
       title = "Temperaturdaten Schweiz",
       subtitle = "Januar 2000") +
  scale_y_continuous(limits = c(-30,30)) +
  scale_x_datetime(date_breaks = "1 week", date_minor_breaks = "1 day", date_labels = "KW%W") +
  facet_wrap(~stn,ncol = 3)
  # ich habe an dieser Stelle theme(legend.position="none") entfernt



```

Folgendermassen kann ich den Plot als png-File abspeichern (ohne Angabe von "plot = " wird einfach der letzte Plot gespeichert)

```{r, eval = F}
ggsave(filename = "11_InfoVis1/plot.png",plot = p)
```

.. und so kann ich einen bestehenden Plot (in einer Variabel) mit einem Layer / einer Option erweitern

```{r, eval = F}
p +
  theme(legend.position="none")

```


Wie üblich wurde diese Änderung nicht gespeichert, sondern nur das Resultat davon ausgeben. Wenn die Änderung in meinem Plot (in der Variabel) abspeichern will, muss ich die Variabel überschreiben:

```{r}
p <- p +
  theme(legend.position="none")
```


Mit `geom_smooth()` kann `ggplot` eine Trendlinie auf der Baiss von Punktdaten berechnen. Die zugrunde liegende statistische Methode kann selbst gewählt werden. Wenn nichts angegeben wird verwendet `ggplot` bei weniger als 1'000 Messungen, die Methode `loess` (local smooths).


```{r, fig.width=8,fig.height=10}
p <- p +
  geom_smooth(colour = "black")

p
```

### Quellen

```{r code=readLines('00_Admin/get_chapter_references.R'), echo=F, eval=T,purl = F, results="asis"}
```

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:11_InfoVis1/Demo_ggplot.Rmd-->

```{r, include=F, purl=F}
library(knitr)

knitr::opts_chunk$set(echo = FALSE,include = TRUE,message = FALSE, collapse=TRUE) 


```

## Übung

In dieser Übung geht es darum, die Grafiken aus dem Blog-post von Marko Kovic ([blog.tagesanzeiger.ch](https://blog.tagesanzeiger.ch/datenblog/index.php/668/je-weniger-auslaender-desto-mehr-ja-stimmen-wirklich)) zu rekonstruieren. Freundlicherweise hat Herr Kovic meist die `ggplot2` Standardeinstellungen benutzt, was die Rekonstruktion relativ einfach macht. 

Die Links im Text verweisen auf die Originalgrafik, die eingebetteten Plots sind meine eigenen Rekonstruktionen. Importiere als erstes den Datensatz [initiative_masseneinwanderung_kanton.csv](11_InfoVis1/data/initiative_masseneinwanderung_kanton.csv) (auf der Blog-Seite erhältlich).


```{r, message=F}

library(tidyverse)
library(ggplot2)
library(stringr)

```


```{r, eval=F}
# Es kann sein, dass man die Codierung des Files spezifizieren muss. Mit `readr::read_delim()` 
# läuft dies mit der Option locale = locale(encoding = "UTF-8") wobei anstelle von UTF-8 die 
# entsprechende Codierung angegeben wird. 
# Tipp: Excel speichert CSV oft in ANSI, welches für den Import in R nicht sonderlich geeignet 
# ist. Falls Probleme auftreten muss das File mittels einer geeigneter Software (Widows: "Editor" 
# oder "Notepad++", Mac: "TextEdit")  und mit einer neuen Codierung (z.B. `UTF-8`) abgespeichert 
# werden.
```

```{r}
kanton <- read_delim("11_InfoVis1/data/initiative_masseneinwanderung_kanton.csv",",",locale = locale(encoding = "UTF-8"))
```



### Aufgabe 1

Rekonstruiere [Grafik 1](https://blog.tagesanzeiger.ch/datenblog/wp-content/uploads/sites/32/2014/03/Kantone-2.png) von Kovic. Erstelle dazu einen Scatterplot wo der Ausländeranteil der Kantone dem Ja-Anteil gegenüber gestellt wird. Speichere den Plot einer Variabel `plot1`.

- nutze `coord_fixed()` um die beiden Achsen in ein fixes Verhältnis zu setzen (1:1).
- setze die Achsen Start- und Endwerte mittels `lims()` oder `scale_y_continuous`bzw. `scale_x_continuous`.
- Optional: Setze analog Kovic die `breaks` (`0.0`, `0.1`...`0.7`) manuell

Rekonstruktion:

```{r}

# Lösung zu Aufgabe 1

# da die Spalten in Kovic's Daten Umlaute und Sonderzeichen enthalten, müssen diese in R mit Graviszeichen 
# angesprochen werden. Dieses Zeichen wirder Schweizer Tastatur [1]  mit 
# Shitft + Gravis (Links von der Backspace taste) + Leerschlag erstellt
# [1] https://de.wikipedia.org/wiki/Tastaturbelegung#Schweiz


# Alternativ können die Spalten im Originalfile oder mit dplyr::rename() umbenannt werden


plot1 <- ggplot(kanton, aes(`Ausländeranteil`, `Ja-Anteil`)) +
  geom_point() +
  coord_fixed(1) +
  scale_y_continuous(breaks = c(0,0.1,0.3,0.5,0.7),limits =  c(0,0.7)) +
  scale_x_continuous(breaks = c(0,0.1,0.3,0.5,0.7),limits =  c(0,0.7)) +
  labs(y = "Anteil Ja-Stimmen")

plot1
```


### Aufgabe 2

Rekonstruiere [Grafik 2](https://blog.tagesanzeiger.ch/datenblog/wp-content/uploads/sites/32/2014/03/Kantone-LOESS-986x923.png). Erweitere dazu `plot1` mit einer Trendlinie.

```{r}
# Lösung zu Aufgabe 2

plot1 +
  geom_smooth()
```



### Aufgabe 3


Importiere die Gemeindedaten [initiative_masseneinwanderung_gemeinde.csv](11_InfoVis1/data/initiative_masseneinwanderung_gemeinde.csv):

```{r, echo = T}
gemeinde <- read_delim("11_InfoVis1/data/initiative_masseneinwanderung_gemeinde.csv",",",locale = locale(encoding = "UTF-8"))
```


Rekonstruiere [Grafik 3](https://blog.tagesanzeiger.ch/datenblog/wp-content/uploads/sites/32/2014/03/Gemeinden-2-986x939.png). Stelle dazu den Ausländeranteil aller Gemeinden dem Ja-Stimmen-Anteil gegenüber. Speichere den Plot als `plot2`

```{r}
# Lösung zu Aufgabe 3

plot2 <- ggplot(gemeinde, aes(`Anteil Ausl`, `Anteil Ja`)) +
  geom_point() +
  labs(x = "Ausländeranteil",y = "Anteil Ja-Stimmen") +
  coord_fixed(1) +
  lims(x = c(0,1), y = c(0,1))

plot2
```


### Aufgabe 4

Rekonstruiere [Grafik 4](https://blog.tagesanzeiger.ch/datenblog/wp-content/uploads/sites/32/2014/03/Gemeinden-GAM-2-986x939.png) indem `plot2` mit einer Trendlinie erweitert wird.

```{r}
# Lösung zu Aufgabe 4

plot2 +
  geom_smooth()
```


### Aufgabe 5

Rekonstruiere [Grafik 5](https://blog.tagesanzeiger.ch/datenblog/wp-content/uploads/sites/32/2014/03/Gemeinden-x-Kantone-2-986x857.png) indem `plot2` mit `facetting` erweitert wird. Die Facets sollen die einzelnen Kantone sein. Speichere den Plot als `plot3`.

```{r}

# Lösung zu Aufgabe 5

plot3 <- plot2 +
  facet_wrap(~Kanton)
plot3
```


### Aufgabe 6

Rekonstruiere [Grafik 6](https://blog.tagesanzeiger.ch/datenblog/wp-content/uploads/sites/32/2014/03/Gemeinden-x-Kantone-LOESS-2-986x857.png) indem `plot3` mit einer Trendlinie erweitert wird.

Rekonstruktion:

```{r, warning=F}

# Lösung zu Aufgabe 6

plot3 +
  geom_smooth()
```


### Aufgabe 7

Rekonstruiere [Grafik 7](https://blog.tagesanzeiger.ch/datenblog/wp-content/uploads/sites/32/2014/03/Gemeinden-x-Quantile-2-986x637.png) indem `plot2`mit `facetting` erweitert wird. Die Facets sollen nun den Grössen-Quantilen entsprechen. Speichere den Plot unter `plot4`.

Rekonstruktion:

```{r}

# Lösung zu Aufgabe 7

plot4 <- plot2 +
  facet_wrap(~Quantile)
plot4
```


### Aufgabe 8

Rekonstruiere [Grafik 8](https://blog.tagesanzeiger.ch/datenblog/wp-content/uploads/sites/32/2014/03/Gemeinden-x-Quantile-LOESS-2-986x637.png) indem `plot4` mit einer Trendlinie ausgestattet wird.

```{r}

# Lösung zu Aufgabe 8

plot4 +
  geom_smooth()
```


### Aufgabe 9 (Fortgeschritten)

Rekonstruiere die [Korrelationstabelle](https://tagi_dwpro.s3.amazonaws.com/UMvkt/2/fs.html).

Tipp: 
- Nutze `group_by()` und `summarise()`
- Nutze `cor.test()` um den Korrelationskoeffizienten sowie den p-Wert zu erhalten. 
- Mit `$estimate` und `$p.value` können die entsprechenden Werte direkt angesprochen werden

Hinweis: aus bisher unerklärlichen Gründen weiche gewisse meiner Werte leicht von den Berechnungen des Herrn Kovics ab.

```{r}

# Lösung zu Aufgabe 9

korr_tab <- gemeinde %>%
  group_by(Kanton) %>%
  summarise(
    Korr.Koeffizient = cor.test(`Anteil Ja`,`Anteil Ausl`,method = "pearson")$estimate,
    Signifikanz_val = cor.test(`Anteil Ja`,`Anteil Ausl`,method = "pearson")$p.value,
    Signifikanz = ifelse(Signifikanz_val < 0.001,"***",ifelse(Signifikanz_val<0.01,"**",ifelse(Signifikanz_val<0.05,"*","-")))
  ) %>%
  select(-Signifikanz_val)

```



```{r,echo=F, purl=F}
knitr::kable(korr_tab)
```


```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:11_InfoVis1/Uebung.Rmd-->

## Lösung

[RCode als Download](11_InfoVis1/RFiles/Uebung.R)


```{r code=readLines('11_InfoVis1/RFiles/Uebung.R'), echo=T, eval=F}
```
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:11_InfoVis1/Uebung_loesung.Rmd-->

# InfoVis2 (22.10.2019)

Die Informationsvisualisierung ist eine vielseitige, effektive und effiziente Methode für die explorative Datenanalyse. Während Scatterplots und Histogramme weitherum bekannt sind, bieten weniger bekannte Informationsvisualisierungs-Typen wie etwa Parallelkoordinatenplots, TreeMaps oder Chorddiagramme originelle alternative Darstellungsformen zur visuellen Analyse von Datensätze, welche stets grösser und komplexer werden. Die Studierenden lernen in dieser Lerneinheit eine Reihe von Informationsvisualisierungstypen kennen, lernen diese zielführend zu gestalten und selber zu erstellen.
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:12_InfoVis2/Abstract.Rmd-->

---
output:
  pdf_document: default
  html_document: default
---

```{r, include=F, purl = F}
library(knitr)
knitr::opts_chunk$set(echo = F,include = T,message = F, warning = F, collapse=TRUE)
# knitr::opts_knit$set(root.dir = "12_InfoVis2") 

```



## Übung A

```{r, message = F, echo = T}
library(tidyverse)
library(lubridate)
```


Laden den `wetter`-Datensatz, bereinige ihn wenn nötig (`NA`-Werte entfernen) und importiere auch den Datensatz `order_52252_legend.csv` und verbinde die Datensätze mit einem join via dem Stationskürzel.



```{r, echo = T, message = F}

wetter <- read_table("09_PrePro1/data/order_52252_data.txt",
                     col_types = list(
                       col_character(),    
                       col_datetime(format = "%Y%m%d%H"),
                       col_double()
                       )
                     )

wetter <- wetter %>%
  filter(!is.na(stn)) %>%
  filter(!is.na(time))

station_meta <- read_delim("09_PrePro1/data/order_52252_legend.csv",";")

wetter <- left_join(wetter,station_meta,by = "stn")

```




### Aufgabe 1

Erstelle zwei Hilfsspalten (convenience variables) "Jahr" und "Monat". Filtere auf ein beliebiges Jahr und zwei beliebige Monate. Speichere den gefilterten Datensatz in einer neuen Variablen ab. Verwende diesen Datensatz für alle folgenden Übungen.

```{r}

# Lösung Aufgabe 1

wetter_fil <- wetter %>%
  mutate(
    year = year(time),
    month = month(time)
  ) %>%
  filter(year == 2000 & month < 3)
```


### Aufgabe 2

Erstelle ein Scatterplot (`time` vs. `tre200h0`) wobei die Punkte aufgrund ihrer Meereshöhe eingefärbt werden sollen. Tiefe Werte sollen dabei blau eingefärbt werden und hohe Werte rot. Verkleinere die Punkte um übermässiges Überplotten der Punkten zu vermeiden. Weiter sollen im Abstand von zwei Wochen die Kalenderwochen auf der Achse erscheinen. 

Speichere den Plot in einer Variabel `p` ab.

```{r}

# Lösung Aufgabe 2

p <- ggplot(wetter_fil, aes(time,tre200h0, colour = Meereshoehe)) +
  geom_point(size = 0.5) +
  labs(x = "Kalenderwoche", y = "Temperatur in ° Celsius") +
  scale_color_continuous(low = "blue", high = "red") +
  scale_x_datetime(date_breaks = "2 week", date_labels = "KW%W") 

p 

```



### Aufgabe 3

Füge am obigen Plot (gespeichert als Variabel `p`) eine schwarze, gestrichelte Trendlinie hinzu und aktualisiere `p` (`p <- p + ...`).

```{r, message=F}

# Lösung Aufgabe 3

p <- p +
  stat_smooth(colour = "black",lty = 2)

p
```


### Aufgabe 4

Positioniere die Legende oberhalb des Plots und lege sie quer (nutze dazu `theme()` mit `legend.direction` und `legend.position`). Speichere diese Änderungen in `p`.

```{r, message=F}

# Lösung Aufgabe 4


p <- p + 
  theme(legend.direction = "horizontal",legend.position = "top")

p
    
```



### Aufgabe 5 (für ambitionierte)

Füge den Temperaturwerten auf der y-Ache ein `°C` hinzu (siehe unten und studiere [diesen Tipp](https://stackoverflow.com/a/35967126/4139249) zur Hilfe). Aktualisiere `p` an dieser Stelle noch nicht.

```{r, message=F}

# Lösung Aufgabe 5

p +
  scale_y_continuous(labels = function(x)paste0(x,"°C")) +
  labs(x = "Kalenderwoche", y = "Temperatur")


```


### Aufgabe 6 (für *noch* ambitioniertere)

Füge dem Plot eine zweite, korrekt ausgerichtete Achse mit Kelvin oder Farenheit hinzu (siehe `sec_axis`). Wenn du es vorherigen Übung schon geschafft hast, setze auch hier die Einheit (`K` rep. `°F`) hinter die Werte auf der Achse. 

$$ K = °C + 273,15$$
$$°F = °C × \frac{9}{5} + 32$$

```{r, message=F}
# Lösung Aufgabe 6

p <- p +
  labs(x = "Kalenderwoche", y = "Temperatur") +
  scale_y_continuous(labels = function(x)paste0(x,"°C"),sec.axis = sec_axis(~.*(9/5)+32,name = "Temperatur",labels = function(x)paste0(x,"° F")))


p
```




### Aufgabe 7

Jetzt verlassen wir den scatterplot und machen einen Boxplot mit den Temperaturdaten. Färbe die Boxplots wieder in Abhängigkeit der Meereshöhe ein. 

- Beachte den Unterschied zwischen `colour =` und `fill =`
- Beachte den Unterschied zwischen `facet_wrap()` und `facet_grid()`
- `facet_grid()` braucht übrigens noch einen Punkt (`.`) zur Tilde (`~`). 
- Beachte den Unterschied zwischen "`.~`" und "`~.`" bei `facet_grid()`
- verschiebe nach Bedarf die Legende

```{r}

# Lösung Aufgabe 7

wetter_fil <- mutate(wetter_fil,monat = month(time,label = T,abbr = F))


ggplot(wetter_fil, aes(stn,tre200h0, fill = Meereshoehe)) +
  geom_boxplot() +
  facet_grid(monat~.) +
  labs(x = "Station", y = "Temperatur") +
  theme(legend.direction = "horizontal",legend.position = "top")

```


### Aufgabe 8

Teile die Stationen in verschiedene Höhenlagen ein (Tieflage [< 450 m], Mittellage [450 - 1000 m] und Hochlage [> 1'000 m]). Vergleiche die Verteilung der Temperaturwerte in den verschiedenen Lagen.  

- Nutze dazu `facet_grid` um die Höhenlage dem Monat gegenüber zu stellen (`Monat~Lage`)
- Passe `scales =` an damit keine leeren Stellen auf der x-Achse entstehen
- Optional: Verwende den vollen Stationsnamen anstelle des Kürzels und drehe diese ab damit sie sich gegenseitig nicht überschreiben

```{r, warning=F}

# Lösung Aufgabe 8

wetter_fil$Lage[wetter_fil$Meereshoehe < 450] <- "Tieflage" 
wetter_fil$Lage[wetter_fil$Meereshoehe >= 450 & wetter_fil$Meereshoehe <1000] <- "Mittellage" 
wetter_fil$Lage[wetter_fil$Meereshoehe >= 1000] <- "Hochlage" 


ggplot(wetter_fil, aes(Name,tre200h0)) +
  geom_boxplot() +
  facet_grid(monat~Lage, scales = "free_x") +
  labs(x = "Lage", y = "Temperatur") +
  theme(axis.text.x = element_text(angle = 45,hjust = 1))

```



### Aufgabe 9


Als letzter wichtiger Plottyp noch zwei Übungen zum Histogramm. Erstelle ein Histogramm `geom_histogram()` mit den Temperaturwerten. Färbe Säulen aufgrund ihrer Höhenlage ein und die Begrenzungslinie weiss. Setze die Klassenbreite auf 1 Grad.

```{r}

# Lösung Aufgabe 9


h <- ggplot(wetter_fil,aes(tre200h0, fill = Lage)) +
  geom_histogram(binwidth = 1, colour = "white") +
  labs(x = "Temperatur in °C", y = "Anzahl")

h
```


### Aufgabe 10

Erstelle `facets` aufgrund der Höhenlage. Setze noch eine Vertikale linie beim Nullpunkt und stelle den x-Achsenabschnit symmetrisch ein (z.B -30 bis + 30°C).

```{r}

# Lösung Aufgabe 10


h + 
  geom_vline(xintercept = 0, lty = 2, alpha = 0.5) +
  facet_wrap(~Lage) +
  lims(x = c(-30,30)) +
  theme(legend.position = "none")

```





```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:12_InfoVis2/Uebung_A.Rmd-->

## Übung A: Lösung

[RCode als Download](12_InfoVis2/RFiles/Uebung_A.R)

```{r code=readLines('12_InfoVis2/RFiles/Uebung_A.R'), echo=T, eval=F}
```


```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:12_InfoVis2/Uebung_A_loesung.Rmd-->


```{r, include=F, purl = F}
library(knitr)
knitr::opts_chunk$set(echo = F,include = T,message = F, warning = F, collapse=TRUE)
# knitr::opts_knit$set(root.dir = "12_InfoVis2") 


output <- knitr::opts_knit$get("rmarkdown.pandoc.to") # html / latex


run_plotly_image = F # set to "TRUE" in order to create static images via plotly_api (max 100/day)
show_static_image = T # set to "TRUE" in order to show the image / "FALSE" to show error message
default_error <- "In der PDF Version kann die interaktive Grafik bis auf weiteres nicht dargestellt werden."

```

## Übung B

In dieser Übung bauen wir einige etwas unübliche Plots aus der Vorlesung nach. Dafür verwenden wir Datensätze, die in R bereits integriert sind. Eine Liste dieser Datensätze findet man [hier](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/00Index.html) oder mit der Hilfe `?datasets`.


Dazu verwenden wir vor allem das Package `plotly` welches im Gegensatz zu `ggplot2` ein paar zusätzliche Plot-Typen kennt und zudem noch interaktiv ist.  Leider scheinen gewisse Browsers (z.B. Firefox) sowie der Viewer Pane mit `plotly` Mühe zu haben. Deshalb empfehlen wir folgendes:

- [Übungsunterlagen für InfoVis2](http://oyster.zhaw.ch:3939/ResearchMethodsUebungen/) in Chrome zu öffnen
- Falls ihr auf dem [RStudio Server](http://oyster.zhaw.ch:8787) arbeitet: hier ebenfalls in Chrome arbeiten
- Falls ihr lokal mit RStudio arbeitet: Mit der Option `options(viewer=NULL)` werden Plots mit dem Standart Browser. 



```{r, echo = F,message=F}

library(tidyverse)
library(plotly)
library(pander)
library(webshot)

```


```{r, echo = F, purl= F}
Sys.setenv("plotly_username" = "rata_zhaw")
Sys.setenv("plotly_api_key" = "ae8Fn1ltcjUGvWYX957J")
```



### Aufgabe 1: Parallel coordinate plots

Erstelle einen [parallel coordinate plot](https://en.wikipedia.org/wiki/Parallel_coordinates). Dafür eignet sich der integrierte Datensatz [`mtcars`](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html):

```{r, echo = F, purl=F}

knitr::kable(head(mtcars))

```

```{r, echo = T, eval=F}

# Nur nötig, wenn ihr mit einer lokalen Installation von RStudio arbeitet
# (also nicht auf dem Server).
options(viewer=NULL)

```


Parallel Coordinates lassen sich mit nativem `ggplot2` nicht herstellen. Es braucht dazu entweder Erweiterungen oder "standalone" Tools. Als "standalone" Tool kann ich `plotly` stark empfehlen. `Plotly` verfügt zwar über eine etwas eigenwillige Syntax, bietet dafür über sehr vielseitige zusätzliche Möglichkeiten. Vor allem aber sind sämtliche `plotly` Grafiken webbasiert und interaktiv. 

Hier findet ihr eine Anleitung zur Herstellung eines Parallel Coordinates Plot mit `plotly`: https://plot.ly/r/parallel-coordinates-plot/

So sieht der fertige Plot aus:

```{r}
# Lösung Aufgabe 1

p <- mtcars %>%
  plot_ly(type = 'parcoords',
          line = list(color = ~mpg,
                      colorscale = list(c(0,'red'),c(1,'blue'))),
          dimensions = list(
            list(label = 'mpg', values = ~mpg),
            list(label = 'disp', values = ~disp),
            list(label = 'hp', values = ~hp),
            list(label = 'drat', values = ~drat),
            list(label = 'wt', values = ~wt),
            list(label = 'qsec', values = ~qsec),
            list(label = 'vs', values = ~vs),
            list(label = 'am', values = ~am),
            list(label = 'gear', values = ~gear),
            list(label = 'carb', values = ~carb)
          )
  )
```


```{r, results="asis", purl = F, echo=F}
aufgabe <- "aufgabe_1"
filename <- paste0("12_InfoVis2/",aufgabe,".png")

if(output == "latex"){
  if(run_plotly_image){plotly_IMAGE(p, format = "png", out_file = filename)} 
  if(show_static_image){pander::pandoc.image(filename)} else{pandoc.strong(default_error)}
} else(p)

```

### Aufgabe 2: Polar Plot mit Biber Daten

Polar Plots (welche man ebenfalls mit Plotly erstellen kann) eignen sich unter anderem für Daten, die zyklischer Natur sind, wie zum Beispiel zeitlich geprägte Daten (Tages-, Wochen-, oder Jahresrhythmen). Aus den Beispiels-Datensätzen  habe ich zwei Datensätze gefunden, die zeitlich geprägt sind:

- [`beaver1` und `beaver2`](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/beavers.html)
[`AirPassenger`](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/AirPassengers.html)

Beide Datensätze müssen noch etwas umgeformt werden, bevor wir sie für einen Radialplot verwenden können. In Aufgabe 2 verwenden wir die Biber-Datensätze, in der nächsten Aufgabe (3) die Passagier-Daten.

Wenn wir die Daten von beiden Bibern verwenden wollen, müssen wir diese noch zusammenfügen:
```{r, echo = T}


beaver1_new <- beaver1 %>%
  mutate(beaver = "nr1")

beaver2_new <- beaver2 %>%
  mutate(beaver = "nr2")

beaver_new <- rbind(beaver1_new,beaver2_new)

```

Zudem müssen wir die Zeitangabe noch anpassen: Gemäss der [Datenbeschreibung](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/beavers.html) handelt es sich bei der Zeitangabe um ein sehr programmier-unfreundliches Format. 3:30 wird als "0330" notiert. Wir müssen diese Zeitangabe, noch in ein Dezimalsystem umwandeln:
```{r, echo = T}
beaver_new <- beaver_new %>%
  mutate(
    hour_dec = (time/100)%/%1,         # Ganze Stunden (mittels ganzzaliger Division)
    min_dec = (time/100)%%1/0.6,       # Dezimalminuten (15 min wird zu 0.25, via Modulo)
    hour_min_dec = hour_dec+min_dec    # Dezimal-Zeitangabe (03:30 wird zu 3.5)
    ) 
```



Der Datensatz: 
```{r, echo = F, purl = F}
knitr::kable(head(beaver_new))
#  formatRound(c("min_dec","hour_min_dec"), 2)
```

So sieht der fertige Plot aus. Rekonstruiere dies mit `plotly`:

```{r}

# Lösung Aufgabe 2

p <- beaver_new %>%
  plot_ly(r = ~temp, t = ~hour_min_dec, color = ~beaver,mode = "lines", type = "scatter") %>%
  layout(
    radialaxis = list(range = c(35,39)),
    angularaxis = list(range = c(0,24)),
    orientation = 270,
    showlegend = F
    )
```

```{r, results="asis", purl = F, echo=F}
aufgabe <- "aufgabe_2"
filename <- paste0("12_InfoVis2/",aufgabe,".png")

if(output == "latex"){
  if(run_plotly_image){plotly_IMAGE(p, format = "png", out_file = filename)} 
  if(show_static_image){pander::pandoc.image(filename)} else{pandoc.strong(default_error)}
} else(p)

```




### Aufgabe 3: Polar Plot mit Passagier-Daten


Analog Aufgabe 2, dieses Mal mit dem Datensatz [`AirPassanger`](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/AirPassengers.html)

`AirPassengers` kommt in einem Format daher, das ich selbst noch gar nicht kannte. Es sieht zwar aus wie ein `data.frame` oder eine `matrix`, ist aber von der Klasse [`ts`](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/ts.html).

```{r, echo = T}
AirPassengers

class(AirPassengers)
```


Damit wir den Datensatz verwenden können, müssen wir ihn zuerst in eine `matrix` umwandeln. Wie das geht habe ich [hier](https://stackoverflow.com/a/5332664/4139249) erfahren.
```{r, echo = T}
AirPassengers2 <- tapply(AirPassengers, list(year = floor(time(AirPassengers)), month = month.abb[cycle(AirPassengers)]), c)
```

Aus der `matrix` muss noch ein Dataframe her, zudem müssen wir aus der breiten Tabelle eine lange Tabelle machen:

```{r, echo = T}


AirPassengers3 <- AirPassengers2 %>%
  as.data.frame() %>%
  rownames_to_column("year") %>%
  gather(month,n,-year) %>%
  mutate(
    # ich nutze einen billigen Trick um ausgeschriebene Monate in Nummern umzuwandeln [1]
    month = factor(month, levels = month.abb,ordered = T),
    month_numb = as.integer(month),
    year = factor(year, ordered = T)
  )


# [1] beachtet an dieser Stelle das Verhalten von as.integer() wenn es sich um factors() handelt. Hier wird das Verhalten genutzt, andersweitig kann es einem zum Verhngnis werden. Das Verhalten wir auch hier verdeutlicht:
# as.integer(as.character("500"))
# as.integer(as.factor("500"))

```


Hier der fertige Plot. Rekonstruiere dies mit `plotly`:
```{r}

# Lösung Aufgabe 3

p <- AirPassengers3 %>%
  plot_ly(r = ~n, t = ~month_numb, color = ~year, mode = "markers", type = "scatter") %>%
  layout(
    showlegend = T,
    angularaxis = list(range = c(0,12)),
    orientation = 270,
    legend = list(traceorder = "reversed")
) 

```



```{r, results="asis", purl = F, echo=F}
aufgabe <- "aufgabe_3"
filename <- paste0("12_InfoVis2/",aufgabe,".png")

if(output == "latex"){
  if(run_plotly_image){plotly_IMAGE(p, format = "png", out_file = filename)} 
  if(show_static_image){pander::pandoc.image(filename)} else{pandoc.strong(default_error)}
} else(p)
```

### Aufgabe 4: 3D Scatterplot

Erstelle einen 3D Scatterplot, ebenfalls mit `plotly`. Nutze dazu den Datensatz `trees`. Ein Beispiel für einen 3D Scatterplot findet ihr [hier](https://plot.ly/r/3d-scatter-plots/).


```{r}

# Lösung Aufgabe 4

  p <- trees %>%
  plot_ly(x = ~Girth, y = ~Height, z = ~Volume)
```



```{r, results="asis", purl = F, echo=F}
aufgabe <- "aufgabe_4"
filename <- paste0("12_InfoVis2/",aufgabe,".png")

if(output == "latex"){
  if(run_plotly_image){plotly_IMAGE(p, format = "png", out_file = filename)} 
  if(show_static_image){pander::pandoc.image(filename)} else{pandoc.strong(default_error)}
} else(p)
```





```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:12_InfoVis2/Uebung_B.Rmd-->

## Übung B: Lösung

[RCode als Download](12_InfoVis2/RFiles/Uebung_B.R)

```{r code=readLines('12_InfoVis2/RFiles/Uebung_B.R'), echo=T, eval=F}
```

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:12_InfoVis2/Uebung_B_loesung.Rmd-->

# Statistik 1 (28.10.2019)

In Statistik 1 lernen die Studierenden, was (Inferenz-) Statistik im Kern leistet und warum sie für wissenschaftliche Erkenntnis (in den meisten Disziplinen) unentbehrlich ist. Nach einer Wiederholung der Rolle von Hypothesen wird erläutert, wie Hypothesentests in der frequentist-Statistik umgesetzt werden, einschliesslich p-Werten und Signifikanz-Levels. Die praktische Statistik beginnt mit den beiden einfachsten Fällen, dem Chi-Quadrat-Test für die Assoziation zwischen zwei kategorialen Variablen und dem t-Test auf Unterschiede in Mittelwerten zwischen zwei Gruppen. Abschliessend beschäftigen wir uns damit, wie man Ergebnisse statistischer Analysen am besten in Abbildungen, Tabellen und Text darstellt.


<!-- TODO: -->
<!-- Referenzen passt noch nicht -->
<!-- hierarchien noch kontrollieren und mit anderen blöcke abgleichen -->
<!-- Beschreibung forschungsprojekte: tabelle besser als csv -->
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:13_Statistik1/Abstract.Rmd-->

```{r, echo = F, purl = F}
knitr::opts_chunk$set(echo = T, collapse=TRUE)
library(knitr)
```


## Demo: Stastische Tests

[Demoscript als Download](13_Statistik1/RFiles/Demo_Tests.R)

### Chi-Quadrat-Test & Fishers Test

```{r}
qchisq(0.95,1)
count<-matrix(c(38,14,11,51),nrow=2)
count
chisq.test(count)
fisher.test(count)
```

### t-Test

```{r}
a<-c(20,19,25,10,8,15,13,18,11,14)
b<-c(12,15,16,7,8,10,12,11,13,10)
blume<-data.frame(a,b)
blume
summary(blume)
boxplot(blume$a,blume$b)
boxplot(blume)
hist(blume$a)
hist(blume$b)
t.test(blume$a,blume$b) #zweiseitig
t.test(blume$a,blume$b, alternative="greater") #einseitig
t.test(blume$a,blume$b, alternative="less") #einseitig
t.test(blume$a,blume$b, var.equal=T) #Varianzen gleich, klassischer t-Test
t.test(blume$a,blume$b, var.equal=F) #Varianzen ungleich, Welch's t-Test, ist auch default, d.h. wenn var.equal nicht                        # definiert wird, wird ein Welch's t-Test ausgeführt. 
t.test(blume$a,blume$b, paired=T) #gepaarter t-Test 
t.test(blume$a,blume$b, paired=T,alternative="greater") #gepaarter t-Test 
shapiro.test(blume$b)
var.test(blume$a,blume$b)
if(!require(car)){install.packages("car")} # installiert das Zusatzpacket car (wenn nicht bereits installiert)
library(car)
leveneTest(blume$a,blume$b,center=mean)
wilcox.test(blume$a,blume$b)
```

Das gleiche mit einem “long table”


```{r}
cultivar<-c(rep("a",10),rep("b",10))
size<-c(a,b)
blume.long<-data.frame(cultivar,size)

rm(size) #Befehl rm entfernt die nicht mehr benötitgten Objekte aus dem Workspace
rm(cultivar)


rm(size) #Befehl rm entfernt die nicht mehr benötitgten Objekte aus dem Workspace
rm(cultivar)


#Das gleiche in einer Zeile
blume.long<-data.frame(cultivar=c(rep("a",10),rep("b",10)),size=c(a,b))
summary(blume.long)             
head(blume.long)

boxplot(size~cultivar, data=blume.long)


boxplot(size~cultivar, data=blume.long)


t.test(size~cultivar, blume.long, var.equal=T)
t.test(size~cultivar, blume.long, var.equal=F)
```


### Base R vs. ggplot2


```{r}

library(tidyverse)
ggplot(blume.long, aes(cultivar,size)) + geom_boxplot()
ggplot(blume.long, aes(cultivar,size)) + geom_boxplot()+theme_classic()
ggplot(blume.long, aes(cultivar,size)) + geom_boxplot(size=1) + theme_classic()+
theme(axis.line = element_line(size=1))+theme(axis.title = element_text(size=14))+
theme(axis.text = element_text(size=14))
ggplot(blume.long, aes(cultivar,size)) + geom_boxplot(size=1) + theme_classic()+
  theme(axis.line = element_line(size=1), axis.ticks = element_line(size=1), 
       axis.text = element_text(size = 20), axis.title = element_text(size = 20))
```


Definieren von mytheme mit allen gewünschten Settings, das man zu Beginn einer Sitzung einmal laden und dann immer wieder ausführen kann (statt des langen Codes)

```{r}
mytheme <- theme_classic() + 
  theme(axis.line = element_line(color = "black", size=1), 
        axis.text = element_text(size = 20, color = "black"), 
        axis.title = element_text(size = 20, color = "black"), 
        axis.ticks = element_line(size = 1, color = "black"), 
        axis.ticks.length = unit(.5, "cm"))

ggplot(blume.long, aes(cultivar,size)) + 
  geom_boxplot(size=1) +
  mytheme

t_test <- t.test(size~cultivar, blume.long)

ggplot(blume.long, aes(cultivar,size)) + 
  geom_boxplot(size=1) + 
  mytheme +
  annotate("text", x = "b", y = 24, label = paste0("italic(p) == ", round(t_test$p.value, 3)), parse = TRUE, size = 8)

ggplot (blume.long, aes(cultivar,size)) + 
  geom_boxplot(size=1) + 
  mytheme +
  labs(x="Cultivar",y="Size (cm)")

```

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:13_Statistik1/Demo_Tests.Rmd-->


## Beschreibung Forschungsprojekt NOVANIMAL (NFP69)

Im Forschungsprojekt NOVANIMAL wird u.a. der Frage nachgegangen, was es braucht, damit Menschen freiwillig weniger tierische Produkte konsumieren? Ein interessanter Ansatzpunkt ist die Ausser-Haus-Verpflegung. Gemäss der ersten in den Jahren 2014/2015 durchgeführten nationalen Ernährungserhebung menuCH essen 70 % der Bevölkerung zwischen 18 und 75 Jahren am Mittag auswärts (Bochud et al. 2017). Daher rückt die Gastronomie als zentraler Akteur einer innovativen und nachhaltigen Ernährungswirtschaft ins Blickfeld. Welche Innovationen in der Gastronomie könnten dazu beitragen, den Pro-Kopf-Verbrauch an tierischen Nahrungsmitteln zu senken? 

Dazu wurde u.a. ein Experiment in zwei Hochschulmensen durchgeführt. Forschungsleitend war die Frage, wie die Gäste dazu bewogen werden können, häufiger vegetarische oder vegane Gerichte zu wählen. Konkret wurde untersucht, wie die Gäste auf ein verändertes Menü-Angebot mit einem höheren Anteil an vegetarischen und veganen Gerichten reagieren. Das Experiment fand während 12 Wochen statt und bestand aus zwei Mensazyklen à 6 Wochen. Über den gesamten Untersuchungszeitraum werden insgesamt 90 verschiedene Gerichte angeboten. In den 6 Referenz- bzw. Basiswochen wurden zwei fleisch- oder fischhaltige Menüs und ein vegetarisches Menü angeboten. In den 6 Interventionswochen wurde das Verhältnis umgekehrt und es wurden ein veganes, ein vegetarisches und ein fleisch- oder fischhaltiges Gericht angeboten. Basis- und Interventionsangebote wechselten wöchentlich ab. Während der gesamten 12 Wochen konnten die Gäste jeweils auf ein Buffet ausweichen und ihre Mahlzeit aus warmen und kalten Komponenten selber zusammenstellen. Die Gerichte wurden über drei vorgegebene Menülinien  (F, K, W) randomisiert angeboten.

![Die Abbildung zeigt das Versuchsdesign der ersten 6 Experimentalwochen (Kalenderwoche 40 bis 45).](13_Statistik1/design_experiment.png)

Mehr Informationen über das Forschungsprojekt NOVANIMAL findet ihr auf dieser [Webpage](https://www.novanimal.ch)


<!-- @ Egel: Diesen Teil habe ich aus der Aufgabenstellung rausgenommen. Könnnen wir den hier platzieren?  -->
### Weitere Erläuterungen zum Datensatz



Der Datensatz beinhaltet knapp 1100 Einträge mit 18 Variablen. Die Daten stammen aus dem Kassensystem des Catering-Unternehmen und stellen eine repräsentative Stichprobe des originalen Datensatzes dar.

Folgende Variablen sind im Datensatz:


```{r, echo = F, message=F}
library(tidyverse)

variablen <- read_delim("13_Statistik1/novanimal_variabeln.csv",";")

knitr::kable(variablen)
  
```


1. Locals (Local F, Local K, Local W) sind nebst den drei "normalen Menü-Linien" zusätzlich angebotene Gerichte
2. hier werden Gerichte mit Fisch & Geflügel als Fleisch zusammengefasst.
3. Vegane Gerichte enthalten ausschliesslich pflanzliche Zutaten. Im Exeriment wurde zwischen Gerichte mit pflanzlichen Fleischsubstituten und authentischen, eigenständigen veganen Gerichten unterschieden
4. Vegetarisch bedeutet ovo-lakto-vegetarisch, d.h. die Gerichte enthalten Eier und/oder Milchprodukte 

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:13_Statistik1/Intro_Daten_egel.Rmd-->

## Übungen 1


###	Übung 1.1: Assoziationstest

Assoziationstest zweier kategorialer Variablen, Dateneingabe und Durchführung von Chi-Quadrat- sowie Fishers exaktem Test mit Daten die selber erhoben wurden oder dem Novanimal Datensatz.

###	Übung 1.2: $\chi^2$-Test

Datensatz [novanimal.csv](13_Statistik1/data/novanimal.csv) 

Unterscheidet sich die Stichprobe des NOVANIMAL-Projekts von der gesamten Population bezüglich Geschlecht und Hochschulzugehörigkeit? 

Die Grundgesamtheit setzt sich aus 719 Studentinnen und 816 Studenten, 345 Mitarbeiterinnen und 339 Mitarbeiter. Die gesamte Population umfasst 2219 Personen mit einer aktiven CampusCard.

- Definiert die Null- ($H_0$) und die Alternativhypothese ($H_1$)
- Führt einen $\chi^2$-Test mit dem Datensatz novanimal.csv durch
- Stellt eure Ergebnisse angemessen dar, d.h. Text mit Abbildung und/oder Tabelle


###	Übung 1.3: t-Test

Werden in den Basis- und Interventionswochen unterschiedlich viele Gerichte verkauft?

- Definiert die Null- ($H_0$) und die Alternativhypothese ($H_1$).
- Führt einen t-Test mit dem Datensatz novanimal.csv durch.
- Welche Form von t-Test musst Du anwenden: einseitig/zweiseitig resp. gepaart/ungepaart?
- Wie gut sind die Voraussetzungen für einen t-Test erfüllt (z.B. Normalverteilung der Residuen und Varianzhomogenität)?
- Stell eure Ergebnisse angemessen dar, d.h. Text mit Abbildung und/oder Tabelle 
- Das gilt für alle Übungen und wird von euch auch an der Prüfung verlangt. Abzugeben sind am Ende 
  
  (a) **Ein lauffähiges R-Skript**
  (b) **begründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation)** 
  (c) **ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit)**

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:13_Statistik1/assigment_stat1.Rmd-->

##  Musterlösung Aufgabe 1.1: Assoziationstest

>Download [R-Skript]("13_Statistik1/RFiles/solution_stat1.R")

**kommentierte Musterlösungen**
```{r, message=T, purl = F, include=FALSE}
## ladet die nötigen Packete und die novanimal.csv Datei in R

library(tidyverse)
nova <- read_delim("13_Statistik1/data/novanimal.csv", delim = ";")

## definiert mytheme für ggplot2 (verwendet dabei theme_classic())

mytheme <- 
  theme_classic() + 
  theme(
    axis.line = element_line(color = "black"), 
    axis.text = element_text(size = 20, color = "black"), 
    axis.title = element_text(size = 20, color = "black"), 
    axis.ticks = element_line(size = 1, color = "black"), 
    axis.ticks.length = unit(.5, "cm")
    )

```



```{r}
# Als eine Möglichkeit, die Aufgabe 1.1 zu bearbeiten, nehmen wir hier den novanimal-Datensatz und gehen der folgenden Frage nach: Gibt es einen Zusammenhang zwischen Geschlecht und der Wahl des Menüinhalts (vegtarisch vs. fleischhaltig) in der Mensa

# berücksichtigt nur vegetarische und fleischhaltige Menüs
# 1) alle Buffet-Menüs weglassen
# 2) alle veganen Gerichte zu vegetarische Gerichte umbenennen
nova2<-subset(nova, !(nova$label_content=="Buffet")) #Subset des Datensatzes ohne Buffet (Da Buffet nicht Fleisch/Vegetarisch zugordnet werden kann)

nova2$label_content[nova2$label_content %in% c('Pflanzlich','Pflanzlich+')] <- "Vegetarisch" # Überschreibt das Label "Pflanzlich","Pflanzlich+" mit "Vegetarisch"

nova2$label_content[grep("Pflanzlich+", nova2$label_content)] <- "Vegetarisch" # alternativer Lösungsweg

nova3<-droplevels(nova2) #entfernt die Kategorien die nicht mehr mehr benutzt werden

#Anzahl Vegetarische/FleischMenüs pro Geschlecht~Vegetarisch Base R
observed<-table(nova2$gender, nova2$label_content) 


#Anzahl Vegetarische/FleischMenüs pro Geschlecht~Vegetarisch Tidyverse
#braucht in diesem speziellen Fall viel mehr Code, da der Chi-Quadrat-Test am liebsten Matrizen will
# unser Vorschlag, nehmt den table Befehl
# untenstehend der vollständige Code in Tidyverse
observed_t <- nova2 %>% 
  group_by(gender, label_content) %>% 
  summarise(tot = n()) %>% 
  ungroup() %>%
  spread(., key = label_content, value = tot) %>%  #
  # set_rownames(.$gender) %>% funktioniert leider nicht mehr mit tibble
  select(-gender) %>%
  as.matrix()


#Chi-squared Test
chi_sq <- chisq.test(observed)
chi_sq

#Fisher's Test 
fisher.test(observed)
```

** Ergebnisse **
Der $\chi^2$-Test sagt uns, dass das Geschlecht und die Kaufentscheidung eines Menüinhalts zusammenhängen. Es gibt signifikante Unterscheide zwischen dem Geschlecht und dem Menüinhalt ($\chi^2$(`r chi_sq$parameter`) = `r round(chi_sq$statistic[[1]], digits = 3)`, *p* > .001). Es sieht so aus, dass Männer rund doppel so viel Fleischgerichte wählen als Frauen (siehe Tabelle 1). Die Ergebnisse müssen jedoch mit Vorsicht interpretiert werden, denn der $\chi^2$-Test gibt uns nur an, dass ein signifikanter Unterschied zwischen Geschlecht und Menüinhalt vorliegt. Um die Unterschiede innerhalb der Gruppen festzustellen bedarf es weiterer Analysen z. B. einer mehrfaktorieller ANOVA mit anschliessenden Post-hoc Tests (siehe Statistik 2, Folien 3 bis 11).

```{r, echo=F}
library(reshape2)
table <- as.data.frame(observed) %>%
  mutate(`Verkaufszahlen (%)` = round(Freq / sum(Freq) * 100, 1)) %>% 
  rename(Geschlecht = Var1, Menüinhalt = Var2, Verkaufszahlen = Freq) %>% 
  mutate(Geschlecht = recode(Geschlecht, "F" = "Frauen", "M" = "Männer"))
knitr::kable(table, caption = "Tabelle 1 \n Verkaufszahlen des Menüinhalts nach Geschlecht")

```


*******

### Musterlösung Aufgabe 1.2: $\chi^2$-Test
> Zur eurer Info: dies ist eine spezielle, aber wichtige Anwendung eines $\chi^2$-Tests
> Meine Empfehlung Kapitel "Single factor classification" von [Manny Gimond](https://mgimond.github.io/Stats-in-R/ChiSquare_test.html)


** Null- und Alternativhypothese**
> Beachtet: ungerichtet vs. gerichtete Hypothesen (z. B. Statistik 1, Folie 24)
> Überblick zu Hypothesentestung dazu: https://www.youtube.com/watch?v=F4c0EjsDvzo 
 
$H_0$: Es gibt keine Unterschiede zwischen der Population und der Stichprobe bezüglich Geschlecht und Hochschulzugehörigkeit. 
\par
$H_1$: Es gibt Unterschiede zwischen der Population und der Stichprobe bezüglich Geschlecht und Hochschulzugehörigkeit.

```{r}
# bereitet eure Daten auf
# gruppiert die Variablen und fasst sie 
# nach Geschlecht und Hochschulzugehörigkeit zusammen 
# fügt Information aus der Aufgabenstellung hinzu: absolute Häufigkeiten der Gesamtheit
# für den Chi-Quardrat-Test ist eine Berechnung der relativen Häufigkeiten nötig

df_t <- group_by(nova, gender, member) %>% 
  summarise(stichprobe = n()) %>% 
  ungroup() %>%
  mutate(canteen_member = c("Mitarbeiterinnen", "Studentinnen", "Mitarbeiter", "Studenten"), # Achtung: Reihenfolge muss stimmten vgl. canteen_member
         gesamtheit = c(345, 719, 339, 816), # Achtung: Reihenfolge muss stimmten vgl. oben
         gesamtheit_pct = gesamtheit / sum(gesamtheit),
         stichprobe_pct = stichprobe / sum(stichprobe)) # Berechnung der relativen Häufigkeiten

# berechnet den Chi-Quadrat-Test

chi_sq <- chisq.test(df_t$stichprobe, p = df_t$gesamtheit_pct) # es werden zwei Informationen übergeben, eure beobachteten Werte (stichprobe) und die in der Grundgesamtheit/Population erwarteten Werte (wichtig als relative Häufigkeiten)

chi_sq

```

** Methoden **

Ziel war es die NOVANIMAL Stichprobe gemäss Geschlecht und Hochschulzugehörigkeit in der Grundgesamtheit besser einzuordnen. Die Grundgesamtheit definiert sich durch alle aktiven CampusCards. Dafür ist ein einfaktorieller $\chi^2$-Test notwendig (siehe [Manny Gimond](https://mgimond.github.io/Stats-in-R/ChiSquare_test.html)). Dieser sagt uns nämlich, ob die beobachteten Häufigkeiten/Frequenzen aus unser Stichprobe mit einer definierten erwarteten Häufigkeit/Frequenz (hier der Grundgesamtheit) übereinstimmen. 

** Ergebnisse **

Der $\chi^2$-Test sagt uns, dass die NOVANIMAL-Stichprobe von der Population signifikant unterscheidet ($\chi^2$(`r chi_sq$parameter`) = `r round(chi_sq$statistic[[1]], digits = 3)`, *p* > .001). Demnach ist unsere Stichprobe bezüglich den Variablen Geschlecht und Hochschulzugehörigkeit nicht repräsentativ für die Grundgesamtheit. Es scheint, dass die Studentinnen unter- und die Studenten übervertreten sind (siehe Tabelle 2).

```{r, echo=F}
table <- df_t %>% 
  rename(Hochschulzugehörigkeit = member, `Anzahl Population` = gesamtheit, `Anzahl Stichprobe` = stichprobe, `Anteil Population (%)` = gesamtheit_pct, `Anteil Stichprobe (%)` = stichprobe_pct) %>%
  dplyr::select(Hochschulzugehörigkeit, `Anzahl Population`, `Anteil Population (%)`, `Anzahl Stichprobe`, `Anteil Stichprobe (%)`) %>% 
  mutate(`Anteil Stichprobe (%)` = round(`Anteil Stichprobe (%)`,2)*100, `Anteil Population (%)` = `Anteil Population (%)`*100)
knitr::kable(table, caption = "Tabelle 2 \nAnzahl und Anteil Beobachtungen in der Population und in der Stichprobe")

```

*******

###  Musterlösung Aufgabe 1.3: t-Test
> Meine Empfehlung Kapitel 2 von [Manny Gimond](https://mgimond.github.io/Stats-in-R/z_t_tests.html) 

** Null- und Alternativhypothese **
<>$H_0$: Es gibt keine Unterschiede in den Verkaufszahlen zwischen Basis- und Interventionswochen. <>
\par
$H_1$: Es gibt Unterschiede in den Verkaufszahlen zwischen Basis- und Interventionswochen.


```{r}
# Gemäss Aufgabenstellung müsset die Daten zuerst nach Kalenderwochen "week" und Bedingungen "condition" zusammengefasst werden

df <- nova %>%
    group_by(week, condit) %>%  
    summarise(tot_sold = n()) 

# überprüft die Voraussetzungen für einen t-Test
ggplot(df, aes(x = condit, y= tot_sold)) + # achtung 0 Punkt fehlt
    geom_boxplot(fill = "white", color = "black", size = 1) + 
    labs(x="\nBedingungen", y="Durchschnittlich verkaufte Gerichte pro Woche\n") + 
    mytheme

# Auf den ersten Blick scheint es keine starken Abweichungen zu einer Normalverteilung zu geben resp. es sind keine extremen schiefen Verteilungen ersichtlich (vgl. Statistik 2, Folien 12-21)

```


```{r}

# führt einen t-Tests durch; 
# es wird angenommen, dass die Verkaufszahlen zwischen den Bedingungen unabhängig sind

t_test <- t.test(tot_sold~condit, data=df)

t.test(df[df$condit == "Basis", ]$tot_sold, 
                 df[df$condit == "Intervention", ]$tot_sold) #alternative Formulierung

```

** Methoden **

Ziel war es die wöchentlichen Verkaufszahlen zwischen den Interventions- und Basiswochen zu vergleichen. Die Annahme war, dass die wöchentlichen Verkaufszahlen unabhängig sind. Daher können die mittleren Verkaufszahlen pro Woche zwischen den beiden Bedingungen mittels t-Test geprüft werden. Obwohl die visuelle Inspektion keine schwerwiegenden Verletzungen der Modelvoraussetzung zeigte, wurde einen Welch t-Test gerechnet. 

** Ergebnisse **

In den Basiswochen werden mehr Gerichte pro Woche verkauft als in den Interventionsowochen (siehe Abbildung 1). Die wöchentlichen Verkaufszahlen zwischen den Bedigungen (Basis oder Intervention) unterscheiden sich gemäss Welch t-Test jedoch nicht signifikant (*t*(`r round(t_test$parameter[[1]],digits = 0)`) = `r round(t_test$statistic, digits = 3)` , *p* = `r round(t_test$p.value, digits=3)`).


```{r, purl=F, message=T, echo = F,  fig.cap="Abbildung1. Die wöchentlichen Verkaufszahlen für die Interventions- und Basiswochen unterscheiden sich nicht signifikant."}
# zeigt die Ergebnisse mit einer Abbildung
ggplot(df, aes(x = condit, y= tot_sold)) + 
  stat_boxplot(geom ='errorbar', width = .25) + # erzeugt sogenannte Whiskers mit Strichen
  geom_boxplot(fill = "white", color = "black", size = 1) +
  labs(x="\nBedingungen", y="Durchschnittlich verkaufte Gerichte pro Woche\n") + 
  mytheme

```


<!-- ******* -->
<!-- ##### letztes Update `r Sys.Date()`, gian-andrea.egeler@zhaw.ch & stefan.widmer@zhaw.ch -->
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:13_Statistik1/solution_stat1.Rmd-->

# Statistik 2 (29.10.2019)

In Statistik 2 lernen die Studierenden die Idee, die Voraussetzungen und die praktische Anwendung „einfacher“ linearer Modelle in R (sowie teilweise ihrer „nicht-parametrischen“ bzw. „robusten“ Äquivalente). Am Anfang steht die Varianzanalyse (ANOVA) als Verallgemeinerung des t-Tests, einschliesslich post-hoc-Tests und mehrfaktorieller ANOVA. Dann geht es um die Voraussetzungen parametrischer (und nicht-parametrischer) Tests und Optionen, wenn diese verletzt sind. Dann beschäftigen wir uns mit Korrelationen, die auf einen linearen Zusammenhang zwischen zwei metrischen Variablen testen, ohne Annahme einer Kausalität. Es folgen einfache lineare Regressionen, die im Prinzip das Gleiche bei klarer Kausalität leisten. Abschliessend besprechen wir, was die grosse Gruppe linearer Modelle (Befehl lm in R) auszeichnet.
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:14_Statistik2/Abstract.Rmd-->


```{r, include=FALSE, purl=F}

knitr::opts_chunk$set(echo = TRUE,include = T, results = "hide",collapse=TRUE)
```

## Demoskript

[Demoscript als Download](14_Statistik2/RFiles/Demo_Tests.R)

 __t-test als ANOVA__ 

```{r}
a<-c(20,19,25,10,8,15,13,18,11,14)
b<-c(12,15,16,7,8,10,12,11,13,10)

blume<-data.frame(cultivar=c(rep("a",10),rep("b",10)),size=c(a,b))

par(mfrow=c(1,1))
boxplot (data=blume, size~cultivar, xlab="Sorte", ylab="Bluetengroesse [cm]")

t.test(size~cultivar, blume, var.equal=T)

aov(size~cultivar,data=blume)
summary(aov(size~cultivar,data=blume))
summary.lm(aov(size~cultivar,data=blume))
```

__Echte ANOVA__

```{r}
c<-c(30,19,31,23,18,25,26,24,17,20)

blume2<-data.frame(cultivar=c(rep("a",10),rep("b",10),rep("c",10)),size=c(a,b,c))

summary(blume2)             
head(blume2)

par(mfrow=c(1,1))
boxplot (data=blume2, size~cultivar, xlab="Sorte", ylab="Blütengrösse [cm]")

aov(size~cultivar,data=blume2)
summary(aov(size~cultivar,data=blume2))
summary.lm(aov(size~cultivar,data=blume2))

aov.1 <- aov(size~cultivar,data=blume2)
summary(aov.1)
summary.lm(aov.1)

#Berechnung Mittelwerte usw. zur Charakterisierung der Gruppen
aggregate(size~cultivar,blume2, function(x) c(Mean = mean(x), SD = sd(x), Min=min(x), Max=max(x)))

lm.1 <- lm(size~cultivar,data=blume2)
summary(lm.1)
```


__Tukeys Posthoc-Test__

```{r}
if(!require(multcomp)){install.packages("multcomp")}
library(multcomp)
summary(glht(aov(size~cultivar, data=blume2),linfct=mcp(cultivar ="Tukey")))
```


__Beispiel Posthoc-Labels in Plot__

```{r}
anova <- aov(Sepal.Width ~ Species, data=iris)
letters <- cld(glht(anova, linfct=mcp(Species="Tukey")))
boxplot(Sepal.Width ~ Species, data=iris)
mtext(letters$mcletters$Letters, at=1:3)

library(tidyverse)
ggplot(iris, aes(Species, Sepal.Width)) + geom_boxplot(size = 1) +
  annotate("text", y = 5, x = 1:3, label = letters$mcletters$Letters)

```


__Klassische Tests der Modellannahmen (NICHT EMPFOHLEN!!!) __

```{r}
attach(blume2)
shapiro.test(size[cultivar == "a"])

var.test(size[cultivar == "a"],size[cultivar == "b"])

if(!require(car)){install.packages("car")}
library(car)
leveneTest(size[cultivar == "a"],size[cultivar == "b"],center=mean)

wilcox.test(size[cultivar == "a"],size[cultivar == "b"])

detach(blume2)
```


__Nicht-parametrische Alternativen, wenn Modellannahmen der ANVOA massiv verletzt sind__

__Zum Vergleich normale ANOVA noch mal__

```{r}
summary(aov(size~cultivar,data=blume2))
```

__Bei starken Abweichungen von der Normalverteilung, aber ähnlichen Varianzen__

__Kruskal-Wallis-Test__

```{r}
kruskal.test(data=blume2, size~cultivar)
if(!require(FSA)){install.packages("FSA")} 
library(FSA)
dunnTest(data=blume2, size~cultivar, method="bh") #korrigierte p-Werte nach Bejamini-Hochberg
```

__Bei erheblicher Heteroskedastizität, aber relative normal/symmetrisch verteilten Residuen__

__Welch-Test__

```{r}
oneway.test(data=blume2, size~cultivar, var.equal=F)
```

__2-faktorielle ANOVA__

```{r}

d<-c(10,12,11,13,10,25,12,30,26,13)
e<-c(15,13,18,11,14,25,39,38,28,24)
f<-c(10,12,11,13,10,9,2,4,7,13)

blume3<-data.frame(cultivar=c(rep("a",20),rep("b",20),rep("c",20)),
                   house=c(rep(c(rep("yes",10),rep("no",10)),3)),size=c(a,b,c,d,e,f))
blume3

boxplot(size~cultivar+house,data=blume3)

summary(aov(size~cultivar+house,data=blume3))
summary(aov(size~cultivar+house+cultivar:house,data=blume3)) 
summary(aov(size~cultivar*house,data=blume3)) #Kurzschreibweise: "*" bedeutet, dass Interaktion zwischen cultivar und house eingeschlossen wird

summary.lm(aov(size~cultivar+house,data=blume3))


interaction.plot(blume3$cultivar,blume3$house,blume3$size)
interaction.plot(blume3$house,blume3$cultivar,blume3$size)

anova(lm(blume3$size~blume3$cultivar*blume3$house),lm(blume3$size~blume3$cultivar+blume3$house))
anova(lm(blume3$size~blume3$house),lm(blume3$size~blume3$cultivar*blume3$house))
```


__Korrelationen__

```{r}
library(car)

blume<-data.frame(a,b)
scatterplot(a~b,blume)

cor.test(a,b,data = blume, method="pearson")
cor.test(a,b,data = blume, method="spearman")
cor.test(a,b,data = blume, method="kendall") 

#Jetzt als Regression
lm.2 <- lm(b~a)
anova(lm.2)
summary(lm.2)

#Model II-Regression
if(!require(lmodel2)){install.packages("lmodel2")} 
library(lmodel2)
lmodel2(b~a)
```

__Beispiele Modelldiagnostik__
```{r}
par(mfrow=c(2,2)) #4 Plots in einem Fenster
plot(lm(b~a))

if(!require(ggfortify)){install.packages("ggfortify")}
library(ggfortify)
autoplot(lm(b~a))

#Modellstatistik nicht OK
g<-c(20,19,25,10,8,15,13,18,11,14,25,39,38,28,24)
h<-c(12,15,10,7,8,10,12,11,13,10,25,12,30,26,13)
par(mfrow=c(1,1))

plot(h~g,xlim=c(0,40),ylim=c(0,30))
abline(lm(h~g))

par(mfrow=c(2,2))
plot(lm(h~g))

#Modelldiagnostik mit ggplot
df <- data.frame(g,h)
ggplot(df, aes(x = g, y = h)) + 
    # scale_x_continuous(limits = c(0,25)) +
    # scale_y_continuous(limits = c(0,25)) +
    geom_point() +
    geom_smooth( method = "lm", color = "black", size = .5, se = F) + 
    theme_classic()

par(mfrow=c(2,2))
plot(lm(h~g))

autoplot(lm(h~g))


```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:14_Statistik2/Demoskript.Rmd-->

## Übungen 2

Repetition: Abzugeben sind am Ende
    
    a. lauffähiges R-Skript
    b. begründeter Lösungsweg (Kombination aus R-Code, R Output 
       und dessen Interpretation)
    c. ausformulierter Methoden- und Ergebnisteil (für eine wiss.Arbeit).
    
- Bitte **erklärt und begründet die einzelnen Schritte,** die ihr unternehmt, um zu eurem Ergebnis zu kommen. Dazu erstellt bitte ein Word-Dokument, in dem ihr Schritt für Schritt den verwendeten **R-Code**, die dazu      gehörigen **Ausgaben von R**, eure **Interpretation** derselben und die sich ergebenden **Schlussfolgerungen** für das weitere Vorgehen dokumentiert.
  
- Dieser **Ablauf** sollte insbesondere beinhalten:
    - Überprüfen der Datenstruktur nach dem Einlesen, welches sind die abhängige(n) und welches die unabängige(n) Variablen etc.
    - Explorative Datenanalyse, um zu sehen, ob evtl. Dateneingabefehler vorliegen oder Datentransformationen vorgenommen werden sollten
    - Auswahl und Begründung eines statistischen Verfahrens
    - Bestimmung des vollständigen/maximalen Models
    - Selektion des/der besten Models/Modelle
    - Durchführen der Modelldiagnostik für dieses
    - Generieren aller Zahlen, Statistiken und Tabellen, die für eine wiss. Ergebnisdarstellung benötigt werden
    
- Formuliert abschliessend einen **Methoden- und Ergebnisteil** (ggf. incl. adäquaten Abbildungen/Tabellen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (je einen ausformulierten Absatz von ca. 60-100 Worten bzw. 3-8 Sätzen). Alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden.

### Übung 2.1: Regression (NatWis)

**Regressionsanalyse mit [decay.csv](14_Statistik2/data/decay.csv)**

Der Datensatz beschreibt in einem physikalischen Experiment die Zahl der radioaktiven Zerfälle pro Minute in Abhängigkeit vom Zeitpunkt (min nach Start des Experimentes).

- Ladet den Datensatz in R und macht eine explorative Datenanalyse.
- Wählt unter den schon gelernten Methoden der Regressionsanalyse einadäquates Vorgehen zur Analyse dieser Daten und führt diese dann durch.
- Prüft anhand der Residuen, ob die Modellvoraussetzungen erfüllt waren
- Stellt die erhaltenen Ergebnisse angemessen dar (Text, Abbildung und/oder Tabelle).
- Kennt ihr ggf. noch eine andere geeignete Herangehensweise?


### Übung 2.2: Einfaktorielle ANOVA (SozOek)

**ANOVA mit [novanimal.csv](13_Statistik1/data/novanimal.csv)**

Führt mit dem Datensatz novanimal.csv eine einfaktorielle ANOVA durch. Gibt es Unterschiede zwischen der Anzahl verkaufter Gerichte (Buffet, Fleisch oder Vegetarisch) pro Woche?

Hinweise für die Analysen:

- Fasst die vier Inhalte der Gerichte zu drei Inhalten zusammen. Das heisst, dass die pflanzlichen Gerichte neu zu den vegetarischen Gerichten gezählt
werden. Konkret könnt ihr das in R mit einer Umbenennung der Inhalte durchführen (z. B. mit grep()).
- Danach muss der Datensatz gruppiert und zusammengefasst werden.
- Unbekannte Menü-Inhalte können vernachlässigt werden.
- Wie gut sind die Voraussetzungen für eine ANOVA erfüllt? Wären allenfalls auch nicht-parametrische Analysen zulässig?
- Führt anschliessend Post-hoc-Vergleiche durch.
- Fasst die Ergebnisse in einem Satz zusammen.


### Übung 2.3N: Mehrfaktorielle ANOVA (NatWis)

**ANOVA mit [kormoran.csv](14_Statistik2/data/kormoran.csv)**

Der Datensatz enthält 40 Beobachtungen zu Tauchzeiten zweier Kormoranunterarten (C = *Phalocrocorax carbo carbo* und S = *Phalacrocorax carbo sinensis*) aus vier Jahreszeiten (F = Frühling, S = Sommer, H = Herbst, W = Winter).

- Lest den Datensatz nach R ein und führt eine adäquate Analyse durch, um
beantworten zu können, wie Unterart und Jahreszeit die Tauchzeit beeinflussen.
- Stellt eure Ergebnisse dann angemessen dar (Text mit Abbildung und/oder Tabelle).
- Gibt es eine Interaktion?

### Übung 2.3S: Mehrfaktorielle ANOVA mit Interaktion (SozOek)

**ANOVA mit [novanimal.csv](13_Statistik1/data/novanimal.csv)**

Können die Unterschiede in den verkauften Gerichten (Buffet, Fleisch oder
Vegetarisch) durch die beiden Bedingungen (Basis- oder Interventionswochen) erklärt werden?

Hinweise für die Analysen:

- Fasst die vier Inhalte der Gerichte zu drei Inhalten zusammen. Das heisst,
dass die pflanzlichen Gerichte neu zu den vegetarischen Gerichten gezählt
werden. Konkret könnt ihr das in R mit einer Umbenennung der Inhalte
durchführen (z. B. mit grep()).
- Danach muss der Datensatz gruppiert und zusammengefasst werden.
- Unbekannte Menü-Inhalte können vernachlässigt werden.
- Wie gut sind die Voraussetzungen für eine ANOVA erfüllt? Wären allenfalls
auch nicht-parametrische Analysen zulässig?
- Führt anschliessend Post-hoc-Vergleiche durch.
- Stellt eure Ergebnisse dann angemessen dar (Text mit Abbildung und/oder Tabelle).

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:14_Statistik2/assigment_stat2.Rmd-->

---
title: "MSc. Research Methods - Statistikteil Lösungen 2019"
author: "Juergen Dengler"
date: "`r format(Sys.Date(), '%B %Y')`"
output:
  pdf_document: default
  html_document:
    df_print: paged
---


## Musterlösung Aufgabe 2.1: Regression

[RCode als Download](14_Statistik2/RFiles/Loesung_Uebung_2.1_v.02.R)

**Übungsaufgabe (hier so ausführlich formuliert, wie dies auch in der Klausur der Fall sein
wird)**

- Laden Sie den Datensatz decay.csv. Dieser enthält die Zahl radioaktiver Zerfälle pro
Zeiteinheit (amount) für Zeitpunkte (time) nach dem Start des Experimentes.
- **Ermitteln Sie ein statistisches Modell, dass die Zerfallshäufigkeit in Abhängigkeit
von der Zeit beschreibt.**
- Bitte erklären und begründen Sie die einzelnen Schritte, die Sie unternehmen, um zu
diesem Ergebnis zu kommen. Dazu erstellen Sie bitte ein Word-Dokument, in das Sie
Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, Ihre
Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere
Vorgehen dokumentieren.
- Dieser Ablauf sollte insbesondere beinhalten:
    - Überprüfen der Datenstruktur nach dem Einlesen, welches sind die abhängige(n) und welches die             unabängige(n) Variablen
    - Explorative Datenanalyse, um zu sehen, ob evtl. Dateneingabefehler vorliegen oder
      Datentransformationen vorgenommen werden sollten
    - Auswahl und Begründung eines statistischen Verfahrens (es gibt hier mehrere
      statistisch korrekte Möglichkeiten!)
    - Ermittlung eines Modells
    - Durchführen der Modelldiagnostik für das gewählte Modell
    - Generieren aller Zahlen, Statistiken und Tabellen, die für eine wiss.
      Ergebnisdarstellung benötigt werden
    - Formulieren Sie abschliessend einen Methoden- und Ergebnisteil (ggf. incl.
      adäquaten Abbildungen) zu dieser Untersuchung in der Form einer
      wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je
      einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und
      Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige
      Redundanz dagegen vermieden werden.
    - **Abzugeben sind am Ende (a) Ein lauffähiges R-Skript; (b) begründeter Lösungsweg
      (Kombination aus R-Code, R Output und dessen Interpretation) und (c)
      ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit).**
      

**kommentierter Lösungsweg**

```{r}
decay <- read.delim("14_Statistik2/data/decay.csv",sep = ",")

decay

# Um die Variablen im Dataframe im Folgenden direkt (ohne $ bzw. ohne "data = data") ansprechen zu können
attach(decay)
summary(decay)
str(decay)
```
Man erkennt, dass es 31 Beobachtungen für die Zeit als Integer von Zerfällen gibt, die als rationale Zahlen angegeben werden (dass die Zahl der Zerfälle nicht ganzzahlig ist, deutet darauf hin, dass sie möglicherweise nur in einem Teil des Zeitintervalls oder für einen Teil des betrachteten Raumes gemessen und dann hochgerechnet wurde.

**Explorative Datenanalyse**
```{r}
boxplot(time)
boxplot(amount)
plot(amount~time)
```

Während der Boxplot für time wunderbar symmetrisch ohne Ausreisser ist, zeigt amount eine stark rechtsschiefe (linkssteile) Verteilung mit einem Ausreiser. Das deutet schon an, dass ein einfaches lineares Modell vermutlich die Modellannahmen verletzen wird. Auch der einfache Scatterplot zeigt, dass ein lineares Modell wohl nicht adäquat ist. Wir rechnen aber erst einmal weiter.

**Einfaches lineares Modell**
```{r}
lm.1<-lm(amount~time)
summary(lm.1)
```
Das sieht erst einmal nach einem Supermodell aus, höchstsignifikant und mit einem hohen R² von fast 77%. ABER: wir müssen uns noch die Modelldiagnostik ansehen...

**Modelldiagnostik**
```{r}
par(mfrow=c(2,2))
plot(lm.1)
```
Hier zeigen die wichtigen oberen Plots beide massive Abweichungen vom „Soll“. Der Plot oben links zeigt eine „Banane“ und beim Q-Q-Plot oben rechts weichen die Punkte rechts der Mitte alle stark nach oben von der Solllinie ab. Wir haben unser Modell also offensichtlich falsch spezifiziert.
Um eine Idee zu bekommen, was falsch ist, plotten wir noch, wie das Ergebnis dieses Modells aussähe:


**Ergebnisplot**
```{r}
par(mfrow=c(1,1))
plot(time,amount)
abline(lm(amount~time),col="red")
abline(lm.1,col="red")
```

Die Punkte links liegen alle über der Regressionslinie, die in der Mitte darunter und die ganz rechts wieder systematisch darüber (darum im Diagnostikplot oben die „Banane“). Es liegt also offensichtlich keine lineare Beziehung vor, sondern eine curvilineare.

Um diese korrekt zu analysieren, gibt es im Prinzip drei Möglichkeiten, wovon am zweiten Kurstag nur eine hatten, während die zweite und dritte in Statistik 3 und 4 folgten. Im Folgenden sind alle drei nacheinander dargestellt (in der Klausur würde es aber genügen, eine davon darzustellen, wenn die Aufgabenstellung wie oben lautet).

**Variante (1): Lineares Modell nach Transformation der abhängigen Variablen**
Dass die Verteilung der abhängigen Variable nicht normal ist, haben wir ja schon bei der explorativen Datenanalyse am Anfang gesehen. Da sie stark linkssteil ist, zugleich aber keine Nullwerte enthält, bietet sich eine Logarithmustransformation an, hier z. B. mit dem natürlichen Logarithmus.

**Loesung 1: log-Transformation der abhaengigen Variablen**
```{r} 
par(mfrow=c(1,2))
boxplot(amount)
boxplot(log(amount))
hist(amount)
hist(log(amount))

#Die log-transformierte Variante rechts sieht sowohl im Boxplot als auch im #Histogramm viel symmetrischer/besser normalverteilt aus. Damit ergibt sich #dann folgendes lineares Modell

lm.2<-lm(log(amount)~time)
summary(lm.2)
```
Jetzt ist der R²-Wert noch höher und der p-Wert noch niedriger als im ursprünglichen linearen Modell ohne Transformation. Das erlaubt aber keine Aussage, da wir Äpfel mit Birnen vergleichen, da die abhängige Variable einmal untransformiert und einmal log-transformiert ist. Entscheidend ist die Modelldiagnostik.

**Modelldiagnostik**
```{r}
par(mfrow=c(2,2))
plot(lm.2)
```

Der Q-Q-Plot sieht jetzt exzellent aus, der Plot rechts oben hat kaum noch eine Banane, nur noch einen leichten Keil. Insgesamt deutlich besser und auf jeden Fall ein statistisch korrektes Modell.


Lösungen 2 und 3 greifen auf Methoden von Statistik 3 und 4 zurück, sie sind hier nur zum Vergleich angeführt

**Loesung 2: quadratische Regression (kam erst in Statistik 3;**
_**koente fuer die Datenverteilung passen, entspricht aber nicht der physikalischen**_

**Gesetzmaessigkeit**

```{r}
model.quad<-lm(amount~time+I(time^2))
summary(model.quad)
```
Hier können wir R² mit dem ursprünglichen Modell vergleichen (beide haben amount als abhängige Grösse) und es sieht viel besser aus. Sowohl der lineare als auch der quadratische Term sind hochsignifikant. Sicherheitshalber vergleichen wir die beiden Modelle aber noch mittels ANOVA.

**Vergleich mit dem einfachen Modell mittels ANOVA (es ginge auch AICc)**
```{r}
anova(lm.1,model.quad)
```
In der Tat ist das komplexere Modell (jenes mit dem quadratischen Term) höchstsignifikant besser. Jetzt brauchen wir noch die Modelldiagnostik.


**Modelldiagnostik**
```{r}
par(mfrow=c(2,2))
plot(model.quad)
```

**Loesung 3 (die beste, hatten wir aber am 2. Tag noch nicht; mit Startwerten muss man ggf. ausprobieren)**

_**mit Startwerten muss man ggf. ausprobieren)**_
```{r}
model.nls<-nls(amount~a*exp(-b*time),start=(list(a=100,b=1)))
summary(model.nls)
```

**Modelldiagnostik**
```{r}
if(!require(nlstools)){install.packages("nlstools")}
library(nlstools)
residuals.nls <- nlsResiduals(model.nls)
plot(residuals.nls)
```

Für nls kann man nicht den normalen Plotbefehl für die Residualdiagnostik nehmen, sondern verwendet das Äquivalent aus nlstools. Die beiden entscheidenden Plots sind jetzt links oben und rechts unten. Der QQ-Plot hat im unteren Bereich einen kleinen Schönheitsfehler, aber ansonsten ist alles OK.

Da alle drei Lösungen zumindest statistisch OK waren, sollen jetzt noch die zugehörigen Ergebnisplots erstellt werden.


**Ergebnisplots**
```{r}
par(mfrow=c(1,1))
xv<-seq(0,30,0.1)
```

1. lineares Modell mit log-transformierter Abhaengiger

```{r}
plot(time,amount)
yv1<-exp(predict(lm.2,list(time=xv)))
lines(xv,yv1,col="red")
```

2. quadratisches Modell
```{r}
plot(time,amount)
yv2<-predict(model.quad,list(time=xv))
lines(xv,yv2,col="blue")
```

3. nicht-lineares Modell
```{r}
plot(time,amount)
yv3<-predict(model.nls,list(time=xv))
lines(xv,yv3,col="green")
```
Optisch betrachtet, geben (2) und (3) den empirischen Zusammenhang etwas besser wieder als (1), da sie im linken Bereich die hohen Werte besser treffen. Man könnte sogar meinen, bei Betrachtung der Daten, dass die Werte ab time = 28 wieder leicht ansteigen, was die quadratische Funktion wiedergibt. Wer sich aber mit Physik etwas auskennt, weiss, dass Version (2) physikalisch nicht zutrifft, da die Zerfallsrate mit der Zeit immer weiter abfällt. Aufgrund der kurzen Messreihe wäre eine quadratische Funktion trotzdem eine statistisch korrekte Interpretation. Mit längeren Messreihen würde sich jedoch schnell zeigen, dass sie nicht zutrifft.
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:14_Statistik2/solution_stat2.1.Rmd-->

## Musterlösung Aufgabe 2.2: einfaktorielle ANOVA

>Download [R-Skript](14_Statistik2/RFiles/solution_stat2.2.R)

**kommentierter Lösungsweg**
```{r, message=FALSE, echo=FALSE, results='hide', warning=FALSE}

library(tidyverse)
library(ggfortify) # zur Testung der Voraussetzungen

## ladet die nötigen Packete und die novanimal.csv Datei in R
nova <- read_delim("13_Statistik1/data/novanimal.csv", delim = ";")

## definiert mytheme für ggplot2 (verwendet dabei theme_classic())
mytheme <- 
  theme_classic() + 
  theme(
    axis.line = element_line(color = "black"), 
    axis.text = element_text(size = 20, color = "black"), 
    axis.title = element_text(size = 20, color = "black"), 
    axis.ticks = element_line(size = 1, color = "black"), 
    axis.ticks.length = unit(.5, "cm")
    )


```



```{r}

df <- nova # klone den originaler Datensatz

# fasst die vier Inhalte der Gerichte zu drei Inhalten zusammen.
df$label_content[grep("Pflanzlich+",df$label_content)] <- "Vegetarisch" 

# gruppiert Daten nach Menü-Inhalt und Woche
df_ <- df %>%
    group_by(label_content, week) %>% 
    summarise(tot_sold = n()) %>%
    drop_na() # lasst die unbekannten Menü-Inhalte weg

# überprüft die Voraussetzungen für eine ANOVA
# Schaut euch die Verteilungen der Mittelwerte an (plus Standardabweichungen)
# Sind Mittelwerte nahe bei Null? Gäbe uns einen weiteren Hinweis auf eine spezielle Binomail-Verteilung (vgl. Statistik 4, FOlie 17)
df_  %>% 
  split(.$label_content) %>% # teilt den Datensatz in 3 verschiedene Datensätze auf
  purrr::map(~ psych::describe(.$tot_sold)) # mit map können andere Funktionen auf den Datensatz angewendet werden (ähnliche Funktion ist aggregate)


# Boxplot
ggplot(df_, aes(x = label_content, y= tot_sold)) + 
  stat_boxplot(geom = "errorbar", width = 0.25) + # Achtung: Reihenfolge spielt hier eine Rolle!
  geom_boxplot(fill="white", color = "black", size = 1, width = .5) +
  labs(x = "\nMenü-Inhalt", y = "Anzahl verkaufte Gerichte pro Woche\n") + 
  mytheme # achtung erster Hinweis einer Varianzheterogenität


# definiert das Modell (Statistik 2: Folien 4-8)
model <- aov(tot_sold ~ label_content, data = df_)

summary.lm(model)

# überprüft die Modelvoraussetzungen
autoplot(model) + mytheme 

```
<br>  
<span style="background-color: #FFFF00">Fazit: Inspektion der Modellvoraussetzung zeigt klare Verletzungen des Residuelplots (zeigt einen "Trichter", siehe Statistik 2: Folie 13-14; 42), somit Voraussetzung der Homoskedastizität verletzt. Mögliche nächste Schritte: Datentransformation oder nicht-parametrischer Test.</span>
<br>
```{r}

# überprüft die Voraussetzungen des Welch-Tests:
# Gibt es eine hohe Varianzheterogenität und ist die relative Verteilung der Residuen gegeben? (siehe Folien Statistik 2: Folie 18)
# Ja Varianzheterogenität ist gegeben, aber die Verteilung der Residuen folgt einem "Trichter", also keiner "normalen/symmetrischen" Verteilung um 0 (siehe Folien Statistik 2: Folie 42)
# Daher ziehe ich eine Transformation der AV dem nicht-parametrischen Test vor
# für weitere Infos: https://data.library.virginia.edu/interpreting-log-transformations-in-a-linear-model/
model1 <- aov(log10(tot_sold) ~ label_content, data = df_)# achtung hier log10, bei Rücktransformation achten

autoplot(model1) + mytheme # scheint ok zu sein

summary.lm(model1) # Referenzkategorie ist der Buffet-Inhalt

TukeyHSD(model1) # (Statistik 2: Folien 9-11)

# achtung Beta-Werte resp. Koeffinzienten sind nicht direkt interpretierbar
# sie müssten zuerst wieder transformiert werden, hier ein Beispiel dafür:
# für Buffet
10^model1$coefficients[1]

# für Fleisch
10^(model1$coefficients[1] + model1$coefficients[2])

# für Vegi
10^(model1$coefficients[1] + model1$coefficients[3])
```


```{r, echo=F, fig.cap="Die wöchentlichen Verkaufzahlen unterscheiden sich je nach Menü-Inhalt stark. Das Modell wurde mit den log-tranformierten Daten gerechnet.", tidy=T}

# plottet die Ergebnisse, die nicht tranformierten Daten werden hier aufgezeigt
# Wichtig: einen Verweis auf die Log-Transformation benötigt es jedoch

# aufbereitung für die Infos der Signifikanzen => Alternative Lösungen findet ihr in der Musterlösung 2.3S
df1 <- data.frame(a = c(1, 1:3,3), b = c(150, 151, 151, 151, 150)) 
df2 <- data.frame(a = c(1, 1,2, 2), b = c(130, 131, 131, 130))
df3 <- data.frame(a = c(2, 2, 3, 3), b = c(140, 141, 141, 140))


ggplot(df_, aes(x = label_content, y= tot_sold)) +
   stat_boxplot(geom = "errorbar", width = .25) +
   geom_boxplot(fill="white", color = "black", size = 1, width = .5) + 
   geom_line(data = df1, aes(x = a, y = b)) + annotate("text", x = 2, y = 152, label = "***", size = 8) + # aus der Information aus dem Tukey Test von oben: Buffet-Vegetarisch
   geom_line(data = df2, aes(x = a, y = b)) + annotate("text", x = 1.5, y = 132, label = "***", size = 8) + # Buffet - Fleisch
   geom_line(data = df3, aes(x = a, y = b)) + annotate("text", x = 2.5, y = 142, label = "**", size = 8)+ # Fleisch - Vegetarisch
   expand_limits(y = 0) + # nimmt das 0 bei der y-Achse mit ein
   labs(x = "\nMenü-Inhalt", y = "Anzahl verkaufte Gerichte pro Woche\n") +
   mytheme 

# hier ein paar interessante Links zu anderen R-Packages, die es ermöglichen signifikante Ergebniss in den Plot zu integrieren
# https://www.r-bloggers.com/add-p-values-and-significance-levels-to-ggplots/
# https://cran.r-project.org/web/packages/ggsignif/vignettes/intro.html
  
   
```

<!-- -------- -->
<!-- letztes Update `r Sys.Date()`, gian-andrea.egeler@zhaw.ch -->
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:14_Statistik2/solution_stat2.2.Rmd-->

## Musterlösung Aufgabe 2.3N: Mehrfaktorielle ANOVA

[RCode als Download](14_Statistik2/RFiles/Loesung_Uebung_2.3N_v.03.R)

**Übungsaufgabe (hier so ausführlich formuliert, wie dies auch in der Klausur der Fall sein
wird)**

- Laden Sie den Datensatz kormoran.txt mit read.table. Dieser enthält Tauchzeiten
(hier ohne Einheit) von Kormoranen in Abhängigkeit von Jahreszeit und Unterart.
Unterarten: Phalacrocorax carbo carbo (C) und Phalacrocorax carbo sinensis (S);
Jahreszeiten: F = Frühling, S = Sommer, H = Herbst, W = Winter.
- **Ihre Gesamtaufgabe ist es, aus diesen Daten ein minimal adäquates Modell zu
ermitteln, das diese Abhängigkeit beschreibt.**
- Bitte erklären und begründen Sie die einzelnen Schritte, die Sie unternehmen, um zu
diesem Ergebnis zu kommen. Dazu erstellen Sie bitte ein Word-Dokument, in das Sie
Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, Ihre
Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere
Vorgehen dokumentieren.
- Dieser Ablauf sollte insbesondere beinhalten:
  - Überprüfen der Datenstruktur nach dem Einlesen, welches sind die abhängige(n)
    und welches die unabängige(n) Variablen, welches statistische Verfahren wenden Sie
    an?
  - Explorative Datenanalyse, um zu sehen, ob schon vor dem Start der Analysen
    Transformationen o.ä. vorgenommen werden sollten
  - Definition eines vollen Modelles, das nach statistischen Kritierien zum minimal
    adäquaten Modell reduziert wird
  - Durchführen der Modelldiagnostik, um zu entscheiden, ob das gewählte Vorgehen
    korrekt war oder ggf. angepasst werden muss
  - Generieren aller Zahlen, Statistiken und Tabellen, die für eine wiss.
    Ergebnisdarstellung benötigt werden
- Formulieren Sie abschliessend einen Methoden- und Ergebnisteil (ggf. incl.
  adäquaten Abbildungen) zu dieser Untersuchung in der Form einer
  wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je
  einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und
  Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige
  Redundanz dagegen vermieden werden.
- **Abzugeben sind am Ende (a) Ein lauffähiges R-Skript; (b) begründeter Lösungsweg
  (Kombination aus R-Code, R Output und dessen Interpretation) und (c)
  ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit).**


**kommentierter Lösungsweg**

```{r}

kormoran <-read.delim("14_Statistik2/data/kormoran.csv",sep = ";")

## Ueberpruefen, ob Einlesen richtig funktioniert hat und welche Datenstruktur vorliegt
str(kormoran)
summary(kormoran)

#Man erkennt, dass es sich um einen Dataframe mit einer metrischen (Tauchzeit) und zwei kategorialen (Unterart, Jahreszeit) Variablen handelt.
#Die adäquate Analyse (1 metrische Abhängige vs. 2 kategoriale Unabhängige) ist damit eine zweifaktorielle ANOVA
#Die Sortierung der Jahreszeiten (default: alphabetisch) ist inhaltlich aber nicht sinnvoll und sollte angepasst werden.

# Um die Variablen im Dataframe im Folgenden direkt (ohne $ bzw. ohne "data = kormoran") ansprechen zu koennen
attach(kormoran)

# Umsortieren der Faktoren, damit sie in den Boxplots eine sinnvolle Reihung haben
Jahreszeit<-factor(Jahreszeit,levels=c("F","S","H","W"))

# Explorative Datenanalyse (zeigt uns die Gesamtverteilung)
boxplot(Tauchzeit)

#Das ist noch OK für parametrische Verfahren (Box ziemlich symmetrisch um Median, Whisker etwas asymmetrisch aber nicht kritisch). Wegen der leichten Asymmetrie (Linksschiefe) könnte man eine log-Transformation ausprobieren.

boxplot(log10(Tauchzeit))

#Der Gesamtboxplot für log10 sieht perfekt symmetrisch aus, das spräche also für eine log10-Transformation. De facto kommt es aber nicht auf den Gesamtboxplot an, sondern auf die einzelnen.

# Explorative Datenanalyse (Check auf Normalverteilung der Residuen und Varianzhomogenitaet)
boxplot(Tauchzeit~Jahreszeit*Unterart)
boxplot(log10(Tauchzeit)~Jahreszeit*Unterart)

#Hier sieht mal die Verteilung für die untransformierten Daten, mal für die transformierten besser aus. Da die Transformation keine klare Verbesserung bringt, bleiben wir im Folgenden bei den untransformierten Daten, da diese leichter (direkter) interpretiert werden können

# Vollständiges Modell mit Interaktion
aov.1 <- aov(Tauchzeit~Unterart*Jahreszeit)
aov.1
summary(aov.1)
#p-Wert der Interaktion ist 0.266

#Das volle (maximale) Modell zeigt, dass es keine signifikante Interaktion zwischen Jahreszeit und Unterart gibt. Wir können das Modell also vereinfachen, indem wir die Interaktion herausnehmen (+ statt * in der Modellspezifikation)

#Modellvereinfachung
aov.2 <- aov(Tauchzeit~Unterart+Jahreszeit)
aov.2
summary(aov.2)

#Im so vereinfachten Modell sind alle verbleibenden Terme signifikant, wir sind also beim „minimal adäquaten Modell“ angelangt

#Anderer Weg, um zu pruefen, ob man das komplexere Modell mit Interaktion behalten soll
anova(aov.1,aov.2)
#in diesem Fall bekommen wir den gleichen p-Wert wie oben (0.266)

#Modelldiagnostik
par(mfrow=c(2,2)) #alle vier Abbildungen in einem 2 x 2 Raster
plot(aov.2)
influence.measures(aov.2) # kann man sich zusätzlich zum "plot" ansehen, um herauszufinden, ob es evtl. sehr einflussreiche Werte mit Cook's D von 1 oder grösser gibt

#Links oben ist alles bestens, d. h. keine Hinweise auf Varianzheterogenität („Keil“) oder Nichtlinearität („Banane“)
#Rechts oben ganz gut, allerdings weichen Punkte 1 und 20 deutlich von der optimalen Gerade ab -> aus diesem Grund können wir es doch noch mal mit der log10-Transformation versuchen (s.u.)
#Rechts unten: kein Punkt hat einen problematischen Einfluss (die roten Linien für Cook’s D > 0.5 und > 1 sind noch nicht einmal im Bildausschnitt.

#Alternative mit log10
aov.3 <-aov(log10(Tauchzeit)~Unterart+Jahreszeit)
aov.3
summary(aov.3)
plot(aov.3)

#Rechts oben: Punkt 20 jetzt auf der Linie, aber Punkt 1 weicht umso deutlicher ab -> keine Verbesserung -> wir bleiben bei den untransformierten Daten.

#Ergebnisdarstellung

#Da wir keine Interaktion zwischen Unterart und Jahreszeit festgestellt haben, brauchen wir auch keinen Interaktionsplot (unnötig kompliziert), statt dessen können wir die Ergebnisse am besten mit zwei getrennten Plots für die beiden Faktoren darstellen. Bitte die Achsenbeschriftungen und den Tukey post-hoc-Test nicht vergessen.

par(mfrow=c(1,1)) #Zurückschalten auf Einzelplots
if(!require(multcomp)){install.packages("multcomp")} 
library(multcomp)

#letters<-cld(glht(aov.2,linfct=mcp(Unterart="Tukey")))
boxplot(Tauchzeit~Unterart,xlab="Unterart",ylab="Tauchzeit")
#mtext(letters$mcletters$Letters,at=1:2)
#genaugenommen braucht man bei nur zwei Kategorien keinen post hoc-Test

letters<-cld(glht(aov.2,linfct=mcp(Jahreszeit="Tukey")))
boxplot(Tauchzeit~Jahreszeit,xlab="Jahreszeit",ylab="Tauchzeit") #Achsenbeschriftung nicht vergessen!
mtext(letters$mcletters$Letters,at=c(1:4))

#Jetzt brauchen wir noch die Mittelwerte bzw. Effektgroessen

#Für den Ergebnistext brauchen wir auch noch Angaben zu den Effektgrössen. Hier sind zwei Möglichkeiten, um an sie zu gelangen.
aggregate(Tauchzeit~Jahreszeit, FUN=mean)
aggregate(Tauchzeit~Unterart, FUN=mean)

summary(lm(Tauchzeit~Jahreszeit))
summary(lm(Tauchzeit~Unterart))
```
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:14_Statistik2/solution_stat2.3n.Rmd-->

## Musterlösung Aufgabe 2.3S: ANOVA mit Interaktion
>**Lese-Empfehlung** Kapitel 7 von [Manny Gimond](https://mgimond.github.io/Stats-in-R/ANOVA.html)

>Download [R-Skript](14_Statistik2/RFiles/solution_stat2.3s.R)

**kommentierter Lösungsweg**
```{r, message=FALSE, echo=FALSE, results='hide', warning=FALSE}

library(tidyverse)
library(ggfortify) # zur Testung der Voraussetzungen


## ladet die nötigen Packete und die novanimal.csv Datei in R
nova <- read_delim("13_Statistik1/data/novanimal.csv", delim = ";")

## definiert mytheme für ggplot2 (verwendet dabei theme_classic())
mytheme <- 
  theme_classic() + 
  theme(
    axis.line = element_line(color = "black"), 
    axis.text = element_text(size = 20, color = "black"), 
    axis.title = element_text(size = 20, color = "black"), 
    axis.ticks = element_line(size = 1, color = "black"), 
    axis.ticks.length = unit(.5, "cm")
    )


```


```{r}
# klone den originaler Datensatz
df <- nova 

# fasst die vier Inhalte der Gerichte zu drei Inhalten zusammen
df$label_content[grep("Pflanzlich+",df$label_content)] <- "Vegetarisch" # ersetzt beide Pflanzlich und Pflanzlich+
 
# gruppiert Daten gemäss Bedingungen, Menü-Inhalt und Wochen
df_ <- df %>%
    group_by(condit, label_content, week) %>%
    summarise(tot_sold = n()) %>%
    drop_na() # lasst die unbekannten Menü-Inhalte weg


# überprüft Voraussetzungen für eine ANOVA
# Boxplots zeigt klare Varianzheterogenität
ggplot(df_, aes(x = interaction(label_content, condit), y = tot_sold)) +
  stat_boxplot(geom = "errorbar", width = .25) +
  geom_boxplot(fill="white", size = 1, width = .5) + 
  labs(x = "\nMenü-Inhalt", y = "Anzahl verkaufte Gerichte pro Woche\n") +
  mytheme

# definiert das Modell mit Interaktion
model2 <- aov(tot_sold ~ label_content * condit, data = df_)

autoplot(model2) + mytheme  # Inspektion der Modellvoraussetzungen sehen nicht schlecht aus => einzig Normalverteilung Q-Q Plot nicht optimal (vgl. Statistik 2: Folie 42)

summary(model2)

# Alternativ gibt es zwei Möglichkeiten:
#1) Transformation der Daten,
model3 <- aov(log10(tot_sold) ~ label_content * condit, data = df_)
autoplot(model3) + mytheme

#2) nicht-parametrischer Test z.B. Kruskal-Wallis-Test (vgl. Statistik 2: Folie 17-18)
inter_action <- interaction(df_$condit, df_$label_content) # zuerst Interaktionsterm definineren, da kruskal.test nicht mit Interaktionen umgehen kann
model4 <- kruskal.test(df_$tot_sold ~ inter_action) 

# in einem nächsten Schritt könnt ihr mit Post-hoc Tests diese Unterschiede genauer betrachten
# es gibt die Möglichkeit mit dunnTest (mit Package FSA)
# mehr Infos hier: https://rcompanion.org/rcompanion/d_06.html
library(FSA)
dunn <- dunnTest(tot_sold ~ inter_action, data = df_, method="bh") # zur Info: dunnTest kann nur mit Faktoren umgehen
dunn

# Infos zu Korrektur für Mehrfachvergleiche (vgl. https://mgimond.github.io/Stats-in-R/ANOVA.html#4_identifying_which_levels_are_different)



```

<span style="background-color: #FFFF00"> Fazit: Die Inspektion des Modells zeigt leichte Verletzungen beim Q-Q Plot, d.h. die Residuen sind nicht normalverteilt. Aufgrund keiner Verbesserung durch eine Transformation der Responsevariable, entscheide ich mich für eine ANOVA mit nicht tranformierten Daten.</span>.

```{r, eval=FALSE}
# Post-hoc Vergleiche
TukeyHSD(model2) # nimmt aber an, dass Residuen normalverteilt sind

# Alternativ
library(DescTools)
PostHocTest(model2, method = "scheffe") # sehr konservativ und auch für ungleiche Gruppengrössen geeignet


```



```{r, echo=F, fig.cap="Box-Whisker-Plots der wöchentlichen Verkaufszahlen pro Menü-Inhalte. Kleinbuchstaben bezeichnen homogene Gruppen auf *p* < .05 nach Tukeys post-hoc-Test."}

# zeigt die Ergebnisse anhand eines Boxplots
library(multcomp)
df_$cond_label <- interaction(df_$condit, df_$label_content) # bei Interaktionen gibt es diesen Trick, um bei den multiplen Vergleiche, die richtigen Buchstaben zu bekommen
model1 <- aov(tot_sold ~ cond_label, data = df_)
letters <-cld(glht(model1, linfct=mcp(cond_label="Tukey")))

ggplot(df_, aes(x = cond_label, y= tot_sold)) +
  stat_boxplot(geom = "errorbar", width = .25) +
  geom_boxplot(fill="white", color = "black", size = 1) + 
  labs(x = "\nMenü-Inhalt", y = "Anzahl verkaufte Gerichte pro Woche\n") +
  scale_y_continuous(breaks = seq(0, 130,25), limits = c(0, 130)) +
  annotate("text", x = 1:6, y = 130, label = letters$mcletters$Letters, size = 8) +
  mytheme 

ggsave("plot1_solution2.3s.pdf",
       height = 12,
       width = 20,
       device = cairo_pdf)

``` 



```{r,echo=F, fig.cap="Wöchentliche Verkaufszahlen aggregiert für die drei Menü-Inhalte."}
# eine weitere Möglichkeit die Ergebnisse darzustellen
m_sell <- na.omit(df_) %>% group_by(condit,label_content) %>% summarise(val = mean(tot_sold)) # berechne die durchschnittlichen Verkaufszahlen pro Bedingung

ggplot(df_, aes(x = condit, y = tot_sold, linetype = label_content, shape = label_content)) + 
    geom_point(data = m_sell, aes(y = val), size = 4) +
    geom_line(data = m_sell, aes(y = val, group = label_content), size = 2) + 
    labs(y = "Durchschnittlich verkaufte Gerichte pro Woche", x = "Bedingungen") + 
    guides(linetype = F, shape = guide_legend(title = "Menü-Inhalt"))+
    scale_y_continuous(breaks = seq(0,120,20), limits = c(0,120))+
    mytheme

ggsave("plot2_solution2.3s.pdf",
       height = 8,
       width = 9,
       device = cairo_pdf)

```

<!-- ******* -->
<!-- ##### letztes Update `r Sys.Date()`, gian-andrea.egeler@zhaw.ch -->
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:14_Statistik2/solution_stat2.3s.Rmd-->

# Statistik 3 (04.11.2019)

Statistik 3 fassen wir zu Beginn den generellen Ablauf inferenzstatistischer Analysen in einem Flussdiagramm zusammen. Dann wird die ANCOVA als eine Technik vorgestellt, die eine ANOVA mit einer linearen Regression verbindet. Danach geht es um komplexere Versionen linearer Regressionen. Hier betrachten wir polynomiale Regressionen, die z. B. einen Test auf unimodale Beziehungen erlaubt, indem man dieselbe Prädiktorvariable linear und quadriert einspeist. Multiple Regressionen versuchen dagegen, eine abhängige Variable durch zwei oder mehr verschieden Prädiktorvariablen zu erklären. Wir thematisieren verschiedene dabei auftretende Probleme und ihre Lösung, insbesondere den Umgang mit korrelierten Prädiktoren und das Aufspüren des besten unter mehreren möglichen statistischen Modellen. Hieran wird auch der informatian theoretician-Ansatz der Statistik und die multimodel inference eingeführt.
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:15_Statistik3/Abstract.Rmd-->

```{r, include=FALSE, purl=F}

knitr::opts_chunk$set(echo = TRUE,include = T, results = "hide",collapse=TRUE)
```

## Demoskript

[Demoscript als Download](15_Statistik3/RFiles/Demo_Tests.R)
**Datensatz [ipomopsis.csv](15_Statistik3/data/ipomopsis.csv)**
**Datensatz [loyn.csv](15_Statistik3/data/loyn.csv)**

**ANCOVA**
Experiment zur Fruchtproduktion (“Fruit”) von Ipomopsis sp. (“Fruit”) in Abhängigkeit Ungrazedvon der Beweidung (Grazing mit 2 Levels: Grazed, Ungrazed) und korrigiert für die Pflanzengrösse vor der Beweidung (hier ausgedrückt als Durchmesser an der Spitze des Wurzelstock: “Root”)

```{r, eval=FALSE}

compensation<-read.table("data/ipomopsis.csv", header=T,sep=",")
```
```{r, eval=FALSE}
summary(compensation)
attach(compensation)

plot(Fruit~Root)
plot(Fruit~Grazing)

tapply(Fruit,Grazing,mean)

aoc.1<-lm(Fruit~Root*Grazing)
summary.aov(aoc.1)

aoc.2<-lm(Fruit~Grazing*Root)
summary.aov(aoc.2)

aoc.3<-lm(Fruit~Grazing+Root)
summary.lm(aoc.3)

# Plotten der Ergebnisse
plot(Fruit~Root,pch=21,bg=(1+as.numeric(Grazing)))
#legend(locator(1),c("grazed","ungrazed"),col=c(2,3),pch=16) # Position von Legende von Hand setzen

#legend(4.5,110,c("grazed","ungrazed"),col=c(2,3),pch=16) # Position von Legende in Code definieren
#legend("topleft",c("grazed","ungrazed"),col=c(2,3),pch=16) # Alternative position von Legende in Code definieren


abline(-127.829,23.56,col="red")
abline(-127.892+36.103,23.56,col="green")
```


**Polynomische Regression**

```{r, eval=FALSE}
e<-c(20,19,25,10,8,15,13,18,11,14,25,39,38,28,24)
f<-c(12,15,10,7,2,10,12,11,13,10,9,2,4,7,13)

summary(lm(f~e))

par(mfrow=c(1,1))
plot(f~e,xlim=c(0,40),ylim=c(0,30))
abline(lm(f~e))

par(mfrow=c(2,2))
plot(lm(f~e))
plot(lm(f~e+I(e^2)))

summary(lm(f~e+I(e^2)))
```

**Multiple lineare Regression basierend auf Logan, Beispiel 9A** 

```{r, eval=FALSE}
loyn <- read.table("data/loyn.csv", header=T,sep=",")
loyn
library(car)

summary(loyn)

lm.1 <- lm (ABUND ~ YR.ISOL + ALT + GRAZE, data=loyn)
summary(lm.1)

aov(lm.1)
par(mfrow=c(2,2))
plot(lm.1)
influence.measures(lm.1)

cor <- cor(loyn[,2:7])
print(cor, digits=2)

cor[abs(cor)<0.6] <- 0
cor
print(cor, digits=3)

vif(lm.1)
```

**Simulation Overfitting**

```{r, eval=FALSE}
test <- data.frame("x"=c(1,2,3,4,5,6),"y"=c(34,21,70,47,23,45))
attach(test)

plot(x,y)
lm0 <- lm(y~1)
lm1 <- lm(y~x)
lm2 <- lm(y~x+I(x^2))
lm3 <- lm(y~x+I(x^2)+I(x^3))
lm4 <- lm(y~x+I(x^2)+I(x^3)+I(x^4))
lm5 <- lm(y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5))
lm6 <- lm(y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)+I(x^6))
summary(lm0)
summary(lm1)
summary(lm2)
summary(lm3)
summary(lm4)
summary(lm5)

xv<-seq(from=0,to=10,by=0.1)

plot(x,y,cex=2,col="black",lwd=3)
yv<-predict(lm1,list(x=xv))
lines(xv,yv,col="red",lwd=3)
yv<-predict(lm2,list(x=xv))
lines(xv,yv,col="blue",lwd=3)
yv<-predict(lm3,list(x=xv))
lines(xv,yv,col="green",lwd=3)
yv<-predict(lm4,list(x=xv))
lines(xv,yv,col="orange",lwd=3)
yv<-predict(lm5,list(x=xv))
lines(xv,yv,col="black",lwd=3)
```

**Modellvereinfachung (mit Loyn-Datensatz)**

```{r, eval=FALSE}
lm.1 <- lm(ABUND ~ YR.ISOL + ALT + GRAZE, data=loyn)
summary(lm.1)
lm.2 <- update(lm.1,~.-YR.ISOL)

summary(lm.2)

anova(lm.1,lm.2)
```

**Hierarchical partitioning**
```{r, eval=FALSE}
if(!require(hier.part)){install.packages("hier.part")}
library(hier.part)

loyn.preds <-with(loyn, data.frame(YR.ISOL,ALT,GRAZE))
par(mfrow=c(1,1))
hier.part(loyn$ABUND,loyn.preds,gof="Rsqu")
```

**Partial regressions**


```{r, eval=FALSE}
avPlots(lm.1,ask=F)
```

**Multimodel inference**

```{r, eval=FALSE}
if(!require(MuMIn)){install.packages("MuMIn")}
library(MuMIn)

global.model <- lm (ABUND ~ YR.ISOL + ALT + GRAZE, data=loyn)
options(na.action="na.fail")
allmodels <- dredge(global.model)
allmodels
importance(allmodels)

avgmodel<-model.avg(get.models(dredge(global.model,rank="AICc"),subset=TRUE))
summary(avgmodel)
```
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:15_Statistik3/Demoskript.Rmd-->

## Übung 3

### Aufgabe 3.1: Multiple Regression

- **Datensatz [Ukraine_bearbeitet.xlsx](15_Statistik3/data/Ukraine_bearbeitet.xlsx)**
- **Datensatz [Ukraine_bearbeitet.csv](15_Statistik3/data/Ukraine_bearbeitet.csv)**

Artenzahlen von Vegetationsaufnahmen in der Ukraine vs. diverse Umweltparameter (farbig gruppiert nach Kategorien, aus Kuzemko et al. 2016) 

- Bestimmt ein minimal adäquates Modell für die Erklärung der Artenzahlen mit allen notwendigen Arbeitsschritten
- Wahlweise könnt ihr mit AICc und dredge oder mit p-Werten und schrittweiser Vereinfachung eines globalen Models arbeiten


```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:15_Statistik3/Uebungen.Rmd-->

---
title: "MSc. Research Methods - Statistikteil Lösungen 2019"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r}
knitr::opts_chunk$set(results = "hide", fig.width = 20, fig.height = 12, warning = F, message = F, fig.pos = 'H')
```

## Musterlösung Aufgabe 3.1: Multiple Regression
[R-Skript als Download](15_Statistik3/RFiles/Loesung_Uebung_3.1_v.02.R)


**Übungsaufgabe (hier so ausführlich formuliert, wie dies auch in der Klausur der Fall sein
wird)**

- Bereiten Sie den Datensatz Ukraine.xlsx für das Einlesen in R vor und lesen Sie ihn
dann ein. Dieser enthält gemittelte Pflanzenartenzahlen (Species.richness) von 199
10 m² grossen Plots (Vegetationsaufnahmen) von Steppenrasen in der Ukraine sowie
zahlreiche Umweltvariablen, deren Bedeutung und Einheiten im Kopf der ExcelTabelle angegeben sind.
- **Ermitteln Sie ein minimal adäquates Modell, das den Artenreichtum in den Plots
durch die Umweltvariablen erklärt.**
- Bitte erklären und begründen Sie die einzelnen Schritte, die Sie unternehmen, um zu
diesem Ergebnis zu kommen. Dazu erstellen Sie bitte ein Word-Dokument, in das Sie
Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, Ihre
Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere
Vorgehen dokumentieren.
- Dieser Ablauf sollte insbesondere beinhalten:
    - Überprüfen der Datenstruktur nach dem Einlesen: welches sind die abhängige(n)
      und welches die unabängige(n) Variablen, sind alle Variablen für die Analyse
      geeignet?
    - Explorative Datenanalyse, um zu sehen, ob die abhängige Variable in der
      vorliegenden Form für die Analyse geeignet ist
    - Definition eines globalen Modelles und dessen Reduktion zu einem minimal
      adäquaten Modell
    - Durchführen der Modelldiagnostik für dieses
    - Generieren aller Zahlen, Statistiken und Tabellen, die für eine wiss.
      Ergebnisdarstellung benötigt werden
    - Formulieren Sie abschliessend einen Methoden- und Ergebnisteil (ggf. incl.
      adäquaten Abbildungen) zu dieser Untersuchung in der Form einer
      wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je
      einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und
      Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige
      Redundanz dagegen vermieden werden.
    - **Abzugeben sind am Ende (a) Ein lauffähiges R-Skript; (b) begründeter Lösungsweg
      (Kombination aus R-Code, R Output und dessen Interpretation) und (c)
      ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit).**
      
      
**Loesung Übung 3.1: Multiple Regression**

Schon vor dem Einlesen kürzt man am besten bereits in Excel die Variablennamen so ab, dass sie noch eindeutig, aber nicht unnötig lang sind, etwa indem man die Einheiten wegstreicht

```{r}
# Aus der Excel-Tabelle wurde das relevante Arbeitsblatt als csv gespeichert
ukraine <-read.delim("15_Statistik3/data/Ukraine_bearbeitet.csv",sep=";")
attach(ukraine)

ukraine
str(ukraine)
summary(ukraine)
```
Man erkennt, dass alle Spalten bis auf die erste mit der Plot ID numerisch (num oder int) und
dass die abhängige Variable in Spalte 2 sowie die Prediktorvariablen in den Spalten 3 bis 23
stehen.

```{r}
#Explorative Datenanalyse der abhängigen Variablen
boxplot(Species.richness)
```

Der Boxplot sieht sehr gut symmetrisch aus. Insofern gibt es keinen Anlass über eine
Transformation nachzudenken. (Da es sich bei Artenzahlen um Zähldaten handelt, müsste man
theoretisch ein glm mit Poisson-Verteilung rechnen; bei einem Mittelwert, der hinreichend von
Null verschieden ist (hier: ca. 40), ist eine Poisson-Verteilung aber praktisch nicht von einer
Normalverteilung zu unterscheiden und wir können uns den Aufwand auch sparen).

```{r}
cor <- cor(ukraine[,3:23])
cor
cor[abs(cor)<0.7] <- 0
cor
```
Die Korrelationsanalyse dient dazu, zu entscheiden, ob die Prädiktorvariablen hinreichend
voneinander unabhängig sind, um alle in das globale Modell hinein zu nehmen. Bei Pearson’s
Korrelationskoeffizienten r, die betragsmässig grösser als 0.7 sind, würde es problematisch.
Alternativ hätten wir auch den VIF (Variance Inflation Factor) als Kriterium für den möglichen
Ausschluss von Variablen aus dem globalen Modell nehmen können.
Diese initiale Korrelationsanalyse zeigt uns aber, dass unsere Daten noch ein anderes Problem
haben: für die drei Korngrössenfraktionen des Bodens (Sand, Silt, Clay) stehen lauter NA’s. Um
herauszufinden, was das Problem ist, geben wir ein:

```{r}
summary(ukraine$Sand)
ukraine[!complete.cases(ukraine), ] # Zeigt zeilen mit NAs ein
```
Da gibt es offensichtlich je ein NA in jeder dieser Zeilen. Jetzt können wir entscheiden,
entweder auf die drei Variablen oder auf die eine Beobachtung zu verzichten. Da wir eh schon
eher mehr unabhängige Variablen haben als wir händeln können, entscheide ich pragmatisch
für ersteres. Wir rechnen die Korrelation also noch einmal ohne diese drei Spalten (es sind die
Nummern 12:14, wie wir aus der anfänglichen Variablenbetrachtung oben wissen).

```{r}
cor <- cor(ukraine[,c(3:11,15:23)])
cor[abs(cor)<0.7] <- 0
cor
```
Wenn man auf cor nun doppel-clickt und es in einem separaten Fenster öffnet, sieht man, wo
es problematische Korrelationen zwischen Variablenpaaren gibt.
Es sind dies Altitude vs. Temperature und N.total vs. C.org. Wir müssen aus jedem dieser Paare
jetzt eine Variable rauswerfen, am besten jene, die weniger gut interpretierbar ist. Ich
entscheide mich dafür Temperature statt Altitude (weil das der direktere ökologische
Wirkfaktor ist) und C.org statt N.total zu behalten (weil es in der Literatur mehr Daten zum
Humusgehalt als zum N-Gehalt gibt, damit eine bessere Vergleichbarkeit erzielt wird). Die
Aussagen, die wir für die beibehaltene Variable erzielen, stehen aber +/- auch für die
entfernte.
Das Problem ist aber, dass wir immer noch 16 Variablen haben, was einen sehr
leistungsfähigen Rechner oder sehr lange Rechenzeit erfordern würde. Wir sollten also unter
15 Variablen kommen. Wir könnten uns jetzt überlegen, welche uns ökologisch am wichtigsten
sind, oder ein noch strengeres Kriterium bei r verwenden, etwa 0.6

```{r}
cor <- cor(ukraine[,c(3:11,15:23)])
cor[abs(cor)<0.6] <- 0
cor
```
Entsprechend „werfen“ wir auch noch die folgenden Variablen „raus“: Temperature.range
(positiv mit Temperature), Precipitation (negativ mit Temperature) sowie Conductivity (positiv
mit pH).

Nun können wir das globale Modell definieren, indem wir alle verbleibenden Variablen
aufnehmen, das sind 13. (Wenn das nicht eh schon so viele wären, dass es uns an die Grenze
der Rechenleistung bringt, hätten wir auch noch darüber nachdenken können, einzelne
quadratische Terme oder Interaktionsterme zu berücksichtigen).

```{r}
global.model <- lm (Species.richness ~
Inclination+Heat.index+Microrelief+Grazing.intensity+Litter+
Stones.and.rocks+Gravel+Fine.soil+pH+CaCO3+C.org+CN.ratio+Temperature)
```

Nun gibt es im Prinzip zwei Möglichkeiten, vom globalen (vollen) Modell zu einem minimal
adäquaten Modell zu kommen. (1) Der Ansatz der „frequentist statistic“, in dem man aus dem
vollen Modell so lange schrittweise Variablen entfernt, bis nur noch signifikante Variablen
verbleiben. (2) Den informationstheoretischen Ansatz, bei dem alle denkbaren Modelle
berechnet und verglichen werden (also alle möglichen Kombinationen von 13,12,…, 1, 0
Parametern). Diese Lösung stelle ich im Folgenden vor:

```{r}
#Multimodel inference
if(!require(MuMIn)){install.packages("MuMIn")}
library(MuMIn)
options(na.action="na.fail")
allmodels<-dredge(global.model)
allmodels
```

Jetzt bekommen wir die besten der insgesamt 8192 möglichen Modelle gelistet mit ihren
Parameterschätzungen und ihrem AICc.

Das beste Modell umfasst 5 Parameter (CaCO3, CN.ratio, Grazing.intensity. Heat.index, Litter).
Allerdings ist das nächstbeste Modell (mit 6 Parametern) nur wenig schlechter (delta AICc =
0.71), was sich in fast gleichen (und zudem sehr niedrigen) Akaike weights bemerkbar macht.
Nach dem Verständnis des Information theoretician approach, sollte man in einer solchen
Situation nicht das eine „beste“ Modell benennen, sondern eine Aussage über die Gruppe der
insgesamt brauchbaren Modelle treffen. Hierzu kann man (a) Importance der Parameter über
alle Modelle hinweg berechnen (= Summe der Akaike weights aller Modelle, die den
betreffenden Parameter enthalten) und/oder (b) ein nach Akaike weights gemitteltes Modell
berechnen.

```{r}
#Importance values der Variablen
importance(allmodels)
```

Demnach ist Heat.index die wichtigste Variable (in 100% aller relevanten Modelle), während
ferner Litter, CaCO3, CN.ratio und Grazing.intensity in mehr als 50% der relevanten Modelle
enthalten sind.

```{r}
#Modelaveraging (Achtung: dauert mit 13 Variablen einige Minuten)
summary(model.avg(get.models(dredge(global.model,rank="AICc"),subset
=TRUE)))
```

Aus dem gemittelten Modell können wir die Richtung der Beziehung (positiv oder negativ) und
ggf. die Effektgrössen (wie verändert sich die Artenzahl, wenn die Prädiktorvariable um eine
Einheit zunimmt?) ermitteln.

```{r}
#Modelldiagnostik nicht vergessen
par(mfrow=c(2,2))
plot(global.model)
plot(lm(Species.richness~Heat.index+Litter+CaCO3+CN.ratio+Grazing.intensity))
```

Wie immer kommt am Ende die Modelldiagnostik. Wir können uns entweder das globale
Modell oder das Modell mit den 5 Variablen mit importance > 50% anschauen. Das Bild sieht
fast identisch aus und zeigt keinerlei problematische Abweichungen, d. h. links oben weder ein
Keil, noch eine Banane, rechts oben eine nahezu perfekte Gerade. 

**Darstellung der Vorgehensweise und Ergebnisse (in einer wiss. Arbeit)
Methoden
Es wurden Pflanzenartenzahlen von Steppenrasen in der Ukraine auf 199 10 m² grossen
Probeflächen erhoben und zu diesen 23 Umweltvariablen erhoben. Da jeweils ein Messwert
fehlte, wurden die Bodenvariablen Sand-, Schluff- und Tongehalt von der weiteren
statistischen Analyse ausgeschlossen. Mittels Pearson’s Korrelationskoeffizient wurde auf
Abhängigkeiten zwischen den Prädiktorvariablen getestet und aus Paaren mit einer
Beziehung mit |r| > 0.6 nur jeweils eine Variable beibehalten. Entsprechend wurden die
hoch mit Jahresmitteltemperatur korrelierten Werte Meereshöhe (negativ),
Temperaturamplitude (positiv) und Niederschlag (negativ) ausgeschlossen, ferner
Leitfähigkeit (positiv mit pH) und Stickstoffgehalt (positiv mit organischem
Kohlenstoffgehalt). Damit umfasste das globale lineare Modell (Funktion lm in R) die in Tab.
1 aufgeführten Variablen. Quadratische Terme oder Interaktionen zwischen Variablen
wurden nicht berücksichtigt. Auf ein glm mit Poisson-Regression wurde verzichtet, da eine
visuelle Inspektion eines Boxplots der Artenzahlen keine relevanten Abweichungen von
einer Normalverteilung ergab.
Die Modellauswahl fand mittels Multimodel Inference (dredge-Funktion im MuMIn package
in R) statt. Die Modellgüte wurde mittels AICc beurteilt. Zur Beurteilung der Bedeutung von
Variablen wurden Importance Values (Summe der Akaike weights in allen Modellen, die die
betreffende Variable beinhalten) ausgerechnet. Die Richtung der Beziehung wurde aus der
Parameterschätzung im globalen Modell bestimmt. Schliesslich wurde ein gemitteltes
Modell (aller möglichen Modelle, gewichtet nach deren Akaike weights) erstellt, um die
Effektgrössen zu bestimmen. Die Adäquanz des gewählten Modells wurde in Residualplots
visuell inspiziert und ergab keine nennenswerten Verletzungen der Voraussetzungen eines
parametrischen Modells.
Ergebnisse
Das beste Modell nach AICc beinhaltete die Variablen CaCO3, CN-Verhältnis,
Beweidungsintensität, Heat load-Index und Streuauflage. Da sich die nächstbesten Modelle
aber um weniger als DeltaAICc = 2 unterscheiden, wurden für die Gesamtbeurteilung die
Importance values sowie die Koeffizienten des über alle 8192 betrachteten Modelle nach
Akaike weights gemittelten average models herangezogen (Tab. 1). Mit einem Importance
value von nahezu 100% war der Heat load-Index die wichtigste Einflussgrösse (negativ). Vier
weitere Umweltvariablen waren ebenfalls in mehr als 50% der statistisch relevanten
Modelle enthalten (in dieser Reihenfolge):**

**Tab. 1. Ergebnisse der Multimodel Inference. Die Variablen sind nach absteigenden
Importance values sortiert. Die Parameterschätzung bezieht sich nur auf die Modelle, welche
die entsprechende Variable beinhalten und ist nach Akaike weights gewichtet. Unter
„Hochkorrelierte Variablen“ sind jene aufgeführt, die wegen ihres engen Zusammenhangs
mit der genannten Variablen nicht ins globale Modell aufgenommen wurden.**

![](15_Statistik3/data/tab1_uebungStatistik3.png)

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:15_Statistik3/solution_stat3.1.Rmd-->

# Statistik 4 (05.11.2019)

Heute geht es hauptsächlich um generalized linear models (GLMs), die einige wesentliche Limitierungen von linearen Modellen überwinden. Indem sie Fehler- und Varianzstrukturen explizit modellieren, ist man nicht mehr an Normalverteilung der Residuen und Varianzhomogenität gebunden. Bei generalized linear regressions muss man sich zwischen verschiedenen Verteilungen und link-Strukturen entscheiden. Spezifisch werden wir uns die Poisson-Regressionen für Zähldaten und die logistische Regression für ja/nein-Daten anschauen. Danach folgt ein Einstieg in nicht-lineare Regressionen, die es erlauben, etwa Potenzgesetze oder Sättigungsfunktionen direkt zu modellieren. Zum Abschluss gibt es einen Ausblick auf Glättungsverfahren (LOWESS) und general additive models (GAMs).
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:16_Statistik4/Abstract.Rmd-->


## Statistik 4 - Demoskript

**(c) Juergen Dengler, 05.11.2019**

[Demoscript als Download](16_Statistik4/RFiles/Demo_Tests.R)


__von LMs zu GLMs__

```{r}
temp<-c(10,12,16,20,24,25,30,33,37)
besucher<-c(40,12,50,500,400,900,1500,900,2000)
strand<-data.frame("Temperatur"=temp,"Besucher"=besucher)
attach(strand)

par(mfrow=c(1,1))
plot(besucher~temp)

lm.strand<-lm(Besucher~Temperatur, data=strand)
summary(lm.strand)

par(mfrow=c(2,2))
plot(lm.strand)

par(mfrow=c(1,1))
xv<-rep(0:40,by=.1)
yv<-predict(lm.strand,list(Temperatur=xv))
plot(Temperatur,Besucher,xlim=c(0,40))
lines(xv,yv,lwd=3,col="blue")

glm.gaussian<-glm(Besucher~Temperatur,family=gaussian)
glm.poisson<-glm(Besucher~Temperatur,family=poisson)

summary(glm.gaussian)
summary(glm.poisson)

#Rücktranformation der Werte auf die orginale Skale (Hier Exponentialfunktion da family=possion als Link-Funktion den natürlichen Logarithmus (log) verwendet)
#Besucher = exp(3.50 + 0.11 Temperatur/°C)
exp(3.500301) #Anzahl besucher bei 0°C
exp(3.500301 + 30*0.112817) #Anzahl besucher bei 30°C

# Test für Overdispersion  
library(AER)
dispersiontest(glm.poisson)

glm.quasi<-glm(Besucher~Temperatur,family=quasipoisson)
summary(glm.quasi)

par(mfrow=c(2,2))
plot(glm.gaussian)
plot(glm.poisson)
plot(glm.quasi)

par(mfrow=c(1,1))
plot(Temperatur,Besucher,xlim=c(0,40))
xv<-rep(0:40,by=.1)

yv<-predict(lm.strand,list(Temperatur=xv))
lines(xv,yv,lwd=3,col="blue")

yv2<-predict(glm.poisson,list(Temperatur=xv))
lines(xv,exp(yv2),lwd=3,col="red")

yv3<-predict(glm.quasi,list(Temperatur=xv))
lines(xv,exp(yv3),lwd=3,col="green")
```


 __Logistische Regression__

```{r}
bathing<-data.frame("temperature"=c(1,2,5,9,14,14,15,19,22,24,25,26,27,28,29),
                    "bathing"=c(0,0,0,0,0,1,0,0,1,0,1,1,1,1,1))
plot(bathing~temperature, data=bathing)

glm.1<-glm(bathing~temperature, data=bathing, family="binomial")
summary(glm.1)

#Modeldiagnostik (wenn nicht signifikant, dann OK)
1 - pchisq (glm.1$deviance,glm.1$df.resid)

#Modellgüte (pseudo-R²)
1 - (glm.1$dev / glm.1$null)

#Steilheit der Beziehung (relative Änderung der odds bei x + 1 vs. x)
exp(glm.1$coefficients[2])

#LD50 (also hier: Temperatur, bei der 50% der Touristen baden)
-glm.1$coefficients[1]/glm.1$coefficients[2]

#Vorhersagen
predicted <- predict(glm.1, type="response")

#Konfusionsmatrix
km <- table(bathing$bathing, predicted > 0.5)
km

#Missklassifizierungsrate
1-sum(diag(km)/sum(km))



#Plotting
xs<-seq(0,30,l=1000)
model.predict<-predict(glm.1,type="response",se=T,newdata=data.frame(temperature=xs))
plot(bathing~temperature,data=bathing,xlab="Temperature (°C)",ylab="% Bathing",pch=16, col="red")
points(model.predict$fit ~ xs,type="l")
lines(model.predict$fit+model.predict$se.fit ~ xs, type="l",lty=2)
lines(model.predict$fit-model.predict$se.fit ~ xs, type="l",lty=2)
```

 __Nicht-lineare Regression __

```{r}
if(!require(AICcmodavg)){install.packages("AICcmodavg")}
if(!require(nlstools)){install.packages("nlstools")}
library(AICcmodavg)
library(nlstools)


loyn <- read.delim("16_Statistik4/data/loyn.csv", sep=",")
attach(loyn)

#Selbstdefinierte Funktion, hier Potenzfunktion
power.model<-nls(ABUND~c*AREA^z,start=(list(c=1,z=0)))
summary(power.model)
AICc(power.model)

#Modeldiagnostik (in nlstools)
plot(nlsResiduals(power.model))

#Vordefinierte "Selbststartfunktionen"#
?selfStart
logistic.model<-nls(ABUND~SSlogis(AREA,Asym,xmid,scal))
summary(logistic.model)
AICc(logistic.model)

#Modeldiagnostik (in nlstools)
plot(nlsResiduals(logistic.model))

#Visualisierung
plot(ABUND~AREA)
par(mfrow=c(1,1))
xv<-seq(0,2000,0.01)

# 1. Potenzfunktion
yv1 <-predict(power.model,list(AREA=xv))
lines(xv,yv1,col="green")

# 2. Logistische Funktion
yv2 <-predict(logistic.model,list(AREA=xv))
lines(xv,yv2,col="blue")

#Visualisierung II
plot(ABUND~log10(AREA))
par(mfrow=c(1,1))

# 1. Potenzfunktion
yv1 <-predict(power.model,list(AREA=xv))
lines(log10(xv),yv1,col="green")

# 2. Logistische Funktion
yv2 <-predict(logistic.model,list(AREA=xv))
lines(log10(xv),yv2,col="blue")

#Model selection among several non-linear models

cand.models<-list()
cand.models[[1]]<-power.model
cand.models[[2]]<-logistic.model

Modnames <- c("Power", "Logistic")

aictab(cand.set = cand.models, modnames = Modnames)
```

 __Smoother__

```{r}
attach(loyn)
log_AREA<-log10(AREA)       
plot(ABUND~log_AREA)
lines(lowess(log_AREA,ABUND,f=0.25),lwd=2,col="red")
lines(lowess(log_AREA,ABUND,f=0.5),lwd=2,col="blue")
lines(lowess(log_AREA,ABUND,f=1),lwd=2,col="green")
```

 __GAMs__

```{r}
if(!require(mgcv)){install.packages("mgcv")}
library(mgcv)

gam.1<-gam(ABUND~s(log_AREA))
gam.1
summary(gam.1)

plot(log_AREA,ABUND,pch=16)
xv<-seq(-1,4,by=0.1)
yv<-predict(gam.1,list(log_AREA=xv))
lines(xv,yv,lwd=2,col="red")

AICc(gam.1)
summary(gam.1)
```
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:16_Statistik4/Demoskript.Rmd-->

## Statistik 4: Übungen

### Übung 4.1: Nicht-lineare Regression (naturwissenschaftlich)

**Datensatz [Curonian_Spit.xlsx](16_Statistik4/data/Curonian_Spit.xlsx)**

Dieser enthält gemittelte Pflanzenartenzahlen (Species.richness) von
geschachtelten Plots (Vegetationsaufnahmen) der Pflanzengesellschaft LolioCynosuretum im Nationalpark Kurische Nehrung (Russland) auf
Flächengrössen (Area) von 0.0001 bis 900 m².

**Ermittelt den funktionellen Zusammenhang (das beste Modell), der die Zunahme der Artenzahl mit der Flächengrösse am besten beschreibt.Berücksichtigt dabei mindestens die Potenzfunktion (power function, die logarithmische Funktion (logarithmic function,und eine Funktion mit Sättigung (saturation, asymptote) eurer Wahl.**


### Übung 4.2N: Multiple logistische Regression (naturwissenschaftlich)

**Datensatz [isolation.csv](16_Statistik4/data/isolation.csv)**

Dieser enthält für 50 Inseln die Information, ob eine bestimmte Vogelart dort vorkommt (incidence = 1) oder nicht vorkommt (incidence = 0). Für jede der Inseln sind zudem zwei Umweltvariablen angegeben: area (Fläche in km²) und
isolation (Entfernung vom Festland in km).

**Ermittelt das minimal adäquate statistische Modell, das die Vorkommenswahrscheinlichkeit der Vogelart in Abhängigkeit von Flächengrösse und Entfernung beschreibt.**


### Übung 4.2S: Multiple logistische Regression (sozioökonomisch)

Führt mit dem Datensatz [novanimal.csv](13_Statistik1/data/novanimal.csv) eine logistische Regression durch.Kann der Fleischkonsum durch das Geschlecht, die Hochschulzugehörigkeit und das Alter erklärt werden?

**Hinweise:**

- Generiert eine neue Variable "Fleisch" (0 = kein Fleisch, 1 = Fleisch)
- Entfernt fehlende Werte aus der Variable "Fleisch"
- Lasst für die Analyse den Menü-Inhalt «Buffet» weg
- Definiert das Modell und wendet es auf den Datensatz an
- Berechnet eine Vorhersage des Modells mit predict()
- Eruiert den Modellfit und die Modellgenauigkeit
- Für Motivierte: Berechnet eine Konfusionsmatrix und zieht euer Fazit daraus (vgl. )
- Stellt eure Ergebnisse dann angemessen dar (Text und/oder Tabelle).
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:16_Statistik4/assigment_stat4.Rmd-->

---
title: "MSc. Research Methods - Statistikteil Lösungen 2019"
author: "Juergen Dengler"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r}
knitr::opts_chunk$set(fig.width = 20, fig.height = 12, warning = F, message = F, fig.pos = 'H',results = "show")
```

## Musterlösung Aufgabe 4.1: Nicht-lineare Regression

[R-Skript als Download](16_Statistik4/RFiles/Loesung_Uebung_4.1_v.02.R)

**Übungsaufgabe (hier so ausführlich formuliert, wie dies auch in der Klausur der Fall sein wird)**

- Laden Sie den Datensatz Curonia_spit.xlsx. Dieser enthält gemittelte            
  Pflanzenartenzahlen (Species.richness) von geschachtelten Plots 
  (Vegetationsaufnahmen) der Pflanzengesellschaft Lolio-Cynosuretum im
  Nationalpark Kurische Nehrung (Russland) auf Flächengrössen (Area) von 0.0001 bis
  900 m².
- **Ermitteln Sie den funktionellen Zusammenhang, der die Zunahme der Artenzahl mit der     Flächengrösse am besten beschreibt. Berücksichtigen Sie dabei mindestens die            Potenzfunktion (power function), die logarithmische Funktion (logarithmic function)     und eine Funktion mit Sättigung (saturation, asymptote) Ihrer Wahl**
- Bitte erklären und begründen Sie die einzelnen Schritte, die Sie unternehmen, um zu
  diesem Ergebnis zu kommen. Dazu erstellen Sie bitte ein Word-Dokument, in das Sie
  Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, Ihre
  Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere
  Vorgehen dokumentieren.
- Dieser Ablauf sollte insbesondere beinhalten:
    - Überprüfen der Datenstruktur nach dem Einlesen, welches sind die abhängige(n)
      und welches die unabängige(n) Variablen
    - Explorative Datenanalyse, um zu sehen, ob eine nicht-lineare Regression überhaupt
      nötig ist und ob evtl. Dateneingabefehler vorliegen vorgenommen werden sollten
    - Definition von mindestens drei nicht-linearen Regressionsmodellen
    - Selektion des/der besten Models/Modelle
    - Durchführen der Modelldiagnostik für die Modelle in der engeren Auswahl, um zu
      entscheiden, ob das gewählte Vorgehen korrekt war oder ggf. angepasst werden
      muss
    - Generieren aller Zahlen, Statistiken und Tabellen, die für eine wiss.
      Ergebnisdarstellung benötigt werden
- Formulieren Sie abschliessend einen Methoden- und Ergebnisteil (ggf. incl.
  adäquaten Abbildungen) zu dieser Untersuchung in der Form einer
  wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je
  einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und
  Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige
  Redundanz dagegen vermieden werden.
- **Abzugeben sind am Ende (a) Ein lauffähiges R-Skript; (b) begründeter Lösungsweg       (Kombination aus R-Code, R Output und dessen Interpretation) und (c)                  ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit).**


### Übung 4.1 - Nicht-lineare Regression -- Lösung

Aus der Excel-Tabelle wurde das relevante Arbeitsblatt als csv gespeichert

```{r}
SAR<-read.delim("16_Statistik4/data/Curonian_Spit.csv",sep=";")
str(SAR)
summary(SAR)
attach(SAR)
#Explorative Datenanalyse
plot(Species.richness~Area)
```

Es liegt in der Tat ein nicht-linearer Zusammenhang vor, der sich gut mit nls analysieren lässt.
Die Daten beinhalten keine erkennbaren Fehler, da der Artenreichtum der geschachtelten
Plots mit der Fläche ansteigt.

```{r}
#Potenzfunktion selbst definiert
if(!require(nlstools)){install.packages("nlstools")}
library(nlstools)
#power.model<-nls(Species.richness~c*Area^z)
#summary(power.model)
```
Falls die Funktion so keine Ergebnisse liefert, oder das Ergebnis unsinnig aussieht, wenn man
es später plottet, müsste man hier geeignete Startwerte angeben, die man aus der
Betrachtung der Daten oder aus Erfahrungen mit der Funktion für ähnliche Datensets gewinnt,etwa so:

```{r}
power.model<-nls(Species.richness~c*Area^z, start=(list(c=1,z=0.2)))
summary(power.model)
```
Das Ergebnis ist identisch

```{r}
#logarithmische Funktion selbst definiert
logarithmic.model<-nls(Species.richness~b0+b1*log10(Area))
summary(logarithmic.model)

# Zu den verschiedenen Funktionen mit Sättigungswert (Asymptote) gehören  Michaelis-Menten, das aymptotische Modell durch den Ursprung und die logistische
# Funktion. Die meisten gibt es in R
# als selbststartende Funktionen, was meist besser funktioniert als
# wenn man sich selbst Gedanken
# über Startwerte usw. machen muss. Man kann sie aber auch selbst definieren
```
_Im Folgenden habe ich ein paar unterschiedliche Sättigungsfunktionen mit verschiedenen Einstellungen durchprobiert, um zu zeigen, was alles passieren kann…_

```{r}
micmen.1<-nls(Species.richness~SSmicmen(Area, Vm, K))
summary(micmen.1)

#Dasselbe selbst definiert (mit default-Startwerten)
micmen.2<-nls(Species.richness~Vm*Area/(K+Area))
summary(micmen.2)
```

Hier ist das Ergebnis deutlich verschieden, ein Phänomen, das einem bei nicht-linearen
Regressionen anders als bei linearen Regressionen immer wieder begegnen kann, da der
Iterationsalgorithmus in lokalen Optima hängen bleiben kann. Oftmals dürfte die eingebaute
Selbststartfunktion bessere Ergebnisse liefern, aber das werden wir unten sehen.

```{r}
#Dasselbe selbst definiert (mit sinnvollen Startwerten, basierend auf dem Plot)
micmen.model3<-
nls(Species.richness~Vm*Area/(K+Area),start=list(Vm=100,K=1))
summary(micmen.model3)
```

Wenn man sinnvollere Startwerte als die default-Werte (1 für alle Parameter) eingibt, hier etwas einen mutmasslichen Asymptoten-Wert (aus der Grafik) von Vm = ca. 100, dann bekommt man das gleiche Ergebnis wie bei der Selbsstartfunktion

```{r}
#Eine asymptotische Funktion durch den Ursprung (mit implementierter Selbststartfunktion)
asym.model<-nls(Species.richness~SSasympOrig(Area, Asym, lrc))
summary(asym.model)

#Logistische Regression als Selbststart-Funktion
#logistic.model<-nls(Species.richness~SSlogis(Area,asym,xmid,scal))
```
<font color="red">**Error in nls(y ~ 1/(1 + exp((xmid - x)/scal)), data = xy, start = list(xmid= aux[1L], : Iterationenzahl überschritt Maximum 50**</font>

Das ist etwas, was einem bei nls immer wieder passieren kann. Die Iteration ist nach der eingestellten max. Iterationszahl noch nicht zu einem Ergebnis konvergiert. Um ein Ergebnis für diese Funktion zu bekommen, müsste man mit den Einstellungen von nls „herumspielen“, etwas bei den Startwerten oder den max. Um das effizient zu machen, braucht man aber etwas Erfahrung Interationszahlen (man kann z. B. manuell die Maximalzahl der Iterationen erhöhen, indem man in den Funktionsaufruf etwa maxiter =100 als zusätzliches Argument reinschreibtn). Da wir aber schon mehrere funktionierende Funktionen mit oberem Grenzwert haben –und damit die Aufgabe erfüllt – lassen wir es hier.

```{r}
#Vergleich der Modellgüte mittels AICc
library(AICcmodavg)
cand.models<-list()
cand.models[[1]]<-power.model
cand.models[[2]]<-logarithmic.model
cand.models[[3]]<-micmen.1
cand.models[[4]]<-micmen.2
cand.models[[5]]<-asym.model

Modnames<-c("Power","Logarithmic","Michaelis-Menten (SS)","Michaelis-Menten","Asymptotic through origin")
aictab(cand.set=cand.models,modnames=Modnames)
```

Diese Ergebnistabelle vergleicht die Modellgüte zwischen den fünf Modellen, die wir in unsere Auswahl reingesteckt haben. Alle haben drei geschätzte Parameter (K), also zwei Funktionsparameter und die Varianz. Das beste Modell (niedrigster AICc bzw. Delta = 0) hat das Potenzmodell (power). Das zweitbeste Modell (logarithmic) hat bereits einen Delta-AICc von mehr als 4, ist daher statistisch nicht relevant. Das zeigt sich auch am Akaike weight, das für das zweite Modell nur noch 2 % ist. Die verschiedenen Modelle mit oberem Grenzwert (3-5) sind komplett ungeeignet.

```{r}
#Modelldiagnostik für das beste Modell
library(nlstools)
plot(nlsResiduals(power.model))
```

Links oben sieht man zwar ein Muster (liegt daran, dass in diesem Fall die Plots geschachtelt, und nicht unabhängig waren), aber jedenfalls keinen problematischen Fall wie einen Bogen oder einen Keil. Der QQ-Plot rechts unten ist völlig OK. Somit haben wir auch keine problematische Abweichung von der Normalverteilung der Residuen. Da es sich bei den einzelnen Punkten allerdings bereits um arithmetische Mittelwerte aus je 8 Beobachtungen handelt, hätte man sich auch einfach auf das Central Limit Theorem beziehen können, das sagt, dass Mittelwerte automatisch einer Normalverteilung folgen.


```{r}
#Ergebnisplot
plot(Area,Species.richness,pch=16,xlab="Flaeche [m2]",ylab="Artenreichtum")
xv<-seq(0,1000,by=0.1)
yv<-predict(power.model,list(Area=xv))
lines(xv,yv,lwd=2,col="red")

#Das ist der Ergebnisplot für das beste Modell. Wichtig ist, dass man die Achsen korrekt beschriftet und nicht einfach die mehr oder weniger kryptischen Spaltennamen aus R nimmt.

#Im Weiteren habe ich noch eine Sättigungsfunktion (Michaelis-Menten mit Selbststarter) zum Vergleich hinzugeplottet

yv2<-predict(micmen.1,list(Area=xv))
lines(xv,yv2,lwd=2,col="blue")
```
Man erkennt, dass die Sättigungsfunktion offensichtlich den tatsächlichen Kurvenverlauf sehr
schlecht widergibt. Im mittleren Kurvenbereich sind die Schätzwerte zu hoch, für grosse
Flächen dann aber systematisch viel zu niedrig.
Man kann die Darstellung im doppeltlogarithmischen Raum wiederholen, um die
Kurvenanpassung im linken Bereich besser differenzieren zu können:

```{r}
#Ergebnisplot Double-log
plot(log10(Area),log10(Species.richness),pch=16,xlab="logA",ylab="log (S)")
xv<-seq(0,1000,by=0.0001)
yv<-predict(power.model,list(Area=xv))
lines(log10(xv),log10(yv),lwd=2,col="red")
yv2<-predict(micmen.1,list(Area=xv))
lines(log10(xv),log10(yv2),lwd=2,col="blue")
```

Auch hier sieht man, dass die rote Kurve zwar nicht perfekt, aber doch viel besser als die blaue Kurve ist.
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:16_Statistik4/solution_stat4.1n.Rmd-->

---
title: "MSc. Research Methods - Statistikteil Lösungen 2019"
author: "Juergen Dengler"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r,include=FALSE, purl=FALSE}
knitr::opts_chunk$set(echo = T,fig.width = 20, fig.height = 12, warning = F, message = F, fig.pos = 'H',results = "show")
```

## Musterlösung Aufgabe 4.2N: Multiple logistische Regression

[R-Skript als Download](16_Statistik4/RFiles/Loesung_Uebung_4.2N_v.01.R)

**Übungsaufgabe (hier so ausführlich formuliert, wie dies auch in der Klausur der Fall sein wird)**

- Laden Sie den Datensatz isolation.csv. Dieser enthält für 50 Inseln die Information, ob
  eine bestimmte Vogelart dort vorkommt (incidence = 1) oder nicht vorkommt
  (incidence = 0). Für jede der Inseln sind zudem zwei Umweltvariablen angegeben:
  area (Fläche in km²) und isolation (Entfernung vom Festland in km).
- **Ermitteln Sie das minimal adäquate statistische Modell, das die              Vorkommenswahrscheinlichkeit der Vogelart in Abhängigkeit von Flächengrösse und Entfernung beschreibt.**
- Bitte erklären und begründen Sie die einzelnen Schritte, die Sie unternehmen, um zu
diesem Ergebnis zu kommen. Dazu erstellen Sie bitte ein Word-Dokument, in das Sie
Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, Ihre
Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere
Vorgehen dokumentieren.
- Dieser Ablauf sollte insbesondere beinhalten:
- Überprüfen der Datenstruktur nach dem Einlesen, welches sind die abhängige(n)
und welches die unabängige(n) Variablen
- Explorative Datenanalyse, um zu sehen, ob evtl. Dateneingabefehler vorliegen oder
Datentransformationen vorgenommen werden sollten
- Auswahl und Begründung eines statistischen Verfahrens
- Bestimmung des vollständigen/maximalen Models
- Selektion des/der besten Models/Modelle
- Durchführen der Modelldiagnostik für d
- Generieren aller Zahlen, Statistiken und Tabellen, die für eine wiss.
Ergebnisdarstellung benötigt werden
- Formulieren Sie abschliessend einen Methoden- und Ergebnisteil (ggf. incl.
adäquaten Abbildungen) zu dieser Untersuchung in der Form einer
wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je
einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und
Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige
Redundanz dagegen vermieden werden.
- **Abzugeben sind am Ende (a) Ein lauffähiges R-Skript; (b) begründeter Lösungsweg
(Kombination aus R-Code, R Output und dessen Interpretation) und (c)
ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit).**

### Übung 4.2 - Multiple logistische Regression**

```{r}
islands <-read.csv("16_Statistik4/data/isolation.csv")
islands
str(islands)
summary(islands)
```

Man erkennt, dass islands 50 Beobachtungen von drei Parametern enthält, wobei incidence
nur 0 oder 1 enthält, während area und isolation metrisch sind.


```{r}
attach(islands)
#Explorative Datenanalyse
boxplot(area)

boxplot(isolation)
```

Die Boxplots der beiden unabhängigen Variablen sind sehr symmetrisch, deshalb besteht kein
Anlass über eine evtl. log-Transformation nachzudenken (Tatsächlich sehen die Boxplots so
aus, als seien Fläche und Entfernung bereits log-transformiert, obgleich das Buch von Crawley (2015), aus dem dieses Beispiel stammt), behauptet, es seien die untransformierten
Originaldaten). Achtung: die Voraussetzung der ungefähren Normalverteilung gilt nur für die abhängige Variable, oder noch genauer für deren Residuen nach Berechnung des Models.
Trotzdem kann u.U. eine Transformation von sehr schief verteilten unabhängigen Variablen
sinnvoll sein, um die Linearität der Beziehung zu verbessern.


```{r}
#Definition der unterschiedlichen Modelle
model.mult <- glm(incidence~area*isolation,binomial)
model.add <- glm(incidence~area+isolation,binomial)
model.area <- glm(incidence~area,binomial)
model.isolation <- glm(incidence~isolation,binomial)
```

Es werden alle vier möglichen Modelle definiert: Mit area, isolation und area:isolationInteraktion (mult), mit area und isolation (add), nur mit area und nur mit isolation.

```{r}
#Modellergebnisse
summary(model.mult)
summary(model.add)

summary(model.area)
summary(model.isolation)
```

Um aus diesen vier Modellen, das minimal adäquate Modell herauszufinden, gibt es zwei
Möglichkeiten: (1) Man nimmt das volle Modell (mult), vereinfacht es schrittweise und
vergleicht jeweils das komplexere mit dem weniger komplexen Modell mittels ANOVA. (2) Man
vergleicht alle vier Modelle auf einmal mittel AICc. Man sollte sich für eine der beiden
Möglichkeiten entscheiden und nicht beide verfolgen. Im Folgenden sind aber beide
darstestellt.

**_Version (1): Schrittweise Modellvereinfachung mittels ANOVA_**

```{r}
anova(model.mult,model.add,test="Chi")
```

Das komplexere Modell mit Interaktionsterm (mult) ist nicht signifikant besser als das Modell, in dem area und isolation additiv enthalten sind (add). Deshalb kann man den Interaktionsterm streichen.

```{r}
anova(model.add,model.area,test="Chi")
anova(model.add,model.isolation,test="Chi")
```

Das additive Modell ist aber signifikant besser als jedes der beiden Modelle mit nur einem Prediktor. Deshalb ist das minimal adäquate Modell incidence ~ area + isolation.

**_Version (2): Modellselektion mittels AICc_**

```{r}
library(AICcmodavg)
cand.models<-list()
cand.models[[1]]<-model.mult
cand.models[[2]]<-model.add
cand.models[[3]]<-model.area
cand.models[[4]]<-model.isolation

Modnames<-c("Area * Isolation","Area +
Isolation","Area","Isolation")
aictab(cand.set=cand.models,modnames=Modnames)
```

Auch mit Modellselektion mittels AICc kommt man zum gleichen Ergebnis. Das minimal
adäquate Modell ist incidence ~ area + isolation.

```{r}
#Modelldiagnostik für das gewählte Modell (wenn nicht signifikant,dann OK)
1 - pchisq(model.add$deviance,model.add$df.resid)

#Nicht signifikant, d. h. kein „lack of fit“.

#Visuelle Inspektion der Linearität
library(car)
crPlots(model.add,ask=F)
```

log odds ratio vs. Prediktorvariablen (area bzw. isolation) ist fast perfekt linear (d. h. grüne Linie liegt fast auf der roten Optimallinie). D. h. auch dieser Sicht ist das Modell korrekt spezifiziert.

```{r}
#Modellgüte (pseudo-R²)
1 - (model.add$dev / model.add$null)

# 58% der Vorkommenswahrscheinlichkeit der Vogelart wird durch die beiden gewählten Prediktorvariablen erklärt.


#Steilheit der Beziehung (relative Änderung der odds von x + 1 vs.x)
#area (2. Koeffizient nach dem Achsenabschnitt)
exp(model.add$coef[2])

#> 1, d. h. Vorkommenswahrscheinlichkeit steigt mit Flächengrösse.

#area (3. Koeffizient nach dem Achsenabschnitt)
exp(model.add$coef[3])

#< 1, d. h. Vorkommenswahrscheinlichkeit sinkt mit zunehmender Isolation.
```

```{r}
#Ergebnisplots
#Da es keine signifikante Interaktion gibt, kann man die separaten Darstellungen der beiden Einzelbeziehungen nehmen.

xs<-seq(0,10,l=1000)
model.predict<-
predict(model.area,type="response",se=T,newdata=data.frame(area=xs))
plot(incidence~area,data=islands,xlab="Fläche
(km²)",ylab="Vorkommenswahrscheinlichkeit",pch=16, col="red")
 points(model.predict$fit ~ xs,type="l")
 lines(model.predict$fit+model.predict$se.fit ~ xs,
type="l",lty=2)
 lines(model.predict$fit-model.predict$se.fit ~ xs,
type="l",lty=2)
 
 model.predict2<-
predict(model.isolation,type="response",se=T,newdata=data.frame(isolation=xs))
plot(incidence~isolation,data=islands,xlab="Entfernung vom Festland
(km)",ylab="Vorkommenswahrscheinlichkeit",pch=16, col="blue")
 points(model.predict2$fit ~ xs,type="l")
 lines(model.predict2$fit+model.predict2$se.fit ~ xs,
type="l",lty=2)
 lines(model.predict2$fit-model.predict2$se.fit ~ xs,
type="l",lty=2)
```


**Darstellung der Vorgehensweise und Ergebnisse (in einer wiss. Arbeit)**

**Methoden**

**Es wurde das Vorkommen (ja/nein) ein Brutvogelart auf 50 Inseln in Abhängigkeit von deren Flächengrösse (in km²) und Entfernung vom Festland (in km) erhoben.**

**Mittels multipler logistischer Regression wurden unterschiedliche Modelle für diese Abhängigkeit berechnet und daraus dann das minimal adäquate Modell mittels AICc ermittelt. Die Adäquanz des gewählten Modells wurde anschliessend mittels G²-Statistik und „component + residual plots“ überprüft und war nicht verletzt.**

**Ergebnisse**

**Das nach AICc beste Modell enthielt Fläche und Entfernung vom Festland als Parameter, nicht jedoch deren Interaktion. Es wies einen delta-AICc zum Modell mit dem Interaktionsterm von 2.22 sowie 5.97 zum Modell nur mit der Fläche und 19.50 zum Modell nur mit der Entfernung auf. Dabei hatte die Fläche (in km²) einen stark positiven, die Isolation (in km) dagegen einen sehr stark negativen Effekt (Abb. 1). Die Gesamtbeziehung Vorkommenswahrscheinlichkeit = 1 / (1 + exp (–(6.64 + 0.58 * Fläche – 1.37 * Entfernung))) erklärt dabei 58% der Variabilität de Daten.**

![](16_Statistik4/data/abb1_4.2N.png)


**Abb. 1: Modellierte Abhängigkeit der Vorkommenswahrscheinlichkeit der Vogelart auf Inseln von den beiden additiv wirkenden Prädiktoren Fläche und Entfernung. Dargestellt ist die Regressionskurve +/- Standardfehler.**
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:16_Statistik4/solution_stat4.2n.Rmd-->

## Musterlösung Aufgabe 4.2S: multiple logistische Regression
> Meine Empfehlung Kapitel 6 von Manny Gimond https://mgimond.github.io/Stats-in-R/Logistic.html

**kommentierter Lösungsweg**
```{r, message=FALSE, echo=FALSE, results='hide', warning=FALSE}

library(tidyverse)
library(ggfortify)

nova <- read_delim("13_Statistik1/data/novanimal.csv", delim = ";")

## definiert mytheme für ggplot2 (verwendet dabei theme_classic())

mytheme <- 
  theme_classic() + 
  theme(
    axis.line = element_line(color = "black"), 
    axis.text = element_text(size = 20, color = "black"), 
    axis.title = element_text(size = 20, color = "black"), 
    axis.ticks = element_line(size = 1, color = "black"), 
    axis.ticks.length = unit(.5, "cm")
    )

```


```{r}

# Genereiert eine Dummyvariable: Fleisch 1, kein Fleisch 0
df <- nova # kopiert originaler Datensatz
df$meat <- ifelse(nova$label_content == "Fleisch", 1, 0)
df_ <- df[df$label_content != "Buffet", ] # entfernt Personen die sich ein Buffet Teller gekauft haben und speichert es in eine neuen Datensatz

# Löscht alle Missings bei der Variable "Fleisch"
df_ <- df_[!is.na(df_$meat), ]

#  sieht euch die Verteilung zwischen Fleisch und  kein Fleisch 
table(df_$meat)

# definiert das logistische Modell und wende es auf den Datensatz an
# modell nicht signifikant, rechnen mal trotzdem weiter
  mod0 <- glm(meat ~ gender + member + age, data = df_, binomial("logit"))
summary.lm(mod0) # Member  und Alter scheinen keinen Einfluss zu nehmen, lassen wir also weg

# neues Modell ohne Alter und Hochschulzugehörigkeit
mod1 <- update(mod0, ~. -member - age)
summary.lm(mod1)

# Modeldiagnostik (wenn nicht signifikant, dann OK)
1 - pchisq(mod1$deviance, mod1$df.resid) # hochsignifikant, d.h. kein gutes Modell

#Modellgüte (pseudo-R²)
1 - (mod1$dev / mod1$null) # sehr kleines pseudo-R²


# Konfusionsmatrix vom  Datensatz
# Model Vorhersage
# hier ein anderes Beispiel: 
predicted <- predict(mod1, df_, type = "response")

# erzeugt eine Tabelle mit den beobachteten
# Fleischesser/Nichtleischesser und den Vorhersagen des Modells
km <- table(df_$meat, predicted > 0.5)
dimnames(km) <- list(
  c("Beobachtung kein Fleisch", "Beobachtung Fleisch"),
  c("Modell kein Fleisch", "Modell Fleisch"))
km

# kalkuliert die Missklassifizierungsrate 
mf <- 1-sum(diag(km)/sum(km)) # ist mit knapp 40% eher hoch
mf


```


<!-- ******* -->
<!-- ##### letztes Update `r Sys.Date()`, gian-andrea.egeler@zhaw.ch -->
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:16_Statistik4/solution_stat4.2s.Rmd-->

# Statistik 5 (11.11.2019)

In Statistik 5 lernen die Studierenden Lösungen kennen, welche die diversen Limitierungen von linearen Modellen überwinden. Während generalized linear models (GLMs) aus Statistik 4 bekannt sind, geht es jetzt um linear mixed effect models (LMMs und generalized linear mixed effect models (GLMMs). Dabei bezeichnet generalized die explizite Modellierung anderer Fehler- und Varianzstrukturen und mixed die Berücksichtigung von Abhängigkeiten bzw. Schachtelungen unter den Beobachtungen. Einfachere Fälle von LMMs, wie split-plot und repeated-measures ANOVAs, lassen sich noch mit dem aov-Befehl in Base R bewältigen, für komplexere Versuchsdesigns/Analysen gibt es spezielle R packages. Abschliessend gibt es eine kurze Einführung in GLMMs, die eine Analyse komplexerer Beobachtungsdaten z. B. mit räumlichen Abhängigkeiten, erlauben. 
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:17_Statistik5/Abstract.Rmd-->

```{r, include=FALSE, purl=F}

knitr::opts_chunk$set(echo = TRUE,include = T, results = "hide",collapse=TRUE)
```
## Statistik 5 - Demoskript 
**Von linearen Modellen zu GLMMs**
**(c) Juergen Dengler**


- [Demoscript als Download](17_Statistik5/RFiles/Statistik 5-Demo_v.03.R)
- Datensatz [spf.csv](17_Statistik5/data/spf.csv)
- Datensatz [DeerEcervi.txt](17_Statistik5/data/DeerEcervi.txt)


 __Split-plot ANOVA__
Based on Logan (2010), Chapter 14

```{r}
spf<-read.delim("17_Statistik5/data/spf.csv", sep = ",")
spf.aov<-aov(Y~A*C+Error(B),spf)
summary(spf.aov)

attach(spf)
interaction.plot(C,A,Y)

#nun als LMM
if(!require(nlme)){install.packages("nlme")}
library(nlme)
spf.lme.1<-lme(Y~A*C,random = ~C | B, spf)
spf.lme.2<-lme(Y~A*C,random = ~1 | B, spf)

anova(spf.lme.1)
anova(spf.lme.2)

summary(spf.lme.1)
summary(spf.lme.2)
```

 __GLMM__
Based on Zuur et al. (2009), chapter 13

```{r}
DeerEcervi <- read.table("17_Statistik5/data/DeerEcervi.txt", header=TRUE, dec=".")

DeerEcervi$Ecervi.01 <- DeerEcervi$Ecervi

#Anzahl Larven hier in Presence/Absence uebersetzt
DeerEcervi$Ecervi.01[DeerEcervi$Ecervi>0]<-1
DeerEcervi$fSex <- factor(DeerEcervi$Sex)

#Hirschlänge hier standardisiert, sonst würde der Achsenabschnitt im Modell für
#einen Hirsch der Länge 0 modelliert, was schlecht interpretierbar ist, 
#jetzt ist der Achsenabschnitt für einen durschnittlich langen Hirsch
DeerEcervi$CLength <- DeerEcervi$Length - mean(DeerEcervi$Length)
DeerEcervi$fFarm <- factor(DeerEcervi$Farm)

#Zunächst als GLM
#Interaktionen mit fFarm nicht berücksichtigt, da zu viele Freiheitsgrade verbraucht würden
DE.glm<-glm(Ecervi.01 ~ CLength * fSex+fFarm, data = DeerEcervi,
            family = binomial)
drop1(DE.glm, test = "Chi")
summary(DE.glm)
anova(DE.glm)
```

 __GLMM__
 
```{r}
if(!require(MASS)){install.packages("MASS")}
library(MASS)
DE.PQL<-glmmPQL(Ecervi.01 ~ CLength * fSex,
                random = ~ 1 | fFarm, family = binomial, data = DeerEcervi)
summary(DE.PQL)


g <- 0.8883697 + 0.0378608 * DeerEcervi$CLength
p.averageFarm1<-exp(g)/(1+exp(g))
I<-order(DeerEcervi$CLength)              #Avoid spaghetti plot
plot(DeerEcervi$CLength,DeerEcervi$Ecervi.01,xlab="Length",
     ylab="Probability of presence of E. cervi L1")
lines(DeerEcervi$CLength[I],p.averageFarm1[I],lwd=3)
p.Upp<-exp(g+1.96*1.462108)/(1+exp(g+1.96*1.462108))
p.Low<-exp(g-1.96*1.462108)/(1+exp(g-1.96*1.462108))
lines(DeerEcervi$CLength[I],p.Upp[I])
lines(DeerEcervi$CLength[I],p.Low[I])

if(!require(lme4)){install.packages("lme4")}
library(lme4)
DE.lme4<-lmer(Ecervi.01 ~ CLength * fSex +(1|fFarm),
              family = binomial, data = DeerEcervi)
summary(DE.lme4)

if(!require(glmmML)){install.packages("glmmML")}
library(glmmML)
DE.glmmML<-glmmML(Ecervi.01 ~ CLength * fSex,
                  cluster = fFarm,family=binomial, data = DeerEcervi)
summary(DE.glmmML)
```
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:17_Statistik5/Demoskript.Rmd-->

## Übungen 5

**Übung 5.1: Split-plot ANOVA**

**Datensatz [splityield.csv](17_Statistik5/data/splityield.csv)**

Versuch zum Ernteertrag (yield) einer Kulturpflanze in Abhängigkeit der drei Faktoren Bewässerung (irrigated vs. control), Düngung (N, NP, P) und Aussaatdichten (low, medium, high). Es gab vier ganze Felder (block), die zwei Hälften mit den beiden Bewässerungstreatments, diese wiederum drei Drittel für die drei Saatdichten und diese schliesslich je drei Drittel für die drei Düngertreatments hatten.


**Aufgaben**

- **Bestimmt das minimal adäquate Modell**
- **Stellt die Ergebnisse dar**



**Übung 5.2: GLMM**

Führt mit dem Datensatz [novanimal.csv](13_Statistik1/data/novanimal.csv) eine logistische Regression durch, wobei ihr die einzelnen Käufer (single campus_card holder) als weitere randomisierte Variable mitberücksichtigt. Kann der Fleischkonsum durch das Geschlecht, die Hochschulzugehörigkeit und das Alter erklärt werden? Vergleich die Ergebnisse mit der eurem multiplen logistische Modell von 4.2

**Kann der Fleischkonsum nun besser durch das Geschlecht, die Hochschulzugehörigkeit und das Alter erklärt werden?**

**Aufgaben**

- **Bestimmt das minimal adäquate Modell**
- **Stellt die Ergebnisse dar**


Ähnliches Vorgehen wie bei der Übung 4.2S:

-   Generiert eine neue Variable "Fleisch" (0 = kein Fleisch, 1 = Fleisch)
-   Entfernt fehlende Werte aus der Variable "Fleisch"
-   Lasst für die Analyse den Menü-Inhalt "Buffet" weg
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:17_Statistik5/assigment_stat5.Rmd-->

## Musterlösung Aufgabe 5.1: Split-plot ANOVA

**Übungsaufgabe (hier so ausführlich formuliert, wie dies auch in der Klausur der Fall sein wird)**

- Ladet den Datensatz splityield.csv. Dieser enthält Versuchsergebnisse eines
  Experiments zum Ernteertrag (yield) einer Kulturpflanze in Abhängigkeit der drei
  Faktoren Bewässerung (irrigated vs. control), Düngung (N, NP, P) und Aussaatdichten
  (low, medium, high). Es gab vier ganze Felder (block), die zwei Hälften mit den beiden
  Bewässerungstreatments (irrigation), diese wiederum drei Drittel für die drei
  Saatdichten (density) und diese schliesslich je drei Drittel für die drei
  Düngertreatments (fertilizer) hatten.
- **Ermittelt das minimal adäquate statistische Modell, das den Ernteertrag in
  Abhängigkeit von den angegebenen Faktoren beschreibt.**
- Bitte erklärt und begründet die einzelnen Schritte, die ihr unternehmt, um zu diesem
  Ergebnis zu kommen. Dazu erstellt bitte ein Word-Dokument, in das ihr Schritt für
  Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, eure
  Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere
  Vorgehen dokumentieren.
- Dieser Ablauf sollte insbesondere beinhalten:
    - Überprüfen der Datenstruktur nach dem Einlesen, welches sind die abhängige(n)
      und welches die unabängige(n) Variablen
    - Explorative Datenanalyse, um zu sehen, ob evtl. Dateneingabefehler vorliegen oder
      Datentransformationen vorgenommen werden sollten
    - Auswahl und Begründung eines statistischen Verfahrens
    - Bestimmung des vollständigen/maximalen Models
    - Selektion des/der besten Models/Modelle
    - Generieren aller Zahlen, Statistiken und Tabellen, die für eine wiss
    
         _(Ergebnisdarstellung benötigt werden)_
         
    - Formuliert abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten
      Abbildungen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit
      (ausformulierte schriftliche Zusammenfassung, mit je einem Absatz von ca. 60-100
      Worten, resp. 3-8 Sätzen für den Methoden- und Ergebnisteil). D. h. alle wichtigen
      Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden.
    - **Abzugeben sind am Ende (a) Ein lauffähiges R-Skript; (b) begründeter Lösungsweg
      (Kombination aus R-Code, R Output und dessen Interpretation) und (c) ausformulierter
      Methoden- und Ergebnisteil (für eine wiss. Arbeit).**
      
[R-Skript als Download](17_Statistik5/RFiles/Loesung_Uebung_5.1_v.03.R)

      

**kommentierter Lösungsweg**
```{r}
splityield <-read.delim("17_Statistik5/data/splityield.csv", sep=",")

#Checken der eingelesenen Daten
splityield
```
Man sieht, dass das Design vollkommen balanciert ist, d.h. jede Kombination irrigation *
density * fertilizer kommt genau 4x vor (in jedem der vier Blöcke A-D einmal).

```{r}
str(splityield)
summary(splityield)
splityield$density<-factor(splityield$density, levels=c("low","medium","high"))
```
Man sieht, dass die Variable yield metrisch ist, während die vier anderen Variablen schon
korrekt als kategoriale Variablen (factors) kodiert sind

```{r}
#Explorative Datenanalyse (auf Normalverteilung, Varianzhomogenität)
boxplot(splityield$yield)
boxplot(yield~fertilizer, data=splityield)
boxplot(yield~irrigation, data=splityield)
boxplot(yield~density, data=splityield)
boxplot(yield~irrigation*density*fertilizer,data=splityield)
```
Die Boxplots sind generell hinreichend symmetrisch, so dass man davon ausgehen kann, dass
keine problematische Abweichung von der Normalverteilung vorliegt. Die Varianzhomogenität
sieht für den Gesamtboxplot sowie für fertilizer und density bestens aus, für irrigation und für
die 3-fach-Interaktion deuten sich aber gewisse Varianzheterogenitäten an, d. h. die Boxen
(Interquartil-Spannen) sind deutlich unterschiedlich lang. Da das Design aber vollkommen
„balanciert“ war, wie wir von oben wissen, sind selbst relativ stark divergierende Varianzen
nicht besonders problematisch. Der Boxplot der Dreifachinteraktion zeigt zudem, dass
grössere Varianzen (~Boxen) mal bei kleinen, mal bei grossen Mittelwerten vorkommen,
womit wir bedenkenlos weitermachen können (Wenn die grossen Varianzen immer bei
grossen Mittelwerten aufgetreten wären, hätten wir eine log- oder Wurzeltransformation von
yield in Betracht ziehen müssen).

```{r}
boxplot(log10(yield)~irrigation*density*fertilizer,data=splityield) #bringt keine Verbesserung
aov.1<-aov(yield~irrigation*density*fertilizer+Error(block/irrigation/density),data=splityield)
```

Das schwierigste an der Analyse ist hier die Definition des Splitt-Plot ANOVA-Modells. Hier
machen wir es mit der einfachsten Möglichkeit, dem aov-Befehl. Um diesen richtig zu
spezifieren, muss man verstanden haben, welches der „random“-Faktor war und wie die
„fixed“ factors ineinander geschachtelt waren. In diesem Fall ist block der random Faktor, in
den zunächst irrigation und dann density geschachtelt sind (die unterste Ebene fertilizer muss
man nicht mehr angeben, da diese in der nächsthöheren nicht repliziert ist).

(Übrigens: das simple 3-faktorielle ANOVA-Modell
aov(yield~irrigation*density*fertilizer,data=splityield) würde unterstellen, dass alle 72 subplot
unabhängig von allen anderen angeordnet sind, also nicht in Blöcken. Man kann ausprobieren,
wie sich das Ergebnis mit dieser Einstellung unterscheidet)

```{r}
summary(aov.1)
```

Wir bekommen p-Werte für die drei Einzeltreatments, die drei 2-fach-Interaktionen und die 3-
fach Interaktion. Keinen p-Wert gibt es dagegen für block, da dieser als „random“ Faktor
spezifiziert wurde.
Signifikant sind für sich genommen irrigation und fertilizer sowie die Interaktionen
irrigation:density und irrigation:fertilizer.

```{r}
# Modelvereinfachung
aov.2<-aov(yield~irrigation+density+fertilizer+irrigation:density+irrigation:fertilizer+
             Error(block/irrigation/density),data=splityield)
summary(aov.2)
```

Jetzt muss man nur noch herausfinden, wie irrigation und fertilizer wirken und wie die
Interaktionen aussehen. Bei multiplen ANOVAs macht man das am besten visuell:

```{r}
#Visualisierung der Ergebnisse
boxplot(yield~fertilizer,data=splityield)
boxplot(yield~irrigation,data=splityield)

interaction.plot(splityield$fertilizer,splityield$irrigation,splityield$yield)
interaction.plot(splityield$density,splityield$irrigation,splityield$yield)
```
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:17_Statistik5/solution_stat5.1.Rmd-->

## Musterlösung Aufgabe 5.2S: GLMM
> Meine Leseempfehlung Kapitel 4.3.1 von [Christopher Molnar](https://christophm.github.io/interpretable-ml-book/extend-lm.html#glm)

> Für Interessierte [hier](https://rpsychologist.com/r-guide-longitudinal-lme-lmer) oder [hier](https://rpubs.com/kaz_yos/glmm1)



```{r, message=FALSE, echo=FALSE, results='hide', warning=FALSE}

library(tidyverse)
library(ggfortify)
library(lme4)

nova <- read_delim("13_Statistik1/data/novanimal.csv", delim = ";")

## definiert mytheme für ggplot2 (verwendet dabei theme_classic())

mytheme <- 
  theme_classic() + 
  theme(
    axis.line = element_line(color = "black"), 
    axis.text = element_text(size = 20, color = "black"), 
    axis.title = element_text(size = 20, color = "black"), 
    axis.ticks = element_line(size = 1, color = "black"), 
    axis.ticks.length = unit(.5, "cm")
    )

```

**kommentierter Lösungsweg**
```{r}

# Genereiert eine Dummyvariable: Fleisch 1, kein Fleisch 0
df <- nova # kopiert originaler Datensatz
df$meat <- ifelse(nova$label_content == "Fleisch", 1, 0)
df_ <- df[df$label_content != "Buffet", ] # entfernt Personen die sich ein Buffet Teller gekauft haben und speichert es in eine neuen Datensatz

# Löscht alle Missings bei der Variable "Fleisch"
df_ <- df_[!is.na(df_$meat), ]

# setzt andere Reihenfolge für die Hochschulzugehörigkeit
df_$member <- factor(df_$member, levels = c("Studierende", "Mitarbeitende"))

#  sieht euch die Verteilung zwischen Fleisch und  kein Fleisch an
# table(df_$meat)

# definiert das logistische Modell mit card_num als random intercept und wendet es auf den Datensatz an (vgl. Statistik 5, Folien 17-23, achtung Beispiel dort ist mit Package nlme )
library(lme4)
mod0 <- glmer(meat ~ gender + member + age + (1|card_num), data = df_, binomial("logit")) # könnte Alter und Geschlecht weglassen (da aber soziodemografische Variablen, lasse ich sie drin)
summary(mod0)
 
# Pseudo R^2
library(MuMIn)
r.squaredGLMM(mod0) 
# das marginale R^2 gibt uns die erklärte Varianz der fixen Effekte: hier 5%
# das conditionale R^2 gibt uns die erklärte Varianz für das ganze Modell (mit fixen und variablen Effekten): hier 26%
# für weitere Informationen: https://rdrr.io/cran/MuMIn/man/r.squaredGLMM.html 

# zusätzliche Informationen, welche für die Interpretation gut sein kann
# berechnet den Standardfehler (mehr infos: https://www.youtube.com/watch?v=r-txC-dpI-E oder hier: https://mgimond.github.io/Stats-in-R/CI.html)
# weitere info: https://stats.stackexchange.com/questions/26650/how-do-i-reference-a-regression-models-coefficients-standard-errors
se <- sqrt(diag(vcov(mod0)))

# zeigt eine Tabelle der Schätzer mit 95% Konfidenzintervall => falls 0 enthalten dann ist der Unterschied statistisch nicht signifikant
tab1 <- cbind(Est = fixef(mod0), LL = fixef(mod0) - 1.96 * se, UL = fixef(mod0) + 1.96 *
    se)

# erzeugt die Odds Ratios
tab2 <- exp(tab1)
```



<!-- ******* -->
<!-- ##### letztes Update `r Sys.Date()`, gian-andrea.egeler@zhaw.ch -->

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:17_Statistik5/solution_stat5.2.Rmd-->

# Konsolidierung Statistik (11.11. - 19.11.2019)

In den vier Blöcken "Konsolidierung Statistik" repetieren die Studierenden wichtige Verfahren der Inferenz-Statistik. Beginnend mit den beiden einfachsten Fällen, dem Chi-Quadrat-Test für die Assoziation zwischen zwei kategorialen Variablen und dem t-Test auf Unterschiede in Mittelwerten zwischen zwei Gruppen, wiederholen die Studierende auch die Varianzanalyse (ANOVA) als Verallgemeinerung des t-Tests, einschliesslich post-hoc-Tests und mehrfaktorieller ANOVA. Weiter geht es mit der Repetition von komplexere Versionen linearer Regressionen und generalized linear models (GLMs), die einige wesentliche Limitierungen von linearen Modellen überwinden. Abschliessend bekommen die Studierenden eine Einführung in die Welt der Ordinationen z.B. PCA.



Der Ablauf der nächsten vier Blöcke:

![](statistikbloecke_2019_konsolidierung.pdf){width=75%}.

<!-- found that here: not working -->
<!-- https://stackoverflow.com/questions/39173714/r-markdown-can-i-insert-a-pdf-to-the-r-markdown-file-as-an-image -->
<!--  ```{r image-ref-for-in-text, echo = FALSE, message=FALSE, out.width='0.75\\linewidth', fig.pos='H', fig.align="center", fig.cap=c(""), echo=FALSE} -->
<!--   -->
<!-- knitr::include_graphics("statistikbloecke_2019_konsolidierung.pdf") -->
<!--  ``` -->

Hier geht es zum [Download](17_Statistik_Konsolidierung1/statistikbloecke_2019_konsolidierung.pdf)
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:17_Statistik_Konsolidierung1/Abstract.Rmd-->

## Übung Konsolidierung Statistik

Für die nächsten drei gemeinsamen Sitzungen (11./18./19. November) könnt ihr auf eure eigene Art die statistischen Methoden nochmals anwenden:

    a. Ihr sucht euch einen eigenen Datensatz
    b. Ihr nehmt einen vorgeschlagenen Datensatz
    c. Ihr nehmt euren Datensatz aus der Fallstudie

**Wichtig:** Beachtet, dass ihr einen Datensatz sucht, mit dem folgende Analyse möglich sind:

1. Assoziationstests
2. Varianzanalysen
3. linerare Modelle


Für eine gute Prüfungsvorbereitung erfasst ihr (Einzel oder als Gruppe) zu jeder obigen Analysemethode einen kurzen Bericht (max. 1/2 bis 1 Seite). Dieser Bericht beinhaltet folgende Punkte:

    a. lauffähiges R-Skript
    b. begründeter Lösungsweg (Kombination aus R-Code, R Output 
       und dessen Interpretation)
    c. ausformulierter Methoden- und Ergebnisteil (für eine wiss.Arbeit)
      * Methodenteil: Begründung wieso ihr euch für diese/n Test/Methode entschieden habt 
      * Ergebnisteil: Interpretation der Ergebnisse (ggf. mit Formel) und einer passenden Abbildung/Tabelle


Das Ziel dieser Übung ist es, 

- dass ihr euch überlegt was für eine Variablenstruktur der Datensatz aufweisen muss, um die obigen Analysen zu durchzuführen
- dass ihr einen eigenen Datensatz findet, welches euer Interesse weckt
- dass ihr verschiedene Quellen zu Open Data gesehen und kennen gelernt habt 
- dass ihr für die Prüfung gut vorbereitet sind

*******
**Open Datasets**
```{r child = "17_Statistik_Konsolidierung1/open_datasets.Rmd"}
```


**Given Datasets**
```{r child = '17_Statistik_Konsolidierung1/suggest_datasets.Rmd'}
```




<!-- ##### letztes Update `r Sys.Date()`, gian-andrea.egeler@zhaw.ch -->
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:17_Statistik_Konsolidierung1/assigment_konstat1.Rmd-->

# Statistik 6 (12.11.2019)

Statistik 6 führt in multivariat-deskriptive Methoden ein, die dazu dienen Datensätze mit multiplen abhängigen und multiplen unabhängigen Variablen effektiv zu analysieren. Dabei betonen Ordinationen kontinuierliche Gradienten und fokussieren auf zusammengehörende Variablen, während Cluster-Analysen Diskontinuitäten betonen und auf zusammengehörende Beobachtungen fokussieren. Es folgt eine konzeptionelle Einführung in die Idee von Ordinationen als einer Technik der deskriptiven Statistik, die Strukturen in multivariaten Datensätzen via Dimensionsreduktion visualisiert. Das Prinzip und die praktische Implementierung wird detailliert am Beispiel der Hauptkomponentenanalyse (PCA) erklärt. Danach folgen kurze Einführungen in weitere Ordinationstechniken für besondere Fälle, welche bestimmte Limitierungen der PCA überwinden, namentlich CA, DCA und NMDS.
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:18_Statistik6/Abstract.Rmd-->

---
output:
  pdf_document: default
  html_document: default
---
```{r, include=FALSE, purl=F}

knitr::opts_chunk$set(echo = TRUE,include = T, results = "hide",collapse=TRUE)
```

## Statistik 6 - Demoskript ## 
**Ordinationen I**
**(c) Juergen Dengler**


[Demoscript als Download](18_Statistik6/Statistik 6-Demo_v.03.R)


__PCA__
```{r, message=FALSE}
if(!require(labdsv)){install.packages("labdsv")}
library(labdsv)
#Für Ordinationen benötigen wir Matrizen, nicht Data.frames

#Generieren von Daten
raw<-matrix(c(1,2,2.5,2.5,1,0.5,0,1,2,4,3,1),nrow=6)
colnames(raw)<-c("spec.1","spec.2")
rownames(raw)<-c("r1","r2","r3","r4","r5","r6")
raw

#originale Daten im zweidimensionalen Raum
x1<-raw[,1]
y1<-raw[,2]
z<-c(rep(1:6))


#Plot Abhängigkeit der Arten vom Umweltgradienten
plot(c(x1,y1)~c(z,z),type="n",axes=T,bty="l",las=1,xlim=c(1,6),ylim=c(0,5),xlab="Umweltgradient",ylab="Deckung der Arten")
points(x1~z,pch=21,type="b")
points(y1~z,pch=16,type="b")

#zentrierte Daten
cent<-scale(raw,scale=F)
x2<-cent[,1]
y2<-cent[,2]

#rotierte Daten
o.pca<-pca(raw)
x3 <- o.pca$scores[,1]
y3 <- o.pca$scores[,2]


#Visualisierung der Schritte im Ordinationsraum
plot(c(y1,y2,y3)~c(x1,x2,x3),type="n",axes=T,bty="l",las=1,xlim=c(-4,4),ylim=c(-4,4),xlab="Art 1",ylab="Art 2")
points(y1~x1,pch=21,type="b",col="green",lwd=2)
points(y2~x2,pch=16,type="b",col="red", lwd=2)
points(y3~x3,pch=17,type="b",col="blue", lwd=2)

#Durchführung der PCA
o.pca<-pca(raw)

#Koordinaten im Ordinationsraum
o.pca$scores

#Korrelationen der Variablen mit den Ordinationsachsen
o.pca$loadings

#Erklärte Varianz der Achsen
E<-o.pca$sdev^2/o.pca$totdev*100
E

#mit prcomp
pca.2<-prcomp(raw,scale=F)
summary(pca.2)
plot(pca.2)
biplot(pca.2)

#mit vegan
if(!require(vegan)){install.packages("vegan")}
library("vegan")
pca.3 <- rda(raw, scale=FALSE) #Die Funktion rda führt ein PCA aus an wenn nicht Umwelt und Artdaten definiert werden
summary(pca.3,axes=0)
biplot(pca.3, scaling=2)


#Mit Beispieldaten aus Wildi (2013)
if(!require(dave)){install.packages("dave")}
library(dave)
data(sveg)
str(sveg)
summary(sveg)
names(sveg)

#PCA: Deckungen Wurzeltransformiert, cor=T erzwingt Nutzung der Korrelationsmatrix
pca.5<-pca(sveg^0.25,cor=T)

#Koordinaten im Ordinationsraum
pca.5$scores

#Korrelationen der Variablen mit den Ordinationsachsen
pca.5$loadings

#Erklärte Varianz der Achsen in Prozent (sdev ist die Wurzel daraus)
E<-pca.5$sdev^2/pca.5$totdev*100
E
E[1:5]

#PCA-Plot der Lage der Beobachtungen im Ordinationsraum
plot(pca.5$scores[,1],pca.5$scores[,2],type="n", asp=1, xlab="PC1", ylab="PC2")
points(pca.5$scores[,1],pca.5$scores[,2],pch=18)

#Subjektive Auswahl von Arten zur Darstellung
sel.sp <- c(3,11,23,39,46,72,77,96)
snames <- names(sveg[,sel.sp])
snames

#PCA-Plot der Korrelationen der Variablen (hier Arten) mit den Achsen (h)
x <- pca.5$loadings[,1]

y<-pca.5$loadings[,2]
plot(x,y,type="n",asp=1)
arrows(0,0,x[sel.sp],y[sel.sp],length=0.08)
text(x[sel.sp],y[sel.sp],snames,pos=1,cex=0.6)


# Mit vegan
pca.6<-rda(sveg^0.25, scale=TRUE)
#Erklärte Varianz der Achsen
summary(pca.6,axes=0)
#PCA-Plot der Lage der Beobachtungen im Ordinationsraum
biplot(pca.6, scaling=1,display = c("sites"),type = c("points"))
#Subjektive Auswahl von Arten zur Darstellung
sel.sp <- c(3,11,23,39,46,72,77,96)
snames <- names(sveg[,sel.sp])
snames
#PCA-Plot der Korrelationen der Variablen (hier Arten) mit den Achsen (h)
scores<-scores(pca.6,display=c("species"))
x<-scores[,1]
y<-scores[,2]
plot(x,y,type="n",asp=1)
arrows(0,0,x[sel.sp],y[sel.sp],length=0.08)
text(x[sel.sp],y[sel.sp],snames,pos=1,cex=0.6)
```


__CA__
```{r, message=FALSE}
ca<-cca(sveg^0.5)
#Arten (o) und Communities (+) plotten
plot(ca)

#Nur Arten plotten
x<-ca$CA$u[,1];y<-ca$CA$u[,2]
plot(x,y)
plot(ca, display = c("species"),type = c("points"))#alternative


#Anteilige Varianz, die durch die ersten beiden Achsen erklärt wird
ca$CA$eig[1:2]/sum(ca$CA$eig)
```

__DCA__
```{r, message=FALSE}
dca <- decorana(sveg,mk=10)
plot(dca$rproj, asp=1)

dca2 <- decorana(sveg,mk=100)
plot(dca2$rproj, asp=1)
```

__NMDS__
```{r, message=FALSE}
#Distanzmatrix als Start erzeugen
mde <-vegdist(sveg,method="euclidean")
mde
mde <-vegdist(sveg,method="bray")#Alternative mit einem für Vegetationsdaten üblichen Dissimilarity index
mde

#Zwei verschiedene NMDS-Methoden
if(!require(MASS)){install.packages("MASS")}
library(MASS)
set.seed(1) #macht man, wenn man bei einer Wiederholung exakt die gleichen Ergebnisse will
imds<-isoMDS(mde,k=2)
set.seed(1)
mmds<-metaMDS(mde,k=2)

plot(imds$points)
plot(mmds$points)

#Stress = S² = Abweichung der zweidimensionalen NMDS-Lösung von der originalen Distanzmatrix
stressplot(imds,mde)
stressplot(mmds,mde)
```
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:18_Statistik6/Demoskript.Rmd-->

## Statistik 6: Übungen

**Übung 6.1: PCA (naturwissenschaftlich)**

**Datensatz [Doubs.RData](18_Statistik6/Doubs.RData)**

Lädt den Datensatz Doubs.RData mit dem folgenden Befehl ins R:
load("Doubs.RData")

Die Umweltvariablen findet ihr im data.frame env die Abundanzen im data.frame spe. Im data.frame fishtrait findet ihr die Vollständigen Namen der Fische

Der Datensatz enthält Daten zum Vorkommen von Fischarten und den zugehörigen Umweltvariablen im Fluss Doubs (Jura). Es gibt 30 Probestellen (sites), an denen jeweils die Abundanzen von 27 Fischarten (auf einer Skalen von 0 bis 5) sowie 11 Umweltvariablen erhoben wurden:

dfs = Distance from source (km)
ele = Elevation (m a.s.l.)
slo = Slope (‰)
dis = Mean annual discharge (m3 s-1)
pH = pH of water
har = Hardness (Ca concentration) (mg L-1)
pho = Phosphate concentration (mg L-1)
nit = Nitrate concentration (mg L-1)
amm = Ammonium concentration (mg L-1)
oxy = Dissolved oxygen (mg L-1)
bod = Biological oxygen demand (mg L-1)

Eure Aufgabe ist nun, in einem ersten Schritt eine PCA für die 11 Umweltvariablen zu rechnen. Da die einzelnen Variablen auf ganz unterschiedlichen Skalen gemessen wurden, ist dazu eine Standardisierung nötig (pca mit der Funktion rda, scale=TRUE). Überlegt, wie viele Achsen wichtig sind und für was sie jeweils stehen.

In einem zweiten Schritt sollen dann die vollständig unkorrelierten PCA-Achsen als Prädiktoren einer multiplen Regression zur Erklärung der Fischartenzahl (Anzahl kann z.B. kann mit dem Befehl specnumber(spe) ermittel werden) verwendet werden (wahlweise lm oder glm). Gebt das minimal adäquate Modell an und interpretiert dieses (wahlweise im frequentist oder information theoretician approach).
(Wer noch mehr probieren möchte, kann zum Vergleich noch eine multiple Regression mit den Originaldaten rechnen).
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:18_Statistik6/assigment_stat6.Rmd-->


## Musterlöseung Aufgabe 6.1: PCA

[R-Skript als Download](18_Statistik6/RFiles/Loesung_Uebung_6.1_v.03.R)

```{r eval=FALSE}
load("18_Statistik6/Doubs.RData")
summary(env)
summary(spe)

#Die Dataframes env und spe enthalten die Umwelt- respective die Artdaten

if(!require(vegan)){install.packages("vegan")}
library("vegan")

```

Die PCA wird im Package vegan mit dem Befehl rda ausgeführt, wobei in diesem scale = TRUE gesetzt warden muss, da die Umweltdaten mit ganz unterschiedlichen Einheiten und Wertebereichen daherkommen

```{r eval=FALSE}
env.pca <- rda(env, scale=TRUE)
env.pca
#In env.pca sieht man, dass es bei 11 Umweltvariablen logischerweise 11 orthogonale Principle Components gibt

summary(env.pca,axes=0)

#Hier sieht man auch die Übersetzung der Eigenvalues in erklärte Varianzen der einzelnen Principle Components

summary(env.pca)
#Hier das ausführliche Summary mit den Art- und Umweltkorrelationen auf den ersten sechs Achsen

screeplot(env.pca, bstick=TRUE, npcs=length(env.pca$CA$eig))
#Visualisierung der Anteile erklärter Varianz, auch im Vergleich zu einem Broken-Stick-Modell
```

- Die Anteile fallen steil ab. Nur die ersten vier Achsen erklären jeweils mehr als 5 % (und zusammen über 90 %)
- Das Broken-stick-Modell würde sogar nur die ersten beiden Achsen als relevant vorschlagen
- Da die Relevanz für das Datenmuster in den Umweltdaten nicht notwendig die Relevanz für die Erklärung der Artenzahlen ist,    nehmen wir ins globale Modell grosszügig die ersten vier Achsen rein (PC1-PC4)
Die Bedeutung der Achsen (benötigt man später für die Interpretation!) findet man in den “species scores” (da so, wie wir die PCA hier gerechnet haben, die Umweltdaten die Arten sind. Zusätzlich oder alternative kann man sich die ersten vier Achsen auch visualisieren, indem man PC2 vs. PC1 (ohne choices), PC3 vs. PC1 oder PC4 vs. PC1 plottet.

```{r eval=FALSE}

par(mfrow=c(2,2))
biplot(env.pca, scaling=1)
biplot(env.pca, choices=c(1,3),scaling=1)
biplot(env.pca, choices=c(1,4),scaling=1)

```

-	PC1 steht v.a. für Nitrat (positiv), Sauerstoff (negativ)
-	PC2 steht v.a. für pH (positiv)
-	PC3 steht v.a. für pH (positiv) und slo (negativ)
- PC4 steht v.a. für pH (negativ) und slo (negativ)

```{r eval=FALSE}
#Wir extrahieren nun die ersten vier PC-Scores aller Aufnahmeflächen

scores<-scores(env.pca,choices=c(1:4),display=c("sites"))
scores

#Berechnung der Artenzahl mittels specnumber; Artenzahl und Scores werden zum Dataframe für die Regressionsanalyse hinzugefügt
doubs <- data.frame(env, scores, species_richness=specnumber(spe))
doubs
str(doubs)

##Lösung mit lm (alternativ ginge Poisson-glm) und frequentist approach (alternativ ginge Multimodelinference mit AICc)
lm.pc.0 <- lm(species_richness ~ PC1+PC2+PC3+PC4, data = doubs)
summary(lm.pc.0)

#Modellvereinfachung: PC4 ist nicht signifikant und wird entfernt
lm.pc.1 <- lm(species_richness ~ PC1+PC2+PC3, data = doubs)
summary(lm.pc.1) #jetzt sind alle Achsen signifikant und werden in das minimal adäquate Modell aufgenommen

#Modelldiagnostik/Modellvalidierung
par(mfrow=c(2,2))
plot(lm.pc.1) 
```

Nicht besonders toll, ginge aber gerade noch. Da wir aber ohnehin Zähldaten haben, können wir es mit einem Poisson-GLM versuchen

#Alternativ mit glm
```{r eval=FALSE}
glm.pc.0 <- glm(species_richness ~ PC1+PC2+PC3+PC4, family = "poisson", data = doubs)
summary(glm.pc.0)
glm.pc.1 <- glm(species_richness ~ PC1+PC2+PC3, family = "poisson", data = doubs)
summary(glm.pc.1)
plot(glm.pc.1) #sieht nicht besser aus als LM, die Normalverteilung ist sogar schlechter
```

LM oder GLM sind für die Analyse möglich, Modellwahl nach Gusto. Man muss jetzt noch die Ergebnisse adäquat aus all den erzielten Outputs zusammenstellen (siehe Ergebnistext). In dieser Aufgabe haben wir ja die PC-Achsen als Alternative zur direkten Modellierung mit den originalen Umweltvariablen ausprobiert. Deshalb (war nicht Teil der Aufgabe), kommt hier noch eine Lösung, wie wir es bisher gemacht hätten.


**Zum Vergleich die Modellierung mit den Originaldaten**

```{r eval=FALSE}
#Korrelationen zwischen Prädiktoren
cor <- cor(doubs[,1:11])
cor[abs(cor)<.7] <-0
cor 

#Die Korrelationsmatrix betrachtet man am besten in Excel.
#Es zeigt sich, dass es zwei grosse Gruppen von untereinander hochkorrelierten Variablen gibt: zum einen dfs-ele-dis-har-nit, zum anderen pho-nit-amm-oxy-bod, während slo und pH mit jeweils keiner anderen Variablen hochkorreliert sind. Insofern behalten wir eine aus der ersten Gruppe (ele), eine aus der zweiten Gruppe (pho) und die beiden «unabhängigen».

#Globalmodell (als hinreichend unabhängige Variablen werden ele, slo, pH und pho aufgenommen)
lm.orig.0 <- lm(species_richness ~ ele+slo+pH+pho, data =doubs)
summary(lm.orig.0)

#Modellvereinfachung: slo als am wenigsten signifikante Variable gestrichen
lm.orig.1 <- lm(species_richness ~ ele+pH+pho, data =doubs)
summary(lm.orig.1)

#Modellvereinfachung: pH ist immer noch nicht signifikant und wird gestrichen
lm.orig.2 <- lm(species_richness ~ ele+pho, data =doubs)
summary(lm.orig.2)

#Modelldiagnostik
par(mfrow=c(2,2))
plot(lm.orig.2) #nicht so gut, besonders die Bananenform in der linken obereren Abbildung

#Nach Modellvereinfachung bleiben zwei signifikante Variablen, ele und pho.

#Da das nicht so gut aussieht, versuchen wir es mit dem theoretisch angemesseneren Modell, einem Poisson-GLM.

#Versuch mit glm
glm.orig.0 <- glm(species_richness ~ ele+pho+pH+slo, family = "poisson", data =doubs)
summary(glm.orig.0)

glm.orig.1 <- glm(species_richness ~ ele+pho+slo, family = "poisson", data =doubs)
summary(glm.orig.1)

glm.orig.2 <- glm(species_richness ~ ele+pho, family = "poisson", data =doubs)
summary(glm.orig.2)
plot(glm.orig.2)

#Das sieht deutlich besser aus als beim LM. Wir müssen aber noch prüfen, ob evtl. Overdispersion vorliegt.

if(!require(AER)){install.packages("AER")}
library(AER)
dispersiontest(glm.orig.2) #signifikante Überdispersion

#Ja, es gibt signifikante Overdispersion (obwohl der Dispersionparameter sogar unter 2 ist, also nicht extrem). Wir können nun entweder quasipoisson oder negativebinomial nehmen.

glmq.orig.2 <- glm(species_richness ~ ele+pho, family = "quasipoisson", data =doubs)
summary(glmq.orig.2)

#Parameterschätzung bleiben gleich, aber Signifikanzen sind niedriger als beim GLM ohne Overdispersion.

plot(glmq.orig.2)
```

Sieht gut aus, wir hätten hier also unser finales Modell.

Im Vergleich der beiden Vorgehensweisen (PC-Achsen vs. Umweltdaten direkt) scheint in diesem Fall die direkte Modellierung der Umweltachsen informativer: Man kommt mit zwei Prädiktoren aus, die jeweils direkt für eine der Hauptvariablen stehen – Meereshöhe und Phosphor – zugleich aber jeweils eine grössere Gruppe von Variablen mit hohen Korrelationen inkludieren, im ersten Fall Variablen, die sich im Flusslauf von oben nach unten systematisch ändern, im zweiten Masse der Nährstoffbelastung des Gewässers. Bei der PCA-Lösung kamen drei signifikante Komponenten heraus, die allerdings nicht so leicht zu interpretieren sind. Dies insbesondere, weil in diesem Fall auf der Ebene PC2 vs. PC1 die Mehrzahl der Umweltparameter ungefähr in 45-Grad-Winkeln angeordnet sind. Im allgemeinen Fall kann aber die Nutzung von PC-Achsen durchaus eine gute Lösung sein.

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:18_Statistik6/solution_stat6.1.Rmd-->

# Statistik 7 (18.11.2019)

In Statistik 7 beschäftigen wir uns zunächst damit, wie wir Ordinationsdiagramme informativer gestalten können, etwa durch die Beschriftung der Beobachtunge, post-hoc-Projektion der Prädiktorvariablen oder Response surfaces. Während wir bislang mit «unconstrained» Ordinationen gearbeitet haben, welche die Gesamtvariabilität in den Beobachtungen visualisieren, beschränken die jeweiligen «constrained»-Varianten derselben Ordinationsmethoden die Betrachtung auf den Teil der Variabilität, welcher durch eine Linearkombination der berücksichtigen Prädiktoren erklärt werden kann. Wir beschäftigen uns im Detail mit der Redundanz-Analyse (RDA), der «constrained»-Variante der PCA und gehen einen kompletten analytischen Ablauf mit Aufbereitung, Interpretation und Visualisierung der Ergebnisse am Beispiel eines gemeinschaftsökologischen Datensatzes (Fischgesellschaften und Umweltfaktoren im Jura-Fluss Doubs) durch
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:19_Statistik7/Abstract.Rmd-->

```{r, include=FALSE, purl=F}

knitr::opts_chunk$set(echo = TRUE,include = T, results = "hide",collapse=TRUE, message = FALSE)
```

## Statistik 7 - Demoskript
- **Ordinationen II**
- **(c) Juergen Dengler**

- [Demoscript als Download](19_Statistik7/Statistik 7-Demo_v.04.R)
- Datensatz [Doubs.RData](18_Statistik6/Doubs.RData)
- Funktion [triplot.rda.R](19_Statistik7/triplot.rda.R)


__Interpretation von Ordinationen (Wildi pp. 96 et seq.)__
Wildi pp. 96 et seq.)
```{r eval=FALSE}
#Plot Arten
if(!require(dave)){install.packages("dave")}
library(dave)
ca<-cca(sveg^0.5)

#Plot mit ausgewählten Arten
sel.spec<-c(3,11,23,31,39,46,72,77,96)
snames<-names(sveg[,sel.spec])
snames
sx <- ca$CA$v[sel.spec,1]
sy <- ca$CA$v[sel.spec,2]
plot(ca$CA$u, asp=1)
points(sx,sy,pch=16)
snames <- make.cepnames(snames)
text(sx,sy,snames,pos=c(1,2,1,1,3,2,4,3,1),cex=0.8)

# Plotte post-hoc gefittete Umweltvariablen
sel.sites <- c("pH.peat", "Acidity.peat", "CEC.peat", "P.peat", "Waterlev.max")
ev <-envfit(ca,ssit[,sel.sites])
plot(ev,add=T,cex=0.8)

#Plot "response surfaces" in der CA
plot(ca$CA$u, asp=1)
ordisurf(ca,ssit$pH.peat,add=T)

plot(ca$CA$u, asp=1)
ordisurf(ca,ssit$Waterlev.av,add=T,col="blue")

#Das gleiche für die DCA (geht nicht, da das "detrending" die Distanzmatrix zerstört)
dca <- decorana(sveg,mk=10)
plot(dca$rproj, asp=1)
ordisurf(dca,ssit$pH.peat,add=T)
ordisurf(dca,ssit$Waterlev.av,add=T,col="blue")

#Das gleiche mit NMDS
mde <-vegdist(sveg,method="euclidean")
mmds<-metaMDS(mde,k=2)
if(!require(MASS)){install.packages("MASS")}
library(MASS)
imds<-isoMDS(mde,k=2)

plot(mmds$points)
ordisurf(mmds,ssit$pH.peat,add=T)
ordisurf(mmds,ssit$Waterlev.av,add=T,col="blue")

plot(imds$points)
ordisurf(imds,ssit$pH.peat,add=T)
ordisurf(imds,ssit$Waterlev.av,add=T,col="blue")
```


__Constrained ordination__
```{r eval=FALSE}
#5 Umweltvariablen gewählt, durch die die Ordination constrained werden soll
ssit
summary(ssit)
s5<-c("pH.peat","P.peat","Waterlev.av","CEC.peat","Acidity.peat")
ssit5<-ssit[s5]

data(sveg)
summary(sveg)

#RDA = constrained PCA
rda <-rda(sveg,ssit5)
plot(rda)

#CCA = constrained CA
cca <-cca(sveg,ssit5)
plot(cca)

#Unconstrained and constrained variance
tot <- cca$tot.chi
constr <- cca$CCA$tot.chi
constr/tot
```


__Mehr Details zu RDA aus Borcard et al. (Numerical ecology with R)__
```{r eval=FALSE}
# Doubs Datensatz in den workspace laden
load("Doubs.RData")  
spe
env
spa
summary(spe)
summary(env)
summary(spa)

# Entfernen der Untersuchungsfläche ohne Arten
spe <- spe[-8, ]
env <- env[-8, ]
spa <- spa[-8, ]

# Karten für 4 Fischarten
dev.new(title = "Four fish species", noRStudioGD = TRUE)
par(mfrow = c(2, 2))
plot(spa, asp = 1, col = "brown", cex = spe$Satr, xlab = "x (km)", ylab = "y (km)", main = "Brown trout")
lines(spa, col = "light blue")
plot(spa, asp = 1, col = "brown", cex = spe$Thth, xlab = "x (km)", ylab = "y (km)", main = "Grayling")
lines(spa, col = "light blue")
plot(spa, asp = 1, col = "brown", cex = spe$Alal, xlab = "x (km)", ylab = "y (km)", main = "Bleak")
lines(spa, col = "light blue")
plot(spa, asp = 1, col = "brown", cex = spe$Titi, xlab = "x (km)", ylab = "y (km)", main = "Tench")
lines(spa, col = "light blue")

# Set aside the variable 'dfs' (distance from the source) for 
# later use
dfs <- env[, 1]
# Remove the 'dfs' variable from the env data frame
env2 <- env[, -1]

# Recode the slope variable (slo) into a factor (qualitative) 
# variable to show how these are handled in the ordinations
slo2 <- rep(".very_steep", nrow(env))
slo2[env$slo <= quantile(env$slo)[4]] <- ".steep"
slo2[env$slo <= quantile(env$slo)[3]] <- ".moderate"
slo2[env$slo <= quantile(env$slo)[2]] <- ".low"
slo2 <- factor(slo2, levels = c(".low", ".moderate", ".steep", ".very_steep"))
table(slo2)

# Create an env3 data frame with slope as a qualitative variable
env3 <- env2
env3$slo <- slo2

# Create two subsets of explanatory variables
# Physiography (upstream-downstream gradient)
envtopo <- env2[, c(1 : 3)]
names(envtopo)
# Water quality
envchem <- env2[, c(4 : 10)]
names(envchem)

# Hellinger-transform the species dataset
library(vegan)
spe.hel <- decostand(spe, "hellinger")
spe.hel

# Redundancy analysis (RDA)
## RDA of the Hellinger-transformed fish species data, constrained
## by all the environmental variables contained in env3
(spe.rda <- rda(spe.hel ~ ., env3)) # Observe the shortcut formula
summary(spe.rda)	# Scaling 2 (default)

# Canonical coefficients from the rda object
coef(spe.rda)
# Unadjusted R^2 retrieved from the rda object
(R2 <- RsquareAdj(spe.rda)$r.squared)
# Adjusted R^2 retrieved from the rda object
(R2adj <- RsquareAdj(spe.rda)$adj.r.squared)

## Triplots of the rda results (lc scores)
## Site scores as linear combinations of the environmental variables
dev.new(title = "RDA scaling 1 and 2 + lc", width = 12, height = 6, noRStudioGD = TRUE)
par(mfrow = c(1, 2))
# Scaling 1
plot(spe.rda,scaling = 1,display = c("sp", "lc", "cn"),main = "Triplot RDA spe.hel ~ env3 - scaling 1 - lc scores")
spe.sc1 <- scores(spe.rda, choices = 1:2, scaling = 1, display = "sp")
arrows(0, 0, spe.sc1[, 1] * 0.92,spe.sc1[, 2] * 0.92,length = 0, lty = 1, col = "red")
text(-0.75, 0.7, "a", cex = 1.5)
# Scaling 2
plot(spe.rda, display = c("sp", "lc", "cn"), main = "Triplot RDA spe.hel ~ env3 - scaling 2 - lc scores")
spe.sc2 <- scores(spe.rda, choices = 1:2, display = "sp")
arrows(0, 0, spe.sc2[, 1] * 0.92, spe.sc2[, 2] * 0.92,length = 0,lty = 1,col = "red")
text(-0.82, 0.55, "b", cex = 1.5)


## Triplots of the rda results (wa scores)
## Site scores as weighted averages (vegan's default)
# Scaling 1 :  distance triplot
dev.new(title = "RDA plot", width = 12, height = 6, noRStudioGD = TRUE)
par(mfrow = c(1,2))
plot(spe.rda, scaling = 1, main = "Triplot RDA spe.hel ~ env3 - scaling 1 - wa scores")
arrows(0, 0, spe.sc1[, 1] * 0.92, spe.sc1[, 2] * 0.92, length = 0, lty = 1, col = "red")
# Scaling 2 (default) :  correlation triplot
plot(spe.rda, main = "Triplot RDA spe.hel ~ env3 - scaling 2 - wa scores")
arrows(0, 0, spe.sc2[, 1] * 0.92, spe.sc2[, 2] * 0.92, length = 0, lty = 1, col = "red")

# Select species with goodness-of-fit at least 0.6 in the 
# ordination plane formed by axes 1 and 2
spe.good <- goodness(spe.rda)
sel.sp <- which(spe.good[, 2] >= 0.6)
sel.sp

# Triplots with homemade function triplot.rda(), scalings 1 and 2
source("triplot.rda.R")
dev.new(title = "RDA plot with triplot.rda", width = 12, height = 6, noRStudioGD = TRUE)

par(mfrow = c(1,2))
triplot.rda(spe.rda, site.sc = "lc", scaling = 1, cex.char2 = 0.7, pos.env = 3, 
            pos.centr = 1, mult.arrow = 1.1, mar.percent = 0.05, select.spe = sel.sp)
text(-0.92, 0.72, "a", cex = 2)
triplot.rda(spe.rda, site.sc = "lc", scaling = 2, cex.char2 = 0.7, pos.env = 3, 
            pos.centr = 1, mult.arrow = 1.1, mar.percent = 0.05, select.spe = sel.sp)
text(-2.82, 2, "b", cex = 2)

# Global test of the RDA result
anova(spe.rda, permutations = how(nperm = 999))
# Tests of all canonical axes
anova(spe.rda, by = "axis", permutations = how(nperm = 999))

## Partial RDA: effect of water chemistry, holding physiography
## constant

# Simple syntax; X and W may be in separate tables of quantitative 
# variables
(spechem.physio <- rda(spe.hel, envchem, envtopo))
summary(spechem.physio)

# Formula interface; X and W variables must be in the same 
# data frame
(spechem.physio2 <- rda(spe.hel ~ pH + har + pho + nit + amm + oxy + bod 
        + Condition(ele + slo + dis), data = env2))

# Test of the partial RDA, using the results with the formula 
# interface to allow the tests of the axes to be run
anova(spechem.physio2, permutations = how(nperm = 999))
anova(spechem.physio2, permutations = how(nperm = 999), by = "axis")

# Partial RDA triplots (with fitted site scores) 
# with function triplot.rda
# Scaling 1
dev.new(title = "Partial RDA",width = 12, height = 6, noRStudioGD = TRUE)
par(mfrow = c(1, 2))
triplot.rda(spechem.physio, site.sc = "lc", scaling = 1, 
            cex.char2 = 0.8, pos.env = 3, mar.percent = 0)
text(-0.58, 0.64, "a", cex = 2)

# Scaling 2
triplot.rda(spechem.physio, site.sc = "lc", scaling = 2, cex.char2 = 0.8, 
            pos.env = 3, mult.spe = 1.1, mar.percent = 0.04)
text(-3.34, 3.64, "b", cex = 2)
```

__Variation partioning__
```{r eval=FALSE}
## Variation partitioning with two sets of explanatory variables

# Explanation of fraction labels (two, three and four explanatory 
# matrices) with optional colours
dev.new(title = "Symbols of variation partitioning fractions", width = 6, height = 2.3, noRStudioGD = TRUE)
par(mfrow = c(1, 3), mar = c(1, 1, 1, 1))
showvarparts(2, bg = c("red", "blue"))
showvarparts(3, bg = c("red", "blue", "yellow"))
showvarparts(4, bg = c("red", "blue", "yellow", "green"))

## 1. Variation partitioning with all explanatory variables
##    (except dfs)
(spe.part.all <- varpart(spe.hel, envchem, envtopo))

# Plot of the partitioning results
par(mfrow = c(1, 1))
dev.new(title = "Variation partitioning - all variables", noRStudioGD = TRUE)
plot(spe.part.all, digits = 2, bg = c("red", "blue"),
     Xnames = c("Chemistry", "Physiography"), 
     id.size = 0.7)
```
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:19_Statistik7/Demoskript.Rmd-->

```{r, include=FALSE, purl=F}

knitr::opts_chunk$set(echo = TRUE,include = T, results = "hide",collapse=TRUE, message = FALSE)
```

## Statistik 7 - Demoskript extended
**Ordinationen II
**(c) Juergen Dengler

[Demoscript als Download](19_Statistik7/Statistik 7-Demo_extended_v02.R)
**Datensatz [Doubs.RData](18_Statistik6/Doubs.RData)**
**Funktion [triplot.rda.R](19_Statistik7/triplot.rda.R)**


```{r eval=FALSE}
#Interpretation von Ordinationen (Wildi pp. 96 et seq.)

#Plot Arten
if(!require(dave)){install.packages("dave")}
library(dave)
ca<-cca(sveg^0.5)

#Plot mit ausgewählten Arten
sel.spec<-c(3,11,23,31,39,46,72,77,96)
snames<-names(sveg[,sel.spec])
snames
sx <- ca$CA$v[sel.spec,1]
sy <- ca$CA$v[sel.spec,2]
plot(ca$CA$u, asp=1)
points(sx,sy,pch=16)
snames <- make.cepnames(snames)
text(sx,sy,snames,pos=c(1,2,1,1,3,2,4,3,1),cex=0.8)

# Plotte post-hoc gefittete Umweltvariablen
sel.sites <- c("pH.peat", "Acidity.peat", "CEC.peat", "P.peat", "Waterlev.max")
ev <-envfit(ca,ssit[,sel.sites])
plot(ev,add=T,cex=0.8)

#Plot "response surfaces" in der CA
plot(ca$CA$u, asp=1)
ordisurf(ca,ssit$pH.peat,add=T)

plot(ca$CA$u, asp=1)
ordisurf(ca,ssit$Waterlev.av,add=T,col="blue")

#Das gleiche für die DCA (geht nicht, da das "detrending" die Distanzmatrix zerstört)
dca <- decorana(sveg,mk=10)
plot(dca$rproj, asp=1)
ordisurf(dca,ssit$pH.peat,add=T)
ordisurf(dca,ssit$Waterlev.av,add=T,col="blue")

#Das gleiche mit NMDS
mde <-vegdist(sveg,method="euclidean")
mmds<-metaMDS(mde,k=2)
if(!require(MASS)){install.packages("MASS")}
library(MASS)
imds<-isoMDS(mde,k=2)

plot(mmds$points)
ordisurf(mmds,ssit$pH.peat,add=T)
ordisurf(mmds,ssit$Waterlev.av,add=T,col="blue")

plot(imds$points)
ordisurf(imds,ssit$pH.peat,add=T)
ordisurf(imds,ssit$Waterlev.av,add=T,col="blue")


# Constrained ordination --------------------------------------------------


#5 Umweltvariablen gewählt, durch die die Ordination constrained werden soll
ssit
summary(ssit)
s5<-c("pH.peat","P.peat","Waterlev.av","CEC.peat","Acidity.peat")
ssit5<-ssit[s5]

data(sveg)
summary(sveg)

#RDA = constrained PCA
rda <-rda(sveg,ssit5)
plot(rda)

#CCA = constrained CA
cca <-cca(sveg,ssit5)
plot(cca)

#Unconstrained and constrained variance
tot <- cca$tot.chi
constr <- cca$CCA$tot.chi
constr/tot


## Mehr Details zu RDA aus Borcard et al. (Numerical ecology with R)

# Doubs Datensatz in den workspace laden
load("Doubs.RData")  
spe
env
spa
summary(spe)
summary(env)
summary(spa)

# Entfernen der Untersuchungsfläche ohne Arten
spe <- spe[-8, ]
env <- env[-8, ]
spa <- spa[-8, ]

# Karten für 4 Fischarten
dev.new(title = "Four fish species", noRStudioGD = TRUE)
par(mfrow = c(2, 2))
plot(spa, asp = 1, col = "brown", cex = spe$Satr, xlab = "x (km)", ylab = "y (km)", main = "Brown trout")
lines(spa, col = "light blue")
plot(spa, asp = 1, col = "brown", cex = spe$Thth, xlab = "x (km)", ylab = "y (km)", main = "Grayling")
lines(spa, col = "light blue")
plot(spa, asp = 1, col = "brown", cex = spe$Alal, xlab = "x (km)", ylab = "y (km)", main = "Bleak")
lines(spa, col = "light blue")
plot(spa, asp = 1, col = "brown", cex = spe$Titi, xlab = "x (km)", ylab = "y (km)", main = "Tench")
lines(spa, col = "light blue")

# Set aside the variable 'dfs' (distance from the source) for 
# later use
dfs <- env[, 1]
# Remove the 'dfs' variable from the env data frame
env2 <- env[, -1]

# Recode the slope variable (slo) into a factor (qualitative) 
# variable to show how these are handled in the ordinations
slo2 <- rep(".very_steep", nrow(env))
slo2[env$slo <= quantile(env$slo)[4]] <- ".steep"
slo2[env$slo <= quantile(env$slo)[3]] <- ".moderate"
slo2[env$slo <= quantile(env$slo)[2]] <- ".low"
slo2 <- factor(slo2, levels = c(".low", ".moderate", ".steep", ".very_steep"))
table(slo2)

# Create an env3 data frame with slope as a qualitative variable
env3 <- env2
env3$slo <- slo2

# Create two subsets of explanatory variables
# Physiography (upstream-downstream gradient)
envtopo <- env2[, c(1 : 3)]
names(envtopo)
# Water quality
envchem <- env2[, c(4 : 10)]
names(envchem)

# Hellinger-transform the species dataset
library(vegan)
spe.hel <- decostand(spe, "hellinger")
spe.hel

# Redundancy analysis (RDA) =======================================

## RDA of the Hellinger-transformed fish species data, constrained
## by all the environmental variables contained in env3
(spe.rda <- rda(spe.hel ~ ., env3)) # Observe the shortcut formula
summary(spe.rda)	# Scaling 2 (default)

# Canonical coefficients from the rda object
coef(spe.rda)
# Unadjusted R^2 retrieved from the rda object
(R2 <- RsquareAdj(spe.rda)$r.squared)
# Adjusted R^2 retrieved from the rda object
(R2adj <- RsquareAdj(spe.rda)$adj.r.squared)

## Triplots of the rda results (lc scores)
## Site scores as linear combinations of the environmental variables
dev.new(title = "RDA scaling 1 and 2 + lc", width = 12, height = 6, noRStudioGD = TRUE)
par(mfrow = c(1, 2))
# Scaling 1
plot(spe.rda,scaling = 1,display = c("sp", "lc", "cn"),main = "Triplot RDA spe.hel ~ env3 - scaling 1 - lc scores")
spe.sc1 <- scores(spe.rda, choices = 1:2, scaling = 1, display = "sp")
arrows(0, 0, spe.sc1[, 1] * 0.92,spe.sc1[, 2] * 0.92,length = 0, lty = 1, col = "red")
text(-0.75, 0.7, "a", cex = 1.5)
# Scaling 2
plot(spe.rda, display = c("sp", "lc", "cn"), main = "Triplot RDA spe.hel ~ env3 - scaling 2 - lc scores")
spe.sc2 <- scores(spe.rda, choices = 1:2, display = "sp")
arrows(0, 0, spe.sc2[, 1] * 0.92, spe.sc2[, 2] * 0.92,length = 0,lty = 1,col = "red")
text(-0.82, 0.55, "b", cex = 1.5)


## Triplots of the rda results (wa scores)
## Site scores as weighted averages (vegan's default)
# Scaling 1 :  distance triplot
dev.new(title = "RDA plot", width = 12, height = 6, noRStudioGD = TRUE)
par(mfrow = c(1,2))
plot(spe.rda, scaling = 1, main = "Triplot RDA spe.hel ~ env3 - scaling 1 - wa scores")
arrows(0, 0, spe.sc1[, 1] * 0.92, spe.sc1[, 2] * 0.92, length = 0, lty = 1, col = "red")
# Scaling 2 (default) :  correlation triplot
plot(spe.rda, main = "Triplot RDA spe.hel ~ env3 - scaling 2 - wa scores")
arrows(0, 0, spe.sc2[, 1] * 0.92, spe.sc2[, 2] * 0.92, length = 0, lty = 1, col = "red")

# Select species with goodness-of-fit at least 0.6 in the 
# ordination plane formed by axes 1 and 2
spe.good <- goodness(spe.rda)
sel.sp <- which(spe.good[, 2] >= 0.6)
sel.sp

# Triplots with homemade function triplot.rda(), scalings 1 and 2
source("triplot.rda.R")
dev.new(title = "RDA plot with triplot.rda", width = 12, height = 6, noRStudioGD = TRUE)

par(mfrow = c(1,2))
triplot.rda(spe.rda, site.sc = "lc", scaling = 1, cex.char2 = 0.7, pos.env = 3, 
            pos.centr = 1, mult.arrow = 1.1, mar.percent = 0.05, select.spe = sel.sp)
text(-0.92, 0.72, "a", cex = 2)
triplot.rda(spe.rda, site.sc = "lc", scaling = 2, cex.char2 = 0.7, pos.env = 3, 
            pos.centr = 1, mult.arrow = 1.1, mar.percent = 0.05, select.spe = sel.sp)
text(-2.82, 2, "b", cex = 2)

# Global test of the RDA result
anova(spe.rda, permutations = how(nperm = 999))
# Tests of all canonical axes
anova(spe.rda, by = "axis", permutations = how(nperm = 999))

## Partial RDA: effect of water chemistry, holding physiography
## constant

# Simple syntax; X and W may be in separate tables of quantitative 
# variables
(spechem.physio <- rda(spe.hel, envchem, envtopo))
summary(spechem.physio)

# Formula interface; X and W variables must be in the same 
# data frame
(spechem.physio2 <- rda(spe.hel ~ pH + har + pho + nit + amm + oxy + bod 
                        + Condition(ele + slo + dis), data = env2))

# Test of the partial RDA, using the results with the formula 
# interface to allow the tests of the axes to be run
anova(spechem.physio2, permutations = how(nperm = 999))
anova(spechem.physio2, permutations = how(nperm = 999), by = "axis")

# Partial RDA triplots (with fitted site scores) 
# with function triplot.rda
# Scaling 1
dev.new(title = "Partial RDA",width = 12, height = 6, noRStudioGD = TRUE)
par(mfrow = c(1, 2))
triplot.rda(spechem.physio, site.sc = "lc", scaling = 1, 
            cex.char2 = 0.8, pos.env = 3, mar.percent = 0)
text(-0.58, 0.64, "a", cex = 2)

# Scaling 2
triplot.rda(spechem.physio, site.sc = "lc", scaling = 2, cex.char2 = 0.8, 
            pos.env = 3, mult.spe = 1.1, mar.percent = 0.04)
text(-3.34, 3.64, "b", cex = 2)


#########################################
###---ab hier im Kurs nicht gezeigt---###

# Variance inflation factors (VIF) in two RDAs
# First RDA of this Chapter: all environmental variables
# except dfs
vif.cca(spe.rda)
# Partial RDA physiographic variables only
spechem.physio<-rda(spe.hel, envchem, envtopo)
vif.cca(spechem.physio) 

## Forward selection of explanatory variables
# RDA with all explanatory variables except dfs
spe.rda.all <- rda(spe.hel ~ ., data = env2)
# Global adjusted R^2
(R2a.all <- RsquareAdj(spe.rda.all)$adj.r.squared)

# Forward selection using forward.sel()
if(!require(adespatial)){install.packages("adespatial")}
library(adespatial)
forward.sel(spe.hel, env2, adjR2thresh = R2a.all)

# Forward selection using vegan's ordistep()
# This function allows the use of factors. 
mod0 <- rda(spe.hel ~ 1, data = env2)
step.forward <- 
  ordistep(mod0, 
           scope = formula(spe.rda.all), 
           direction = "forward", 
           permutations = how(nperm = 499)
  )
RsquareAdj(step.forward)

# Backward elimination using vegan's ordistep()
step.backward <-
  ordistep(spe.rda.all, permutations = how(nperm = 499))
# With redundant argument direction = "backward":
# step.backward <-
#  ordistep(spe.rda.all,
#           direction = "backward",
#           permutations = how(nperm = 499)
RsquareAdj(step.backward)

# Forward selection using vegan's ordiR2step()
# using a double stopping criterion (Blanchet et al. 2008a)
# and object env containing only quantitative variables.
step2.forward <- 
  ordiR2step(mod0, 
             scope = formula(spe.rda.all), 
             direction = "forward", 
             R2scope = TRUE,
             permutations = how(nperm = 199)
  )
RsquareAdj(step2.forward)

# Forward selection using vegan's ordiR2step()
# using a double stopping criterion (Blanchet et al. 2008a)
# and object env3 containing a factor.
mod00 <- rda(spe.hel ~ 1, data = env3)
spe.rda2.all <- rda(spe.hel ~ ., data = env3)
step3.forward <- 
  ordiR2step(mod00, 
             scope = formula(spe.rda2.all), 
             direction = "forward", 
             permutations = how(nperm = 199)
  )
RsquareAdj(step3.forward)
# Note that the adjusted R^2 of the complete model is smaller
# than that of the complete RDA with only quantitative
# variables.
# Some information has been lost when transforming the
# quantitative slo variable into a factor with 4 levels.

# Partial forward selection with variable slo held constant
mod0p <- rda(spe.hel ~ Condition(slo), data = env2)
mod1p <- rda(spe.hel ~ . + Condition(slo), data = env2)
step.p.forward <- 
  ordiR2step(mod0p, 
             scope = formula(mod1p), 
             direction = "forward", 
             permutations = how(nperm = 199)
  )


## Parsimonious RDA
(spe.rda.pars <- rda(spe.hel ~ ele + oxy + bod, data = env2))
anova(spe.rda.pars, permutations = how(nperm = 999))
anova(spe.rda.pars, permutations = how(nperm = 999), by = "axis")
(R2a.pars <- RsquareAdj(spe.rda.pars)$adj.r.squared)
# Compare the variance inflation factors
vif.cca(spe.rda.all)
vif.cca(spe.rda.pars)

# Triplots of the parsimonious RDA (with fitted site scores)
dev.new(title = "Parsimonious RDA scaling 1", width = 7, height = 12, noRStudioGD = TRUE)
par(mfrow = c(2, 1))
# Scaling 1
triplot.rda(spe.rda.pars, 
     site.sc = "lc", 
     scaling = 1, 
     cex.char2 = 0.8, 
     pos.env = 2, 
     mult.spe = 0.9, 
     mult.arrow = 0.92, 
     mar.percent = 0.01)

# Scaling 2
triplot.rda(spe.rda.pars, 
     site.sc = "lc", 
     scaling = 2, 
     cex.char2 = 0.8, 
     pos.env = 2, 
     mult.spe = 1.1, 
mar.percent = -0.02)


## Environmental reconstruction (calibration) with RDA

# New (fictitious) objects with fish abundances
# Variables(species) must match those in the original data set in 
# name, number and order
# New site 1 is made from rounded means of species in sites 1 to 15
site1.new <- round(apply(spe[1:15, ], 2, mean))
# New site 2 is made from rounded means of species in sites 16 - 29
site2.new <- round(apply(spe[16:29, ], 2, mean))
obj.new <- t(cbind(site1.new, site2.new))

# Hellinger transformation of the new sites
obj.new.hel <- decostand(obj.new, "hel")

# Calibration
calibrate(spe.rda.pars, obj.new.hel)

# Compare with real values at sites 7 to 9 and 22 to 24: 
env2[7:9, c(1, 9, 10)]
env2[22:24, c(1, 9, 10)]

###---bis hier im Kurs nicht gezeigt---###
##########################################

# Explanation of fraction labels (two, three and four explanatory 
# matrices) with optional colours
dev.new(title = "Symbols of variation partitioning fractions", width = 6, height = 2.3, noRStudioGD = TRUE)
par(mfrow = c(1, 3), mar = c(1, 1, 1, 1))
showvarparts(2, bg = c("red", "blue"))
showvarparts(3, bg = c("red", "blue", "yellow"))
showvarparts(4, bg = c("red", "blue", "yellow", "green"))

## 1. Variation partitioning with all explanatory variables
##    (except dfs)
(spe.part.all <- varpart(spe.hel, envchem, envtopo))

# Plot of the partitioning results
par(mfrow = c(1, 1))
dev.new(title = "Variation partitioning - all variables", noRStudioGD = TRUE)
plot(spe.part.all, digits = 2, bg = c("red", "blue"),
     Xnames = c("Chemistry", "Physiography"), 
     id.size = 0.7)

#########################################
###---ab hier im Kurs nicht gezeigt---###

## 2. Variation partitioning after forward selection of explanatory 
##    variables
# Separate forward selection in each subset of environmental 
# variables
spe.chem <- rda(spe.hel, envchem)
R2a.all.chem <- RsquareAdj(spe.chem)$adj.r.squared
forward.sel(spe.hel, 
            envchem, 
            adjR2thresh = R2a.all.chem, 
            nperm = 9999
)

spe.topo <- rda(spe.hel, envtopo)
R2a.all.topo <- RsquareAdj(spe.topo)$adj.r.squared
forward.sel(spe.hel, 
            envtopo, 
            adjR2thresh = R2a.all.topo, 
            nperm = 9999
)

# Parsimonious subsets of explanatory variables, based on forward 
# selections
names(envchem)
envchem.pars <- envchem[, c(4, 6, 7)]
names(envtopo)
envtopo.pars <- envtopo[, c(1, 2)]

# Variation partitioning
(spe.part <- varpart(spe.hel, envchem.pars, envtopo.pars))
dev.new(title = "Variation partitioning - parsimonious subsets", noRStudioGD = TRUE)
plot(spe.part, digits = 2,  bg = c("red", "blue"), 
     Xnames = c("Chemistry", "Physiography"), 
     id.size = 0.7)

# Tests of all testable fractions
# Test of fraction [a+b]
anova(rda(spe.hel, envchem.pars), permutations = how(nperm = 999))
# Test of fraction [b+c]
anova(rda(spe.hel, envtopo.pars), permutations = how(nperm = 999))
# Test of fraction [a+b+c]
env.pars <- cbind(envchem.pars, envtopo.pars)
anova(rda(spe.hel, env.pars), permutations = how(nperm = 999))
# Test of fraction [a]
anova(rda(spe.hel, envchem.pars, envtopo.pars), 
      permutations = how(nperm = 999)
)
# Test of fraction [c]
anova(rda(spe.hel, envtopo.pars, envchem.pars), 
      permutations = how(nperm = 999))


## 3. Variation partitioning without the 'nit' variable
envchem.pars2 <- envchem[, c(6, 7)]
(spe.part2 <- varpart(spe.hel, envchem.pars2, envtopo.pars))
dev.new(title = "Variation partitioning - parsimonious subset 2", noRStudioGD = TRUE)
plot(spe.part2, digits = 2)


## Two-way MANOVA by RDA

# Creation of a factor 'elevation' (3 levels, 9 sites each)
ele.fac <- gl(3, 9, labels = c("high", "mid", "low"))
# Creation of a factor mimicking 'pH'
pH.fac <- 
  as.factor(c(1, 2, 3, 2, 3, 1, 3, 2, 1, 2, 1, 3, 3, 2, 
              1, 1, 2, 3, 2, 1, 2, 3, 2, 1, 1, 3, 3))
# Is the two-way factorial design balanced?
table(ele.fac, pH.fac)

# Creation of Helmert contrasts for the factors and the interaction
ele.pH.helm <- 
  model.matrix(~ ele.fac * pH.fac, 
               contrasts = list(ele.fac = "contr.helmert", 
                                pH.fac = "contr.helmert"))[, -1]
ele.pH.helm
ele.pH.helm2 <- 
  model.matrix(~ ele.fac + pH.fac, 
               contrasts = list(ele.fac = "contr.helmert", 
                                pH.fac = "contr.helmert"))[, -1]
colnames(ele.pH.helm2)

# Check property 1 of Helmert contrasts : all variables sum to 0
apply(ele.pH.helm, 2, sum)
# Check property 2 of Helmert contrasts: their crossproducts 
# must be 0 within and between groups (factors and interaction)
crossprod(ele.pH.helm)

# Verify multivariate homogeneity of within-group covariance
# matrices using the betadisper() function (vegan package)
# implementing Marti Anderson's testing method (Anderson 2006)

# To avoid the rist of heterogeneity of variances with respect to 
# one factor because of the dispersion in the other (in case of 
# interaction), creation of a factor crossing the two factors, i.e. 
# defining the cell-by-cell attribution of the data
cell.fac <- gl(9, 3) 

spe.hel.d1 <- dist(spe.hel[1:27, ])

# Test of homogeneity of within-cell dispersions
(spe.hel.cell.MHV <- betadisper(spe.hel.d1, cell.fac))
anova(spe.hel.cell.MHV)     # Parametric test (not recommended here)
permutest(spe.hel.cell.MHV)

# Alternatively, test homogeneity of dispersions within each
# factor. 
# These tests ore more robust with this small example because 
# there are now 9 observations per group instead of 3. 
# Factor "elevation"
(spe.hel.ele.MHV <- betadisper(spe.hel.d1, ele.fac))
anova(spe.hel.ele.MHV)     # Parametric test (not recommended here)
permutest(spe.hel.ele.MHV) # Permutation test
# Factor "pH"
(spe.hel.pH.MHV <- betadisper(spe.hel.d1, pH.fac))
anova(spe.hel.pH.MHV)
permutest(spe.hel.pH.MHV) # Permutation test


## Step-by-step procedure using function rda()

# Test the interaction first. Factors ele and pH (columns 1-4)  
# are assembled to form the matrix of covariables for the test.
interaction.rda <- 
  rda(spe.hel[1:27, ], 
      ele.pH.helm[, 5:8], 
      ele.pH.helm[, 1:4])
anova(interaction.rda, permutations = how(nperm = 999))

# Test the main factor ele. The factor pH and the interaction
# are assembled to form the matrix of covariables. 
factor.ele.rda <- 
  rda(spe.hel[1:27, ], 
      ele.pH.helm[, 1:2], 
      ele.pH.helm[, 3:8])
anova(factor.ele.rda, 
      permutations = how(nperm = 999), 
      strata = pH.fac
)

# Test the main factor pH. The factor ele and the interaction
# are assembled to form the matrix of covariables. 
factor.pH.rda <- 
  rda(spe.hel[1:27, ], 
      ele.pH.helm[, 3:4], 
      ele.pH.helm[, c(1:2, 5:8)]) 
anova(factor.pH.rda, 
      permutations = how(nperm = 999), 
      strata = ele.fac)

# RDA with the significant factor ele
ele.rda.out <- rda(spe.hel[1:27, ]~ ., as.data.frame(ele.fac))
# Triplot with "wa" sites related to factor centroids, and species 
# arrows
dev.new(title = "Multivariate ANOVA - elevation", noRStudioGD = TRUE)
plot(ele.rda.out, 
     scaling = 1, 
     display = "wa", 
     main = "Multivariate ANOVA, factor elevation - scaling 1 - 
     wa scores")
ordispider(ele.rda.out, ele.fac, 
           scaling = 1, 
           label = TRUE, 
           col = "blue"
)
spe.sc1 <- 
  scores(ele.rda.out, 
         scaling = 1, 
         display = "species")
arrows(0, 0, 
       spe.sc1[, 1] * 0.3, 
       spe.sc1[, 2] * 0.3, 
       length = 0.1, 
       angle = 10, 
       col = "red"
)
text(
  spe.sc1[, 1] * 0.3, 
  spe.sc1[, 2] * 0.3, 
  labels = rownames(spe.sc1), 
  pos = 4, 
  cex = 0.8, 
  col = "red"
)

## Permutational MANOVA using adonis2()
adonis2(spe.hel[1:27, ] ~ ele.fac * pH.fac, 
        method = "euc", 
        by = "term"
)


## RDA with a single second degree explanatory variable

# Create a matrix of dfs and its orthogonal second degree term 
# using function poly()
dfs.df <- poly(dfs, 2)
colnames(dfs.df) <- c("dfs", "dfs2")
# Verify that the polynomial terms are orthogonal
cor(dfs.df)
# Find out if both variables are significant
forward.sel(spe.hel, dfs.df)

# RDA and test
spe.dfs.rda <- rda(spe.hel ~ ., as.data.frame(dfs.df))
anova(spe.dfs.rda)

# Triplot using "lc" (model) site scores and scaling 2
dev.new(title = "RDA w. 2nd-degree variable - scaling 2", noRStudioGD = TRUE)
triplot.rda(spe.dfs.rda, 
            site.sc = "lc", 
            scaling = 2, 
            plot.sites = FALSE, 
            pos.env = 1, 
            mult.arrow = 0.9, 
            move.origin = c(-0.25, 0), 
            mar.percent = 0)



## Polynomial RDA, second degree, with forward selection
source ('https://raw.githubusercontent.com/zdealveindy/anadat-r/master/scripts/NumEcolR2/polyvars.R')
## among all environmental variables
env.square <- polyvars(env2, degr = 2)
names(env.square)
spe.envsq.rda <- rda(spe.hel ~ ., env.square)
R2ad <- RsquareAdj(spe.envsq.rda)$adj.r.squared
spe.envsq.fwd <- 
  forward.sel(spe.hel, 
              env.square, 
              adjR2thresh = R2ad)
spe.envsq.fwd
envsquare.red <- env.square[, sort(spe.envsq.fwd$order)]
(spe.envsq.fwd.rda <- rda(spe.hel ~., envsquare.red))
RsquareAdj(spe.envsq.fwd.rda)
summary(spe.envsq.fwd.rda)

# Triplot using lc (model) site scores and scaling 2
dev.new(title = "RDA w. 2nd-order variables - scaling 2", noRStudioGD = TRUE)
triplot.rda(spe.envsq.fwd.rda, 
            site.sc = "lc", 
            scaling = 2, 
            plot.sites = FALSE, 
            pos.env = 1, 
            mult.arrow = 0.9, 
            mult.spe = 0.9,
            mar.percent = 0)


## Distance-based redundancy analysis (db-RDA) :  square-rooted
## percentage difference response matrix and 'ele' factor with 
## factor pH and interaction as covariables

# Rename columns of matrix of Helmert contrasts (for convenience)
colnames(ele.pH.helm) <- 
  c("ele1", "ele2", "pH1", "pH2", "ele1pH1", "ele1pH2", 
    "ele2pH1", "ele2pH2" )

# Create the matrix of covariables. MUST be of class matrix, 
# NOT data.frame
covariables <- ele.pH.helm[, 3:8]

# Compute the dissimilarity response matrix with vegans vegdist()
spe.bray27 <- vegdist(spe[1:27, ], "bray")
# or with function dist.ldc() of adespatial
spe.bray27 <- dist.ldc(spe[1:27, ], "percentdiff")

# 1. dbrda() on the square-rooted dissimilarity matrix
bray.ele.dbrda <-  dbrda(
  sqrt(spe.bray27) ~ ele.pH.helm[, 1:2] + Condition(covariables))
anova(bray.ele.dbrda, permutations = how(nperm = 999)) 

# 2. capscale() with raw (site by species) data
# Rename factor (cosmetic for plot)
ele.fac. <- ele.fac
bray.env.cap <- 
  capscale(spe[1:27, ] ~ ele.fac. + Condition(covariables), 
           data = as.data.frame(ele.pH.helm), 
           distance = "bray", 
           add = "lingoes", 
           comm = spe[1:27, ])
anova(bray.env.cap, permutations = how(nperm = 999))
# Plot with "wa" scores to see dispersion of sites around the 
# factor levels
triplot.rda(bray.env.cap, site.sc = "wa", scaling = 1)

# The results of the two analyses are slightly different because
# (1) the test is not performed in the same manner and (2) the 
# correction to make the response matrix Euclidean is not the same.


### NOT IN THE BOOK ###

## Alternative ways of computing db-RDA

# 1. capscale() with raw (site by species) data
# Alternate coding with explicit covariables coming from same
# object as the constraining variables : 
bray.env.capscale <- 
  capscale(spe[1:27, ] ~ ele1 + ele2 + 
             Condition(pH1 + pH2 + ele1pH1 + ele1pH2 + ele2pH1 + ele2pH2), 
           data = as.data.frame(ele.pH.helm), 
           distance = "bray", 
           add = "cailliez", 
           comm = spe[1:27, ])
anova(bray.env.capscale, permutations = how(nperm = 999))

# 2. PCoA with Lingoes (1971) correction
#    Explicit steps
if(!require(ape)){install.packages("ape")}
library(ape)
spe.bray27.lin <- pcoa(spe.bray27, correction = "lingoes") 
spe.bray27.lingoes <- spe.bray27.lin$vectors.cor 
# Test of the factor ele. Factor pH and interaction, Helmert-coded,
# form the matrix of covariables
spe.L.ele.dbrda <- 
  rda(spe.bray27.lingoes, 
      ele.pH.helm[, 1:2], 
      covariables) 
anova(spe.L.ele.dbrda, permutations = how(nperm = 999))

# Same by staying in {vegan} and using wcmdscale() : 
spe.lingoes2 <- wcmdscale(spe.bray27, add = "lingoes") 
anova(rda(spe.lingoes2 ~ ele.pH.helm[, 1:2] + Condition(covariables)))

### END NOT IN THE BOOK ###



# -----------------------------------------------------------------
# The Code It Yourself corner #3

myRDA <- function(Y, X)
{
  
  ## 1. Preparation of the data
  
  Y.mat <- as.matrix(Y)
  Yc <- scale(Y.mat, scale = FALSE)
  
  X.mat <- as.matrix(X)
  Xcr <- scale(X.mat)
  
  # Dimensions
  n <- nrow(Y)
  p <- ncol(Y)
  m <- ncol(X)
  
  ## 2. Computation of the multivariate linear regression
  
  # Matrix of regression coefficients (eq. 11.9)
  B <- solve(t(Xcr) %*% Xcr) %*% t(Xcr) %*% Yc
  
  # Matrix of fitted values (eq. 11.10)
  Yhat <- Xcr %*% B
  
  # Matrix of residuals
  Yres <- Yc - Yhat
  
  
  ## 3. PCA on fitted values
  
  # Covariance matrix (eq. 11.12)
  S <- cov(Yhat)
  
  # Eigenvalue decomposition
  eigenS <- eigen(S)
  
  # How many canonical axes?
  kc <- length(which(eigenS$values > 0.00000001))
  
  # Eigenvalues of canonical axes
  ev <- eigenS$values[1 : kc]
  # Total variance (inertia) of the centred matrix Yc
  trace = sum(diag(cov(Yc)))
  
  # Orthonormal eigenvectors (contributions of response 
  # variables, scaling 1)
  U <- eigenS$vectors[, 1 : kc]
  row.names(U) <- colnames(Y)
  
  # Site scores (vegan's wa scores, scaling 1; eq.11.17)
  F <- Yc %*% U
  row.names(F) <- row.names(Y)
  
  # Site constraints (vegan's 'lc' scores, scaling 1; 
  # eq. 11.18)
  Z <- Yhat %*% U
  row.names(Z) <- row.names(Y)
  
  # Canonical coefficients (eq. 11.19)
  CC <- B %*% U
  row.names(CC) <- colnames(X)
  
  # Explanatory variables
  # Species-environment correlations
  corXZ <- cor(X, Z)
  
  # Diagonal matrix of weights
  D <- diag(sqrt(ev / trace))
  
  # Biplot scores of explanatory variables
  coordX <- corXZ %*% D    # Scaling 1
  coordX2 <- corXZ         # Scaling 2
  row.names(coordX) <- colnames(X)
  row.names(coordX2) <- colnames(X)
  
  # Scaling to sqrt of the relative eigenvalue
  # (for scaling 2)
  U2 <- U %*% diag(sqrt(ev))
  row.names(U2) <- colnames(Y)
  F2 <- F %*% diag(1/sqrt(ev))
  row.names(F2) <- row.names(Y)
  Z2 <- Z %*% diag(1/sqrt(ev))
  row.names(Z2) <- row.names(Y)
  
  # Unadjusted R2
  R2 <- sum(ev/trace)
  # Adjusted R2
  R2a <- 1 - ((n - 1)/(n - m - 1)) * (1 - R2)
  
  
  # 4. PCA on residuals
  # Write your own code as in Chapter 5. It could begin 
  # with : 
  #     eigenSres <- eigen(cov(Yres))
  #     evr <- eigenSres$values
  
  
  # 5. Output
  
  result <- 
    list(trace, R2, R2a, ev, CC, U, F, Z, coordX, 
         U2, F2, Z2, coordX2)
  names(result) <- 
    c("Total_variance", "R2", "R2adj", "Can_ev", 
      "Can_coeff", "Species_sc1", "wa_sc1", "lc_sc1", 
      "Biplot_sc1", "Species_sc2", "wa_sc2", "lc_sc2", 
      "Biplot_sc2") 
  
  result
}


doubs.myRDA <- myRDA(spe.hel, env2)
summary(doubs.myRDA)
# Retrieve adjusted R-square
doubs.myRDA$R2adj

###################################################################



# Canonical correspondence analysis (CCA) =========================

## CCA of the raw fish species data, constrained by all the 
## environmental variables in env3
(spe.cca <- cca(spe ~ ., env3))
summary(spe.cca)	# Scaling 2 (default)

# Unadjusted and adjusted R^2 - like statistics
RsquareAdj(spe.cca)

## CCA triplots (using lc site scores)
dev.new(title = "CCA triplot - lc scores", width = 6, height = 12, noRStudioGD = TRUE)
par(mfrow = c(2, 1))
# Scaling 1: species scores scaled to the relative eigenvalues, 
# sites are weighted averages of the species
plot(spe.cca, 
     scaling = 1, 
     display = c("sp", "lc", "cn"), 
     main = "Triplot CCA spe ~ env3 - scaling 1"
)
text(-2.3, 4.1, "a", cex = 1.5)
# Default scaling 2: site scores scaled to the relative 
# eigenvalues, species are weighted averages of the sites
plot(spe.cca, 
     display = c("sp", "lc", "cn"), 
     main = "Triplot CCA spe ~ env3 - scaling 2")
text(-2.3, 2.6, "b", cex = 1.5)

dev.new(title = "CCA biplot - without species", width = 9, height = 5, noRStudioGD = TRUE)
par(mfrow = c(1, 2))
# CCA scaling 1 biplot without species (using lc site scores)
plot(spe.cca, 
     scaling = 1, 
     display = c("lc", "cn"), 
     main = "Biplot CCA spe ~ env3 - scaling 1"
)
# CCA scaling 2 biplot with species but without sites
plot(spe.cca, 
     scaling = 2, 
     display = c("sp", "cn"), 
     main = "Biplot CCA spe ~ env3 - scaling 2"
)

# Permutation test of the overall analysis
anova(spe.cca, permutations = how(nperm = 999))
# Permutation test of each axis
anova(spe.cca, by = "axis", permutations = how(nperm = 999))


## CCA-based forward selection using vegan's ordistep()

# This function allows the use of factors like 'slo' in env3
cca.step.forward <- 
  ordistep(cca(spe ~ 1, data = env3), 
           scope = formula(spe.cca), 
           direction = "forward", 
           permutations = how(nperm = 199))


## Parsimonious CCA using ele, oxy and bod
spe.cca.pars <- cca(spe ~ ele + oxy + bod, data = env3)
anova(spe.cca.pars, permutations = how(nperm = 999))
anova(spe.cca.pars, permutations = how(nperm = 999), by = "axis")
# R-square like statistics
RsquareAdj(spe.cca.pars)
# Compare variance inflation factors
vif.cca(spe.cca)
vif.cca(spe.cca.pars)

# -----------------------------------------------------------------

## A simple function to compute CCA-based variation partitioning
## with bootstrap adjusted R-square

# This function is limited to two explanatory matrices

varpart.cca <- function(Y, X, W)
{
  # Computation of the three CCA
  yx.cca <- cca(Y, X)
  yw.cca <- cca(Y, W)
  yxw.cca <- cca(Y, cbind(X, W))
  
  # Computation of adjusted inertia
  fract.ab <- RsquareAdj(yx.cca)$adj.r.squared
  fract.bc <- RsquareAdj(yw.cca)$adj.r.squared
  fract.abc <- RsquareAdj(yxw.cca)$adj.r.squared
  fract.a <- fract.abc-fract.bc
  fract.b <- fract.ab-fract.a
  fract.c <- fract.abc-fract.ab
  fract.d <- 1-fract.abc
  
  # Output of results
  res <- matrix(0, 7, 1)
  rownames(res) <- 
    c("[ab]", "[bc]", "[abc]", "[a]", "[b]", "[c]", "[d]")
  colnames(res) <- "Value"
  res[1, 1] <- round(fract.ab, 4)
  res[2, 1] <- round(fract.bc, 4)
  res[3, 1] <- round(fract.abc, 4)
  res[4, 1] <- round(fract.a, 4)
  res[5, 1] <- round(fract.b, 4)
  res[6, 1] <- round(fract.c, 4)
  res[7, 1] <- round(fract.d, 4)
  
  res
}

# -----------------------------------------------------------------
## Three-dimensional interactive ordination plots
## (requires vegan3d package)
if(!require(vegan3d)){install.packages("vegan3d")}
library(vegan3d)

# Plot of the sites only (wa scores)
ordirgl(spe.cca.pars, type = "t", scaling = 1)

# Connect weighted average scores to linear combination scores
orglspider(spe.cca.pars, scaling = 1, col = "purple")

# Plot the sites (wa scores) with a clustering result
# Colour sites according to cluster membership
gr <- cutree(hclust(vegdist(spe.hel, "euc"), "ward.D2"), 4)
ordirgl(spe.cca.pars, 
        type = "t", 
        scaling = 1, 
        ax.col = "black", 
        col = gr + 1
)
# Connect sites to cluster centroids
orglspider(spe.cca.pars, gr, scaling = 1)

# Complete CCA 3D triplot
ordirgl(spe.cca.pars, type = "t", scaling = 2)
orgltext(spe.cca.pars, 
         display = "species", 
         type = "t", 
         scaling = 2, 
         col = "cyan"
)

# Plot species groups (Jaccard dissimilarity, useable in R mode)
gs <- 
  cutree(
    hclust(vegdist(t(spe), method = "jaccard"), "ward.D2"), 
    k = 4)
ordirgl(spe.cca.pars, 
        display = "species", 
        type = "t", 
        col = gs + 1)


## Linear discriminant analysis (LDA)

# Ward clustering result of Hellinger-transformed species data, 
# cut into 4 groups
gr <- cutree(hclust(vegdist(spe.hel, "euc"), "ward.D2"), k = 4)

# Environmental matrix with only 3 variables (ele, oxy and bod)
env.pars2 <- as.matrix(env2[, c(1, 9, 10)])

# Verify multivariate homogeneity of within-group covariance
# matrices using the betadisper() function {vegan}
env.pars2.d1 <- dist(env.pars2)
(env.MHV <- betadisper(env.pars2.d1, gr))
anova(env.MHV)
permutest(env.MHV)	# Permutational test

# Log transform ele and bod
env.pars3 <- cbind(log(env2$ele), env2$oxy, log(env2$bod))
colnames(env.pars3) <- c("ele.ln", "oxy", "bod.ln") 
rownames(env.pars3) <- rownames(env2)
env.pars3.d1 <- dist(env.pars3)
(env.MHV2 <- betadisper(env.pars3.d1, gr))
permutest(env.MHV2)

# Preliminary test :  do the means of the explanatory variable 
# differ among groups?
# Compute Wilk'S lambda test
# First way: with function Wilks.test() of package rrcov, Ï2 test
Wilks.test(env.pars3, gr)
# Second way: with function manova() of stats, which uses
#             an F-test approximation
lw <-  manova(env.pars3 ~ as.factor(gr))
summary(lw, test = "Wilks")


## Computation of LDA - identification functions (on unstandardized 
## variables)

env.pars3.df <- as.data.frame(env.pars3)
(spe.lda <- lda(gr ~ ele.ln + oxy + bod.ln, data = env.pars3.df))
# Alternate coding without formula interface :  
#    spe.lda <- lda(env.pars3.df, gr)
# The result object contains the information necessary to interpret 
# the LDA
summary(spe.lda)

# Display the group means for the 3 variables
spe.lda$means

# Extract the unstandardized identification functions (matrix C, 
# eq. 11.33 in Legendre and Legendre 2012)
(C <- spe.lda$scaling)

# Classification of two new objects (identification)
# A new object is created with two sites: 
#     (1) ln(ele) = 6.8, oxygen = 9 and ln(bod) = 0.8 
# and (2) ln(ele) = 5.5, oxygen = 10 and ln(bod) = 1.0
newo <- data.frame(c(6.8, 5.5), c(9, 10), c(0.8, 1))
colnames(newo) <- colnames(env.pars3)
newo
(predict.new <- predict(spe.lda, newdata = newo))


## Computation of LDA - discrimination functions (on standardized 
## variables)

env.pars3.sc <- as.data.frame(scale(env.pars3.df))
spe.lda2 <- lda(gr ~ ., data = env.pars3.sc)

# Display the group means for the 3 variables
spe.lda2$means

# Extract the classification functions
(C2 <- spe.lda2$scaling)

# Compute the canonical eigenvalues
spe.lda2$svd^2

# Position the objects in the space of the canonical variates
(Fp2 <- predict(spe.lda2)$x)
# alternative way :  Fp2 <- as.matrix(env.pars3.sc) %*% C2

# Classification of the objects
(spe.class2 <- predict(spe.lda2)$class)

# Posterior probabilities of the objects to belong to the groups
# (rounded for easier interpretation)
(spe.post2 <- round(predict(spe.lda2)$posterior, 2))

# Contingency table of prior versus predicted classifications
(spe.table2 <- table(gr, spe.class2))

# Proportion of correct classification (classification success)
diag(prop.table(spe.table2, 1))

# Plot the LDA results using the homemade function plot.lda()
source('https://raw.githubusercontent.com/zdealveindy/anadat-r/master/scripts/NumEcolR2/plot.lda.R')
if(!require(ellipse)){install.packages("ellipse")}
library(ellipse)
dev.new(title = "Discriminant analysis", noRStudioGD = TRUE)
plot.lda(lda.out = spe.lda2, 
         groups = gr, 
         plot.sites = 2, 
         plot.centroids = 1, 
         mul.coef = 2.35)

# LDA with jackknife-based classification (i.e., leave-one-out 
# cross-validation)
(spe.lda.jac <- 
    lda(gr ~ ele.ln + oxy + bod.ln, 
        data = env.pars3.sc, 
        CV = TRUE))
summary(spe.lda.jac)

# Numbers and proportions of correct classification
spe.jac.class <- spe.lda.jac$class
spe.jac.table <- table(gr, spe.jac.class)
# Classification success
diag(prop.table(spe.jac.table, 1))


# NOT IN THE BOOK :  ==============================================
# Example of Legendre and Legendre (2012, p. 683)

grY <- c(1, 1, 1, 2, 2, 3, 3)
x1 <- c(1, 2, 2, 8, 8, 8, 9)
x2 <- c(2, 2, 1, 7, 6, 3, 3)
X <- as.data.frame(cbind(x1, x2))

# Computation of unstandardized identification functions
unstand.lda <- lda(grY ~ ., data = X)

# Computation of standardized discriminant functions
X.sc <- as.data.frame(scale(X))
stand.lda <- lda(grY ~ ., data = X.sc)

# END NOT IN THE BOOK ============================================= 



## Principal response curves (PRC)

# Code from the prc() help file, with additional comments
# Chlorpyrifos experiment and experimental design :  Pesticide
# treatment in ditches (replicated) and followed over from 4 weeks
# before to 24 weeks after exposure 

# Extract the data (available in vegan)
data(pyrifos)

# Create factors for time (week) and treatment (dose). Create an
# additional factor "ditch" representing the mesocosm, for testing 
# purposes
week <-
  gl(11, 12, 
     labels = c(-4, -1, 0.1, 1, 2, 4, 8, 12, 15, 19, 24))
dose <- 
  factor(rep(c(0.1, 0, 0, 0.9, 0, 44, 6, 0.1, 44, 0.9, 0, 6), 
             11))
ditch <- gl(12, 1, length = 132)

# PRC
mod <- prc(pyrifos, dose, week)
mod            # Modified RDA
summary(mod)   # Results formatted as PRC

# PRC plot; at the right of it, only species with large total
# (log-transformed) abundances are reported
logabu <- colSums(pyrifos)
dev.new(title = "PRC", noRStudioGD = TRUE)
plot(mod, select = logabu > 200,main = "PRC")

# Statistical test
# Ditches are randomized, we have a time series, and are only
# interested in the first axis
ctrl <- 
  how(plots = Plots(strata = ditch, type = "free"), 
      within = Within(type = "series"), nperm = 999)
anova(mod, permutations = ctrl, first = TRUE)

## Predictive co-correspondence analysis (CoCA)
if(!require(cocorresp)){install.packages("cocorresp")}
library(cocorresp)

data(bryophyte)
data(vascular)

# Co-correspondence analysis is computed using the function coca()
# The default option is method = "predictive" 
(carp.pred <- coca(bryophyte ~ ., data = vascular))

# Leave-one-out cross-validation
crossval(bryophyte, vascular)

# Permutation test
(carp.perm <- permutest(carp.pred, permutations = 99))

# Only two significant axes: refit
(carp.pred <- coca(bryophyte ~ ., data = vascular, n.axes = 2))

# Extract the site scores and the species loadings used in
# the biplots
carp.scores <- scores(carp.pred)
load.bryo <- carp.pred$loadings$Y
load.plant <- carp.pred$loadings$X

# We have generated two plots. As in ter Braak and Schaffers
# (2004, Fig. 3), in both plots the site scores are derived
# from the vascular plants (carp.scores$sites$X) and the
# species scores are the "loadings with respect to
# normalized site scores"

# Printing options:
?plot.predcoca  

dev.new(title = "Predictive co-correspondence analysis (CoCA)", width = 12, height = 6, noRStudioGD = TRUE)
par(mfrow = c(1, 2))
plot(carp.pred, 
     type = "none", 
     main = "Bryophytes", 
     xlim = c(-2, 3), 
     ylim = c(-3, 2))

points(carp.scores$sites$X, pch = 16, cex = 0.5)
text(load.bryo, 
     labels = rownames(load.bryo), 
     cex = 0.7, 
     col = "red"
) 
plot(carp.pred, 
     type = "none", 
     main = "Vascular plants", 
     xlim = c(-2, 3), 
     ylim = c(-3, 2)
)
points(carp.scores$sites$X, pch = 16, cex = 0.5)
text(load.plant, 
     labels = rownames(load.plant), 
     cex = 0.7, 
     col = "blue"
) 

# Detach package cocorresp to avoid conflicts with ade4:
detach("package:cocorresp", unload = TRUE)
# If not sufficient:
unloadNamespace("cocorresp")


## Canonical correlation analysis (CCorA)

# Preparation of data (transformations to make variable 
# distributions approximately symmetrical)
envchem2 <- envchem
envchem2$pho <- log(envchem$pho)
envchem2$nit <- sqrt(envchem$nit)
envchem2$amm <- log1p(envchem$amm)
envchem2$bod <- log(envchem$bod)
envtopo2 <- envtopo
envtopo2$ele <- log(envtopo$ele)
envtopo2$slo <- log(envtopo$slo)
envtopo2$dis <- sqrt(envtopo$dis)

# CCorA (on standardized variables)
chem.topo.ccora <- 
  CCorA(envchem2, envtopo2, 
        stand.Y = TRUE, 
        stand.X = TRUE, 
        permutations = how(nperm = 999))
chem.topo.ccora
dev.new(title = "Canonical correlation analysis (CCorA)", width = 9, height = 6, noRStudioGD = TRUE)
biplot(chem.topo.ccora, plot.type = "biplot")


## Co-inertia analysis
# PCA on both matrices using ade4 functions
if(!require(ade4)){install.packages("ade4")}
library(ade4)

dudi.chem <- dudi.pca(envchem2, 
                      scale = TRUE, 
                      scannf = FALSE)
dudi.topo <- dudi.pca(envtopo2, 
                      scale = TRUE, 
                      scannf = FALSE)
# Cumulated relative variation of eigenvalues
cumsum(dudi.chem$eig / sum(dudi.chem$eig))
# Cumulated relative variation of eigenvalues
cumsum(dudi.topo$eig / sum(dudi.topo$eig))

# Are the row weights equal in the 2 analyses?
all.equal(dudi.chem$lw, dudi.topo$lw)

# Co-inertia analysis
coia.chem.topo <- coinertia(dudi.chem, dudi.topo, 
                            scannf = FALSE, 
                            nf = 2)
summary(coia.chem.topo)

# Relative variation on first eigenvalue
coia.chem.topo$eig[1] / sum(coia.chem.topo$eig)
# Permutation test
randtest(coia.chem.topo, nrepet = 999)

# Plot results
dev.new(title = "Co-inertia analysis", noRStudioGD = TRUE)
plot(coia.chem.topo, main = "Co-inertia analysis")


# Multiple factor analysis (MFA) ==================================

# MFA on 3 groups of variables : 
# Regroup the 3 tables (Hellinger-transformed species, 
# physiographic variables, chemical variables) 
tab3 <- data.frame(spe.hel, envtopo, envchem)
dim(tab3)
# Number of variables in each group
(grn <- c(ncol(spe), ncol(envtopo), ncol(envchem)))

# Close the previous graphic windows
graphics.off()
# Compute the MFA without multiple plots
if(!require(FactoMineR)){install.packages("FactoMineR")}
library(FactoMineR)

t3.mfa <- MFA(
  tab3,
  group = grn,
  type = c("c", "s", "s"),
  ncp = 2,
  name.group = c("Fish community", "Physiography", "Water quality"),
  graph = FALSE
)
t3.mfa
summary(t3.mfa)
t3.mfa$ind

# Plot the results
dev.new(title = "Partial axes", noRStudioGD = TRUE)
plot(t3.mfa,
     choix = "axes",
     habillage = "group",
     shadowtext = TRUE)
dev.new(title = "Quantitative variables", noRStudioGD = TRUE)
plot(
  t3.mfa,
  choix = "ind",
  partial = "all",
  habillage = "group")
dev.new(title = "Sites", noRStudioGD = TRUE)
plot(t3.mfa,
     choix = "var",
     habillage = "group",
     shadowtext = TRUE)
dev.new(title = "Groups", noRStudioGD = TRUE)
plot(t3.mfa, choix = "group")


# RV coefficients with tests (p-values above the diagonal of 
# the matrix)
rvp <- t3.mfa$group$RV
rvp[1, 2] <- coeffRV(spe.hel, scale(envtopo))$p.value
rvp[1, 3] <- coeffRV(spe.hel, scale(envchem))$p.value
rvp[2, 3] <- coeffRV(scale(envtopo), scale(envchem))$p.value
round(rvp[-4, -4], 6)

# Eigenvalues, scree plot and broken stick model
ev <- t3.mfa$eig[, 1]
names(ev) <- paste("MFA", 1 : length(ev))
source('https://raw.githubusercontent.com/zdealveindy/anadat-r/master/scripts/NumEcolR2/screestick.R')
dev.new(title = "MFA eigenvalues and broken stick model", noRStudioGD = TRUE)
screestick(ev, las = 2)

# Alternative to the standard, automatic MFA plots : 
# Plot only the significant variables (correlations)

# Select the most characteristic variables
aa <- dimdesc(t3.mfa, axes = 1:2, proba = 0.0001)

# Plot
dev.new(title = "MFA :  correlations among significant variables", noRStudioGD = TRUE)
varsig <- 
  t3.mfa$quanti.var$cor[unique(c(rownames(aa$Dim.1$quanti), 
                                 rownames(aa$Dim.2$quanti))), ]
plot(varsig[, 1:2], 
     asp = 1, 
     type = "n", 
     xlim = c(-1, 1),
     ylim = c(-1, 1)
)
abline(h = 0, lty = 3)
abline(v = 0, lty = 3)
symbols(0, 0, circles = 1, inches = FALSE, add = TRUE)
arrows(0, 0, varsig[, 1], varsig[, 2], length = 0.08, angle = 20)
for (v in 1 : nrow(varsig))
{
  if (abs(varsig[v, 1]) > abs(varsig[v, 2]))
  {
    if (varsig[v, 1] >=  0) pos <- 4
    else pos <- 2
  }
  else
  {
    if (varsig[v, 2] >=  0) pos <- 3
    else pos <- 1
  }
  text(varsig[v, 1], varsig[v, 2], 
       labels = rownames(varsig)[v], 
       pos = pos)
}



## RLQ and fourth-corner analyses
data(aravo)
dim(aravo$spe)
dim(aravo$traits)
dim(aravo$env)

# Preliminary analyses: CA, Hill-Smith and PCA
afcL.aravo <- dudi.coa(aravo$spe, scannf = FALSE)
acpR.aravo <- dudi.hillsmith(aravo$env, 
                             row.w = afcL.aravo$lw,
                             scannf = FALSE)
acpQ.aravo <- dudi.pca(aravo$traits, 
                       row.w = afcL.aravo$cw,
                       scannf = FALSE)

# RLQ analysis
rlq.aravo <- rlq(
  dudiR = acpR.aravo, 
  dudiL = afcL.aravo, 
  dudiQ = acpQ.aravo,
  scannf = FALSE)
dev.new(title = "RLQ", noRStudioGD = TRUE)
plot(rlq.aravo)
# Traits by environment crossed table
rlq.aravo$tab


# Since the plots are crowded, one can plot them one by one 
# in large graphical windows.
dev.new(title = "RLQ - site (L) scores", noRStudioGD = TRUE)
s.label(rlq.aravo$lR,  plabels.boxes.draw = FALSE,  ppoints.alpha = 0, psub.text = "a", psub.cex = 2,  psub.position = "topleft")

dev.new(title = "RLQ - species abundances", noRStudioGD = TRUE)
s.label(rlq.aravo$lQ,  plabels.boxes.draw = FALSE,   ppoints.alpha = 0,  psub.text = "b", psub.cex = 2,  psub.position = "topleft")

dev.new(title = "RLQ - environmental variables", noRStudioGD = TRUE)
s.arrow(rlq.aravo$l1, psub.text = "c", psub.cex = 2, psub.position = "topleft")
dev.new(title = "RLQ - species traits", noRStudioGD = TRUE)
s.arrow(rlq.aravo$c1,psub.text = "d",psub.cex = 2, psub.position = "topleft")

# Global test
randtest(rlq.aravo, nrepet = 999, modeltype = 6)

## Fourth-corner analysis (takes time with 49999 permutations!)
fourth.aravo <- fourthcorner(
  tabR = aravo$env, 
  tabL = aravo$spe, 
  tabQ = aravo$traits,
  modeltype = 6,
  p.adjust.method.G = "none", 
  p.adjust.method.D = "none", 
  nrepet = 49999)
# Correction for multiple testing, here using FDR
if(!require(ade4)){install.packages("ade4")}
library(ade4)
fourth.aravo.adj <- p.adjust.4thcorner(
  fourth.aravo,
  p.adjust.method.G = "fdr", 
  p.adjust.method.D = "fdr", 
  p.adjust.D = "global") 

# Plot significant associations
dev.new(title = "Fourth-corner analysis", noRStudioGD = TRUE)
plot(fourth.aravo.adj, alpha = 0.05, stat = "D2")

# Biplot combining RLQ and fourth-corner results
dev.new(title = "Combining RLQ and Fourth-corner results", noRStudioGD = TRUE)
plot(fourth.aravo.adj, 
     x.rlq = rlq.aravo, 
     alpha = 0.05, 
     stat = "D2", 
     type = "biplot")


###################################################################

# RLQ and fourth-corner analyses (Doubs data) ######
summary(fishtraits)
rownames(fishtraits)
names(spe)
names(fishtraits)
tra <- fishtraits[ , 6:15]
tra

# Preliminary analyses: CA, Hill-Smith and PCA
afcL.doubs <- dudi.coa(spe, scannf = FALSE)
acpR.doubs <- dudi.hillsmith(env3,
                             row.w = afcL.doubs$lw,
                             scannf = FALSE)
acpQ.doubs <- dudi.pca(tra, 
                       row.w = afcL.doubs$cw,
                       scannf = FALSE)

# RLQ analysis
rlq.doubs <- rlq(
  dudiR = acpR.doubs, 
  dudiL = afcL.doubs, 
  dudiQ = acpQ.doubs,
  scannf = FALSE)
dev.new(title = "RLQ",width = 12,height = 7,noRStudioGD = TRUE)
plot(rlq.doubs)
# Traits by environment crossed table
rlq.doubs$tab


# Since the plots are crowded, one can plot them one by one 
# in large graphical windows:
dev.new(title = "RLQ - site (L) scores", noRStudioGD = TRUE)
s.label(rlq.doubs$lR, 
        plabels.boxes.draw = FALSE, 
        ppoints.alpha = 0,
        psub.text = "a",
        psub.cex = 2, 
        psub.position = "topleft")
dev.new(title = "RLQ - species abundances", noRStudioGD = TRUE)
s.label(rlq.doubs$lQ, 
        plabels.boxes.draw = FALSE, 
        ppoints.alpha = 0,
        psub.text = "b",
        psub.cex = 2, 
        psub.position = "topleft")
dev.new(title = "RLQ - environmental variables", noRStudioGD = TRUE)
s.arrow(rlq.doubs$l1,
        psub.text = "c",
        psub.cex = 2, 
        psub.position = "topleft")
dev.new(title = "RLQ - species traits", noRStudioGD = TRUE)
s.arrow(rlq.doubs$c1,
        psub.text = "d",
        psub.cex = 2, 
        psub.position = "topleft")

# Global test
randtest(rlq.doubs, nrepet = 999, modeltype = 6)


# Fourth-corner analysis (takes time with 49999 permutations!) ######
?fourthcorner

fourth.doubs2 <- fourthcorner(
  tabR = env3,
  tabL = spe,
  tabQ = tra,
  modeltype = 2,
  p.adjust.method.G = "fdr",
  p.adjust.method.D = "fdr",
  nrepet = 49999
)
fourth.doubs2
summary(fourth.doubs2)

fourth.doubs <- fourthcorner(
  tabR = env2, 
  tabL = spe, 
  tabQ = tra,
  modeltype = 6,
  p.adjust.method.G = "none", 
  p.adjust.method.D = "none", 
  nrepet = 49999)
# Correction for multiple testing, here using FDR
fourth.doubs.adj <- p.adjust.4thcorner(
  fourth.doubs,
  p.adjust.method.G = "fdr", 
  p.adjust.method.D = "fdr", 
  p.adjust.D = "global") 

fourth.doubs.adj
summary(fourth.doubs.adj)

# Plot
dev.new(title = "Fourth-corner analysis", noRStudioGD = TRUE)
plot(fourth.doubs.adj, alpha = 0.05, stat = "D2")
plot(fourth.doubs2, stat = "D2")
plot(fourth.doubs2, stat = "G")

# Biplot combining RLQ and fourth-corner results
dev.new(title = "Combining RLQ and Fourth-corner results", noRStudioGD = TRUE)
plot(fourth.doubs.adj, 
     x.rlq = rlq.doubs, 
     alpha = 0.05, 
     stat = "D2", 
     type = "biplot")

plot(fourth.doubs2,
     x.rlq = rlq.doubs,
     alpha = 0.05,
     stat = "D2",
     type = "biplot")
```
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:19_Statistik7/Demoskirpt_extended.Rmd-->


## Statistik 7: Übungen

### Übung 7.1: RDA (naturwissenschaftlich)

Moordatensatz in **library(dave)** :

- sveg (Vegetationsdaten)
- ssit (Umweltdaten)

**Führt eine RDA mit allen in der Vorlesung gezeigten Schritten durch und interpretiert die Ergebnisse.**

**Von den Umweltvariablen entfallen x.axis & y.axis**

**Für die partielle RDA und die Varianzpartitionierung bildet zwei Gruppen:**

- Physiographie (Waterlev.max, Waterlev.av, Waterlev.min, log.peat.lev, log
  slope.deg)
- Chemie (alle übrigen)
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:19_Statistik7/assigment_stat7.Rmd-->


## Musterlösung Aufgabe 7.1: RDA

- [R-Skript als Download](19_Statistik7/RFiles/Loesung_Uebung_7.1_v.03.R)

- **Übungsaufgabe (hier so ausführlich formuliert, wie dies auch in der Klausur der Fall sein wird)**

- Ladet die library dave, welche den Moordatensatz enthält. sveg beinhaltet presenceabsence-Daten  aller untersuchten Arten in den Plots; ssit beinhaltet 18 metrische Umweltdaten sowie Koordinaten der Plots
- **Führt eine RDA und eine Varianzpartizionierung in die Variablengruppen Physiographie**  (Waterlev.max, Waterlev.av, Waterlev.min, log.peat.lev, log
  slope.deg) **und Chemie** (alle übrigen) **durch.**
- Formuliert abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden.
- Während im Text normalerweise die Variablen ausgeschrieben werden solltet, genügt es hier (da ihr die entsprechenden Infos nicht bekommen habt und nur raten könntet), wenn ihr die Abkürzungen aus dem dataframe nehmt.

### Übung 7.1 - RDA -- Lösung

```{r eval=FALSE}
# Load the Moordatensatz data.
__Moordatensatz laden__ 
if(!require(dave)){install.packages("dave")}
library(dave)
data(sveg)
data(ssit)
summary(sveg)
summary(ssit)
str(ssit)

# x.axis and y.axis vom data frame data frame ssit entfernen
env2 <- ssit[, -c(19,20)]
```
Betrachtung der Daten zeigt, dass die Koordinaten in Spalten 19 und 20 sind, die daraufhin entfernt werden.

```{r eval=FALSE}

# Generiere zwei subset der erklärenden Variablen
# Physiografie (upstream-downstream-Gradient)
envtopo <- env2[, c(11 : 15)]
names(envtopo)
# Chemie
envchem <- env2[, c(1:10,16:18)]
names(envchem)

# Hellinger-transform the species dataset
library(vegan)
spe.hel <- decostand(sveg, "hellinger")
spe.hel
```

Vorstehend wurden die Variablen in die zwei Gruppen Chemistry und Physiography aufgteilt. Die Hellilnger-Transformation wird gemeinhin empfohlen (wobei dahingestellt sei, ob sie auch bei presence-absence-Daten nötig ist).
Die weiteren Analysen führen wir mit der default-Einstellung „Scaling 2“ durch. (Je nach Bedarf bzw. persönlichen Vorlieben könnte auch Scaling 1 genommen werden).


```{r eval=FALSE}
## RDA of the Hellinger-transformed mire species data, constrained
## by all the environmental variables contained in env2


__Redundancy analysis (RDA)__

## RDA der Hellinger-transformireten Moorarten-Daten, constrained
## mit allen Umweltvarialben die in env2 enthalten sind
(spe.rda <- rda(spe.hel ~ ., env2)) # Observe the shortcut formula

summary(spe.rda)	# Skalierung 2 (default)

# Canonical coefficients from the rda object
coef(spe.rda)
# Unadjusted R^2 retrieved from the rda object
(R2 <- RsquareAdj(spe.rda)$r.squared)
# Adjusted R^2 retrieved from the rda object
(R2adj <- RsquareAdj(spe.rda)$adj.r.squared)
```

Man erhält R²adj. = 0.376
Jetzt kann man den Triplot erstellen

```{r eval=FALSE}
## Triplots of the rda results (lc scores)
## Site scores as linear combinations of the environmental variables
dev.new(title = "RDA scaling 1 and 2 + lc",width = 15,height = 6,noRStudioGD = TRUE)
par(mfrow = c(1, 2))

# 1 und 2 Achse
plot(spe.rda, display = c("sp", "lc", "cn"), 
     main = "Triplot RDA spe.hel ~ env2 - scaling 2 - lc scores")
spe.sc2 <- scores(spe.rda, choices = 1:2, display = "sp")
arrows(0, 0, spe.sc2[, 1] * 0.92, spe.sc2[, 2] * 0.92,length = 0,
       lty = 1,col = "red")
text(-0.82, 0.55, "b", cex = 1.5)

#1 und 3 Achse
plot(spe.rda, display = c("sp", "lc", "cn"), choices = c(1,3),
     main = "Triplot RDA spe.hel ~ env2 - scaling 2 - lc scores")
spe.sc2 <- scores(spe.rda, choices =c(1,3), display = "sp")
arrows(0, 0, spe.sc2[, 1] * 0.92, spe.sc2[, 2] * 0.92,length = 0,
       lty = 1,col = "red")
text(-0.82, 0.55, "b", cex = 1.5)


## Triplots of the rda results (wa scores)
## Site scores as weighted averages (vegan's default)
# Scaling 1 :  distance triplot
dev.new(title = "RDA scaling 2 + wa",width = 7,height = 6,noRStudioGD = TRUE)

# Scaling 2 (default) :  correlation triplot
plot(spe.rda, main = "Triplot RDA spe.hel ~ env3 - scaling 2 - wa scores")
arrows(0, 0, spe.sc2[, 1] * 0.92, spe.sc2[, 2] * 0.92, length = 0, lty = 1, col
```

Auswahl der höchstkorrelierten Arten (Grenzwert kann subjektiv nach Bedarf gesetzt werden, hier 0.5).

```{r eval=FALSE}
# Select species with goodness-of-fit at least 0.6 in the 
# ordination plane formed by axes 1 and 2
spe.good <- goodness(spe.rda)
sel.sp <- which(spe.good[, 2] >= 0.6)
sel.sp

# Triplots with homemade function triplot.rda()
source("19_Statistik7/triplot.rda.R")

# Triplots with homemade function triplot.rda(), scaling 2
setwd("S:/pools/n/N-zen_naturmanag_lsfm/FS_Vegetationsanalyse/Lehre (Module)/MSc. Research Methods/Statistik Dengler 2019/DataSets")
source("triplot.rda.R")

dev.new(title = "RDA plot with triplot.rda",width = 7,height = 6, noRStudioGD = TRUE)

triplot.rda(spe.rda, site.sc = "lc", cex.char2 = 0.7, pos.env = 3, 
            pos.centr = 1, mult.arrow = 1.1, mar.percent = 0.05, select.spe = sel.sp)


# Global test of the RDA result
anova(spe.rda, permutations = how(nperm = 999))
# Tests of all canonical axes
anova(spe.rda, permutations = how(nperm = 999))
anova(spe.rda, by = "axis", permutations = how(nperm = 999))
```

Die ersten drei RDA-Achsen sind also signifikant. Man könnte also auch noch eine
Visualisierung von RDA 3 vs. RDA 1 machen.

- **Partielle RDA**
- **Simple syntax; X and W may be in separate tables of quantitative** 
- **variables**

```{r eval=FALSE}
spechem.physio <- rda(spe.hel, envchem, envtopo)
summary(spechem.physio)

# Formula interface; X and W variables must be in the same 
# data frame
(spechem.physio2 <- 
    rda(spe.hel ~ pH.peat + log.ash.perc + Ca_peat +Mg_peat + Na_peat
       + K_peat + Acidity.peat + CEC.peat + Base.sat.perc + P.peat
       + pH.water + log.cond.water + log.Ca.water
       + Condition(Waterlev.max + Waterlev.av + Waterlev.min + log.peat.lev
       + log.slope.deg), data = env2))

# Test of the partial RDA, using the results with the formula 
# interface to allow the tests of the axes to be run
anova(spechem.physio2, permutations = how(nperm = 999))
anova(spechem.physio2, permutations = how(nperm = 999), by = "axis")

# Partial RDA triplots (with fitted site scores) 
# with function triplot.rda
dev.new(title = "Partial RDA",width = 7,height = 6,noRStudioGD = TRUE)

triplot.rda(spechem.physio, site.sc = "lc", scaling = 2, 
            cex.char2 = 0.8, pos.env = 3, mult.spe = 1.1, mar.percent = 0.04)
text(-3.34, 3.64, "b", cex = 2)
```

__Varianzpartitionierung__

```{r eval=FALSE}
## 1. Variation partitioning with all explanatory variables
(spe.part.all <- varpart(spe.hel, envchem, envtopo))

# Plot of the partitioning results
dev.new(title = "Variation partitioning",width = 7,height = 7,noRStudioGD = TRUE)

plot(spe.part.all, digits = 2, bg = c("red", "blue"),
     Xnames = c("Chemistry", "Physiography"), id.size = 0.7)
```
Die durch die erhobenen Umweltvariablen insgesamt erklärte Varianz (37.6%, s.o.) entfällt zu 19.4% auf chemische Variablen, 3.6% auf physiographische Variablen und zu 14.6% auf gemeinsame Erklärung.
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:19_Statistik7/solution_stat7.1.Rmd-->

# Statistik 8 (19.11.2019)

In Statistik 8 lernen die Studierenden Clusteranalysen/Klassifikationen als eine den Ordinationen komplementäre Technik der deskriptiven Statistik multivariater Datensätze kennen. Es gibt Partitionierungen (ohne Hierarchie), divisive und agglomerative Clusteranalysen (die jeweils eine Hierarchie produzieren). Etwas genauer gehen wir auf die k-means Clusteranalyse (eine Partitionierung) und eine Reihe von agglomerativen Clusterverfahren ein. Hierbei hat das gewählte Distanzmass und der Modus für die sukzessive Fusion von Clustern einen grossen Einfluss auf das Endergebnis. Wir besprechen ferner, wie man die Ergebnisse von Clusteranalysen adäquat visualisieren und mit anderen statistischen Prozeduren kombinieren kann.
Im Abschluss von Statistik 8 werden wir dann die an den acht Statistiktagen behandelten Verfahren noch einmal rückblickend betrachten und thematisieren, welches Verfahren wann gewählt werden sollte. Ebenfalls ist Platz, um den adäquaten Ablauf statistischer Analysen vom Einlesen der Daten bis zur Verschriftlichung der Ergebnisse, einschliesslich der verschiedenen zu treffenden Entscheidungen, zu thematisieren.
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:20_Statistik8/Abstract.Rmd-->

```{r, include=FALSE, purl=F}

knitr::opts_chunk$set(echo = TRUE,include = T, results = "hide",collapse=TRUE, message = FALSE)
```

## Statistik 8 - Demoskript
**Cluster-Analysen**
**(c) Juergen Dengler**

- [Demoscript als Download](20_Statistik8/Statistik 8-Demo_v.03.R)
- Datensatz [Doubs.RData](18_Statistik6/Doubs.RData)
- Funktion drawmap.R [drawmap.R](20_Statistik8/drawmap.R)
- Funktion hcoplot.R [hcoplot.R](20_Statistik8/hcoplot.R)


__k-means clustering__

```{r eval=FALSE}
# das Moordatenset aus Wildi...
if(!require(dave)){install.packages("dave")}
library(dave)
pca<-pca(sveg^0.25,cor=T)
ca<-cca(sveg^0.5)

kmeans.1 <- kmeans(sveg,4)
kmeans.1
plot(ca$CA$u, asp=1,col=kmeans.1[[1]])

kmeans.2 <- kmeans(sveg,3)
plot(pca$scores[,1],pca$scores[,2],type="n", asp=1, xlab="PC1", ylab="PC2")
points(pca$scores[,1],pca$scores[,2],pch=18,col=kmeans.2[[1]])
plot(pca$scores[,1],pca$scores[,3],type="n", asp=1, xlab="PC1", ylab="PC3")
points(pca$scores[,1],pca$scores[,3],pch=18,col=kmeans.2[[1]])

# k-means partitioning, 2 to 10 groups
KM.cascade <-cascadeKM(sveg,  inf.gr = 2, sup.gr = 10, iter = 100,criterion = "ssi")
summary(KM.cascade)
KM.cascade$results
KM.cascade$partition

# k-means visualisation
plot(KM.cascade, sortg = TRUE)
```


__Agglomarative Clusteranalyse__
mit Daten und Skripten aus Borcard et al. (2018)

```{r eval=FALSE}
load("Doubs.Rdata")

# Remove empty site 8
spe <- spe[-8, ]
env <- env[-8, ]
spa <- spa[-8, ]
latlong <- latlong[-8, ]
```

__Dendogramme berechnen und ploten__
```{r eval=FALSE}
## Hierarchical agglomerative clustering of the species abundance 

# Compute matrix of chord distance among sites
spe.norm <- decostand(spe, "normalize")
spe.ch <- vegdist(spe.norm, "euc")

# Attach site names to object of class 'dist'
attr(spe.ch, "Labels") <- rownames(spe)

par(mfrow = c(1, 1))

# Compute single linkage agglomerative clustering
spe.ch.single <- hclust(spe.ch, method = "single")
# Plot a dendrogram using the default options
plot(spe.ch.single, labels = rownames(spe), main = "Chord - Single linkage")

# Compute complete-linkage agglomerative clustering
spe.ch.complete <- hclust(spe.ch, method = "complete")
plot(spe.ch.complete, labels = rownames(spe), main = "Chord - Complete linkage")

# Compute UPGMA agglomerative clustering
spe.ch.UPGMA <- hclust(spe.ch, method = "average")
plot(spe.ch.UPGMA, labels = rownames(spe), main = "Chord - UPGMA")

# Compute centroid clustering
spe.ch.centroid <- hclust(spe.ch, method = "centroid")
plot(spe.ch.centroid, labels = rownames(spe),  main = "Chord - Centroid")

# Compute Ward's minimum variance clustering
spe.ch.ward <-hclust(spe.ch, method = "ward.D2")
plot(spe.ch.ward, labels = rownames(spe),  main = "Chord - Ward")

# Compute beta-flexible clustering using cluster::agnes()
# beta = -0.1
spe.ch.beta1 <- agnes(spe.ch, method = "flexible",par.method = 0.55)
# beta = -0.25
spe.ch.beta2 <- agnes(spe.ch, method = "flexible", par.method = 0.625)
# beta = -0.5
spe.ch.beta3 <- agnes(spe.ch, method = "flexible",par.method = 0.75)
# Change the class of agnes objects
class(spe.ch.beta1)
spe.ch.beta1 <- as.hclust(spe.ch.beta1)
class(spe.ch.beta1)
spe.ch.beta2 <- as.hclust(spe.ch.beta2)
spe.ch.beta3 <- as.hclust(spe.ch.beta3)

par(mfrow = c(2, 2))
plot(spe.ch.beta1, labels = rownames(spe), main = "Chord - Beta-flexible (beta=-0.1)")
plot(spe.ch.beta2, labels = rownames(spe), main = "Chord - Beta-flexible (beta=-0.25)")
plot(spe.ch.beta3,  labels = rownames(spe),  main = "Chord - Beta-flexible (beta=-0.5)")

# Compute Ward's minimum variance clustering
spe.ch.ward <- hclust(spe.ch, method = "ward.D2")
plot(spe.ch.ward, labels = rownames(spe), main = "Chord - Ward")
```


__Cophenetic correlations__

```{r eval=FALSE}
# Single linkage clustering
spe.ch.single.coph <- cophenetic(spe.ch.single)
cor(spe.ch, spe.ch.single.coph)

# Complete linkage clustering
spe.ch.comp.coph <- cophenetic(spe.ch.complete)
cor(spe.ch, spe.ch.comp.coph)

# Average clustering
spe.ch.UPGMA.coph <- cophenetic(spe.ch.UPGMA)
cor(spe.ch, spe.ch.UPGMA.coph)

# Ward clustering
spe.ch.ward.coph <- cophenetic(spe.ch.ward)
cor(spe.ch, spe.ch.ward.coph)

# Shepard-like diagrams
par(mfrow = c(2, 2))
plot(
  spe.ch,
  spe.ch.single.coph,
  xlab = "Chord distance",
  ylab = "Cophenetic distance",
  asp = 1,
  xlim = c(0, sqrt(2)),
  ylim = c(0, sqrt(2)),
  main = c("Single linkage", paste("Cophenetic correlation =",
                                   round(
                                     cor(spe.ch, spe.ch.single.coph), 3
                                   )))
)
abline(0, 1)
lines(lowess(spe.ch, spe.ch.single.coph), col = "red")
plot(
  spe.ch,
  spe.ch.comp.coph,
  xlab = "Chord distance",
  ylab = "Cophenetic distance",
  asp = 1,
  xlim = c(0, sqrt(2)),
  ylim = c(0, sqrt(2)),
  main = c("Complete linkage", paste("Cophenetic correlation =",
                                     round(
                                       cor(spe.ch, spe.ch.comp.coph), 3
                                     )))
)
abline(0, 1)
lines(lowess(spe.ch, spe.ch.comp.coph), col = "red")
plot(
  spe.ch,
  spe.ch.UPGMA.coph,
  xlab = "Chord distance",
  ylab = "Cophenetic distance",
  asp = 1,
  xlim = c(0, sqrt(2)),
  ylim = c(0, sqrt(2)),
  main = c("UPGMA", paste("Cophenetic correlation =",
                          round(
                            cor(spe.ch, spe.ch.UPGMA.coph), 3
                          )))
)
abline(0, 1)
lines(lowess(spe.ch, spe.ch.UPGMA.coph), col = "red")
plot(
  spe.ch,
  spe.ch.ward.coph,
  xlab = "Chord distance",
  ylab = "Cophenetic distance",
  asp = 1,
  xlim = c(0, sqrt(2)),
  ylim = c(0, max(spe.ch.ward$height)),
  main = c("Ward", paste("Cophenetic correlation =",
                         round(
                           cor(spe.ch, spe.ch.ward.coph), 3
                         )))
)
abline(0, 1)
lines(lowess(spe.ch, spe.ch.ward.coph), col = "red")
```


__Optimale Anzahl Cluster__
```{r eval=FALSE}
## Select a dendrogram (Ward/chord) and apply three criteria
## to choose the optimal number of clusters

# Choose and rename the dendrogram ("hclust" object)
hc <- spe.ch.ward
# hc <- spe.ch.beta2
# hc <- spe.ch.complete

par(mfrow = c(1, 2))

# Average silhouette widths (Rousseeuw quality index)
Si <- numeric(nrow(spe))
for (k in 2:(nrow(spe) - 1))
{
  sil <- silhouette(cutree(hc, k = k), spe.ch)
  Si[k] <- summary(sil)$avg.width
}

k.best <- which.max(Si)
plot(1:nrow(spe),Si,type = "h",
  main = "Silhouette-optimal number of clusters",
  xlab = "k (number of clusters)",
  ylab = "Average silhouette width")
axis(1,k.best,paste("optimum", k.best, sep = "\n"),
  col = "red",
  font = 2,
  col.axis = "red")
points(k.best,max(Si),pch = 16,col = "red",cex = 1.5)

# Optimal number of clusters according to matrix correlation 
# statistic (Pearson)

# Homemade function grpdist from Borcard et al. (2018)
grpdist <- function(X)
{
  require(cluster)
  veg <- as.data.frame(as.factor(X))
  distgr <- daisy(veg,"gower")
  distgr
} 

kt <- data.frame(k = 1:nrow(spe), r = 0)
for (i in 2:(nrow(spe) - 1)) 
{
  gr <- cutree(hc, i)
  distgr <- grpdist(gr)
  mt <- cor(spe.ch, distgr, method = "pearson")
  kt[i, 2] <- mt
}

k.best <- which.max(kt$r)
plot(kt$k,kt$r,type = "h",
  main = "Matrix correlation-optimal number of clusters",
  xlab = "k (number of clusters)",
  ylab = "Pearson's correlation")
axis(1,k.best,paste("optimum", k.best, sep = "\n"),
  col = "red",font = 2,col.axis = "red")
points(k.best,max(kt$r),pch = 16,col = "red",cex = 1.5)

# Optimal number of clusters according as per indicator species
# analysis (IndVal, Dufrene-Legendre; package: labdsv)
IndVal <- numeric(nrow(spe))
ng <- numeric(nrow(spe))
for (k in 2:(nrow(spe) - 1))
{
  iva <- indval(spe, cutree(hc, k = k), numitr = 1000)
  gr <- factor(iva$maxcls[iva$pval <= 0.05])
  ng[k] <- length(levels(gr)) / k
  iv <- iva$indcls[iva$pval <= 0.05]
  IndVal[k] <- sum(iv)
}

k.best <- which.max(IndVal[ng == 1]) + 1
col3 <- rep(1, nrow(spe))
col3[ng == 1] <- 3

par(mfrow = c(1, 2))
plot(1:nrow(spe),IndVal,
  type = "h",
  main = "IndVal-optimal number of clusters",
  xlab = "k (number of clusters)",
  ylab = "IndVal sum",
  col = col3)
axis(1,k.best,paste("optimum", k.best, sep = "\n"),
  col = "red",font = 2,col.axis = "red")

points(which.max(IndVal),max(IndVal),pch = 16,col = "red",cex = 1.5)
text(28, 15.7, "a", cex = 1.8)

plot(1:nrow(spe),ng,
  type = "h",
  xlab = "k (number of clusters)",
  ylab = "Ratio",
  main = "Proportion of clusters with significant indicator species",
  col = col3)
axis(1,k.best,paste("optimum", k.best, sep = "\n"),
     col = "red",font = 2,col.axis = "red")
points(k.best,max(ng),pch = 16,col = "red",cex = 1.5)
text(28, 0.98, "b", cex = 1.8)
```


__Final dendrogram with the selected clusters__
```{r eval=FALSE}
# Choose the number of clusters
k <- 4
# Silhouette plot of the final partition
spech.ward.g <- cutree(spe.ch.ward, k = k)
sil <- silhouette(spech.ward.g, spe.ch)
rownames(sil) <- row.names(spe)

plot(sil,main = "Silhouette plot - Chord - Ward", cex.names = 0.8,col = 2:(k + 1),nmax = 100)

# Reorder clusters
if(!require(gclus)){install.packages("gclus")}
library("gclus")
spe.chwo <- reorder.hclust(spe.ch.ward, spe.ch)

# Plot reordered dendrogram with group labels
par(mfrow = c(1, 1))
plot(spe.chwo,hang = -1,
  xlab = "4 groups",
  sub = "",
  ylab = "Height",
  main = "Chord - Ward (reordered)",
  labels = cutree(spe.chwo, k = k))
rect.hclust(spe.chwo, k = k)

# Plot the final dendrogram with group colors (RGBCMY...)
# Fast method using the additional hcoplot() function:
# Usage:
# hcoplot(tree = hclust.object,
#   diss = dissimilarity.matrix,
#   lab = object labels (default NULL),
#   k = nb.clusters,
#   title = paste("Reordered dendrogram from",deparse(tree$call),
#   sep="\n"))
source("20_Statistik8/hcoplot.R")
hcoplot(spe.ch.ward, spe.ch, lab = rownames(spe), k = 4)

# Plot the Ward clusters on a map of the Doubs River
# (see Chapter 2)
source("20_Statistik8/drawmap.R")
drawmap(xy = spa,clusters = spech.ward.g, main = "Four Ward clusters along the Doubs River")
```

__Miscellaneous graphical outputs__
```{r eval=FALSE}
# Heat map of the dissimilarity matrix ordered with the dendrogram
heatmap(as.matrix(spe.ch),Rowv =NULL , symm = TRUE,margin = c(3, 3))

# Ordered community table
# Species are ordered by their weighted averages on site scores.
# Dots represent absences.
library(vegan)
or <- vegemite(spe, spe.chwo)
```
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:20_Statistik8/Demoskript.Rmd-->

## Statistik 8: Übungen

### Übung 8.1: Clusteranalyse (sozioökonomisch)

- Datensatz [crime2.csv](20_Statistik8/crime2.csv)

Raten von 7 Kriminalitätsformen pro 100000 Einwohner und Jahr für die Bundesstaaten der USA

**(a)  Führt eine k-means- und eine agglomerative Clusteranalyse eurer Wahl durch.**

**(b) Überlegt in beiden Fällen, wie viele Cluster sinnvoll sind** (k-means z. B.visuelle Betrachtung einer PCA, agglomerative Clusteranalyse z. B. SilhouettePlot).

**(c)** Abschliessend entscheidet euch für eine Clusterung und **vergleicht die erhaltenen Cluster bezüglich der Kriminalitätsformen mittels ANOVA und interpretiert die Cluster entsprechend.**


**Hinweis:**

Wegen der sehr ungleichen Varianzen muss auf jeden Fall eine Standardisierung stattfinden, damit Distanzen zwischen den verschiedenen Kriminalitätsraten sinnvoll berechnet werden können



```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:20_Statistik8/assigment_stat8.Rmd-->


## Musterlösung Aufgabe 8.1: Clusteranalysen

- [R-Skript als Download](20_Statistik8/Loesung_Uebung_8.1.R)

- **Übungsaufgabe (hier so ausführlich formuliert, wie dies auch in der Klausur der Fall sein wird)**

- Ladet den Datensatz crime2.csv. Dieser enthält Raten von 7   Kriminatlitätsformen pro 100000 Einwohner und Jahr für die Bundesstaaten der USA.
- Führt eine k-means- und eine agglomerative Clusteranalyse eurer Wahl durch. Bitte beachet, dass wegen der sehr ungleichen Varianzen in jedem Fall eine Standardisierung stattfinden muss, damit die Distanzen zwischen den verschiedenen Kriminalitätsraten sinnvoll berechnet werden können.
- **Überlegt in beiden Fällen, wie viele Cluster sinnvoll sind** (k-means: z. B. visuelle Betrachtung einer PCA, agglomertive Clusteranalyse: z. B. Silhoutte-Plot).
- Entscheidet euch dann für eine der beiden Clusterungen und vergleicht dann die
erhaltenen Cluster bezüglich der Kriminalitätsformen und interpretiert die Cluster entsprechend.
- Bitte erklärt und begründet die einzelnen Schritte, die ihr unternehmt, um zu diesem Ergebnis zu kommen. Dazu erstellt bitte ein Word-Dokument, in das ihr Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, eure Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere Vorgehen dokumentieren.
- Formuliert abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden.
- **Abzugeben sind am Ende (a) Ein lauffähiges R-Skript; (b) begründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation) und (c) ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit).**


### Übung 8.1 - Clusteranalysen -- Lösung

```{r eval=FALSE}
crime <-read.csv("20_Statistik8/crime2.csv",sep=";")
crime
```

Im mitgelieferten R-Skript habe ich die folgenden Analysen zunächst mit untransformierten, dann mit standardisierten Kriminalitätsraten berechnet. Ihr könnt die Ergebnisse vergleichen und seht, dass sie sehr unterschiedlich ausfallen.

```{r eval=FALSE}
crimez <- crime
crimez[,c(2:8)] <- lapply(crime[, c(2:8)], scale)
crimez
```

„scale“ führt eine Standardisierung (z-Transformation) durch, so dass alle Variablen anschiessen einen Mittelwert von 0 und eine SD von 1 haben, ausgenommen natürlich die 1. Spalte mit den Kürzeln der Bundesstaaten.
Anschliessend wird das SSI-Kriterium getestet und zwar für Partitionierungen von 2 bis 6 Gruppen (wie viele Gruppen man maximal haben will, muss man pragmatisch nach der jeweiligen Fragestelltung entscheiden).

```{r eval=FALSE}
library(vegan)
crimez.KM.cascade <-
 cascadeKM(
 crimez[,c(2:8)],
 inf.gr = 2,
 sup.gr = 6,
 iter = 100,
 criterion = "ssi"
 )
summary(crimez.KM.cascade)
crimez.KM.cascade$results
crimez.KM.cascade$partition

# k-means visualisation
library(cclust)
plot(crimez.KM.cascade, sortg = TRUE)
```

Nach SSI ist die 4-Gruppenlösung die beste, mit dieser wird also weitergerechnet.

```{r eval=FALSE}
# 4 Kategorien sind nach SSI offensichtlich besonders gut
modelz <- kmeans(crimez[,c(2:8)],4)
modelz

#File für ANOVA (Originaldaten der Vorfälle, nicht die ztransformierten)
crime.KM4 <- data.frame(crime,modelz[1])
crime.KM4$cluster <-as.factor(crime.KM4$cluster)
crime.KM4
str(crime.KM4)
```

**Von den agglomerativen Clusterverfahren habe ich mich für Ward’s minimum variance clustering entschieden, da dieses allgemein als besonders geeignet gilt.**

Vor der Berechnung von crime.norm und crime.ch muss man die Spalte mit den
Bundesstaatenkürzeln entfern.

```{r eval=FALSE}
#Agglomerative Clusteranalyse
crime2 <- crime[,-1]
crime.norm <- decostand(crime2, "normalize")
crime.ch <- vegdist(crime.norm, "euc")
# Attach site names to object of class 'dist'
attr(crime.ch, "Labels") <- crime[,1]

#Ward's minimum variance clustering
crime.ch.ward <- hclust(crime.ch, method = "ward.D2")
par(mfrow = c(1, 1))
plot(crime.ch.ward,
 labels = crime[,1],
 main = "Chord - Ward")

# Choose and rename the dendrogram ("hclust" object)
hc <- crime.ch.ward
# hc <- spe.ch.beta2
# hc <- spe.ch.complete
dev.new(
 title = "Optimal number of clusters",
 width = 12,
 height = 8,
 noRStudioGD = TRUE
)
dev.off()
par(mfrow = c(1, 2))


# Average silhouette widths (Rousseeuw quality index)
library(cluster)
Si <- numeric(nrow(crime))
for (k in 2:(nrow(crime) - 1))
{
 sil <- silhouette(cutree(hc, k = k), crime.ch)
 Si[k] <- summary(sil)$avg.width
}
k.best <- which.max(Si)
plot(
 1:nrow(crime),
 Si,
 type = "h",
 main = "Silhouette-optimal number of clusters",
 xlab = "k (number of clusters)",
 ylab = "Average silhouette width"
)
axis(
 1,
 k.best,
 paste("optimum", k.best, sep = "\n"),
 col = "red",
 font = 2,
 col.axis = "red"
)
points(k.best,
 max(Si),
 pch = 16,
 col = "red",
 cex = 1.5
)
```

Demnach wären beim Ward’s-Clustering nur zwei Gruppen die optimale Lösung.

**Für die Vergleiche der Bundesstaatengruppen habe ich mich im Folgenden für die k-meansClusterung mit 4 Gruppen entschieden.**

Damit die Boxplots und die ANOVA direkt interpretierbar sind, werden für diese, anders als für die Clusterung, die untransformierten Incidenz-Werte verwendet (also crime statt crimez). Die Spalte mit der Clusterzugehörigkeit im Fall von k-means mit 4 Clustern hängt man als Spalte an (Achtung: muss als Faktor definiert werden!).

Anschliessend kann man die 7 ANOVAs rechnen, die Posthoc-Vergleiche durchführen und die zugehörigen Boxplots mit Buchstaben für die homogenen Gruppen erzeugen. Sinnvollerweise gruppiert man die Abbildungen gleich, z. B. je 2 x 2. Das Skript ist hier simple für jede Verbrechensart wiederholt (nur die erste und letzte gezeigt). Erfahrenere R-Nutzer können das Ganze hier natürlich durch eine Schleife abkürzen.

```{r eval=FALSE}
library(multcomp)
par(mfrow=c(2,2))
ANOVA.Murder <- aov(Murder~cluster,data=crime.KM4)
summary (ANOVA.Murder)
letters <- cld(glht(ANOVA.Murder, linfct=mcp(cluster="Tukey")))
boxplot(Murder~cluster, data=crime.KM4, xlab="Cluster",
ylab="Murder")
mtext(letters$mcletters$Letters, at=1:4)

ANOVA.Vehicle <- aov(Vehicle~cluster,data=crime.KM4)
summary (ANOVA.Vehicle)
letters <- cld(glht(ANOVA.Vehicle, linfct=mcp(cluster="Tukey")))
boxplot(Vehicle~cluster, data=crime.KM4, xlab="Cluster",
ylab="Vehicle")
mtext(letters$mcletters$Letters, at=1:4)
```

Die Boxplots erlauben jetzt auch eine Beurteilung der Modelldiagnostik: sind die Residuen hinreichen normalverteilt (symmetrisch) und sind die Varianzen zwischen den Kategorien einigermassen ähnlich. Mit der Symmetrie/Normalverteilung sieht es OK aus. Die Varianzhomogenität ist nicht optimal – meist deutlich grössere Varianz bei höheren Mittelwerten. Eine log-Transformation hätte das verbessert und könnte hier gut begründet werden. Da die p-Werte sehr niedrig waren und die Varianzheterogenität noch nicht extrem war, habe ich aber von einer Transformation abgesehen, da jede Transformation die Interpretation der Ergebnisse erschwert. Jetzt muss man nur noch herausfinden, welche Bundesstaaten überhaupt zu welchem der vier
Cluster gehören, sonst ist das ganze schöne Ergebnis nutzlos. Z. B. kann man in R auf den Dataframe clicken und ihn nach cluster sortieren.
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:20_Statistik8/solution_stat8.1.Rmd-->

# Raumanalyse 1 (25.11.2019)

Die erste Übung zur Raumanalyse illustriert das einfache Laden und Anzeigen von Geodaten im Vektor- und Raster-Datenformat. Zusätzlich veranschaulicht die Übung den Umgang mit Koordinatensystemen sowie die Vektor-Raster-Konvertierung. Einfach erste Analysen umfassen den Spatial Join (Annotieren von Punkten mit Attributen von die Punkte einbettenden Vektordaten) sowie Puffer-Operationen. Zum Abschluss thematisiert die Übung die Aggregationsabhängigkeit räumlicher Daten durch die Illustration des Modifiable Areal Unit Problem (MAUP). Inhaltlich orientiert sich die Übung an Bodeneigenschaften für den Untersuchungsraum Schweiz.
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:21_RaumAn1/Abstract.Rmd-->

```{r, include=FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = FALSE,include = TRUE,message = FALSE, collapse=TRUE, warning=FALSE, results = "markup", fig.width = 7, fig.height = 5) 
```


## Übung: Spatial Join, Puffer und MAUP


### Thematische Einbettung

Mit dieser Übung wirst Du Schritt fürfür Schritt an einfache Methoden der Raumanalyse herangeführt. Dies umfasst das Laden und Plotten von Vektor- und Rasterdaten sowie den wichtigen Umgang mit Koordinatensystemen. Darauf folgen einige Übungen zur räumlichen Anreicherung von Punktdaten unter Verwendung des Konzepts des "Spatial Join". Dazu arbeiten wir mit einem Datensatz aus Punktstichproben zur Wasserverfügbarkeit in Schweizer Böden. Es soll untersucht werden, ob ein Zusammenhang besteht zwischen der an den Punkten gemessenen Wasserverfügbarkeit und dem Join-Layer Bodeneignung/Skelettanteil. Im zweiten Teil der Übung wirst Du die Messwerte zur Wasserverfügbarkeit auf unterschiedlichen räumlichen Skalen aggregieren (Kantone und Bezirke) und prüfen, ob ggf. das Modifiable Areal Unit Problem auftritt. 

### Vorbereitung

Es gibt bereits eine Vielzahl von Packages um in R mit räumlichen Daten zu arbeiten, die ihrerseits wiederum auf weiteren Packages basieren (Stichwort `dependencies`). 

- Für *Vektordaten* dominierte lange das Package `sp`, welches nun aber schrittweise durch `sf` abgelöst wird. Wir werden wenn immer möglich mit `sf` arbeiten und nur in Ausnahmefällen auf andere Packages zurück greifen.
- Für *Rasterdaten* exisitert das Package `raster`

Für die Integration von Vektor und Rasterdaten existiert das Package `stars`: **S**patio**t**emporal **A**rrays for **R**aster and **V**ector Datacubes. Diese Tools sind teilweise sehr gut dem Tidyverse-Workflow (`group_by`, `mutate`, `summarise`, `%>%`) integriert. Lade zu beginn die folgenden notwenigen Packages (installiere die fehlenden Packages mit `install.package("packagename")`).
 
```{r warning=FALSE, echo = TRUE}
library(sf)
library(tidyverse)
library(stars)
library(raster)
```


### Aufgabe 1: Daten runterladen und importieren

```{r}
## -- Aufgabe 1: Daten runterladen und importieren -- ##
```

Lade zunächst die Datensätze unter folgenden Links herunter und importiere sie mit dem Befehl `read_sf()` in R:

- [bodeneignung_skelett.gpkg](21_RaumAn1/data/bodeneignung_skelett.gpkg): Datensatz des Bundesamt für Landwirtschaft, modifiziert ([weitere Informationen](https://www.blw.admin.ch/blw/de/home/politik/datenmanagement/geografisches-informationssystem-gis/download-geodaten.html))
- [kantone.gpkg](21_RaumAn1/data/kantone.gpkg): Ein Datensatz der Swisstopo, modifiziert ([weitere Informationen](https://shop.swisstopo.admin.ch/de/products/landscape/boundaries3D))
- [bezirke.gpkg](21_RaumAn1/data/bezirke.gpkg): Ein Datensatz der Swisstopo, modifiziert ([weitere Informationen](https://shop.swisstopo.admin.ch/de/products/landscape/boundaries3D))
- [wasserverfuegbarkeit_boden.gpkg](21_RaumAn1/data/wasserverfuegbarkeit_boden.gpkg): Ein Datensatz der WSL, modifiziert ([weiteren Informationen](https://www.wsl.ch/de/ueber-die-wsl/programme-und-initiativen/abgeschlossene-programme-und-grossprojekte/wuk.html))
  
Es handelt sich um Geodatensätze im Format Geopackage ("\*.gpkg"), eine alternatives Datenformat zum bekannteren Format "Shapefiles". Lade nun die Datensätze wie folgt ein:

```{r, echo = TRUE}

# Pfad muss natürlich angepasst werden
wasser <- read_sf("21_RaumAn1/data/wasserverfuegbarkeit_boden.gpkg")
kantone <- read_sf("21_RaumAn1/data/kantone.gpkg")
bezirke <- read_sf("21_RaumAn1/data/bezirke.gpkg") 
skelettgehalt <- read_sf("21_RaumAn1/data/bodeneignung_skelett.gpkg")

```


Schau Dir die importierten Datensätze an, nutzt dafür `View()`, `str()`, `class()`. Studiere ausserdem die weiteren Informationen zu den Datensätzen.


### Aufgabe 2: Daten Visualisieren

```{r}
## -- Aufgabe 2: Daten Visualisieren -- ##
```

Wir hatten anfangs erwähnt, dass Geodaten mit `sf` und `raster` sich teilweise sehr schön in die bekannten Tidyverse workflows integrieren lassen. Das merkt man schnell, wenn man die Daten visualisieren möchte. In InfoVis 1 & 2 haben wir intensiv mit `ggplot2` gearbeitet und dort die Layers `geom_point()` und `geom_line()` kennen gelernt. Zusätzlich beinhaltet `ggplot` die Möglichkeit, mit `geom_sf()` Vektordaten direkt und sehr einfach zu plotten. Führe die angegebenen R-Befehle aus und studiere die entstehenden Plots. Welche Unterschiede findest Du? Wie erklärst Du diese Unterschiede?

```{r echo = TRUE}
ggplot(bezirke) + 
  geom_sf()
```

```{r echo = TRUE}
ggplot(wasser) + 
  geom_sf()
```




### Aufgabe 3 Koordinatensysteme *zuweisen*

```{r}
## -- Aufgabe 3 Koordinatensysteme zuweisen -- ##
```


In den obigen beiden sehr einfachen Kartogrammen fallen verschiedene Dinge auf: 

- die X/Y Achsen weisen zwei ganz unterschiedlichen Zahlenbereiche auf (vergleiche die Achsenbeschriftungen), und
- der Umriss der Schweiz scheint im zweiten Plot "gestaucht" zu sein.

Dies hat natürlich damit zu tun, dass die beiden Datensätze in unterschiedlichen Koordinatensystemen erfasst wurden. Koordinatensysteme werden mit CRS (**C**oordinate **R**eference **S**ystem) abgekürzt. Mit `st_crs()` könnnen die zugewiesenen Koordinatensysteme abgefragt werden.

```{r, echo = TRUE}
st_crs(wasser)
st_crs(bezirke)
```

Leider sind in unserem Fall keine Koordinatensysteme zugewiesen. Mit etwas Erfahrung kann man das Koordinatensystem aber erraten, so viele kommen nämlich gar nicht in Frage. Am häufigsten trifft man hierzulande eines der drei folgenden Koordinatensysteme an:

- [CH1903 LV03](https://epsg.io/21781): das alte Koordinatensystem der Schweiz
- [CH1903+ LV95](https://epsg.io/2056): das neue Koordinatensystem der Schweiz
- [WGS84](https://epsg.io/4326): ein häufig genutztes weltumspannendes geodätisches Koordinatensystem, sprich die Koordinaten werden in Länge und Breite angegeben (Lat/Lon).

Nun gilt es, anhand der Koordinaten die in der Spalte `geometry` ersichtlich sind das korrekte Koordinatensystem festzustellen. Wenn man sich auf [epsg.io/map](https://epsg.io/map#srs=4326&x=8.407288&y=46.773731&z=9&layer=streets) die Schweiz anschaut, kann man die Koordinaten in verschiedenen Koordinatensystem betrachten.

**Bedienungshinweise**:

![Koordinanten (des Fadenkreuzes) werden im ausgewählten Koordinatensystem dargestellt](21_RaumAn1/koordinatenpaar.jpg)



![Das Koordinatensystem, in welchem die Koordinaten dargestellt werden sollen, kann mit "Change" angepasst werden](21_RaumAn1/koordinatensystem.jpg)

![Für Enthusiasten: Schau Dir die Schweiz in verschiedenen Koordinatensystemen an, in dem Du auf "Reproject Map" klickst](21_RaumAn1/reproject.jpg)


Wenn man diese Koordinaten mit den Koordinaten unserer Datensätze vergleicht, dann ist schnell klar, dass es sich beim Datensatz `wasser` um das Koordinatensystem WGS84 handelt und bei `bezirke` das Koordinatensystem CH1903+ LV95. Diese Koordinatensyteme weisen wir nun mit `st_set_crs()` und dem entsprechenden *EPSG*-Code (siehe die jeweiligen Links) zu.

```{r, echo = TRUE}
wasser <- st_set_crs(wasser, 4326)
bezirke <- st_set_crs(bezirke, 2056)

# zuweisen mit st_set_crs(), abfragen mit st_crs()
st_crs(wasser)
```

Weise auch für die anderen Datensätze (`kantone` und `skelettgehalt`) das *korrekte* Koordinatensytem zu.


```{r, warning=FALSE}
kantone <- st_set_crs(kantone, 2056)
skelettgehalt <- st_set_crs(skelettgehalt, 2056)
```

Jetzt wo das CRS der Datensätze bekannt ist, können diese in einem gemeinsamen Plot visualisiert werden, `ggplot` kümmert sich darum die unterschiedlichen Koordinatensysteme zu vereinheitlichen. Probier das aus, indem du `kantone` und `wasser` in einem `ggplot` kombinierst.

Die Achsen werden dann immer in WGS84 beschriftet. Wenn das stört, kann man `coord_sf(datum = 2056)` in einem weiteren Layer spezifizieren. Oder aber man blendet die Achsenbeschriftung mit `theme_void()` komplett aus. Versuche beide Varianten.

```{r}
ggplot() + 
  geom_sf(data = kantone) +
  geom_sf(data = wasser)
```




### Aufgabe 4: Koordinatensyteme *transformieren*

```{r}
## -- Aufgabe 4: Koordinatensyteme transformieren -- ##
```

In der vorherigen Übung haben wir das bestehende Koordinatensystem *zugewiesen*. Dabei haben wir die bestehenden Koordinaten (in der Spalte `geom`) *nicht* manipuliert. Ganz anders ist eine *Transformation* der Daten von einem Koordinatensystem in das andere. Bei einer Transformation werden die Koordinaten in das neue Koordinatensystem umgerechnet und somit manipuliert. Aus praktischen Gründen wollen <!-- Was sind diese Gründe? genauer erläutern? -->
wir all unsere Daten ins neue Schweizer Koordinatensystem CH1903+ LV95 transfomieren. Transformiere den Datensatz `wasser` mit `st_transform()`in CH1903+ LV95, nutze dafür den korrekten EPSG-Code.

Vor der Transformation (betrachte die Spalte `geom` sowie die Attribute `epsg (SRID)` und `proj4string`): 

```{r, echo = TRUE}
wasser
```


```{r}
wasser <- st_transform(wasser, 2056)
```


Nach der Transformation (betrachte die Spalte `geom`):

```{r, echo = TRUE}
wasser
```


### Aufgabe 5:

```{r}
## -- Aufgabe 5 -- ##
```

Wir wollen nun wissen, ob die Wasserverfügbarkeit im Boden mit dem Skelettgehalt zusammen hängt. Dazu nutzen wir die GIS-Technik *Spatial Join*, die in der Vorlesung beschrieben wurde. In `sf` können wir Spatial Joins mit der Funktion `st_join` durchführen, dabei gibt es nur `left` sowie `inner`-Joins (vgl. PrePro 1 & 2). So müssen die Punkte "Links", also an erste Stelle aufgeführt werden, da wir ja Attribute *an* die Punkte anheften wollen.

```{r, echo = FALSE}
wasser_skelett <- st_join(wasser,skelettgehalt)
wasser_skelett

```

Führe den obigen Spatial Join aus und erstelle anschliessend einen Boxplot pro Skelett-Kategorie. Für `ggplot` boxplots ist es sinnvoll, den Skelettgehalt vorgängig von `numeric` in `factor` zu konvertieren (falls Du nicht mehr weisst weshalb, schau nochmals nach in PrePro und InfoVis).

Nun haben wir das Ziel der Aufgabe erreicht und die Messpunkte durch räumliche Zusatzinformation aufgewertet. Wir werden das Resultat an dieser Stelle aber nicht weiter interpretieren, dass wäre Teil einer Bodenkunde Vorlesung.

```{r}

wasser_skelett %>%
  mutate(SKELETT = factor(SKELETT)) %>%
  ggplot() +  
  geom_boxplot(aes(SKELETT,wasserverfuegbarkeit))
```



### Aufgabe 6: Spatial Join mit Flächen

```{r}
## -- Aufgabe 6: Spatial Join mit Flächen -- ##
```

In der letzten Aufgabe haben wir für jede Probe aus `wasser` den Skelettgehalt des darunterliegenden Polygons ermittelt. Für Proben, die gerade an der Grenze zu einem Polygon mit einem anderen Skelettgehalt liegen ist dieser Wert aber nicht sehr aussagekräftig. 

So könnte es zum Beispiel wichtiger sein zu wissen, was *der dominierende Skelettgehalt innerhalb eines 2 km Radius* um die Probe ist. In den kommenden Teilaufgaben lösen wir diese Herausforderung.

#### Teilaufgabe A: Punkte mit Puffer versehen

Dafür müssen wir die Punkte mit einem Puffer versehen. Dies erreichen wir mit `st_buffer()`. Erstelle einen Datensatz `wasser_2km`, in dem jeder Punkt mit 2'000 m gepuffert wurde. Visualisiere dann diesen Datensatz. Beachte, dass es sich nun nicht mehr um Punkte, sondern um Flächen handelt (`POLYGON`).

```{r}
wasser_2km <- st_buffer(wasser,2000)
```

```{r}
ggplot(wasser_2km) + geom_sf(fill = "blue")
```



#### Teilaufgabe B: Vektordatensatz in Raster konvertieren

Um Flächen miteinander zu verrechnen ("Was ist der dominierende Skelettgehalt im Umkreis von 2km?") ist es einfacher, wenn der Skelettgehalt-Datensatz im Raster-Datenformat daher kommt. Dazu wandeln wir den Vektordatensatz `skelettgehalt` mit einer Vektor-nach-Raster-Konvertierung in den Rasterdatensatz `skelett_raster`. Hierzu brauchen wir die Funktion `fasterize()` (**f**ast **rasterize**) aus der gleichnamigen Library. Installiere diese Library (wenn nötig) und importiere sie in die aktuelle Session mit `library(fasterize)`.

In einem ersten Schritt müssen wir eine Raster-Vorlage erstellen, welche dazu dient, die räumliche Ausdehnung und die Auflösung (Zellengrösse) des resultierenden Datensatzes festzulegen. 


```{r, echo = TRUE}
library(fasterize)

raster_template <- raster(extent(skelettgehalt), resolution = 1000)
```

Danach wird mit `fasterize` der Polygon-Datensatz in ein Raster konvertiert. Mit `field = ` kann festgelegt werden, aus welcher Spalte die Werte des Output Datensatzes entnommen werden sollten. GIS-Experten werden sich erinnern, dass im Gegensatz zu Flächen in einem Vektordatensatz, welche viele verschiedene Attributen haben können, ein Raster nur noch ein Attribut (hier `SKELETT`).

```{r, echo = TRUE}
skelett_raster <- fasterize(skelettgehalt,raster_template,field = "SKELETT")

ggplot() + 
  geom_stars(data = st_as_stars(skelett_raster)) +
  coord_equal() # geom_stars() ist (noch) nicht so clever wie geom_sf() das CRS wird nicht berücksichtigt
```

#### Teilaufgabe C: Rasterwerte extrahieren

Mit `raster::extract()` könnnen nun die Rasterwerte aus dem Rasterdatensatz extrahiert werden. In `fun = ` kann festgelegt werden, ob und mit welcher Funktion die vielen Rasterzellen pro Polygon aggregiert werden sollen. Wir möchten nur den häufigsten Wert zurück erhalten, sprich den [Modus](https://de.wikipedia.org/wiki/Modus_(Statistik)) dafür gibt es in `R` leider keine eingebaute Funktion, weshalb wir unsere eigene basteln müssen:

```{r, echo = TRUE}
mode <- function(x,na.rm = FALSE) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}
```

Nun können wir mit `raster::extract()` den Modus in jedem Puffer berechnen.

```{r, echo = TRUE}
wasser_skelett$skelett_mode <- extract(skelett_raster,wasser_2km,fun = mode)[,1]
```

Jetzt könnnen wir prüfen, wie oft die beiden Join-Varianten übereinstimmen. Erstelle dazu einen Facet-Plot, indem für jede Skelett-Kategorie die Modus-Kategorie (sprich die häufigste Kategorie) im den Punkt umgebenden Puffer darstellt.

```{r}
ggplot(wasser_skelett) + 
  geom_bar(aes(skelett_mode)) + 
  labs(x = "Skelettgehalt gem. Variante 1", y = "Anzahl",
       title = "Vergleich zwischen den beiden Join Varianten") +
  facet_wrap(~SKELETT,labeller = label_both)
```


### Aufgabe 7: Spatial Join mit Kantone, Bezirke


```{r}
## -- Aufgabe 7: Spatial Join mit Kantone, Bezirke -- ##
```

Zum Abschluss der Übung wenden wir uns nun noch der Aggregationsabhängigkeit von Geodaten zu. Dazu wollen wir die Daten zur Wasserverfügbarkeit auf verschiedenen Massstäben aggregieren. Als Aggregationseinheiten verwenden wir zwei politische Gliederungen der Schweit - Kantone und Bezirke. Die Frage stellt sich, ob die Daten ev. das MAUP illustrieren.

Hier könnten wir wir nochmals `st_join()` verwenden, aber da wir diesmal Polygone im Fokus haben (Bezirke, Kantone) und mehrere Punkte in einem Polygon vorkommen können, ist dieser Weg etwas umständlich. Wir nutzen deshalb die Funktion `aggregate()`, und spezifizieren `x = wasser`, `by = kantone` und `FUN = mean`.

Hinweis: Das Beschriften der Kantone ist fakultativ und nicht *ganz* trivial.

```{r}
library(ggrepel)
```

```{r}

aggregate(x = wasser, by = kantone, FUN = mean) %>%
  ggplot() + 
  geom_sf(aes(fill = wasserverfuegbarkeit)) + 
  geom_sf(data = wasser,size = 0.1) +  
  geom_text_repel(
    data = summarise(group_by(kantone,NAME)),
    aes(label = NAME, geometry = geom),
    stat = "sf_coordinates"
  ) +
  labs(title = "Mittlere Wasserverfügbarkeit im Boden", subtitle = "pro Kanton", fill = "") +
  theme_void() +
  theme(legend.position = "bottom",legend.direction = "horizontal") 


aggregate(wasser, bezirke, mean) %>%
  ggplot() + 
  geom_sf(aes(fill = wasserverfuegbarkeit)) + 
  geom_sf(data = wasser,size = 0.1) +
  geom_sf(data = kantone, fill = NA, colour = "grey") +
  labs(title = "Mittlere Wasserverfügbarkeit im Boden", subtitle = "pro Bezirk", fill = "") +
  theme_void() +
  theme(legend.position = "bottom",legend.direction = "horizontal") 


```



### Aufgabe 8 (für Ambitionierte)


```{r}
## -- Aufgabe 8 (für Ambitionierte) -- ##
```

Politische Grenzen sind für die meisten natürlichen Phänomene irrelevant. Wir könnten deshalb auch eine regelmässige Kachelung (sog. Tesselierung) des Untersuchungsgebietes vornehmen. Dafür könnten wir mit `st_make_grid()` eine Kachelung für das gewählte Untersuchungsgebiet (`x = `) in einer bestimmten Grösse (`cellsize = `)  als Quadrate (`square = TRUE`) oder sogar mit hübschen Hexagonen (`square = FALSE`) durchführen. Probier's aus!

Eine weitere, zusätzliche (und sehr anspruchsvolle) Herausforderung ist das Zeichnen und Beschriften der Kantosgrenzen. Hierzu geben wir für Wagemutige gerne Tips.

```{r}
hex <- st_make_grid(kantone,cellsize = 20000, square = FALSE) %>%
  st_sf() %>%
  aggregate(x = wasser, by = .,mean) %>%
  st_intersection(st_union(kantone))

hex2 <- st_join(hex,kantone,largest = TRUE) %>%
  st_set_precision(100) %>% 
  group_by(Abk) %>% summarise()

ggplot() +
  geom_sf(data = hex, aes(fill = wasserverfuegbarkeit), colour = NA) +
  scale_fill_continuous(na.value = NA) +
  labs(title = "Mittlere Wasserverfügbarkeit im Boden", subtitle = "20km Hexagon", fill = "") +
  geom_sf(data = hex2, colour = "grey", fill = NA) +
  geom_sf_text(data = hex2, aes(label = Abk), size = 3, colour = "grey") +
  theme_void() +
  theme(legend.position = "bottom",legend.direction = "horizontal")




```



```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:21_RaumAn1/Uebung.Rmd-->

## Übung -- Lösung

[R-Script als Download](21_RaumAn1/RFiles/Uebung.R)

```{r code=readLines('21_RaumAn1/RFiles/Uebung.R'), echo=T, eval=F}
```
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:21_RaumAn1/Loesung.Rmd-->

# Raumanalyse 2 (26.11.2019)

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:22_RaumAn2/Abstract.Rmd-->

## Thematische Einbettung und Vorbereitung


In dieser zweiten Übung wirst Du wiederum Geodatensätze verarbeiten und darstellen. Wir starten mit einem Punktdatensatz zu einem Messnetz zur Erhebung der Luftqualität in der Schweiz (Stickstoffdioxid NO2 um genau zu sein). Im Gegensatz zum Punktdatensatz zur Wasserverfügbarkeit aus der vorherigen Übung, sind die Messstellen des Messnetzes zur Luftqualität sehr unregelmässig im Raum verteilt. Trotzdem möchten wir versuchen ein kontinuierliches Raster von Luftqualitätswerten für die ganze Schweiz zu interpolieren. Wir starten mit der einfachen Interpolations-Methode Inverse Distance Weighting IDW. Danach wollen wir für den gleichen Datensatz nach dem Ansatz der nächsten Nachbarn die Thiessen Polygone konstruieren. Im zweiten Teil der Übung wollen wir Dichteverteilung untersuchen. Dabei untersuchen wir einen Datensatz mit Bewegungsdaten eines Rotmilans in der Schweiz. Mittels einer Kernel Density Estimation (KDE) berechnen wir eine kontinuierliche Dichteverteilung, über die wir eine Annäherung an das Habitat des untersuchten Greifvogels berechnen können. Bevor wir aber starten, schauen wir uns die Punktdatensätze genauer an indem wir die G-Function berechnen und plotten.

Importiere zunächst die Daten. Weise dann die korrekten Koordinatensysteme zu (CRS zuweisen).

- [luftqualitaet.gpkg](22_RaumAn2/data/luftqualitaet.gpkg): Luftqualitätsmessungen NO2, ([weitere Informationen](https://opendata.swiss/en/dataset/luftqualitaet))
- [kantone.gpkg](21_RaumAn1/data/kantone.gpkg): Ein Datensatz der Swisstopo ([weitere Informationen](https://shop.swisstopo.admin.ch/de/products/landscape/boundaries3D))
- [rotmilan.gpkg](22_RaumAn2/data/rotmilan.gpkg): Der Datensatz `rotmilan.gpkg` stammt aus einem grösseren Forschungsprojekt der Vogelwarte Sempach [Mechanismen der Populationsdynamik beim Rotmilan](https://www.vogelwarte.ch/de/projekte/oekologische-forschung/mechanismen-der-populationsdynamik-beim-rotmilan). Der Datensatz wurde über die Plattform [movebank](http://www.movebank.org/panel_embedded_movebank_webapp?gwt_fragment=page%3Dsearch_map_linked%2CindividualIds%3D676571254%2Clat%3D44.465151013519666%2Clon%3D9.096679687499508%2Cz%3D5) zur Verfügung gestellt. Es handelt sich dabei um ein einzelnes Individuum, welches seit 2017 mit einem Sender versehen ist und über ganz Mitteleuropa zieht. Wir arbeiten in dieser Übung nur mit denjenigen Datenpunkten, die in der Schweiz erfasst wurden. Wer den ganzen Datensatz analysieren möchte, kann sich diesen über den Movebank-Link runterladen.


```{r, echo = TRUE}
library(gstat)
library(sf)
library(tidyverse)
library(lubridate)
library(stars)

luftqualitaet <- read_sf("22_RaumAn2/data/luftqualitaet.gpkg")
kantone <- read_sf("21_RaumAn1/data/kantone.gpkg")
rotmilan <- read_sf("22_RaumAn2/data/rotmilan.gpkg")


luftqualitaet <- st_set_crs(luftqualitaet,2056)
kantone <- st_set_crs(kantone, 2056)
rotmilan <- st_set_crs(rotmilan, 2056)
```




## Übung A: Analyse von Punktverteilungen

```{r}
###################################################
## -- Übung A - Analyse von Punktverteilungen -- ##
###################################################
```

### Aufgabe 1: G-Function


```{r}
## -- Aufgabe 1: G-Function-- ##
```

Als erstes berechnen wir die G-Function für die Rotmilanpositionen wie auch für die Luftqualitätsmessungen NO2:

**Schritt 1:** Da es sich beim Rotmilan um relativ viele Datenpunkte handelt, nehmen wir mit `st_sample()` ein subset aus dem Datensatz. Dies verkürzt die Rechenzeit substantiell und verändert die Resultate kaum.

```{r, echo = TRUE}
rotmilan_sample <- sample_n(rotmilan,1000)
```

**Schritt 2:** Mit `st_distance()` können Distanzen zwischen zwei `sf` Datensätze berechnet werden. Wird nur ein Datensatz angegeben, wird eine Kreuzmatrix erstellt wo die Distanzen zwischen allen Features zu allen anderen Features dargestellt werden. Wir nützen diese Funktion zur Berechnung der nächsten Nachbarn.

```{r, echo = TRUE}
rotmilan_distanzmatrix <- st_distance(rotmilan_sample)

# zeige die ersten 6 Zeilen und Spalten der Matrix
# jeder Distanz wurde 2x Gemessen (vergleiche Wert [2,1] mit [1,2])
# die Diagonale ist die Distanz zu sich selber (gleich 0)
rotmilan_distanzmatrix[1:6,1:6] 
```


**Schritt 3:** Nun wollen wir wissen, wie gross die kürzeste Distanz von jedem Punkt zu seinem nächsten Nachbarn beträgt, also die kürzeste Distanz pro Zeile. Zuerst aber müssen wir die diagonalen Werte noch entfernen, denn diese stellen ja jeweils die Distanz zu sich selber dar und sind immer `0`. Danach kann mit `apply()` eine Funktion (`FUN = min`) über die Zeilen (`MARGIN = 1`) einer Matrix (`X = rotmilan_distanzmatrix`) gerechnet werden. Zusätzlich müssen wir noch `na.rm = TRUE` setzen, damit `NA` Werte von der Berechnung ausgeschlossen werden. Das Resultat ist ein Vektor mit gleich vielen Werten wie Zeilen in der Matrix.

```{r, echo = TRUE}
diag(rotmilan_distanzmatrix) <- NA # entfernt alle diagonalen Werte

rotmilan_mindist <- apply(rotmilan_distanzmatrix,1,min, na.rm = TRUE)
```

Nun wollen wir die kumulierte Häufigkeit der Werte in einer Verteilungsfunktion (engl: [Empirical Cumulative Distribution Function, ECDF](https://en.wikipedia.org/wiki/Empirical_distribution_function)) darstellen. Dafür müssen wir den Vektor zuerst noch in einen Dataframe packen, damit `ggplot` damit klar kommt:

```{r, echo = TRUE}
rotmilan_mindist_df <- data.frame(distanzen = rotmilan_mindist)

ggplot() + geom_step(data = rotmilan_mindist_df, aes(distanzen),stat = "ecdf")
```

Führe nun die gleichen Schritte mit `luftqualitaet` durch und vergleiche die ECDF-Plots. Hinweis: `st_sample()` ist bei `luftqualitaet` nicht nötig, da es sich hier um einen kleineren Datensatz handelt.

```{r}
luftqualitaet_distanzmatrix <- st_distance(luftqualitaet)

diag(luftqualitaet_distanzmatrix) <- NA

luftqualitaet_mindist <- apply(luftqualitaet_distanzmatrix,1,min,na.rm = TRUE)

luftqualitaet_mindist_df <- data.frame(distanzen = luftqualitaet_mindist, data = "Luftqualität")

rotmilan_mindist_df$data <- "Rotmilan"

mindist_df <- rbind(luftqualitaet_mindist_df,rotmilan_mindist_df)

ggplot() + 
  geom_step(data = mindist_df, aes(distanzen, colour = data),stat = "ecdf") +
  labs(colour = "Datensatz")
```






```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:22_RaumAn2/Uebung_A.Rmd-->

## Übung A -- Lösung

[R-Script als Download](22_RaumAn2/RFiles/Uebung_A.R)

```{r code=readLines('22_RaumAn2/RFiles/Uebung_A.R'), echo=T, eval=F}
```
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:22_RaumAn2/Loesung_A.Rmd-->

## Übung B: Räumliche Interpolation

```{r, echo = TRUE}
library(gstat)
library(sf)
library(tidyverse)
library(lubridate)
library(stars)

luftqualitaet <- read_sf("22_RaumAn2/data/luftqualitaet.gpkg")
kantone <- read_sf("21_RaumAn1/data/kantone.gpkg")
rotmilan <- read_sf("22_RaumAn2/data/rotmilan.gpkg")


luftqualitaet <- st_set_crs(luftqualitaet,2056)
kantone <- st_set_crs(kantone, 2056)
rotmilan <- st_set_crs(rotmilan, 2056)
```

```{r}
#############################################
## -- Übung B - Räumliche Interpolation -- ##
#############################################
```

### Aufgabe 2: Räumliche Interpolation mit IDW

```{r}
## -- Aufgabe 2: Räumliche Interpolation mit IDW -- ##
```

Die Library `gstat` bietet verschiedene Möglichkeiten, Datenpunkte zu interpolieren, unter anderem auch den **IDW**. Leider ist das Package *noch* nicht so benutzerfreundlich wie `sf`: Das Package wird aber aktuell überarbeitet und in mittlerer Zukunft sollte es ebenso einfach  zugänglich sein. Damit Ihr Euch nicht mit den Eigenheiten dieser Library umschlagen müsst, haben wir eine Function vorbereitet, die Euch die Verwendung der IDW-Interpolation erleichtern soll.

Wir nehmen Euch damit etwas Komplexität weg und liefern Euch ein pfannenfertiges Werkzeug. Das hat auch Nachteile und wir ermutigen alle, die dafür Kapazität haben, unsere Function eingehend zu studieren und allenfalls ganz auf die Function zu verzichten und stattdessen direkt `gstat` zu verwenden. Egal für welche Variante Ihr Euch entscheidet, installiert vorgängig die Library `gstat`. Liest anschliessend die Funktion `my_idw` ein damit ihr sie nutzen könnt.


```{r, echo = TRUE}
my_idw <- function(groundtruth,column,cellsize, nmax = Inf, maxdist = Inf, idp = 2, extent = NULL){
  require(gstat)
  require(sf)
  require(raster)
  if(is.null(extent)){
    extent <- groundtruth
  }
  
  samples <- st_make_grid(extent,cellsize,what = "centers") %>% st_as_sf()
  my_formula <- formula(paste(column,"~1"))
  idw_sf <- gstat::idw(formula = my_formula,groundtruth,newdata = samples,nmin = 1, maxdist = maxdist, idp = idp)
  
  idw_matrix <- cbind(st_coordinates(idw_sf),idw_sf$var1.pred)
  
  
  ras <- raster::rasterFromXYZ(idw_matrix)
  
  if(all(grepl("polygon",st_geometry_type(extent),ignore.case = TRUE))){
    ras <- mask(ras,st_as_sf(st_zm(extent)))
  }
  ras
}
```


Nun könnt Ihr mit `my_idw()` den Datensatz `luftqualitaet` folgendermassen interpolieren. 
```{r, echo = TRUE, eval = FALSE}
my_idw(groundtruth = luftqualitaet,column = "value",cellsize = 1000)

```

Folgende Parameter stehen Euch zur Verfügung:

- Notwendige Parameter:
  - `groundtruth`: Punktdatensatz mit den Messwerten (`sf`-Objekt)
  - `column`: Name der Spalte mit den Messwerten (in Anführungs- und Schlusszeichen)
  - `cellsize`: Zellgrösse des output Rasters

- Optionale Parameter
  - `nmax`: Maximale Anzahl Punkte, die für die Interpolation berücksichtigt werden sollen. Default: `Inf` (alle Werte im gegebenen Suchradius)
  - `maxdist`: Suchradius, welcher für die Interpolation verwendet werden soll. Default `Inf` (alle Werte bis `nmax`)
  - `idp`: **I**nverse **D**istance **P**ower: die Potenz, mit der der Nenner gesteigert werden soll. Default: `2`. Werte werden im Kehrwert des Quadrates gewichtet: $\frac{1}{dist^{idp}}$.
  - `extent`: Gebiet, für welches die Interpolation durchgeführt werden soll. Default `NULL` (die Ausdehnung von `groundtruth`). Wenn `extent` ein Polygon ist, wird die Interpolation für dieses Gebiet "geclipped"


Rechnet so den IDW für die Luftqualitätsmessungen mit verschiedenen Parametern und visualisiert jeweils die Resultate (die Beschriftung der Werte ist fakultativ).

```{r}


nmax = Inf
maxdist = 40000
idp = 2

idw <- my_idw(luftqualitaet,"value",1000,nmax = nmax,maxdist = maxdist,idp = idp, extent = kantone)

luftqualitaet_extreme <- luftqualitaet %>%
  arrange(value) %>%
  slice(c(1:5,(n()-4):n()))

ggplot() +
  geom_stars(data = st_as_stars(idw)) +
  ggrepel::geom_text_repel(
    data = luftqualitaet_extreme,
    aes(label = value, geometry = geom),
    stat = "sf_coordinates",
    min.segment.length = 0
  ) +
  scale_fill_viridis_c(na.value = NA) +
  labs(title = "Luftqualitätswerte Schweiz NO2, Interpoliert mit IDW",
       fill = "μg/m3",
       subtitle = paste("nmax: ",nmax,"\nmaxdist: ",maxdist,"\nidp: ",idp,sep = " ")
  ) +
  theme_void() +
  coord_equal()


nmax = 50
maxdist = Inf
idp = 1

idw <- my_idw(luftqualitaet,"value",1000,nmax = nmax,maxdist = maxdist,idp = idp, extent = kantone)

ggplot() +
  geom_stars(data = st_as_stars(idw)) +
  ggrepel::geom_text_repel(
    data = luftqualitaet_extreme,
    aes(label = value, geometry = geom),
    stat = "sf_coordinates",
    min.segment.length = 0
  ) +
  scale_fill_viridis_c(na.value = NA) +
  labs(title = "Luftqualitätswerte Schweiz NO2, Interpoliert mit IDW",
       fill = "μg/m3",
       subtitle = paste("nmax: ",nmax,"\nmaxdist: ",maxdist,"\nidp: ",idp,sep = " ")
  ) +
  theme_void()+
  coord_equal()


nmax = 50
maxdist = Inf
idp = 2

idw <- my_idw(luftqualitaet,"value",1000,nmax = nmax,maxdist = maxdist,idp = idp, extent = kantone)

ggplot() +
  geom_stars(data = st_as_stars(idw)) +
  ggrepel::geom_text_repel(
    data = luftqualitaet_extreme,
    aes(label = value, geometry = geom),
    stat = "sf_coordinates",
    min.segment.length = 0
  ) +
  scale_fill_viridis_c(na.value = NA) +
  labs(title = "Luftqualitätswerte Schweiz NO2, Interpoliert mit IDW",
       fill = "μg/m3",
       subtitle = paste("nmax: ",nmax,"\nmaxdist: ",maxdist,"\nidp: ",idp,sep = " ")
  ) +
  theme_void()+
  coord_equal()


nmax = 50
maxdist = Inf
idp = 3

idw <- my_idw(luftqualitaet,"value",1000,nmax = nmax,maxdist = maxdist,idp = idp, extent = kantone)

ggplot() +
  geom_stars(data = st_as_stars(idw)) +
  ggrepel::geom_text_repel(
    data = luftqualitaet_extreme,
    aes(label = value, geometry = geom),
    stat = "sf_coordinates",
    min.segment.length = 0
  ) +
  scale_fill_viridis_c(na.value = NA) +
  labs(title = "Luftqualitätswerte Schweiz NO2, Interpoliert mit IDW",
       fill = "μg/m3",
       subtitle = paste("nmax: ",nmax,"\nmaxdist: ",maxdist,"\nidp: ",idp,sep = " ")
  ) +
  theme_void()

```



### Aufgabe 3: Interpolation mit Nearest Neighbour / Thiessen Polygone

```{r}
## -- Aufgabe 3: Interpolation mit Nearest Neighbour / Thiessen Polygone -- ##
```

Eine weitere einfache Möglichkeit zur Interpolation bietet die Erstellung eines Voronoi-Diagrammes, auch als Thiessen-Polygone oder Dirichlet-Zerlegung bekannt. `sf` liefert dazu die Funktion `st_voronoi()`, die einen Punktdatensatz annimmt und eben um die Punkte die Thiessenpolygone konstruiert. Dazu braucht es lediglich einen kleinen Vorverarbeitungsschritt: `sf` möchte für jedes Feature, also für jede *Zeile* in unserem Datensatz, ein Voronoidiagramm. Das macht bei uns wenig Sinn, weil jede Zeile nur aus einem Punkt besteht. Deshalb müssen wir vorher `luftqualitaet` mit `st_union()` von einem `POINT` in ein `MULTIPOINT` Objekt konvertieren, in welchem alle Punkte in einer Zeile zusammengefasst sind.

```{r, echo = TRUE}

luftqualitaet_union <- st_union(luftqualitaet)

thiessenpolygone <- st_voronoi(luftqualitaet_union)

thiessenpolygone <- st_set_crs(thiessenpolygone, 2056)

ggplot() + 
  geom_sf(data = kantone) +
  geom_sf(data = thiessenpolygone, fill = NA)

```

`st_voronoi` hat die Thiessenpolygone etwas weiter gezogen als wir sie wollen. Dies ist allerdings eine schöne Illustration der Randeffekte von Thiessenpolygonen, die zum Rand hin (wo es immer weniger Punkte hat) sehr gross werden können. Wir können die Polygone auf die Ausdehnung der Schweiz mit `st_intersection()` clippen. Auch hier braucht es zwei kleine Vorverarbeitungsschritte:

1. wie vorher müssen wir die einzelnen Kantons-Polygone miteinander verschmelzen. Dies erreichen wir mit `st_union()`. Wir speichern den Output als `schweiz`, was als Resultat ein einzelnes Polygon der Schweizergrenze retourniert.
2. für die Thiessen-Polygone machen wir genau das Umgekehrte: `st_voronoi()` liefert ein einzelnes Feature mit allen Polygonen, welches sich nicht gerne clippen lässt. Mit `st_cast()` wird die `GEOMETRYCOLLECTION` in Einzelpolygone aufgeteilt. 

```{r, echo = TRUE}
schweiz <- st_union(kantone)
thiessenpolygone <- st_cast(thiessenpolygone)

thiessenpolygone_clip <- st_intersection(thiessenpolygone,schweiz)

ggplot() + 
  geom_sf(data = kantone) +
  geom_sf(data = thiessenpolygone_clip, fill = NA)

```

Jetzt müssen wir nur noch den jeweiligen Wert für jedes Polygon ermitteln. Dies erreichen wir wieder durch `st_join`. Auch hier ist noch ein *kleiner* Vorverarbeitungsschritt nötig: Wir konvertieren das `sfc` Objekt (nur Geometrien) in ein `sf` Objekt (Geometrien mit Attributtabelle).

```{r, echo = TRUE}

thiessenpolygone_clip <- st_as_sf(thiessenpolygone_clip)
thiessenpolygone_clip <- st_join(thiessenpolygone_clip,luftqualitaet)

ggplot() + 
  geom_sf(data = kantone) +
  geom_sf(data = thiessenpolygone_clip, aes(fill = value)) +
  scale_fill_viridis_c() +
  theme_void()
```
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:22_RaumAn2/Uebung_B.Rmd-->

## Übung B -- Lösung

[R-Script als Download](22_RaumAn2/RFiles/Uebung_B.R)

```{r code=readLines('22_RaumAn2/RFiles/Uebung_B.R'), echo=T, eval=F}
```
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:22_RaumAn2/Loesung_B.Rmd-->

## Übung C: Dichteverteilungen

```{r, echo = TRUE}
library(gstat)
library(sf)
library(tidyverse)
library(lubridate)
library(stars)

luftqualitaet <- read_sf("22_RaumAn2/data/luftqualitaet.gpkg")
kantone <- read_sf("21_RaumAn1/data/kantone.gpkg")
rotmilan <- read_sf("22_RaumAn2/data/rotmilan.gpkg")


luftqualitaet <- st_set_crs(luftqualitaet,2056)
kantone <- st_set_crs(kantone, 2056)
rotmilan <- st_set_crs(rotmilan, 2056)
```

```{r}
########################################
## -- Übung C - Dichteverteilungen -- ##
########################################
```

### Aufgabe 4: Rotmilan Bewegungsdaten visualisieren

```{r}
## -- Aufgabe 4: Rotmilan Bewegungsdaten visualisieren -- ##
```

Die erste Frage, die bei solchen Bewegungsstudien typischerweise gestellt wird, lautet: *Wo hält sich das Tier hauptsächlich auf?* Um diese Frage zu beantworten, kann man als erstes einfach die Datenpunkte in einer einfachen Karte visualisieren.

```{r}
ggplot(kantone) + 
  geom_sf() + 
  geom_sf(data = rotmilan) +
  theme_void()
```




### Aufgabe 5: Kernel Density Estimation berechnen

```{r}
## -- Aufgabe 5: Kernel Density Estimation berechnen -- ##
```

In einer ersten Annäherung funktioniert dies, doch wir sehen hier ein klassisches Problem des "Overplotting". Das heisst, dass wir durch die Überlagerung vieler Punkte in den dichten Regionen nicht abschätzen können, wie viele Punkte dort effektiv liegen und ggf. übereinander liegen. Es gibt hier verschiedene Möglichkeiten, die Punktdichte klarer zu visualisieren. Eine unter Biologen sehr beliebte Methode ist die Dichteverteilung mit einer Kernel Density Estimation (KDE). Dies v.a. darum, weil mit KDE das Habitat (Streifgebiet) eines Tieres abgeschätzt werden kann. Homeranges werden oft mit KDE95 und Core Areas mit KDE50 definiert ([Fleming C., Calabrese J., 2016](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12673)).

Ähnlich wie beim IDW sind auch die verfügbaren KDE-Funktionen in R etwas kompliziert in der Handhabung. Damit wir dieses Verfahren aber dennoch auf unsere Rotmilan-Daten anwenden können, haben wir eine eigene KDE-Funktion erstellt, die wir Euch zur Verfügung stellen. Die Funktion beruht auf den Libraries `MASS`, `raster`, `sf` und `stars`. Die letzen drei solltet Ihr bereits installiert haben. Überprüft nun, ob `MASS` ebenfalls schon installiert ist und holt dies bei Bedarf nach. Im Anschluss könnt Ihr die nachstehende Funktion einlesen:

```{r, echo = TRUE}
my_kde <- function(points,cellsize, bandwith, extent = NULL){
  require(MASS)
  require(raster)
  require(sf)
  require(stars)
  if(is.null(extent)){
    extent_vec <- st_bbox(points)[c(1,3,2,4)]
  } else{
    extent_vec <- st_bbox(extent)[c(1,3,2,4)]
  }
  
  n_y <- ceiling((extent_vec[4]-extent_vec[3])/cellsize)
  n_x <- ceiling((extent_vec[2]-extent_vec[1])/cellsize)
  
  extent_vec[2] <- extent_vec[1]+(n_x*cellsize)-cellsize
  extent_vec[4] <- extent_vec[3]+(n_y*cellsize)-cellsize

  coords <- st_coordinates(points)
  matrix <- kde2d(coords[,1],coords[,2],h = bandwith,n = c(n_x,n_y),lims = extent_vec)
  raster(matrix)
}
```


Die Parameter der Funktion sollten relativ klar sein:

- `points`: Ein Punktdatensatz aus der Class `sf`
- `cellsize`: Die Zellgrösse des output-Rasters
- `bandwith`: Der Suchradius für die Dichteberechnung
- `extent` (optional): Der Perimeter, in dem die Dichteverteilung berechnet werden soll. Wenn kein Perimeter angegeben wird, wird die "bounding box" von `points` genutzt.

Wenn wir nun mit `my_kde()` die Dichteverteilung berechnen, erhalten wir einen Raseterdatensatz zurück, der sich mit `base::plot()` schnell visualisieren lässt.

```{r, echo = TRUE}
schweiz <- st_union(kantone)
rotmilan_kde <- my_kde(rotmilan,cellsize = 1000,bandwith = 10000, extent = schweiz)

rotmilan_kde

plot(rotmilan_kde)
```

Um den Raster-Datensatz in `ggplot()` zu integrieren, müssen wir ihn zu einem `stars`-Objekt konvertieren und können danach `geom_stars()` verwenden.

```{r, echo = TRUE}
ggplot() + 
  geom_stars(data = st_as_stars(rotmilan_kde)) +
  geom_sf(data = kantone, fill = NA) +
  scale_fill_viridis_c() +
  theme_void() +
  theme(legend.position = "none")
```

Die Kernel Density Estimation ist nun sehr stark von den tiefen Werten dominiert, da die Dichte in den meisten Zellen unseres Untersuchungsgebiets nahe bei Null liegt. 

Wir können die tiefen Werte ausblenden, indem wir nur die höchsten 5% der Werte darstellen. Dafür berechnen wir mit `raster::quantile` die 95. Perzentile aller Rasterzellen und nutzen dies um die Farbskala in `ggplot` zu limitieren (*KDE95*). Zusätzlich hilft eine logarithmische Transformation der KDE-Werte, die Farbskala etwas sichtbarer zu machen.

```{r, echo = TRUE}
q95 <- raster::quantile(rotmilan_kde,probs = 0.95)

ggplot() + 
  geom_sf(data = kantone, fill = NA) +
  geom_stars(data = st_as_stars(rotmilan_kde),alpha = 0.8) +
  scale_fill_viridis_c(trans = "log10",limits = c(q95,NA),na.value = NA) +
  theme_void() +
  labs(fill = "KDE",title = "Dichteverteilung von Bewegungsdaten eines Rotmilans",subtitle = "Jahre 2017-2019") +
  theme(legend.position = "top", legend.direction = "horizontal")
```



### Aufgabe 6: Dichteverteilung mit Thiessen Polygonen

```{r}
## -- Aufgabe 6: Dichteverteilung mit Thiessen Polygonen -- ##
```

Thiessen Polygone bieten eine spannende Alternative um Unterschiede in der Dichteverteilung von Punktdatensätzen zu visualisieren. 
Wir wollen dies nun ausprobieren und konstruieren zum Schluss die Thiessenpolygone für die Rotmilan-Daten für das Untersuchungsgebiet Schweiz.


```{r, echo = TRUE}

thiessenpolygone <- rotmilan %>%
  st_union() %>%
  st_voronoi()

ggplot() + 
  geom_sf(data = kantone) + 
  geom_sf(data = thiessenpolygone, fill = NA) + 
  geom_sf(data = rotmilan) +
  theme_void()
```






```{r, echo = TRUE}
schweiz <- st_union(kantone)


thiessenpolygone

thiessenpolygone <- st_cast(thiessenpolygone)

thiessenpolygone

# Dieser Schritt kann eine Weile dauern
thiessenpolygone_clip <- st_intersection(thiessenpolygone,schweiz)

```

Wenn wir jetzt die Thiessenpolygone (ohne Punkte) darstellen, wird deutlicher, wie die Dichteverteilung im Innern des Clusters aussieht. 

```{r}
ggplot() + 
  geom_sf(data = schweiz) + 
  geom_sf(data = thiessenpolygone_clip) + 
  theme_void()
```
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:22_RaumAn2/Uebung_C.Rmd-->

## Übung C -- Lösung

[R-Script als Download](22_RaumAn2/RFiles/Uebung_C.R)

```{r code=readLines('22_RaumAn2/RFiles/Uebung_C.R'), echo=T, eval=F}
```
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:22_RaumAn2/Loesung_C.Rmd-->

