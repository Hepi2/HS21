--- 
title: "Research Methods"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
#output: bookdown::gitbook
documentclass: book
bibliography: [00_Admin/book.bib, 00_Admin/packages.bib]
biblio-style: apalike
link-citations: yes
description: "Begleitmaterial zum Modul 'Research Methods' "
---

# Einleitung


Das Modul „Research Methods“ vermittelt vertiefte Methodenkompetenzen für praxisorientiertes und angewandtes wissenschaftliches Arbeiten im Fachbereich „Umwelt und Natürliche Ressourcen“ auf MSc-Niveau. Die Studierenden erarbeiten sich vertiefte Methodenkompetenzen für die analytische Betrachtung der Zusammenhänge im Gesamtsystem „Umwelt und Natürliche Ressourcen“. Die Studierenden erlernen die methodischen Kompetenzen, auf denen die nachfolgenden Module im MSc Programm UNR aufbauen. Das Modul vermittelt einerseits allgemeine, fächerübergreifende methodische Kompetenzen (z.B. Wissenschaftstheorie, computer-gestützte Datenverar-beitung und Statistik).

Auf dieser Plattform (RStudio Connect) werden die Unterlagen für die R-Übungsteile bereitgestellt. Es werden sukzessive sowohl Demo-Files, Aufgabenstellungen und Lösungen veröffentlicht.




```{r, include=F, message=F}
# Set Root Directory / Working directory to Project folder for all Files (if M-K)
knitr::opts_knit$set(root.dir = getwd())
```


```{r, include=F, message=F}


grepl_loop <- function(vector,remove){
  for(remove_i in remove){
    vector <- vector[!grepl(remove_i,vector)]
  }
  return(vector)
}




# Allow duplicate Labels so that calling purl() does not create an error
# https://stackoverflow.com/q/36868287/4139249
options(knitr.duplicate.label = 'allow')

# purl all Rmd Documents (with some exceptions) and store them in a Subfolder /RFiles
# Document cannot be knitted if the folder "RFiles" does not exist!
library(stringr)

keywords <- c("ResearchMethods","_Rcode","99_","index","Archive","Admin","main","Abstract")


rmds <- list.files(pattern = ".Rmd",recursive = T)

rmds <- grepl_loop(rmds,keywords)


# 2019-08-15 rata: folgender Teil ist auskommentiert, um die Komplexität des ganzen zu verkleinern
# for (file in rmds){
#   file_r <- gsub("Rmd","R",file)                          # change fileextension from .rmd to r
#   file_r <- str_split_fixed(file_r,"/",Inf)               # split path at /
#   file_r <- append(file_r, "RFiles",length(file_r)-1)     # append Foldername "RFiles" in 2nd last pos
#   file_r <- paste(file_r,collapse = "/")                  # collapse vector to string
#   if(file.exists(file_r)){
#     file.remove(file_r)
#   }
#   knitr::purl(file,documentation = 0,output = file_r)
# }

```




```{r include=FALSE, message=F}
# automatically create a bib database for R packages
# 2019-08-15 rata: folgender Teil ist auskommentiert, um die Komplexität des ganzen zu verkleinern
# knitr::write_bib(c(
#   .packages(), 'bookdown', 'knitr','forcats','carData', 'rmarkdown','tidyverse','plotly','car','ggfortify','boot','pander','scales','multicomp','ggExtra','lubridate','dplyr','purrr','readr','tidyr','tibble','ggplot2','webshot','bindrcpp','GGally','hier.part','gtools','MuMIn','nlme','lme4','languageR','lmerTest','rms','SparseM','Hmisc','Formula','survival','lattice','Matrix'
# ), '00_Admin/packages.bib')

```
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:index.Rmd-->

# PrePro1 (14.10.2019)

Die Datenkunde 2.0 gibt den Studierenden das Wissen und die Fertigkeiten an die Hand, selbst erhobene und bezogene Daten für Ihre eigenen Analysen vorzubereiten und anzureichern (preprocessing). Die Einheit vermittelt zentrale Datenverarbeitungskompetenzen und thematisiert bekannte Problemzonen der umweltwissenschaftlichen Datenverarbeitung – immer mit einer „hands-on“ Perspektive auf die begleitenden R-Übungen. Die Studierenden lernen die Eigenschaften ihrer Datensätze in der Fachsprache korrekt zu beschreiben. Sie lernen ausserdem Metadaten zu verstehen und die Implikationen derselben für ihre eigenen Analyseprojekte kritisch zu beurteilen. Zentrale Konzepte der Lerneinheit sind Skalenniveaus, Datentypen, Zeitdaten und Typumwandlungen.


```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:09_PrePro1/Abstract.Rmd-->


```{r, include=FALSE, purl=F}

knitr::opts_chunk$set(echo = TRUE,include = T, collapse=TRUE)
```

## Demo: Datentypen, Tabellen

[R-Code als Download](09_PrePro1/RFiles/Demo_Datentypen.R)

### Datentypen 


#### Numerics

Unter die Kategorie `numeric` fallen in R zwei Datentypen:

- `double`: Gleitkommazahl (z.B. 10.3, 7.3)
- `integer`: Ganzzahl (z.B. 10, 7)

##### Doubles

Folgendermassen wird eine Gleitkommazahl einer Variabel zuweisen:

```{r}
x <- 10.3

x

typeof(x)
```



Statt `<-`kann auch `=` verwendet werden. Dies funktioniert aber nicht in allen Situationen, und ist zudem leicht mit `==` zu verwechseln.

```{r}
y = 7.3

y
```



Ohne explizite Zuweisung nimmt R immer den Datentyp `double`an:

```{r}
z <- 42
typeof(z)
is.integer(z)
is.numeric(z)
is.double(z)

```

#### Ganzzahl / Integer 


Erst wenn man eine Zahl explizit als `integer` definiert (mit `as.integer()` oder `L`), wird sie auch als solches abgespeichert.

```{r}
a <- as.integer(z)
is.numeric(a)
is.integer(a)

c <- 8L
is.numeric(c)
is.integer(c)
```




```{r}
typeof(a)

is.numeric(a)
is.integer(a)
```



Mit `c()` können eine Reihe von Werten in einer Variabel zugewiesen werden (als `vector`). Es gibt zudem auch `character vectors`. 

```{r}
vector <- c(10,20,33,42,54,66,77)
vector
vector[5]
vector[2:4]

vector2 <- vector[2:4]
```



Eine Ganzzahl kann explizit mit `as.integer()` definiert werden.

```{r}
a <- as.integer(7)
b <- as.integer(3.14)
a
b
typeof(a)
typeof(b)
is.integer(a)
is.integer(b)

```

Eine Zeichenkette kann als Zahl eingelesen werden.

```{r}
c <- as.integer("3.14")
c
typeof(c)
```


#### Logische Abfragen 

Wird auch auch als boolesch (Eng. **boolean**) bezeichnet.

```{r}
e <- 3
f <- 6
g <- e > f
e
f
g
typeof(g)

```

#### Logische Operationen


```{r}
sonnig <- TRUE
trocken <- FALSE

sonnig & !trocken
```

Oft braucht man auch das Gegenteil / die Negation eines Wertes. Dies wird mittels `!` erreicht

```{r}
u <- TRUE
v <- !u 
v
```



#### Zeichenketten

Zeichenketten (Eng. **character**) stellen Text dar

```{r}
s <- as.character(3.14)
s
typeof(s)
```



Zeichenketten verbinden / zusammenfügen (Eng. **concatenate**)

```{r}
fname <- "Hans"
lname <- "Muster"
paste(fname,lname)

fname2 <- "hans"
fname == fname2
```


#### `Factors`

Mit `Factors` wird in R eine Sammlung von Zeichenketten bezeichnet, die sich wiederholen, z.B. Wochentage (es gibt nur 7 unterschiedliche Werte für "Wochentage").

```{r}
wochentage <- c("Montag","Dienstag","Mittwoch","Donnerstag","Freitag","Samstag","Sonntag",
                "Montag","Dienstag","Mittwoch","Donnerstag","Freitag","Samstag","Sonntag")

typeof(wochentage)

wochentage_fac <- as.factor(wochentage)

wochentage
wochentage_fac


```

Wie man oben sieht, unterscheiden sich `character vectors` und `factors` v.a. dadurch, dass letztere über sogenannte `levels` verfügt. Diese `levels` entsprechen den Eindeutigen (`unique`) Werten.

```{r}
levels(wochentage_fac)

unique(wochentage)
```



#### Zeit/Datum

Um in R mit Datum/Zeit Datentypen umzugehen, müssen sie als `POSIXct` eingelesen werden (es gibt alternativ noch `POSIXlt`, aber diese ignorieren wir mal). Anders als Beispielsweise bei Excel, sollten in R Datum und Uhrzeit immer in **einer Spalte** gespeichert werden.

```{r}
datum <- "2017-10-01 13:45:10"

as.POSIXct(datum)

```

Wenn das die Zeichenkette in dem obigen Format (Jahr-Monat-Tag Stunde:Minute:Sekunde) daher kommt, braucht `as.POSIXct`keine weiteren Informationen. Sollte das Format von dem aber Abweichen, muss man der Funktion das genaue Schema jedoch mitteilen. Der Syntax dafür kann via `?strptime` nachgeschlagen werden.

```{r}
datum <- "01.10.2017 13:45"

as.POSIXct(datum,format = "%d.%m.%Y %H:%M")

```

Beachtet, dass in den den obigen Beispiel R automatisch eine Zeitzone angenommen hat (`CEST`). R geht davon aus, dass die Zeitzone der **System Timezone** (`Sys.timezone()`) entspricht.


### Data Frames und Conveniance Variabeln

Eine `data.frame` ist die gängigste Art, Tabellarische Daten zu speichern. 

```{r}
df <- data.frame(
  Stadt = c("Zürich","Genf","Basel","Bern","Lausanne"),
  Einwohner = c(396027,194565,175131,140634,135629),
  Ankunft = c("1.1.2017 10:00","1.1.2017 14:00",
              "1.1.2017 13:00","1.1.2017 18:00","1.1.2017 21:00")
)

str(df)

```

In der obigen `data.frame` wurde die Spalte `Einwohner` als Fliesskommazahl abgespeichert. Dies ist zwar nicht tragisch, aber da wir wissen das es sich hier sicher um Ganzzahlen handelt, können wir das korrigieren. Wichtiger ist aber, dass wir die Ankunftszeit (Spalte`Ankunft`) von  einem `Factor` in ein Zeitformat (`POSIXct`) umwandeln. 


```{r}
df$Einwohner <- as.integer(df$Einwohner)

df$Einwohner

df$Ankunft <- as.POSIXct(df$Ankunft, format = "%d.%m.%Y %H:%M")

df$Ankunft
```


Diese Rohdaten können nun helfen, um Hilfsvariablen (**convenience variables**) zu erstellen. Z.B. können wir die Städte einteilen in gross, mittel und klein. 

```{r}
df$Groesse[df$Einwohner > 300000] <- "gross"
df$Groesse[df$Einwohner <= 300000 & df$Einwohner > 150000] <- "mittel"
df$Groesse[df$Einwohner <= 150000] <- "klein"

```



Oder aber, die Ankunftszeit kann von der Spalte `Ankunft`abgeleitet werden. Dazu brauchen wir aber das Package `lubridate`

```{r, message = F}
library(lubridate)
```


```{r}
df$Ankunft_stunde <- hour(df$Ankunft)
```

### Quellen

```{r code=readLines('00_Admin/get_chapter_references.R'), echo=F, eval=T,purl = F, results="asis"}
```




```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:09_PrePro1/Demo_Datentypen.Rmd-->


```{r, include=FALSE, purl = F}
knitr::opts_chunk$set(echo = F, include = T, collapse=TRUE, warning = F)
```


## Übung A

R ist ohne Zusatzpackete nicht mehr denkbar. Die allermeisten Packages werden auf [CRAN](https://cran.r-project.org/) gehostet und können leicht mittels `install.packages()` installiert werden. Eine sehr wichtige Sammlung von Packages wird von RStudio entwickelt. Unter dem Namen [Tidyverse](https://www.tidyverse.org/) werden eine Reihe von Packages angeboten, den R-Alltag enorm erleichtert. Wir werden später näher auf das "Tidy"-Universum eingehen, an dieser Stelle können wir die Sammlung einfach mal installieren.

```
install.packages("tidyverse")
```

Um ein `package` in R verwenden zu können, gibt es zwei Möglichkeiten: 

- entweder man lädt es zu Beginn der R-session mittles `library()`. 
- oder man ruft eine `function` mit vorangestelltem Packetname sowie zwei Doppelpunkten auf. `dplyr::filter()` ruft die Funktion `filter()` des Packets `dplyr` auf. 

Letztere Notation ist vor allem dann sinnvoll, wenn sich zwei unterschiedliche Funktionen mit dem gleichen namen in verschiedenen pacakges existieren. `filter()` existiert als Funktion einersits im package `dplyr` sowie in  `stats`. Dieses Phänomen nennt man "masking". 


Zu beginn laden wir die nötigen Pakete:


```{r,message = F}
library(tidyverse)
# Im Unterschied zu `install.packages()` werden bei `library()` keine Anführungs- 
# und Schlusszeichen gesetzt.


library(lubridate)
# Im Unterschied zu install.packages("tidyverse") wird bei library(tidyverse) 
# das package lubridate nicht berücksichtigt
```


### Aufgabe 1

Erstelle eine `data.frame` mit nachstehenden Daten.

Tipps:

- Eine leere `data.frame` zu erstellen ist schwieriger als wenn erstellen und befüllen der `data.frame` in einem Schritt erfolgt
- R ist dafür gedacht, Spalte für Spalte zu arbeiten ([warum?](http://www.noamross.net/blog/2014/4/16/vectorization-in-r--why.html)), nicht Reihe für Reihe. Versuche dich an dieses Schema zu halten.

```{r}

# Lösung Aufgabe 1

df <- data.frame(
  Tierart = c("Fuchs","Bär","Hase","Elch"),
  Anzahl = c(2,5,1,3),
  Gewicht = c(4.4, 40.3,1.1,120),
  Geschlecht = c("m","f","m","m"),
  Beschreibung = c("Rötlich","Braun, gross", "klein, mit langen Ohren","Lange Beine, Schaufelgeweih")
  )

```


```{r, echo = F, purl=F}
knitr::kable(df)
```



### Aufgabe 2

Was für Datentypen wurden (in Aufgabe 1) von R automatisch angenommen? Sind diese sinnvoll? 

Tipp: Nutze dazu `str()`

```{r}
# Lösung Aufgabe 2

str(df)

# Anzahl wurde als `double` interpretiert, ist aber eigentlich ein `integer`. 
# Mit data.frame() wurde Beschreibung wurde als `factor` interpretiert, ist 
# aber eigentlich `character`
```


```{r}


typeof(df$Anzahl)

df$Anzahl <- as.integer(df$Anzahl)
df$Beschreibung <- as.character(df$Beschreibung)

```


### Aufgabe 3


Nutze die Spalte `Gewicht` um die Tiere in 3 Gewichtskategorien einzuteilen: 

- leicht: < 5kg
- mittel: 5 - 100 kg
- schwer: > 100kg


```{r}

# Lösung Aufgabe 3

df$Gewichtsklasse[df$Gewicht > 100] <- "schwer"
df$Gewichtsklasse[df$Gewicht <= 100 & df$Gewicht > 5] <- "mittel"
df$Gewichtsklasse[df$Gewicht <= 5] <- "leicht"

```


```{r, purl=F}
knitr::kable(df)
```




### Aufgabe 4

Importiere den Datensatz [order_52252_data.txt](09_PrePro1/data/order_52252_data.txt). Es handelt sich dabei um die stündlich gemittelten Temperaturdaten an verschiedenen Standorten in der Schweiz im Zeitraum 2000 - 2005. Wir empfehlen `read_table()`^[@wickham2017, Kapitel 8 bzw. http://r4ds.had.co.nz/data-import.html)] anstelle von `read.table()`.

```{r, message = F}
# Lösung Aufgabe 4

wetter <- readr::read_table("09_PrePro1/data/order_52252_data.txt")
```




```{r, purl=F}
knitr::kable(head(wetter,10))
```


### Aufgabe 5

Schau dir die Rückmeldung von `read_table()`an. Sind die Daten korrekt interpretiert worden?


```{r}
# Lösung Aufgabe 5
# Die Spalte 'time' wurde als 'integer' interpretiert. Dabei handelt es
# sich offensichtlich um Zeitangaben.
```



### Aufgabe 6

Die Spalte `time` ist eine Datum/Zeitangabe im Format JJJJMMTTHH (siehe [meta.txt](09_PrePro1/data/meta.txt)). Damit R dies als Datum-/Zeitangabe erkennt, müssen wir die Spalte in einem R-Format (`POSIXct`) einlesen und dabei R mitteilen, wie sie aktuell formatiert ist. Lies die Spalte mit `as.POSIXct()` (oder `parse_datetime`) ein und spezifiziere sowohl `format` wie auch `tz`. 

Tipps: 

- Wenn keine Zeitzone festgelegt wird, trifft `as.POSIXct()` eine Annahme (basierend auf `Sys.timezone()`). In unserem Fall handelt es sich aber um Werte in UTC (siehe [meta.txt](09_PrePro1/data/meta.txt))
- `as.POSIXct`erwartet `character`: Wenn du eine Fehlermeldung hast die `'origin' must be supplied` (o.ä) heisst, hast du der Funktion vermutlich einen `Numeric` übergeben.

```{r}
# Lösung Aufgabe 6

# mit readr
parse_datetime(as.character(wetter$time[1:10]), format = "%Y%m%d%H")


# mit as.POSIXct()
wetter$time <- as.POSIXct(as.character(wetter$time), format = "%Y%m%d%H",tz = "UTC")

```


```{r, purl=F}
knitr::kable(head(wetter,10))
```




### Aufgabe 7


Erstelle zwei neue Spalten mit Wochentag (Montag, Dienstag, etc) und Kalenderwoche. Verwende dazu die neu erstellte `POSIXct`-Spalte


```{r}

# Lösung Aufgabe 7

wetter$wochentag <- wday(wetter$time,label = T)
wetter$kw <- week(wetter$time)

```


```{r, purl=F}
knitr::kable(head(wetter,10))
```





### Aufgabe 8


Erstelle eine neue Spalte basierend auf die Temperaturwerte mit der Einteilung "kalt" (Unter Null Grad) und "warm" (über Null Grad)

```{r}

# Lösung Aufgabe 8

wetter$temp_kat[wetter$tre200h0>0] <- "warm"
wetter$temp_kat[wetter$tre200h0<=0] <- "kalt"
```


```{r, purl=F}
knitr::kable(head(wetter,10))
```


```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:09_PrePro1/Uebung_A.Rmd-->

## Übung A Lösung

[R-Script als Download](09_PrePro1/RFiles/Uebung_A.R)

```{r code=readLines('09_PrePro1/RFiles/Uebung_A.R'), echo=T, eval=F}
```

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:09_PrePro1/Uebung_A_loesung.Rmd-->


```{r, include=FALSE, purl = F}

knitr::opts_chunk$set(echo = F, include = T, collapse=TRUE)
# knitr::opts_knit$set(root.dir = "09_PrePro1") 

```



## Übung B 




```{r, message = F}
library(tidyverse)
```



Fahre mit dem Datensatz `wetter` aus Übung A fort. 
```{r, purl=F}
wetter <- read_table("09_PrePro1/data/order_52252_data.txt",
                  col_types = list(
                    col_factor(levels = NULL),    
                    col_datetime(format = "%Y%m%d%H"),
                    col_double()
                    )
                  )
```


### Aufgabe 1

Nutze `plot()` um die Temparaturkurve zu visualisieren. Verwende aber vorher `filter()` um dich auf eine Station (z.B. "`ABO`") zu beschränken (es handelt sich sonst um zuviele Datenpunkte).



```{r}
# Lösung Aufgabe 1

wetter_fil <- dplyr::filter(wetter, stn == "ABO")

plot(wetter_fil$time,wetter_fil$tre200h0, type = "l")

```


Nun schauen wir uns das plotten mit `ggplot2` an. Ein simpler Plot wie der in der vorherigen Aufgabe ist in `ggplot2` zugegebenermassen *etwas* komplizierter. `ggplot2` wird aber rasch einfacher, wenn die Grafiken komplexer werden. Wir empfehlen deshalb stark, `ggplot2` zu verwenden.

Schau dir ein paar online Tutorials zu `ggplot2` an (siehe ^[@wickham2017, Kapitel 1 bzw. [http://r4ds.had.co.nz/data-visualisation.html](http://r4ds.had.co.nz/data-visualisation.html) oder hier ein sehr schönes Video: [Learn R: An Introduction to ggplot2](https://youtu.be/YxKr2a-Y1WE?t=1m40s)]) 
und reproduziere den obigen Plot mit `ggplot2`


```{r}

p <- ggplot(wetter_fil, aes(time,tre200h0)) +
  geom_line()

p
```



### Aufgabe 2

Spiele mit Hilfe der erwähnten Tutorials mit dem Plot etwas rum. Versuche die x-/y-Achsen zu beschriften sowie einen Titel hinzu zu fügen.

```{r}
# Lösung Aufgabe 2
p <- p +
  labs(x = "Datum", y = "Temperatur", title = "Stündlich gemittelte Temperaturwerte")

p
```


### Aufgabe 3

Reduziere den x-Achsenausschnitt auf einen kleineren Zeitraum, beispielsweise einn beliebigen Monat. Verwende dazu `lims()` zusammen mit `as.POSIXct()` oder mache ein Subset von deinem Datensatz mit einer convenience-Variabel und `filter()`.

```{r}
# Lösung Aufgabe 3

limits <- as.POSIXct(c("2002-01-01 00:00:00","2002-02-01 00:00:00"),tz = "UTC")

p +
  lims(x = limits)
```



```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:09_PrePro1/Uebung_B.Rmd-->

## Übung B Lösung

[R-Code als Download](09_PrePro1/Uebung_B.R)

```{r code=readLines('09_PrePro1/RFiles/Uebung_B.R'), echo=T, eval=F}
```

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:09_PrePro1/Uebung_B_loesung.Rmd-->

# PrePro2 (15.10.2019)

Die Lerneinheit vermittelt zentralste Fertigkeiten zur Vorverarbeitung von strukturierten Daten in der umweltwissenschaftlichen Forschung: Datensätze verbinden (Joins) und umformen („reshape“, „split-apply-combine“). Im Anwendungskontext haben Daten selten von Anfang an diejenige Struktur, welche für die statistische Auswertung oder für die Informationsvisualisierung erforderlich wäre. In dieser Lerneinheit lernen die Studierenden die für diese oft zeitraubenden Preprocessing-Schritte notwendigen Konzepte und R-Werkzeuge kennen und kompetent anzuwenden.
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:10_PrePro2/Abstract.Rmd-->

```{r, include=F, purl=F}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,include = T, collapse=TRUE)
```

## Ergänzungen zu PrePro 1

### Integer mit "L"

In `R` kann eine Zahl mit dem Suffix "L" explizit als Integer spezifiziert werden. 

```{r, parse = F}
typeof(42)
typeof(42L)
```


Warum dazu der Buchstabe "L" verwendet wird ist nirgends offiziell Dokumentiert (zumindest haben wir nichts gefunden). Die gängigste Meinung, die auch [von renommierten R-Profis vertreten wird](https://hypatia.math.ethz.ch/pipermail/r-devel/2017-June/074467.html
) ist, dass damit `Long integer` abgekürzt wird.



### Arbeiten mit RStudio "Project"

Wir empfehlen die Verwendung von "Projects" innerhalb von RStudio. RStudio legt für jedes Projekt dann einen Ordner an, in welches die Projekt-Datei abgelegt wird (Dateiendung .Rproj). Sollen innerhalb des Projekts dann R-Skripts geladen oder erzeugt werden, werden diese dann auch im angelegten Ordner abgelegt. Mehr zu RStudio Projects findet ihr  [hier](https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects).


Das Verwenden von Projects bringt verschiedene Vorteile, wie zum Beispiel:

- Festlegen der Working Directory ohne die Verwendung des expliziten Pfades (`setwd()`). Das ist sinnvoll, da sich dieser Pfad ändern kann (Zusammenarbeit mit anderen Usern, Ausführung des Scripts zu einem späteren Zeitpunkt) 
- Automatisches Zwischenspeichern geöffneter Scripts und Wiederherstellung der geöffneten Scripts bei der nächsten Session
- Festlegen verschiedener projektspezifischer Optionen
- Verwendung von Versionsverwaltungssystemen (Github oder SVN)



### Arbeiten mit `factors`

Wie bereits angedeutet, ist das Arbeiten mit `factors` etwas gewöhnungsbedürftig. Wir gehen hier auf ein paar Stolpersteine ein.

```{r}
zahlen <- factor(c("null","eins","zwei","drei"))

zahlen
```

Offensichtlich sollten diese `factors` geordnet sein, R weiss davon aber nichts. Eine Ordnung kann man mit dem Befehl `ordered = T` festlegen. 

Beachtet: `ordered = T` kann nur bei der Funktion `factor()` spezifiziert werden, nicht bei `as.factor()`. Ansonsten sind `factor()` und `as.factor()` sehr ähnlich.


```{r}
zahlen <- factor(zahlen,ordered = T)

zahlen
```

Beachtet das "<"-Zeichen zwischen den Levels. Die Zahlen werden nicht in der korrekten Reihenfolge, sondern Alphabetisch geordnet. Die richtige Reihenfolge kann man mit `levels = ` festlegen.

```{r}
zahlen <- factor(zahlen,ordered = T,levels = c("null","eins","zwei","drei","vier"))

zahlen
```

Wie auch schon erwähnt werden `factors` als `character` Vektor dargestellt, aber als Integers gespeichert. Das führt zu einem scheinbaren Wiederspruch wenn man den Datentyp auf unterschiedliche Weise abfragt.
```{r}
typeof(zahlen)

is.integer(zahlen)
```


Mit `typeof()` wird eben diese Form der Speicherung abgefragt und deshalb mit `integer` beantwortet. Da es sich aber nicht um einen eigentlichen Integer Vektor handelt, wird die Frage `is.integer()` mit `FALSE` beantwortet. Das ist etwas verwirrend, beruht aber darauf, dass die beiden Funktionen die Frage von unterschiedlichen Perspektiven beantworten. In diesem Fall schafft `class()` Klarheit:

```{r}
class(zahlen)
```


Wirklich verwirrend wird es, wenn `factors` in numeric umgewandelt werden sollen.

```{r}
zahlen
as.integer(zahlen)
```

Das die Übersetzung der auf Deutsch ausgeschriebenen Nummern in nummerische Zahlen nicht funktionieren würde, war ja klar. Weniger klar ist es jedoch, wenn die `factors` bereits aus nummerischen Zahlen bestehen.

```{r}
zahlen2 <- factor(c("3","2","1","0"))

as.integer(zahlen2)

```

In diesem Fall müssen die `factors` erstmals in `character` umgewandelt werden.

```{r}
zahlen2 <- factor(c("3","2","1","0"))

as.integer(as.character(zahlen2))
```




### Heikle Annahmen - bessere Alternativen

Aus oben beschriebenen Grund ist es auch problematisch, dass `data.frame()` sowie alle `read.*` Funktionen (`read.table`, `read.csv` etc) immer davon ausgehen, dass `strings` als `factors` interpretiert werden sollten. Es gibt in Base R einige Funktionen, welche Annahmen treffen die problematisch sein können. Ein weiteres Beispiel ist die Annahme der Zeitzone und Verwendung von Sommerzeit bei `as.POSIXct()`.

Oft gibt es dafür im Tidyverse alternative Funktionen, in denen diese Probleme besser gelöst sind. Wir empfehlen, wenn immer Möglich die Tidyverse-Alternativen zu verwenden. Beispiele:

- `data_frame()` statt `data.frame()` 
- `read_*` statt `read.*`
- `parse_datetime` statt `as.POSIXct()`


Beim Import von Daten kann es sinnvoll sein, die Datentypen der Spalten bereits _im Importbefehl_ zu spezifizieren. So vermeidet man die anschliessende  Typumwandlung und die damit verbundenen Fehlerquellen. Zudem wird der Importprozess beschleunigt, da R keine Zeit daran verschwenden muss die Datentypen (aufgrund der ersten 1000 Zeilen) zu erraten.

```{r,message=FALSE, eval = FALSE}

library(tidyverse)

df1 <- read_table("09_PrePro1/data/order_52252_data.txt",
                  col_types = list(
                    col_character(),                  # Macht aus der 1.Spalte ein character
                    col_datetime(format = "%Y%m%d%H"),# Macht aus der 2.Spalte ein POSIXct
                    col_double()                      # Macht aus der 3.Spalte ein double
                    )
                  )


df1 <- read_table("09_PrePro1/data/order_52252_data.txt",
                  col_types = list(
                    col_factor(levels = NULL),        # Macht aus der 1.Spalte ein factor
                    col_datetime(format = "%Y%m%d%H"),# Macht aus der 2.Spalte ein POSIXct
                    col_double()                      # Macht aus der 3.Spalte ein double
                    )
                  )



```


```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:10_PrePro2/Nachtrag.Rmd-->


```{r, include=F, purl=F}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,include = TRUE, collapse=TRUE)
```



## Demo: `tidyverse`

[Demoscript als Download](10_PrePro2/RFiles/Demo_Tidyverse.R)


Hier möchten wir euch mit einer Sammlung von Tools vertraut machen, die spezifisch für das Daten prozessieren in Data Science entwickelt wurden. Der Prozess und das Modell ist hier^[http://r4ds.had.co.nz/introduction.html#] schön beschrieben.
Die Sammlung von Tools wird unter dem Namen [tidyverse](https://www.tidyverse.org/) vertrieben, welches wir ja schon zu Beginn der ersten Übung installiert und geladen haben. Die Tools erleichtern den Umgang mit Daten ungeheuer und haben sich mittlerweile zu einem "must have" im Umgang mit Daten in R entwickelt. 

Wir können Euch nicht sämtliche Möglichkeiten von tidyverse zeigen. Wir fokussieren uns deshalb auf einzelne Komponenten^[`dplyr, ggplot2, tidyr, stringr, magrittr, lubridate`] und zeigen ein paar Funktionalitäten, die wir oft verwenden und Euch ggf. noch nicht bekannt sind. Wer sich vertieft mit dem Thema auseinandersetzen möchte, der sollte sich unbedingt das Buch @wickham2017 beschaffen. Eine umfangreiche, aber nicht ganz vollständige Version gibt es online^[http://r4ds.had.co.nz/], das vollständige eBook kann über die Bibliothek bezogen werden^[https://ebookcentral.proquest.com/lib/zhaw/detail.action?docID=4770093].



### Split-Apply-Combine

#### Packete laden

```{r,message=F}
library(tidyverse)
```

Mit `library(tidyverse)` werden nicht alle Packete geladen, die mit `install.packages(tidyverse)` intalliert wurden ([warum?](https://community.rstudio.com/t/which-packages-get-loaded/298)). Unter anderem muss `lubridate` noch separat geladen werden:

```{r, message=F}
library(lubridate) 
```



#### Daten Laden

Wir laden die Wetterdaten von der letzten Übung.

```{r}

wetter <- read_table("09_PrePro1/data/order_52252_data.txt",
                  col_types = list(
                    col_factor(levels = NULL),    
                    col_datetime(format = "%Y%m%d%H"),
                    col_double()
                    )
                  )

```


#### Kennwerte berechnen
Wir möchten den Mittelwert aller gemessenen Temperaturwerte berechnen. Dazu könnten wir folgenden Befehl verwenden:

```{r}
mean(wetter$tre200h0, na.rm = TRUE) 
```

Die Option `na.rm = T` bedeutet, dass NA Werte von der Berechnung ausgeschlossen werden sollen. 

Mit der selben Herangehensweise können diverse Werte berechnet werden (z.B. das Maximum (`max()`), Minimum (`min()`), Median (`median()`) u.v.m.). 

Diese Herangehensweise funktioniert nur dann gut, wenn wir die Kennwerte über *alle* Beobachtungen (Zeilen) für eine Variable (Spalte) berechnen wollen. Sobald wir die Beobachtungen gruppieren wollen, wird es schwierig. Zum Beispiel, wenn wir die durchschnittliche Temperatur *pro Jahr* berechnen wollen.


#### Convenience Variablen

Um diese Aufgabe zu lösen, muss zuerst das "Jahr" berechne werden (das Jahr ist die *convenience variabel*).   Hierfür brauchen wir die Funktion `year()` (von `lubridate`). 

Nun kann kann die **convenience Variable** "Jahr" erstellt werden. Ohne `dpylr` wird eine neue Spalte wird folgendermassen hinzugefügt. 
```{r}
wetter$year <- year(wetter$time)
```


Mit `dplyr` (siehe ^[@wickham2017, Kapitel 10 / http://r4ds.had.co.nz/transform.html]) sieht der gleiche Befehl folgendermassen aus:
```{r}
wetter <- mutate(wetter,year = year(time))
```

Der grosse Vorteil von `dplyr` ist an dieser Stelle noch nicht ersichtlich. Dieser wird aber später klar.


#### Kennwerte nach Gruppen berechnen

Jetzt kann man die `data.frame` mithilfe der Spalte "Jahr" filtern. 
```{r}
mean(wetter$tre200h0[wetter$year == 2000], na.rm = TRUE)
```

Dies müssen wir pro Jahr wiederholen, was natürlich sehr umständlich ist, v.a. wenn man eine Vielzahl an Gruppen hat (z.B. Kalenderwochen statt Jahre). Deshalb nutzen wir das package `dplyr`. Damit geht die Aufgabe (Temperaturmittel pro Jahr berechnen) folgendermassen:


```{r}
summarise(group_by(wetter,year),temp_mittel = mean(tre200h0, na.rm = TRUE))
```


#### Verketten vs. verschachteln

Auf Deutsch übersetzt heisst die obige Operation folgendermassen: 

1) nimm den Datensatz `wetter`
2) Bilde Gruppen pro Jahr  (`group_by(wetter,year)`) 
3) Berechne das Temperaturmittel (`mean(tre200h0)`)

Diese Übersetzung `R`-> Deutsch unterscheidet sich vor allem darin, dass die Operation auf Deutsch *verkettet* ausgesprochen wird (Operation 1->2->3) während der Computer *verschachtelt* liest 3(2(1)). Um `R` näher an die gesprochene Sprache zu bringen, kann man den `%>%`-Operator verwenden  (siehe ^[@wickham2017, Kapitel 14 / http://r4ds.had.co.nz/pipes.html]). 
```{r, eval = F}

summarise(group_by(wetter,year),temp_mittel = mean(tre200h0))

# wird zu:

wetter %>%                                #1) nimm den Datensatz "wetter"
  group_by(year) %>%                      #2) Bilde Gruppen pro Jahr
  summarise(temp_mittel = mean(tre200h0)) #3) berechne das Temperaturmittel 

```


Dieses Verketten mittels `%>%` macht den Code einiges schreib- und leserfreundlicher, und wir werden ihn in den nachfolgenden Übungen verwenden. Dabei handelt es sich um das package `magrittr`, welches mit `tidyverse` mitgeliefert wird. 

Zu `dplyr` und `magrittr`gibt es etliche Tutorials online (siehe^[@wickham2017, Kapitel 10 / http://r4ds.had.co.nz/transform.html, oder [Hands-on dplyr tutorial..](https://youtu.be/jWjqLW-u3hc)]), deshalb werden wir diese Tools nicht in allen Details erläutern. Nur noch folgenden wichtigen Unterschied zu zwei wichtigen Funktionen in `dpylr`: `mutate()` und `summarise()`.

- `summarise()` fasst einen Datensatz zusammen. Dabei reduziert sich die Anzahl Beobachtungen (Zeilen) auf die Anzahl Gruppen (z.B. eine zusammengefasste Beobachtung (Zeile) pro Jahr). Zudem reduziert sich die Anzahl Variablen (Spalten) auf diejenigen, die in der "summarise" Funktion spezifiziert wurde (z.B. `temp_mittel`).
- mit `mutate` wird ein `data.frame` vom Umfang her belassen, es werden lediglich *zusätzliche* Variablen (Spalten) hinzugefügt (siehe Beispiel unten).

```{r, eval=T}
# Maximal und minimal Temperatur pro Kalenderwoche
wetter %>%                              #1) nimm den Datensatz "wetter"
  filter(stn == "ABO") %>%              #2) filter auf Station namnes "ABO"
  mutate(kw = week(time)) %>%       #3) erstelle eine neue Spalte "kw"
  group_by(kw) %>%                      #4) Nutze die neue Spalte um Guppen zu bilden
  summarise(
    temp_max = max(tre200h0, na.rm = TRUE),#5) Berechne das Maximum 
    temp_min = min(tre200h0, na.rm = TRUE) #6) Berechne das Minimum
    )   
```


#### Resultate plotten

Mit diesen Tools können wir nun auch eine neue Grafik plotten, ähnlich wie in der Übung 1. Dafür müssen wir die ganzen Operationen aber zuerst in einer Variabel speichern (bis jetzt hat R zwar alles schön berechnet, aber uns nur auf die Konsole ausgegeben).

```{r, warning = F}

wetter_sry <- wetter %>%                              
  mutate(
    kw = week(time)
    ) %>%
  filter(stn == "ABO") %>%
  group_by(kw) %>%                      
  summarise(
    temp_max = max(tre200h0),               
    temp_min = min(tre200h0),
    temp_mean = mean(tre200h0)
    )  
```

Dieses Mal plotten wir nur mit `ggplot2` (siehe ^[@wickham2017, Kapitel 1 / http://r4ds.had.co.nz/data-visualisation.html oder hier ein sehr schönes Video: [Learn R: An Introduction to ggplot2](https://youtu.be/YxKr2a-Y1WE?t=1m40s)]) 
 
```{r}

ggplot() +
  geom_line(data = wetter_sry, aes(kw,temp_max), colour = "yellow") +
  geom_line(data = wetter_sry, aes(kw,temp_mean), colour = "pink") +
  geom_line(data = wetter_sry, aes(kw,temp_min), colour = "black") +
  labs(y = "temp")

```


Das sieht schon mal gut aus. Nur, wir mussten pro Linie einen eigene Zeile schreiben (`geom_line()`) und dieser eine Farbe zuweisen. Bei drei Werten ist das ja ok, aber wie sieht es denn aus wenn es Hunderte sind? Da hat ggplot natürlich eine Lösung, dafür müssen aber alle Werte in *einer* Spalte daher kommen. Das ist ein häufiges Problem: Wir haben eine *breite* Tabelle (viele Spalten), bräuchten aber eine *lange* Tabelle (viele Zeilen).


### Reshaping data

#### Breit -> lang

Da kommt `tidyverse` wieder ins Spiel. Die Umformung von Tabellen *breit*->*lang* erfolgt mittels `tidyr`(siehe ^[http://r4ds.had.co.nz/tidy-data.html#gathering]). Auch dieses package funktioniert wunderbar mit piping (`%>%`). 

```{r}
wetter_sry_long <- wetter_sry %>%
  gather(Key, Value, c(temp_max,temp_min,temp_mean))

```

Im Befehl `gather()` braucht es drei Werte:

- beliebiger Name der neuen Variablen (Spalte) für die *Schlüssel*: "temp_mean", "temp_min"... (ich verwenden den Namen: `Key`)
- beliebiger Name der neuen Variablen (Spalte) für die effektiven *Werte*: 5°C, 10°C (ich verwenden den Namen: `Value`)
- Name der (bestehenden) Variablen (Spalten), die zusammen gefasst werden sollten: `(hier: temp_max,temp_min,temp_mean`)


Die ersten 6 Zeilen von `wetter_sry`:
```{r, echo = F, purl = F}
kable(head(wetter_sry,6))
```

Die ersten 6 Zeilen von `wetter_sry_long`:
```{r, echo = F, purl=F}
kable(head(arrange(wetter_sry_long,kw),6))

```

Beachte: `wetter_sry_long` umfasst 159 Beobachtungen (Zeilen), das sind 3 mal soviel wie `wetter_sry`, da wir ja drei Spalten zusammengefasst haben.
```{r}
nrow(wetter_sry)
nrow(wetter_sry_long)
```


Statt die Variablen (Spalten) zu benennen, die zusammengefasst werden sollten, wäre es in unserem Fall einfacher, die Variablen (Spalten) zu benennen die *nicht* zusammengefasst werden sollen (`kw`):

```{r,}
wetter_sry_long <- wetter_sry %>%
  gather(Key, Value, -kw)
```

Nun können wir den obigen Plot viel einfacher erstellen:

```{r}
ggplot(wetter_sry_long, aes(kw,Value, colour = Key)) +
  geom_line()
```

Beachtet, dass wir gegenüber dem letzten Plot `colour` nun *innerhalb* von `aes()` festlegen und nicht mit einem expliziten Farbwert, sondern mit dem Verweis auf die Spalte `key`.


#### Lang -> breit

Um unsere *lange* Tabelle wieder zurück in eine *breite* zu überführen, brauchen wir lediglich einen Befehl (`spread`):

```{r}
wetter_sry_long %>%
  spread(Key,Value)
```


### Quellen

```{r code=readLines('00_Admin/get_chapter_references.R'), echo=F, eval=T,purl = F, results="asis"}
```

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:10_PrePro2/Demo_Tidyverse.Rmd-->

```{r include=FALSE, purl=F}
knitr::opts_chunk$set(echo = TRUE, include = F, collapse=TRUE)
# knitr::opts_knit$set(root.dir = "10_PrePro2") 

select <- dplyr::select


```

## Übung A


```{r,message=F}
library(tidyverse)
library(lubridate)
library(stringr)
```


### Aufgabe 1

Lade die Wetterdaten aus der letzten Übung.

```{r}
# Lösung Aufgabe 1

wetter <- read_table("09_PrePro1/data/order_52252_data.txt",
                  col_types = list(
                    col_factor(levels = NULL),    
                    col_datetime(format = "%Y%m%d%H"),
                    col_double()
                    )
                  )
```


### Aufgabe 2

Bereinige den Datensatz. Entferne z.B. alle Zeilen, bei dem der Stationsnahme oder Temperaturwerte fehlen 

```{r}
# Lösung Aufgabe 2

wetter <- wetter %>%
  filter(!is.na(stn)) %>%
  filter(!is.na(tre200h0))

```



### Aufgabe 3

Überführe die **lange** Tabelle über in eine breite. Dabei sollte jede Station eine eigene Spalte enthalten (`key`), gefüllt mit den Temperaturwerten (`value`).  Speichere diese Tabelle in einer neuen Variabel.

```{r}

# Lösung Aufgabe 3

wetter_spread <- spread(wetter, stn,tre200h0)


```



### Aufgabe 4



Importiere die Datei [order_52252_legend.csv](09_PrePro1/data/order_52252_legend.csv) (z.B. mit `read_delim`).

Hinweis: Wenn Umlaute und Sonderzeichen nicht korrekt dargestellt werden (z.B. in Gen*è*ve), hat das vermutlich mit der [Zeichencodierung](https://de.wikipedia.org/wiki/Zeichenkodierung) zu tun. Das File ist aktuell in 'ANSI' Codiert, welche für gewisse Betriebssysteme / R-Versionen ein Problem darstellt. Um das Problem zu umgehen muss man das File mit einem Editor öffnen (Windows 'Editor' oder 'Notepad++', Mac: 'TextEdit') und mit einer neuen Codierung (z.B 'UTF-8') abspeichern. Danach kann die Codierung spezifitiert werden (bei `read_delim(): mit `locale = locale(encoding = "UTF-8")`)

```{r}

# Lösung Aufgabe 4

wetter_legende <- read_delim("09_PrePro1/data/order_52252_legend.csv",delim = ";", locale = locale(encoding = "UTF-8"))

```



### Aufgabe 5


Die x-/y-Koordinaten sind aktuell in einer Spalte erfasst. Um mit den Koordinaten sinnvoll arbeiten zu können, brauchen wir die Koordinaten getrennt. Trenne die `x` und `y` Koordinaten aus der Spalte `Koordinaten` (Tipp: nutze dafür `tidyr::separate()`).

```{r}

# Lösung Aufgabe 5

# Variante mit str_split_fixed()
koordinaten <- str_split_fixed(wetter_legende$Koordinaten, "/", 2)

colnames(koordinaten) <- c("x","y")

wetter_legende <- cbind(wetter_legende,koordinaten)



# Variante mit tidyr::separate
# ich lösche die Spalten wieder, damit ich die tidyr lösung zeigen kann
wetter_legende$x <- NULL 
wetter_legende$y <- NULL

wetter_legende <- wetter_legende %>%
  separate(Koordinaten,c("x","y"),"/")

```


### Aufgabe 6

Nun wollen wir den Datensatz `wetter`mit den Informationen aus `wetter_legende`anreichern. Uns interessiert aber nur das Stationskürzel, der Name, die x/y Koordinaten sowie die Meereshöhe. Lösche die nicht benötigten Spalten (oder selektiere die benötigten Spalten).

Tipp: Nutze `select()` von `dplyr`

```{r, message=F}

# Lösung Aufgabe 6

wetter_legende <- dplyr::select(wetter_legende, stn, Name, x,y,Meereshoehe)
```


### Aufgabe 7

Nun ist der Datensatz `wetter_legende`genügend vorbereitet. Jetzt kann er mit dem Datensatz `wetter` verbunden werden. Überlege dir, welcher Join dafür sinnvoll ist und mit welchem Attribut wir "joinen" können.

Nutze die Join-Möglichkeiten von `dplyr` (Hilfe via `?dplyr::join`)  um die Datensätze `wetter` und `wetter_legende`zu verbinden.

```{r}

# Lösung Aufgabe 7

wetter <- left_join(wetter,wetter_legende,by = "stn")

# Jointyp: Left-Join auf 'wetter', da uns nur die Stationen im Datensatz 'wetter' interessieren.
# Attribut: "stn"
```

### Aufgabe 8

Berechne die Durchschnittstemperatur pro Station. Nutze dabei `dplyr::summarise()` und wenn möglich `%>%`. Speichere das Resultat in einer neuen Variabel.


```{r,warning=F}

# Lösung Aufgabe 8

wetter_sry <- wetter %>%
  group_by(stn) %>%
  summarise(temp_mean = mean(tre200h0))
```

### Aufgabe 9

Nun wollen wir das Resultat aus Aufgabe 7 nutzen, um die Durchschnittstemperatur der Meereshöhe gegenüber zu stellen. Dummerweise ging das Attribut `Meereshoehe` bei der `summarise()` Operation verloren (da bei `summarise()` alle Spalten weg fallen, die **nicht** in `group_by()` definiert wurden). Um die Spalte `Meereshoehe` beizubehalten, muss sie also unter `group_by()` aufgelistet werden. 

Wiederhole Übung 7 und siehe zu, dass die Meereshöhe beibehalten wird. Stelle danach in einem Scatterplot (wenn möglich mit `ggplot()`) die Meereshöhe der Durchschnittstemperatur gegenüber.

```{r}
# Lösung Aufgabe 9

wetter_sry <- wetter %>%
  group_by(stn,Meereshoehe) %>%
  summarise(temp_mean = mean(tre200h0))

# Achtung: wenn mehrere Argumente in group_by() definiert werden führt das 
# üblicherweise zu Untergruppen. In unserem Fall hat jede Station nur EINE 
# Meereshöhe, deshalb wird die Zahl der Gruppen nicht erhöht.
```


```{r}

ggplot(wetter_sry, aes(temp_mean,Meereshoehe)) +
  geom_point()
```


```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:10_PrePro2/Uebung_A.Rmd-->

## Übung A: Lösung

[R-Code als Download](10_PrePro2/RFiles/Uebung_A.R)


```{r code=readLines('10_PrePro2/RFiles/Uebung_A.R'), echo=T, eval=F, include = T}
```

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:10_PrePro2/Uebung_A_loesung.Rmd-->

```{r, include=FALSE, purl = F}
knitr::opts_chunk$set(collapse=TRUE)
# knitr::opts_knit$set(root.dir = "10_PrePro2")

```

## Übung B


```{r,message=F}
library(tidyverse)
library(lubridate)
library(stringr)
```



### Aufgabe 1

Gegeben sind die Daten von drei Sensoren ([sensor1.csv](10_PrePro2/data/sensor1.csv), [sensor2.csv](10_PrePro2/data/sensor2.csv), [sensor3.csv](10_PrePro2/data/sensor3.csv)). Lade die Datensätze runter und lese sie ein.



```{r, message=F}
# Lösung Aufgabe 1

sensor1 <- read_delim("10_PrePro2/data/sensor1.csv",";")
sensor2 <- read_delim("10_PrePro2/data/sensor2.csv",";")
sensor3 <- read_delim("10_PrePro2/data/sensor3.csv",";")

```

### Aufgabe 2



Füge die drei Tabellen zu **einer** zusammen. Dazu kannst du entweder die  Spalten (Variablen) mittels `join()` oder die Zeilen (Beobachtungen) mittels `rbind()` zusammen "kleben".  Überführe zudem die Spalte `Datetime` in ein `POSIXct`-Format. Das ursprüngliche Format lautet:`DDMMYYYY_HHMM`


```{r}

# Lösung Aufgabe 2 (Var 1: Spalten [Variabeln] zusammen 'kleben')
sensor_all <- sensor1 %>%
  rename(sensor1 = Temp) %>%              # Spalte "Temp" in "sensor1" umbenennen
  full_join(sensor2,by = "Datetime") %>%    
  rename(sensor2 = Temp) %>%
  full_join(sensor3, by = "Datetime") %>%
  rename(sensor3 = Temp) %>%
  mutate(Datetime = as.POSIXct(Datetime,format = "%d%m%Y_%H%M"))
```




```{r}

# Lösung Aufgabe 2 (Var 2: Zeilen [Beobachtungen] zusammen 'kleben)

sensor1$sensor <- "sensor1"
sensor2$sensor <- "sensor2"
sensor3$sensor <- "sensor3"

sensor_all <- rbind(sensor1,sensor2,sensor3)

sensor_all <- sensor_all %>%
  mutate(
    Datetime = as.POSIXct(Datetime,format = "%d%m%Y_%H%M")
  ) %>%
  spread(sensor, Temp)

```


```{r,  echo = F, eval = T, purl=F}
# Die neue Tabelle sollte folgendermassen aussehen:
knitr::kable(sensor_all)
```



### Aufgabe 3

Importiere die Datei [sensor_1_fail.csv](10_PrePro2/data/sensor_fail.csv) in `R`.


```{r, message = F}

# Lösung Aufgabe 3

sensor_fail <- read_delim("10_PrePro2/data/sensor_fail.csv", delim = ";")

```


```{r,  echo = F, eval = T, purl=F}
knitr::kable(sensor_fail)
```


`sensor_fail.csv` hat eine Variabel `SensorStatus`: `1` bedeutet der Sensor misst, `0` bedeutet der Sensor miss nicht. Fälschlicherweise wurde auch dann der Messwert `Temp = 0` erfasst, wenn `Sensorstatus = 0`. Richtig wäre hier `NA` (not available). Korrigiere den Datensatz entsprechend.


```{r}

# Lösungsweg 1
sensor_fail$Datetime <- as.POSIXct(sensor_fail$Datetime,format = "%d%m%Y_%H%M")

sensor_fail$`Hum_%`[sensor_fail$SensorStatus == 0] <- NA
sensor_fail$Temp[sensor_fail$SensorStatus == 0] <- NA
```


```{r, message = F}

# Lösungsweg 2

sensor_fail <- read_delim("10_PrePro2/data/sensor_fail.csv", delim = ";")


sensor_fail_corr <- sensor_fail %>%
  mutate(
    Datetime = as.POSIXct(Datetime,format = "%d%m%Y_%H%M")
  ) %>%
  rename(Humidity = `Hum_%`) %>%         # Weil R "%" in Headers nicht mag
  gather(key,val, c(Temp, Humidity)) %>%
  mutate(
    val = ifelse(SensorStatus == 0,NA,val)
  ) %>%
  spread(key,val)
  
```


### Aufgabe 4


Warum spielt das es eine Rolle, ob `0` oder `NA` erfasst wird? Vergleiche dazu die Mittlere Temperatur / Feuchtigkeit vor und nach der Korrektur. 

```{r}

# Lösung Aufgabe 4

# Mittelwerte der unkorrigierten Sensordaten (`NA` als `0`)
mean(sensor_fail$Temp)
mean(sensor_fail$`Hum_%`)

```



```{r}
# Mittelwerte der korrigierten Sensordaten (`NA` als `NA`). Hier müssen wir die Option 
# `na.rm = T` (Remove NA = T) wählen, denn `mean()` (und ähnliche Funktionen) retourieren 
# immer `NA`, sobald ein **einzelner** Wert in der Reihe `NA`ist.
mean(sensor_fail_corr$Temp, na.rm = T)
mean(sensor_fail_corr$Humidity, na.rm = T)

```



```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:10_PrePro2/Uebung_B.Rmd-->

## Übung B: Lösung

[R-Code als Download](10_PrePro2/RFiles/Uebung_B.R)


```{r code=readLines('10_PrePro2/RFiles/Uebung_B.R'), echo=T, eval=F, include = T}
```

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:10_PrePro2/Uebung_B_loesung.Rmd-->

# InfoVis1 (21.10.2019)

Die konventionelle schliessende Statistik arbeitet in der Regel konfirmatorisch, sprich aus der bestehenden Theorie heraus werden Hypothesen formuliert, welche sodann durch Experimente geprüft und akzeptiert oder verworfen werden. Die Explorative Datenanalyse (EDA) nimmt dazu eine antagonistische Analyseperspektive ein und will in den Daten zunächst Zusammenhänge aufdecken, welche dann wiederum zur Formulierung von prüfbaren Hypothesen führen kann. Die Einheit stellt dazu den klassischen 5-stufigen EDA-Prozess nach Tukey (1980!) vor. Abschliessend wird dann noch die Brücke geschlagen zur modernen Umsetzung der EDA in Form von Visual Analytics.
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:11_InfoVis1/Abstract.Rmd-->

```{r, include=F, purl=F}

knitr::opts_chunk$set(echo = T,include = T,message = F, collapse=TRUE)
# knitr::opts_knit$set(root.dir = "11_InfoVis1") 

```




## EDA Beispiel Vorlesung

[Demoscript als Download](11_InfoVis1/RFiles/Demo_EDA.R)


```{r,message=F}


library(tidyverse)
library(scales)

# create some data about age and height of people
people <- data.frame(
  ID = c(1:30),
  
  age = c(5.0, 7.0, 6.5 ,9.0, 8.0, 5.0, 8.6, 7.5, 9.0, 6.0,
          63.5 ,65.7, 57.6, 98.6, 76.5, 78.0, 93.4, 77.5, 256.6, 512.3,
          15.5, 18.6, 18.5, 22.8, 28.5, 39.5, 55.9, 50.3, 31.9, 41.3),
  
  height = c(0.85, 0.93, 1.1, 1.25, 1.33, 1.17, 1.32, 0.82, 0.89, 1.13,
             1.62, 1.87, 1.67, 1.76, 1.56, 1.71, 1.65, 1.55, 1.87, 1.69,
             1.49, 1.68, 1.41, 1.55, 1.84, 1.69, 0.85, 1.65, 1.94, 1.80),
  
  weight = c(45.5, 54.3, 76.5, 60.4, 43.4, 36.4, 50.3, 27.8, 34.7, 47.6,
             84.3, 90.4, 76.5, 55.6, 54.3, 83.2, 80.7, 55.6, 87.6, 69.5,
             48.0, 55.6, 47.6, 60.5, 54.3, 59.5, 34.5, 55.4, 100.4, 110.3)
)



# build a scatterplot for a first inspection
ggplot(people, aes(x=age, y=height)) + 
  geom_point() + 
  scale_y_continuous(limits=c(0.75, 2.0)) +
  theme(axis.text=element_text(size=12), axis.title=element_text(size=20,face="bold"))

# Go to help page: http://docs.ggplot2.org/current/ -> Search for icon of fit-line
# http://docs.ggplot2.org/current/geom_smooth.html

# build a scatterplot for a first inspection, with regression line
ggplot(people, aes(x=age, y=height)) + 
  geom_point() + 
  scale_y_continuous(limits=c(0, 2.0)) +
  geom_smooth(method="loess", fill='lightblue', size=0.5, alpha=0.5) +
  theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face="bold"))

?stem

# stem and leaf plot
stem(people$height)
stem(people$height, scale=2)

# explore the two variables with box-whiskerplots
summary(people$age)
boxplot(people$age)

boxplot(people$age)

summary(people$height)
boxplot(people$height)

boxplot(people$height)


# explore data with a histgram
ggplot(people, aes(x=age)) + 
  geom_histogram(stat="bin", fill='green', binwidth=20) + 
  theme_bw() + labs(x = '\nage', y = 'count\n') +
  theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face="bold")) 

density(x = people$height)


# re-expression: use log or sqrt axes
#
# Find here guideline about scaling axes 
# http://www.cookbook-r.com/Graphs/Axes_(ggplot2)/
# http://docs.ggplot2.org/0.9.3.1/scale_continuous.html


# logarithmic axis: respond to skewness in the data, e.g. log10 
ggplot(people, aes(x=age, y=height)) + 
  geom_point() + 
  scale_y_continuous(limits=c(0, 2.0)) +
  geom_smooth(method="lm", fill='lightblue', size=0.5, alpha=0.5) +
  theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face="bold")) +
  scale_x_log10()

# logarithmic axis: show multiplicative factors, e.g. log2
ggplot(people, aes(x=age, y=height)) + 
  geom_point() + 
  scale_y_continuous(limits=c(0, 2.0)) +
  geom_smooth(method="lm", fill='lightblue', size=0.5, alpha=0.5) +
  theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face="bold")) +
  scale_x_continuous(trans = log2_trans(),
                   breaks = trans_breaks("log2", function(x) 2^x),
                   labels = trans_format("log2", math_format(2^.x)))


# outliers: Remove very small and very old people
peopleTemp <- subset(people, ID != 27) # Diese Person war zu klein.
peopleClean <- subset(peopleTemp, age < 100) # Fehler in der Erhebung des Alters

# re-explore cleaned data with a histgram
ggplot(peopleClean, aes(x=age)) + 
  geom_histogram(stat="bin", fill='#6baed6', binwidth=10) + 
  theme_bw() + labs(x = '\nage', y = 'count\n') +
  theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face="bold"))

ggplot(peopleClean, aes(x=age, y=height)) + 
  geom_point() + 
  scale_y_continuous(limits=c(0, 2.0)) +
  geom_smooth(method="lm", fill='lightblue', size=0.5, alpha=0.5) +
  theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face="bold"))

# with custom binwidth
ggplot(peopleClean, aes(x=age)) + 
  geom_histogram(stat="bin", fill='#6baed6', binwidth=10) + 
  theme_bw() + labs(x = '\nAlter', y = 'Anzahl\n')



# quadratic axis
ggplot(peopleClean, aes(x=age, y=height)) + 
  geom_point() + scale_y_continuous(limits=c(0, 2.0)) +
  geom_smooth(method="lm", fill='lightblue', size=0.5, alpha=0.5) +
  theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face="bold")) +
  scale_x_sqrt()




# subset "teenies": No trend
kids <- subset(peopleClean, age < 15)

ggplot(kids, aes(x=age, y=height)) + 
  geom_point() + 
  scale_y_continuous(limits=c(0, 2.0)) +
  geom_smooth(method="lm", fill='lightblue', size=0.5, alpha=0.5) +
  theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face="bold"))


# subset "teenies": No trend
oldies <- subset(peopleClean, age > 55)

ggplot(oldies, aes(x=age, y=height)) + 
  geom_point() + 
  scale_y_continuous(limits=c(0, 2.0)) +
  geom_smooth(method="lm", fill='lightblue', size=0.5, alpha=0.5) +
  theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face="bold"))


# Onwards towards multidimensional data

# Finally, make a scatterplot matrix
pairs(peopleClean[,2:4], panel=panel.smooth)

pairs(peopleClean[,2:4], panel=panel.smooth)

# Or as a bubble chart
peopleClean$radius <- sqrt( peopleClean$weight/ pi )
symbols(peopleClean$age, peopleClean$height, circles=peopleClean$radius)

symbols(peopleClean$age, peopleClean$height, circles=peopleClean$radius)

```

### Quellen

```{r code=readLines('00_Admin/get_chapter_references.R'), echo=F, eval=T,purl = F, results="asis"}
```
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:11_InfoVis1/Demo_EDA.Rmd-->


```{r, include=F, purl=F}

knitr::opts_chunk$set(echo = T,include = T,message = F, collapse=TRUE) # 
# knitr::opts_knit$set(root.dir = "11_InfoVis1") 

```

```{r, message = F}
library(tidyverse)
library(lubridate)

```

## Demo: `ggplot2`

[Demoscript als Download](11_InfoVis1/RFiles/Demo_ggplot.R)

Als erstes laden wir den Wetterdatensatz von der Übung Prepro1 ein.

```{r}
wetter <- read_table("09_PrePro1/data/order_52252_data.txt",
                  col_types = list(
                    col_factor(levels = NULL),    
                    col_datetime(format = "%Y%m%d%H"),
                    col_double()
                    )
                  )
```


```{r, echo = F, eval = T, purl=F}

knitr::kable(head(wetter))

```

Der Datensatz hat `r nrow(wetter)` Zeilen. Bevor wir mit plotten beginnen, müssen wir den Datensatz etwas filtern da die Plots ansonsten zu schwerfällig werden. Wir filtern deshalb auf Januar 2000.

```{r}
wetter_fil <- wetter %>%
  mutate(
    year = year(time),
    month = month(time)
    ) %>%
  filter(year == 2000 & month == 1)
```


Ein ggplot wird durch den Befehl `ggplot()` initiiert. Hier wird einerseits der Datensatz festgelegt, auf dem der Plot beruht (`data = `), sowie die Variablen innerhalb des Datensatzes, die Einfluss auf den Plot ausüben (`mapping = aes()`). 

Weiter braucht es *mindestens* ein "Layer" der beschreibt, wie die Daten dargestellt werden sollen (z.B. `geom_point()`).

Anders als bei "Piping" (`%>%`) wird ein Layer mit `+` hinzugefügt.

```{r}
# Datensatz: "wetter_fil" | Beeinflussende Variabeln: "time" und "tre200h0"
ggplot(data = wetter_fil, mapping = aes(time,tre200h0)) +
  # Layer: "geom_point" entspricht Punkten in einem Scatterplot 
  geom_point()                                                 

```


Da ggplot die Eingaben in der Reihenfolge `data = ` und dann `mapping = `erwartet, können wir diese Spezifizierungen auch weglassen.

```{r, eval=F}

ggplot(wetter_fil, aes(time,tre200h0)) +
  geom_point()

```

Nun wollen wir die unterschiedlichen Stationen unterschiedlich einfärben. Da wir Variablen definieren wollen, welche Einfluss auf die Grafik haben sollen, gehört diese Information in `aes()`.

```{r}
ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) +
  geom_point()
```

Wir können noch einen Layer mit Linien hinzufügen:

```{r}
ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) +
  geom_point() +
  geom_line()

```

Weiter können wir die Achsen beschriften und einen Titel hinzufügen. Zudem lasse ich die Punkte (`geom_point()`) nun weg, da mir diese nicht gefallen.

```{r}
ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) +
  geom_line() +
  labs(x = "Woche",
       y = "Temperatur in Grad C°", 
       title = "Temperaturdaten Schweiz",
       subtitle = "Januar 2000")
```

Man kann auch Einfluss auf die x-/y-Achsen nehmen. Dabei muss man zuerst festlegen, was für ein Achsentyp der Plot hat (vorher hat `ggplot` eine Annahme auf der Basis der Daten getroffen). 




Bei unserer y-Achse handelt es sich um numerische Daten, `ggplot` nennt diese: `scale_y_continuous()`. Unter [ggplot2.tidyverse.org](http://ggplot2.tidyverse.org/reference/#section-scales) findet man noch andere x/y-Achsentypen (`scale_x_irgenwas` bzw. `scale_y_irgendwas`).

```{r}
ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) +
  geom_line() +
  labs(x = "Woche",
       y = "Temperatur in Grad C°", 
       title = "Temperaturdaten Schweiz",
       subtitle = "Januar 2000") +    
  scale_y_continuous(limits = c(-30,30))    # y-Achsenabschnitt bestimmen

```


Das gleiche Spiel kann man für die y-Achse betreiben. Bei unserer y-Achse handelt es sich ja um unsere `POSIXct` Daten. `ggplot` nennt diese: `scale_x_datetime()`. 
```{r}
ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) +
  geom_line() +
  labs(x = "Woche",
       y = "Temperatur in Grad C°", 
       title = "Temperaturdaten Schweiz",
       subtitle = "Januar 2000") +    
  scale_y_continuous(limits = c(-30,30)) +
  scale_x_datetime(date_breaks = "1 week", 
                   date_minor_breaks = "1 day", 
                   date_labels = "KW%W")

```


Mit `theme` verändert man das allgmeine Layout der Plots. Beispielsweise kann man mit `theme_classic()` `ggplot`-Grafiken etwas weniger "Poppig" erscheinen lassen: so sind sie besser für Bachelor- / Masterarbeiten sowie Publikationen geeignet. `theme_classic()` kann man indiviudell pro Plot anwenden, oder für die aktuelle Session global setzen (s.u.)

Individuell pro Plot:
```{r}
ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) +
  geom_line() +
  labs(x = "Woche",
       y = "Temperatur in Grad C°", 
       title = "Temperaturdaten Schweiz",
       subtitle = "Januar 2000") +    
  scale_y_continuous(limits = c(-30,30)) +
  scale_x_datetime(date_breaks = "1 week", 
                   date_minor_breaks = "1 day", 
                   date_labels = "KW%W") +
  theme_classic()
```

Global (für alle nachfolgenden Plots der aktuellen Session):

```{r}
theme_set(theme_classic())
```


Sehr praktisch sind auch die Funktionen für "Small multiples". Dies erreicht man mit `facet_wrap()` (oder `facet_grid()`, mehr dazu später). Man muss mit einem Tilde-Symbol "`~`" nur festlegen, welche *Variable* für das Aufteilen des Plots in kleinere Subplots verantwortlich sein soll. 

```{r, fig.width=8,fig.height=10}
ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) +
  geom_line() +
  labs(x = "Woche",
       y = "Temperatur in Grad C°", 
       title = "Temperaturdaten Schweiz",
       subtitle = "Januar 2000") +    
  scale_y_continuous(limits = c(-30,30)) +
  scale_x_datetime(date_breaks = "2 weeks", 
                   date_minor_breaks = "1 day", 
                   date_labels = "KW%W") +
  facet_wrap(~stn)

```


Auch `facet_wrap` kann man auf seine Bedürfnisse anpassen. Da wir 24 Stationen haben möchte ich lieber 3 pro Zeile, damit es schön aufgeht. Dies erreiche ich mit `ncol = 3`.

Zudem brauchen wir die Legende nicht mehr, da der Stationsnamen über jedem Facet steht. Ich setze deshalb `theme(legend.position="none")` 

```{r, fig.width=8,fig.height=10}
ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) +
  geom_line() +
  labs(x = "Woche",
       y = "Temperatur in Grad C°", 
       title = "Temperaturdaten Schweiz",
       subtitle = "Januar 2000") +  
  scale_y_continuous(limits = c(-30,30)) +
  scale_x_datetime(date_breaks = "1 week", date_minor_breaks = "1 day", date_labels = "KW%W") +
  facet_wrap(~stn,ncol = 3) +
  theme(legend.position="none")
```


Genau wie `data.frames` und andere Objekte, kann man einen ganzen Plot auch in einer Variabel speichern. Dies kann nützlich sein um einen Plot zu exportieren (als png, jpg usw.) oder sukzessive erweitern wie in diesem Beispiel.

```{r, message = F}
p <- ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) +
  geom_line() +
  labs(x = "Woche",
       y = "Temperatur in Grad C°", 
       title = "Temperaturdaten Schweiz",
       subtitle = "Januar 2000") +
  scale_y_continuous(limits = c(-30,30)) +
  scale_x_datetime(date_breaks = "1 week", date_minor_breaks = "1 day", date_labels = "KW%W") +
  facet_wrap(~stn,ncol = 3)
  # ich habe an dieser Stelle theme(legend.position="none") entfernt



```

Folgendermassen kann ich den Plot als png-File abspeichern (ohne Angabe von "plot = " wird einfach der letzte Plot gespeichert)

```{r, eval = F}
ggsave(filename = "11_InfoVis1/plot.png",plot = p)
```

.. und so kann ich einen bestehenden Plot (in einer Variabel) mit einem Layer / einer Option erweitern

```{r, eval = F}
p +
  theme(legend.position="none")

```


Wie üblich wurde diese Änderung nicht gespeichert, sondern nur das Resultat davon ausgeben. Wenn die Änderung in meinem Plot (in der Variabel) abspeichern will, muss ich die Variabel überschreiben:

```{r}
p <- p +
  theme(legend.position="none")
```


Mit `geom_smooth()` kann `ggplot` eine Trendlinie auf der Baiss von Punktdaten berechnen. Die zugrunde liegende statistische Methode kann selbst gewählt werden. Wenn nichts angegeben wird verwendet `ggplot` bei weniger als 1'000 Messungen, die Methode `loess` (local smooths).


```{r, fig.width=8,fig.height=10}
p <- p +
  geom_smooth(colour = "black")

p
```

### Quellen

```{r code=readLines('00_Admin/get_chapter_references.R'), echo=F, eval=T,purl = F, results="asis"}
```

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:11_InfoVis1/Demo_ggplot.Rmd-->

```{r, include=F, purl=F}
library(knitr)

knitr::opts_chunk$set(echo = F,include = T,message = F, collapse=TRUE) # 
# knitr::opts_knit$set(root.dir = "11_InfoVis1") 


```

## Übung

In dieser Übung geht es darum, die Grafiken aus dem Blog-post von Marko Kovic ([blog.tagesanzeiger.ch](https://blog.tagesanzeiger.ch/datenblog/index.php/668/je-weniger-auslaender-desto-mehr-ja-stimmen-wirklich)) zu rekonstruieren. Freundlicherweise hat Herr Kovic meist die `ggplot2` Standardeinstellungen benutzt, was die Rekonstruktion relativ einfach macht. 

Die Links im Text verweisen auf die Originalgrafik, die eingebetteten Plots sind meine eigenen Rekonstruktionen. Importiere als erstes den Datensatz [initiative_masseneinwanderung_kanton.csv](11_InfoVis1/data/initiative_masseneinwanderung_kanton.csv) (auf der Blog-Seite erhältlich).


```{r, message=F}

library(tidyverse)
library(ggplot2)
library(stringr)

```


```{r, eval=F}
# Es kann sein, dass man die Codierung des Files spezifizieren muss. Mit `readr::read_delim()` 
# läuft dies mit der Option locale = locale(encoding = "UTF-8") wobei anstelle von UTF-8 die 
# entsprechende Codierung angegeben wird. 
# Tipp: Excel speichert CSV oft in ANSI, welches für den Import in R nicht sonderlich geeignet 
# ist. Falls Probleme auftreten muss das File mittels einer geeigneter Software (Widows: "Editor" 
# oder "Notepad++", Mac: "TextEdit")  und mit einer neuen Codierung (z.B. `UTF-8`) abgespeichert 
# werden.
```

```{r}
kanton <- read_delim("11_InfoVis1/data/initiative_masseneinwanderung_kanton.csv",",",locale = locale(encoding = "UTF-8"))
```



### Aufgabe 1

Rekonstruiere [Grafik 1](https://blog.tagesanzeiger.ch/datenblog/wp-content/uploads/sites/32/2014/03/Kantone-2.png) von Kovic. Erstelle dazu einen Scatterplot wo der Ausländeranteil der Kantone dem Ja-Anteil gegenüber gestellt wird. Speichere den Plot einer Variabel `plot1`.

- nutze `coord_fixed()` um die beiden Achsen in ein fixes Verhältnis zu setzen (1:1).
- setze die Achsen Start- und Endwerte mittels `lims()` oder `scale_y_continuous`bzw. `scale_x_continuous`.
- Optional: Setze analog Kovic die `breaks` (`0.0`, `0.1`...`0.7`) manuell

Rekonstruktion:

```{r}

# Lösung zu Aufgabe 1

# da die Spalten in Kovic's Daten Umlaute und Sonderzeichen enthalten, müssen diese in R mit Graviszeichen 
# angesprochen werden. Dieses Zeichen wirder Schweizer Tastatur [1]  mit 
# Shitft + Gravis (Links von der Backspace taste) + Leerschlag erstellt
# [1] https://de.wikipedia.org/wiki/Tastaturbelegung#Schweiz


# Alternativ können die Spalten im Originalfile oder mit dplyr::rename() umbenannt werden


plot1 <- ggplot(kanton, aes(`Ausländeranteil`, `Ja-Anteil`)) +
  geom_point() +
  coord_fixed(1) +
  scale_y_continuous(breaks = c(0,0.1,0.3,0.5,0.7),limits =  c(0,0.7)) +
  scale_x_continuous(breaks = c(0,0.1,0.3,0.5,0.7),limits =  c(0,0.7)) +
  labs(y = "Anteil Ja-Stimmen")

plot1
```


### Aufgabe 2

Rekonstruiere [Grafik 2](https://blog.tagesanzeiger.ch/datenblog/wp-content/uploads/sites/32/2014/03/Kantone-LOESS-986x923.png). Erweitere dazu `plot1` mit einer Trendlinie.

```{r}
# Lösung zu Aufgabe 2

plot1 +
  geom_smooth()
```



### Aufgabe 3


Importiere die Gemeindedaten [initiative_masseneinwanderung_gemeinde.csv](11_InfoVis1/data/initiative_masseneinwanderung_gemeinde.csv):

```{r, echo = T}
gemeinde <- read_delim("11_InfoVis1/data/initiative_masseneinwanderung_gemeinde.csv",",",locale = locale(encoding = "UTF-8"))
```


Rekonstruiere [Grafik 3](https://blog.tagesanzeiger.ch/datenblog/wp-content/uploads/sites/32/2014/03/Gemeinden-2-986x939.png). Stelle dazu den Ausländeranteil aller Gemeinden dem Ja-Stimmen-Anteil gegenüber. Speichere den Plot als `plot2`

```{r}
# Lösung zu Aufgabe 3

plot2 <- ggplot(gemeinde, aes(`Anteil Ausl`, `Anteil Ja`)) +
  geom_point() +
  labs(x = "Ausländeranteil",y = "Anteil Ja-Stimmen") +
  coord_fixed(1) +
  lims(x = c(0,1), y = c(0,1))

plot2
```


### Aufgabe 4

Rekonstruiere [Grafik 4](https://blog.tagesanzeiger.ch/datenblog/wp-content/uploads/sites/32/2014/03/Gemeinden-GAM-2-986x939.png) indem `plot2` mit einer Trendlinie erweitert wird.

```{r}
# Lösung zu Aufgabe 4

plot2 +
  geom_smooth()
```


### Aufgabe 5

Rekonstruiere [Grafik 5](https://blog.tagesanzeiger.ch/datenblog/wp-content/uploads/sites/32/2014/03/Gemeinden-x-Kantone-2-986x857.png) indem `plot2` mit `facetting` erweitert wird. Die Facets sollen die einzelnen Kantone sein. Speichere den Plot als `plot3`.

```{r}

# Lösung zu Aufgabe 5

plot3 <- plot2 +
  facet_wrap(~Kanton)
plot3
```


### Aufgabe 6

Rekonstruiere [Grafik 6](https://blog.tagesanzeiger.ch/datenblog/wp-content/uploads/sites/32/2014/03/Gemeinden-x-Kantone-LOESS-2-986x857.png) indem `plot3` mit einer Trendlinie erweitert wird.

Rekonstruktion:

```{r, warning=F}

# Lösung zu Aufgabe 6

plot3 +
  geom_smooth()
```


### Aufgabe 7

Rekonstruiere [Grafik 7](https://blog.tagesanzeiger.ch/datenblog/wp-content/uploads/sites/32/2014/03/Gemeinden-x-Quantile-2-986x637.png) indem `plot2`mit `facetting` erweitert wird. Die Facets sollen nun den Grössen-Quantilen entsprechen. Speichere den Plot unter `plot4`.

Rekonstruktion:

```{r}

# Lösung zu Aufgabe 7

plot4 <- plot2 +
  facet_wrap(~Quantile)
plot4
```


### Aufgabe 8

Rekonstruiere [Grafik 8](https://blog.tagesanzeiger.ch/datenblog/wp-content/uploads/sites/32/2014/03/Gemeinden-x-Quantile-LOESS-2-986x637.png) indem `plot4` mit einer Trendlinie ausgestattet wird.

```{r}

# Lösung zu Aufgabe 8

plot4 +
  geom_smooth()
```


### Aufgabe 9 (Fortgeschritten)

Rekonstruiere die [Korrelationstabelle](https://tagi_dwpro.s3.amazonaws.com/UMvkt/2/fs.html).

Tipp: 
- Nutze `group_by()` und `summarise()`
- Nutze `cor.test()` um den Korrelationskoeffizienten sowie den p-Wert zu erhalten. 
- Mit `$estimate` und `$p.value` können die entsprechenden Werte direkt angesprochen werden

Hinweis: aus bisher unerklärlichen Gründen weiche gewisse meiner Werte leicht von den Berechnungen des Herrn Kovics ab.

```{r}

# Lösung zu Aufgabe 9

korr_tab <- gemeinde %>%
  group_by(Kanton) %>%
  summarise(
    Korr.Koeffizient = cor.test(`Anteil Ja`,`Anteil Ausl`,method = "pearson")$estimate,
    Signifikanz_val = cor.test(`Anteil Ja`,`Anteil Ausl`,method = "pearson")$p.value,
    Signifikanz = ifelse(Signifikanz_val < 0.001,"***",ifelse(Signifikanz_val<0.01,"**",ifelse(Signifikanz_val<0.05,"*","-")))
  ) %>%
  select(-Signifikanz_val)

```



```{r,echo=F, purl=F}
knitr::kable(korr_tab)
```


```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:11_InfoVis1/Uebung.Rmd-->

## Lösung

[RCode als Download](11_InfoVis1/RFiles/Uebung.R)


```{r code=readLines('11_InfoVis1/RFiles/Uebung.R'), echo=T, eval=F}
```
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:11_InfoVis1/Uebung_loesung.Rmd-->

# InfoVis2 (22.10.2019)

Die Informationsvisualisierung ist eine vielseitige, effektive und effiziente Methode für die explorative Datenanalyse. Während Scatterplots und Histogramme weitherum bekannt sind, bieten weniger bekannte Informationsvisualisierungs-Typen wie etwa Parallelkoordinatenplots, TreeMaps oder Chorddiagramme originelle alternative Darstellungsformen zur visuellen Analyse von Datensätze, welche stets grösser und komplexer werden. Die Studierenden lernen in dieser Lerneinheit eine Reihe von Informationsvisualisierungstypen kennen, lernen diese zielführend zu gestalten und selber zu erstellen.
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:12_InfoVis2/Abstract.Rmd-->

---
output:
  pdf_document: default
  html_document: default
---

```{r, include=F, purl = F}
library(knitr)
knitr::opts_chunk$set(echo = F,include = T,message = F, warning = F, collapse=TRUE)
# knitr::opts_knit$set(root.dir = "12_InfoVis2") 

```



## Übung A

```{r, message = F, echo = T}
library(tidyverse)
library(lubridate)
```


Laden den `wetter`-Datensatz, bereinige ihn wenn nötig (`NA`-Werte entfernen) und importiere auch den Datensatz `order_52252_legend.csv` und verbinde die Datensätze mit einem join via dem Stationskürzel.



```{r, echo = T, message = F}

wetter <- read_table("09_PrePro1/data/order_52252_data.txt",
                     col_types = list(
                       col_character(),    
                       col_datetime(format = "%Y%m%d%H"),
                       col_double()
                       )
                     )

wetter <- wetter %>%
  filter(!is.na(stn)) %>%
  filter(!is.na(time))

station_meta <- read_delim("09_PrePro1/data/order_52252_legend.csv",";")

wetter <- left_join(wetter,station_meta,by = "stn")

```




### Aufgabe 1

Erstelle zwei Hilfsspalten (convenience variables) "Jahr" und "Monat". Filtere auf ein beliebiges Jahr und zwei beliebige Monate. Speichere den gefilterten Datensatz in einer neuen Variablen ab. Verwende diesen Datensatz für alle folgenden Übungen.

```{r}

# Lösung Aufgabe 1

wetter_fil <- wetter %>%
  mutate(
    year = year(time),
    month = month(time)
  ) %>%
  filter(year == 2000 & month < 3)
```


### Aufgabe 2

Erstelle ein Scatterplot (`time` vs. `tre200h0`) wobei die Punkte aufgrund ihrer Meereshöhe eingefärbt werden sollen. Tiefe Werte sollen dabei blau eingefärbt werden und hohe Werte rot. Verkleinere die Punkte um übermässiges Überplotten der Punkten zu vermeiden. Weiter sollen im Abstand von zwei Wochen die Kalenderwochen auf der Achse erscheinen. 

Speichere den Plot in einer Variabel `p` ab.

```{r}

# Lösung Aufgabe 2

p <- ggplot(wetter_fil, aes(time,tre200h0, colour = Meereshoehe)) +
  geom_point(size = 0.5) +
  labs(x = "Kalenderwoche", y = "Temperatur in ° Celsius") +
  scale_color_continuous(low = "blue", high = "red") +
  scale_x_datetime(date_breaks = "2 week", date_labels = "KW%W") 

p 

```



### Aufgabe 3

Füge am obigen Plot (gespeichert als Variabel `p`) eine schwarze, gestrichelte Trendlinie hinzu und aktualisiere `p` (`p <- p + ...`).

```{r, message=F}

# Lösung Aufgabe 3

p <- p +
  stat_smooth(colour = "black",lty = 2)

p
```


### Aufgabe 4

Positioniere die Legende oberhalb des Plots und lege sie quer (nutze dazu `theme()` mit `legend.direction` und `legend.position`). Speichere diese Änderungen in `p`.

```{r, message=F}

# Lösung Aufgabe 4


p <- p + 
  theme(legend.direction = "horizontal",legend.position = "top")

p
    
```



### Aufgabe 5 (für ambitionierte)

Füge den Temperaturwerten auf der y-Ache ein `°C` hinzu (siehe unten und studiere [diesen Tipp](https://stackoverflow.com/a/35967126/4139249) zur Hilfe). Aktualisiere `p` an dieser Stelle noch nicht.

```{r, message=F}

# Lösung Aufgabe 5

p +
  scale_y_continuous(labels = function(x)paste0(x,"°C")) +
  labs(x = "Kalenderwoche", y = "Temperatur")


```


### Aufgabe 6 (für *noch* ambitioniertere)

Füge dem Plot eine zweite, korrekt ausgerichtete Achse mit Kelvin oder Farenheit hinzu (siehe `sec_axis`). Wenn du es vorherigen Übung schon geschafft hast, setze auch hier die Einheit (`K` rep. `°F`) hinter die Werte auf der Achse. 

$$ K = °C + 273,15$$
$$°F = °C × \frac{9}{5} + 32$$

```{r, message=F}
# Lösung Aufgabe 6

p <- p +
  labs(x = "Kalenderwoche", y = "Temperatur") +
  scale_y_continuous(labels = function(x)paste0(x,"°C"),sec.axis = sec_axis(~.*(9/5)+32,name = "Temperatur",labels = function(x)paste0(x,"° F")))


p
```




### Aufgabe 7

Jetzt verlassen wir den scatterplot und machen einen Boxplot mit den Temperaturdaten. Färbe die Boxplots wieder in Abhängigkeit der Meereshöhe ein. 

- Beachte den Unterschied zwischen `colour =` und `fill =`
- Beachte den Unterschied zwischen `facet_wrap()` und `facet_grid()`
- `facet_grid()` braucht übrigens noch einen Punkt (`.`) zur Tilde (`~`). 
- Beachte den Unterschied zwischen "`.~`" und "`~.`" bei `facet_grid()`
- verschiebe nach Bedarf die Legende

```{r}

# Lösung Aufgabe 7

wetter_fil <- mutate(wetter_fil,monat = month(time,label = T,abbr = F))


ggplot(wetter_fil, aes(stn,tre200h0, fill = Meereshoehe)) +
  geom_boxplot() +
  facet_grid(monat~.) +
  labs(x = "Station", y = "Temperatur") +
  theme(legend.direction = "horizontal",legend.position = "top")

```


### Aufgabe 8

Teile die Stationen in verschiedene Höhenlagen ein (Tieflage [< 450 m], Mittellage [450 - 1000 m] und Hochlage [> 1'000 m]). Vergleiche die Verteilung der Temperaturwerte in den verschiedenen Lagen.  

- Nutze dazu `facet_grid` um die Höhenlage dem Monat gegenüber zu stellen (`Monat~Lage`)
- Passe `scales =` an damit keine leeren Stellen auf der x-Achse entstehen
- Optional: Verwende den vollen Stationsnamen anstelle des Kürzels und drehe diese ab damit sie sich gegenseitig nicht überschreiben

```{r, warning=F}

# Lösung Aufgabe 8

wetter_fil$Lage[wetter_fil$Meereshoehe < 450] <- "Tieflage" 
wetter_fil$Lage[wetter_fil$Meereshoehe >= 450 & wetter_fil$Meereshoehe <1000] <- "Mittellage" 
wetter_fil$Lage[wetter_fil$Meereshoehe >= 1000] <- "Hochlage" 


ggplot(wetter_fil, aes(Name,tre200h0)) +
  geom_boxplot() +
  facet_grid(monat~Lage, scales = "free_x") +
  labs(x = "Lage", y = "Temperatur") +
  theme(axis.text.x = element_text(angle = 45,hjust = 1))

```



### Aufgabe 9


Als letzter wichtiger Plottyp noch zwei Übungen zum Histogramm. Erstelle ein Histogramm `geom_histogram()` mit den Temperaturwerten. Färbe Säulen aufgrund ihrer Höhenlage ein und die Begrenzungslinie weiss. Setze die Klassenbreite auf 1 Grad.

```{r}

# Lösung Aufgabe 9


h <- ggplot(wetter_fil,aes(tre200h0, fill = Lage)) +
  geom_histogram(binwidth = 1, colour = "white") +
  labs(x = "Temperatur in °C", y = "Anzahl")

h
```


### Aufgabe 10

Erstelle `facets` aufgrund der Höhenlage. Setze noch eine Vertikale linie beim Nullpunkt und stelle den x-Achsenabschnit symmetrisch ein (z.B -30 bis + 30°C).

```{r}

# Lösung Aufgabe 10


h + 
  geom_vline(xintercept = 0, lty = 2, alpha = 0.5) +
  facet_wrap(~Lage) +
  lims(x = c(-30,30)) +
  theme(legend.position = "none")

```





```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:12_InfoVis2/Uebung_A.Rmd-->

## Übung A: Lösung

[RCode als Download](12_InfoVis2/RFiles/Uebung_A.R)

```{r code=readLines('12_InfoVis2/RFiles/Uebung_A.R'), echo=T, eval=F}
```


```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:12_InfoVis2/Uebung_A_loesung.Rmd-->


```{r, include=F, purl = F}
library(knitr)
knitr::opts_chunk$set(echo = F,include = T,message = F, warning = F, collapse=TRUE)
# knitr::opts_knit$set(root.dir = "12_InfoVis2") 


output <- knitr::opts_knit$get("rmarkdown.pandoc.to") # html / latex


run_plotly_image = F # set to "TRUE" in order to create static images via plotly_api (max 100/day)
show_static_image = T # set to "TRUE" in order to show the image / "FALSE" to show error message
default_error <- "In der PDF Version kann die interaktive Grafik bis auf weiteres nicht dargestellt werden."

```

## Übung B

In dieser Übung bauen wir einige etwas unübliche Plots aus der Vorlesung nach. Dafür verwenden wir Datensätze, die in R bereits integriert sind. Eine Liste dieser Datensätze findet man [hier](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/00Index.html) oder mit der Hilfe `?datasets`.


Dazu verwenden wir vor allem das Package `plotly` welches im Gegensatz zu `ggplot2` ein paar zusätzliche Plot-Typen kennt und zudem noch interaktiv ist.  Leider scheinen gewisse Browsers (z.B. Firefox) sowie der Viewer Pane mit `plotly` Mühe zu haben. Deshalb empfehlen wir folgendes:

- [Übungsunterlagen für InfoVis2](http://oyster.zhaw.ch:3939/ResearchMethodsUebungen/) in Chrome zu öffnen
- Falls ihr auf dem [RStudio Server](http://oyster.zhaw.ch:8787) arbeitet: hier ebenfalls in Chrome arbeiten
- Falls ihr lokal mit RStudio arbeitet: Mit der Option `options(viewer=NULL)` werden Plots mit dem Standart Browser. 



```{r, echo = F,message=F}

library(tidyverse)
library(plotly)
library(pander)
library(webshot)

```


```{r, echo = F, purl= F}
Sys.setenv("plotly_username" = "rata_zhaw")
Sys.setenv("plotly_api_key" = "ae8Fn1ltcjUGvWYX957J")
```



### Aufgabe 1: Parallel coordinate plots

Erstelle einen [parallel coordinate plot](https://en.wikipedia.org/wiki/Parallel_coordinates). Dafür eignet sich der integrierte Datensatz [`mtcars`](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html):

```{r, echo = F, purl=F}

knitr::kable(head(mtcars))

```

```{r, echo = T, eval=F}

# Nur nötig, wenn ihr mit einer lokalen Installation von RStudio arbeitet
# (also nicht auf dem Server).
options(viewer=NULL)

```


Parallel Coordinates lassen sich mit nativem `ggplot2` nicht herstellen. Es braucht dazu entweder Erweiterungen oder "standalone" Tools. Als "standalone" Tool kann ich `plotly` stark empfehlen. `Plotly` verfügt zwar über eine etwas eigenwillige Syntax, bietet dafür über sehr vielseitige zusätzliche Möglichkeiten. Vor allem aber sind sämtliche `plotly` Grafiken webbasiert und interaktiv. 

Hier findet ihr eine Anleitung zur Herstellung eines Parallel Coordinates Plot mit `plotly`: https://plot.ly/r/parallel-coordinates-plot/

So sieht der fertige Plot aus:

```{r}
# Lösung Aufgabe 1

p <- mtcars %>%
  plot_ly(type = 'parcoords',
          line = list(color = ~mpg,
                      colorscale = list(c(0,'red'),c(1,'blue'))),
          dimensions = list(
            list(label = 'mpg', values = ~mpg),
            list(label = 'disp', values = ~disp),
            list(label = 'hp', values = ~hp),
            list(label = 'drat', values = ~drat),
            list(label = 'wt', values = ~wt),
            list(label = 'qsec', values = ~qsec),
            list(label = 'vs', values = ~vs),
            list(label = 'am', values = ~am),
            list(label = 'gear', values = ~gear),
            list(label = 'carb', values = ~carb)
          )
  )
```


```{r, results="asis", purl = F, echo=F}
aufgabe <- "aufgabe_1"
filename <- paste0("12_InfoVis2/",aufgabe,".png")

if(output == "latex"){
  if(run_plotly_image){plotly_IMAGE(p, format = "png", out_file = filename)} 
  if(show_static_image){pander::pandoc.image(filename)} else{pandoc.strong(default_error)}
} else(p)

```

### Aufgabe 2: Polar Plot mit Biber Daten

Polar Plots (welche man ebenfalls mit Plotly erstellen kann) eignen sich unter anderem für Daten, die zyklischer Natur sind, wie zum Beispiel zeitlich geprägte Daten (Tages-, Wochen-, oder Jahresrhythmen). Aus den Beispiels-Datensätzen  habe ich zwei Datensätze gefunden, die zeitlich geprägt sind:

- [`beaver1` und `beaver2`](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/beavers.html)
[`AirPassenger`](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/AirPassengers.html)

Beide Datensätze müssen noch etwas umgeformt werden, bevor wir sie für einen Radialplot verwenden können. In Aufgabe 2 verwenden wir die Biber-Datensätze, in der nächsten Aufgabe (3) die Passagier-Daten.

Wenn wir die Daten von beiden Bibern verwenden wollen, müssen wir diese noch zusammenfügen:
```{r, echo = T}


beaver1_new <- beaver1 %>%
  mutate(beaver = "nr1")

beaver2_new <- beaver2 %>%
  mutate(beaver = "nr2")

beaver_new <- rbind(beaver1_new,beaver2_new)

```

Zudem müssen wir die Zeitangabe noch anpassen: Gemäss der [Datenbeschreibung](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/beavers.html) handelt es sich bei der Zeitangabe um ein sehr programmier-unfreundliches Format. 3:30 wird als "0330" notiert. Wir müssen diese Zeitangabe, noch in ein Dezimalsystem umwandeln:
```{r, echo = T}
beaver_new <- beaver_new %>%
  mutate(
    hour_dec = (time/100)%/%1,         # Ganze Stunden (mittels ganzzaliger Division)
    min_dec = (time/100)%%1/0.6,       # Dezimalminuten (15 min wird zu 0.25, via Modulo)
    hour_min_dec = hour_dec+min_dec    # Dezimal-Zeitangabe (03:30 wird zu 3.5)
    ) 
```



Der Datensatz: 
```{r, echo = F, purl = F}
knitr::kable(head(beaver_new))
#  formatRound(c("min_dec","hour_min_dec"), 2)
```

So sieht der fertige Plot aus. Rekonstruiere dies mit `plotly`:

```{r}

# Lösung Aufgabe 2

p <- beaver_new %>%
  plot_ly(r = ~temp, t = ~hour_min_dec, color = ~beaver,mode = "lines", type = "scatter") %>%
  layout(
    radialaxis = list(range = c(35,39)),
    angularaxis = list(range = c(0,24)),
    orientation = 270,
    showlegend = F
    )
```

```{r, results="asis", purl = F, echo=F}
aufgabe <- "aufgabe_2"
filename <- paste0("12_InfoVis2/",aufgabe,".png")

if(output == "latex"){
  if(run_plotly_image){plotly_IMAGE(p, format = "png", out_file = filename)} 
  if(show_static_image){pander::pandoc.image(filename)} else{pandoc.strong(default_error)}
} else(p)

```




### Aufgabe 3: Polar Plot mit Passagier-Daten


Analog Aufgabe 2, dieses Mal mit dem Datensatz [`AirPassanger`](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/AirPassengers.html)

`AirPassengers` kommt in einem Format daher, das ich selbst noch gar nicht kannte. Es sieht zwar aus wie ein `data.frame` oder eine `matrix`, ist aber von der Klasse [`ts`](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/ts.html).

```{r, echo = T}
AirPassengers

class(AirPassengers)
```


Damit wir den Datensatz verwenden können, müssen wir ihn zuerst in eine `matrix` umwandeln. Wie das geht habe ich [hier](https://stackoverflow.com/a/5332664/4139249) erfahren.
```{r, echo = T}
AirPassengers2 <- tapply(AirPassengers, list(year = floor(time(AirPassengers)), month = month.abb[cycle(AirPassengers)]), c)
```

Aus der `matrix` muss noch ein Dataframe her, zudem müssen wir aus der breiten Tabelle eine lange Tabelle machen:

```{r, echo = T}


AirPassengers3 <- AirPassengers2 %>%
  as.data.frame() %>%
  rownames_to_column("year") %>%
  gather(month,n,-year) %>%
  mutate(
    # ich nutze einen billigen Trick um ausgeschriebene Monate in Nummern umzuwandeln [1]
    month = factor(month, levels = month.abb,ordered = T),
    month_numb = as.integer(month),
    year = factor(year, ordered = T)
  )


# [1] beachtet an dieser Stelle das Verhalten von as.integer() wenn es sich um factors() handelt. Hier wird das Verhalten genutzt, andersweitig kann es einem zum Verhngnis werden. Das Verhalten wir auch hier verdeutlicht:
# as.integer(as.character("500"))
# as.integer(as.factor("500"))

```


Hier der fertige Plot. Rekonstruiere dies mit `plotly`:
```{r}

# Lösung Aufgabe 3

p <- AirPassengers3 %>%
  plot_ly(r = ~n, t = ~month_numb, color = ~year, mode = "markers", type = "scatter") %>%
  layout(
    showlegend = T,
    angularaxis = list(range = c(0,12)),
    orientation = 270,
    legend = list(traceorder = "reversed")
) 

```



```{r, results="asis", purl = F, echo=F}
aufgabe <- "aufgabe_3"
filename <- paste0("12_InfoVis2/",aufgabe,".png")

if(output == "latex"){
  if(run_plotly_image){plotly_IMAGE(p, format = "png", out_file = filename)} 
  if(show_static_image){pander::pandoc.image(filename)} else{pandoc.strong(default_error)}
} else(p)
```

### Aufgabe 4: 3D Scatterplot

Erstelle einen 3D Scatterplot, ebenfalls mit `plotly`. Nutze dazu den Datensatz `trees`. Ein Beispiel für einen 3D Scatterplot findet ihr [hier](https://plot.ly/r/3d-scatter-plots/).


```{r}

# Lösung Aufgabe 4

  p <- trees %>%
  plot_ly(x = ~Girth, y = ~Height, z = ~Volume)
```



```{r, results="asis", purl = F, echo=F}
aufgabe <- "aufgabe_4"
filename <- paste0("12_InfoVis2/",aufgabe,".png")

if(output == "latex"){
  if(run_plotly_image){plotly_IMAGE(p, format = "png", out_file = filename)} 
  if(show_static_image){pander::pandoc.image(filename)} else{pandoc.strong(default_error)}
} else(p)
```





```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:12_InfoVis2/Uebung_B.Rmd-->

## Übung B: Lösung

[RCode als Download](12_InfoVis2/RFiles/Uebung_B.R)

```{r code=readLines('12_InfoVis2/RFiles/Uebung_B.R'), echo=T, eval=F}
```

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:12_InfoVis2/Uebung_B_loesung.Rmd-->

# Statistik 1 (28.10.2019)

In Statistik 1 lernen die Studierenden, was (Inferenz-) Statistik im Kern leistet und warum sie für wissenschaftliche Erkenntnis (in den meisten Disziplinen) unentbehrlich ist. Nach einer Wiederholung der Rolle von Hypothesen wird erläutert, wie Hypothesentests in der frequentist-Statistik umgesetzt werden, einschliesslich p-Werten und Signifikanz-Levels. Die praktische Statistik beginnt mit den beiden einfachsten Fällen, dem Chi-Quadrat-Test für die Assoziation zwischen zwei kategorialen Variablen und dem t-Test auf Unterschiede in Mittelwerten zwischen zwei Gruppen. Abschliessend beschäftigen wir uns damit, wie man Ergebnisse statistischer Analysen am besten in Abbildungen, Tabellen und Text darstellt.


<!-- TODO: -->
<!-- Referenzen passt noch nicht -->
<!-- hierarchien noch kontrollieren und mit anderen blöcke abgleichen -->
<!-- Beschreibung forschungsprojekte: tabelle besser als csv -->
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:13_Statistik1/Abstract.Rmd-->

```{r, echo = F, purl = F}
knitr::opts_chunk$set(echo = T, collapse=TRUE)
library(knitr)
```


## Demo: Stastische Tests

[Demoscript als Download](13_Statistik1/RFiles/Demo_Tests.R)

### Chi-Quadrat-Test & Fishers Test

```{r}
qchisq(0.95,1)
count<-matrix(c(38,14,11,51),nrow=2)
count
chisq.test(count)
fisher.test(count)
```

### t-Test

```{r}
a<-c(20,19,25,10,8,15,13,18,11,14)
b<-c(12,15,16,7,8,10,12,11,13,10)
blume<-data.frame(a,b)
blume
summary(blume)
boxplot(blume$a,blume$b)
boxplot(blume)
hist(blume$a)
hist(blume$b)
t.test(blume$a,blume$b) #zweiseitig
t.test(blume$a,blume$b, alternative="greater") #einseitig
t.test(blume$a,blume$b, alternative="less") #einseitig
t.test(blume$a,blume$b, var.equal=T) #Varianzen gleich, klassischer t-Test
t.test(blume$a,blume$b, var.equal=F) #Varianzen ungleich, Welch's t-Test, ist auch default, d.h. wenn var.equal nicht                        # definiert wird, wird ein Welch's t-Test ausgeführt. 
t.test(blume$a,blume$b, paired=T) #gepaarter t-Test 
t.test(blume$a,blume$b, paired=T,alternative="greater") #gepaarter t-Test 
shapiro.test(blume$b)
var.test(blume$a,blume$b)
if(!require(car)){install.packages("car")} # installiert das Zusatzpacket car (wenn nicht bereits installiert)
library(car)
leveneTest(blume$a,blume$b,center=mean)
wilcox.test(blume$a,blume$b)
```

Das gleiche mit einem “long table”


```{r}
cultivar<-c(rep("a",10),rep("b",10))
size<-c(a,b)
blume.long<-data.frame(cultivar,size)

rm(size) #Befehl rm entfernt die nicht mehr benötitgten Objekte aus dem Workspace
rm(cultivar)


rm(size) #Befehl rm entfernt die nicht mehr benötitgten Objekte aus dem Workspace
rm(cultivar)


#Das gleiche in einer Zeile
blume.long<-data.frame(cultivar=c(rep("a",10),rep("b",10)),size=c(a,b))
summary(blume.long)             
head(blume.long)

boxplot(size~cultivar, data=blume.long)


boxplot(size~cultivar, data=blume.long)


t.test(size~cultivar, blume.long, var.equal=T)
t.test(size~cultivar, blume.long, var.equal=F)
```


### Base R vs. ggplot2


```{r}

library(tidyverse)
ggplot(blume.long, aes(cultivar,size)) + geom_boxplot()
ggplot(blume.long, aes(cultivar,size)) + geom_boxplot()+theme_classic()
ggplot(blume.long, aes(cultivar,size)) + geom_boxplot(size=1) + theme_classic()+
theme(axis.line = element_line(size=1))+theme(axis.title = element_text(size=14))+
theme(axis.text = element_text(size=14))
ggplot(blume.long, aes(cultivar,size)) + geom_boxplot(size=1) + theme_classic()+
  theme(axis.line = element_line(size=1), axis.ticks = element_line(size=1), 
       axis.text = element_text(size = 20), axis.title = element_text(size = 20))
```


Definieren von mytheme mit allen gewünschten Settings, das man zu Beginn einer Sitzung einmal laden und dann immer wieder ausführen kann (statt des langen Codes)

```{r}
mytheme <- theme_classic() + 
  theme(axis.line = element_line(color = "black", size=1), 
        axis.text = element_text(size = 20, color = "black"), 
        axis.title = element_text(size = 20, color = "black"), 
        axis.ticks = element_line(size = 1, color = "black"), 
        axis.ticks.length = unit(.5, "cm"))

ggplot(blume.long, aes(cultivar,size)) + 
  geom_boxplot(size=1) +
  mytheme

t_test <- t.test(size~cultivar, blume.long)

ggplot(blume.long, aes(cultivar,size)) + 
  geom_boxplot(size=1) + 
  mytheme +
  annotate("text", x = "b", y = 24, label = paste0("italic(p) == ", round(t_test$p.value, 3)), parse = TRUE, size = 8)

ggplot (blume.long, aes(cultivar,size)) + 
  geom_boxplot(size=1) + 
  mytheme +
  labs(x="Cultivar",y="Size (cm)")

```

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:13_Statistik1/Demo_Tests.Rmd-->


## Beschreibung Forschungsprojekt NOVANIMAL (NFP69)

Im Forschungsprojekt NOVANIMAL wird u.a. der Frage nachgegangen, was es braucht, damit Menschen freiwillig weniger tierische Produkte konsumieren? Ein interessanter Ansatzpunkt ist die Ausser-Haus-Verpflegung. Gemäss der ersten in den Jahren 2014/2015 durchgeführten nationalen Ernährungserhebung menuCH essen 70 % der Bevölkerung zwischen 18 und 75 Jahren am Mittag auswärts (Bochud et al. 2017). Daher rückt die Gastronomie als zentraler Akteur einer innovativen und nachhaltigen Ernährungswirtschaft ins Blickfeld. Welche Innovationen in der Gastronomie könnten dazu beitragen, den Pro-Kopf-Verbrauch an tierischen Nahrungsmitteln zu senken? 

Dazu wurde u.a. ein Experiment in zwei Hochschulmensen durchgeführt. Forschungsleitend war die Frage, wie die Gäste dazu bewogen werden können, häufiger vegetarische oder vegane Gerichte zu wählen. Konkret wurde untersucht, wie die Gäste auf ein verändertes Menü-Angebot mit einem höheren Anteil an vegetarischen und veganen Gerichten reagieren. Das Experiment fand während 12 Wochen statt und bestand aus zwei Mensazyklen à 6 Wochen. Über den gesamten Untersuchungszeitraum werden insgesamt 90 verschiedene Gerichte angeboten. In den 6 Referenz- bzw. Basiswochen wurden zwei fleisch- oder fischhaltige Menüs und ein vegetarisches Menü angeboten. In den 6 Interventionswochen wurde das Verhältnis umgekehrt und es wurden ein veganes, ein vegetarisches und ein fleisch- oder fischhaltiges Gericht angeboten. Basis- und Interventionsangebote wechselten wöchentlich ab. Während der gesamten 12 Wochen konnten die Gäste jeweils auf ein Buffet ausweichen und ihre Mahlzeit aus warmen und kalten Komponenten selber zusammenstellen. Die Gerichte wurden über drei vorgegebene Menülinien  (F, K, W) randomisiert angeboten.

![Die Abbildung zeigt das Versuchsdesign der ersten 6 Experimentalwochen (Kalenderwoche 40 bis 45).](13_Statistik1/design_experiment.png)

Mehr Informationen über das Forschungsprojekt NOVANIMAL findet ihr auf dieser [Webpage](https://www.novanimal.ch)


<!-- @ Egel: Diesen Teil habe ich aus der Aufgabenstellung rausgenommen. Könnnen wir den hier platzieren?  -->
### Weitere Erläuterungen zum Datensatz



Der Datensatz beinhaltet knapp 1100 Einträge mit 18 Variablen. Die Daten stammen aus dem Kassensystem des Catering-Unternehmen und stellen eine repräsentative Stichprobe des originalen Datensatzes dar.

Folgende Variablen sind im Datensatz:


```{r, echo = F, message=F}
library(tidyverse)

variablen <- read_delim("13_Statistik1/novanimal_variabeln.csv",";")

knitr::kable(variablen)
  
```


1. Locals (Local F, Local K, Local W) sind nebst den drei "normalen Menü-Linien" zusätzlich angebotene Gerichte
2. hier werden Gerichte mit Fisch & Geflügel als Fleisch zusammengefasst.
3. Vegane Gerichte enthalten ausschliesslich pflanzliche Zutaten. Im Exeriment wurde zwischen Gerichte mit pflanzlichen Fleischsubstituten und authentischen, eigenständigen veganen Gerichten unterschieden
4. Vegetarisch bedeutet ovo-lakto-vegetarisch, d.h. die Gerichte enthalten Eier und/oder Milchprodukte 

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:13_Statistik1/Intro_Daten_egel.Rmd-->

## Übungen 1


###	Übung 1.1: Assoziationstest

Assoziationstest zweier kategorialer Variablen, Dateneingabe und Durchführung von Chi-Quadrat- sowie Fishers exaktem Test mit Daten die selber erhoben wurden oder dem Novanimal Datensatz.

###	Übung 1.2: $\chi^2$-Test

Datensatz [novanimal.csv](13_Statistik1/data/novanimal.csv) 

Unterscheidet sich die Stichprobe des NOVANIMAL-Projekts von der gesamten Population bezüglich Geschlecht und Hochschulzugehörigkeit? 

Die Grundgesamtheit setzt sich aus 719 Studentinnen und 816 Studenten, 345 Mitarbeiterinnen und 339 Mitarbeiter. Die gesamte Population umfasst 2219 Personen mit einer aktiven CampusCard.

- Definiert die Null- ($H_0$) und die Alternativhypothese ($H_1$)
- Führt einen $\chi^2$-Test mit dem Datensatz novanimal.csv durch
- Stellt eure Ergebnisse angemessen dar, d.h. Text mit Abbildung und/oder Tabelle


###	Übung 1.3: t-Test

Werden in den Basis- und Interventionswochen unterschiedlich viele Gerichte verkauft?

- Definiert die Null- ($H_0$) und die Alternativhypothese ($H_1$).
- Führt einen t-Test mit dem Datensatz novanimal.csv durch.
- Welche Form von t-Test musst Du anwenden: einseitig/zweiseitig resp. gepaart/ungepaart?
- Wie gut sind die Voraussetzungen für einen t-Test erfüllt (z.B. Normalverteilung der Residuen und Varianzhomogenität)?
- Stell eure Ergebnisse angemessen dar, d.h. Text mit Abbildung und/oder Tabelle 
- Das gilt für alle Übungen und wird von euch auch an der Prüfung verlangt. Abzugeben sind am Ende 
  
  (a) **Ein lauffähiges R-Skript**
  (b) **begründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation)** 
  (c) **ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit)**

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:13_Statistik1/assigment_stat1.Rmd-->

---
title: "Modul Research Methods HS19"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE, purl = F, message=F}
knitr::opts_chunk$set(fig.width = 20, fig.height = 12, warning = F, message = F, error = F, include = T, fig.pos = 'H')

```

## Musterlösungen 1.1 - 1.3

```{r, message=T, purl = F, include=FALSE}
## ladet die nötigen Packete und die novanimal.csv Datei in R

library(tidyverse)
nova <- read_delim("13_Statistik1/data/novanimal.csv", delim = ";")

## definiert mytheme für ggplot2 (verwendet dabei theme_classic())

mytheme <- 
  theme_classic() + 
  theme(
    axis.line = element_line(color = "black"), 
    axis.text = element_text(size = 20, color = "black"), 
    axis.title = element_text(size = 20, color = "black"), 
    axis.ticks = element_line(size = 1, color = "black"), 
    axis.ticks.length = unit(.5, "cm")
    )

```


###  Musterlösung Aufgabe 1.1: Assoziationstest

```{r}
# Als eine Möglichkeit, die Aufgabe 1.1 zu bearbeiten, nehmen wir hier den novanimal-Datensatz und gehen der folgenden Frage nach: Gibt es einen Zusammenhang zwischen Geschlecht und der Wahl des Menüinhalts (vegtarisch vs. fleischhaltig) in der Mensa

# berücksichtigt nur vegetarische und fleischhaltige Menüs
# 1) alle Buffet-Menüs weglassen
# 2) alle veganen Gerichte zu vegetarische Gerichte umbenennen
nova2<-subset(nova, !(nova$label_content=="Buffet")) #Subset des Datensatzes ohne Buffet (Da Buffet nicht Fleisch/Vegetarisch zugordnet werden kann)

nova2$label_content[nova2$label_content %in% c('Pflanzlich','Pflanzlich+')] <- "Vegetarisch" # Überschreibt das Label "Pflanzlich","Pflanzlich+" mit "Vegetarisch"

nova2$label_content[grep("Pflanzlich+", nova2$label_content)] <- "Vegetarisch" # alternativer Lösungsweg

nova3<-droplevels(nova2) #entfernt die Kategorien die nicht mehr mehr benutzt werden

#Anzahl Vegetarische/FleischMenüs pro Geschlecht~Vegetarisch Base R
observed<-table(nova2$gender, nova2$label_content) 


#Anzahl Vegetarische/FleischMenüs pro Geschlecht~Vegetarisch Tidyverse
#braucht in diesem speziellen Fall viel mehr Code, da der Chi-Quadrat-Test am liebsten Matrizen will
# unser Vorschlag, nehmt den table Befehl
# untenstehend der vollständige Code in Tidyverse
observed_t <- nova2 %>% 
  group_by(gender, label_content) %>% 
  summarise(tot = n()) %>% 
  ungroup() %>%
  spread(., key = label_content, value = tot) %>%  #
  # set_rownames(.$gender) %>% funktioniert leider nicht mehr mit tibble
  select(-gender) %>%
  as.matrix()


#Chi-squared Test
chi_sq <- chisq.test(observed)
chi_sq

#Fisher's Test 
fisher.test(observed)
```

#### Ergebnisteil
Der $\chi^2$-Test sagt uns, dass das Geschlecht und die Kaufentscheidung eines Menüinhalts zusammenhängen. Es gibt signifikante Unterscheide zwischen dem Geschlecht und dem Menüinhalt ($\chi^2$(`r chi_sq$parameter`) = `r round(chi_sq$statistic[[1]], digits = 3)`, *p* > .001). Es sieht so aus, dass Männer rund doppel so viel Fleischgerichte wählen als Frauen (siehe Tabelle 1). Die Ergebnisse müssen jedoch mit Vorsicht interpretiert werden, denn der $\chi^2$-Test gibt uns nur an, dass ein signifikanter Unterschied zwischen Geschlecht und Menüinhalt vorliegt. Um die Unterschiede innerhalb der Gruppen festzustellen bedarf es weiterer Analysen z. B. einer mehrfaktorieller ANOVA mit anschliessenden Post-hoc Tests (siehe Statistik 2, Folien 3 bis 11).

```{r, echo=F}
library(reshape2)
table <- as.data.frame(observed) %>%
  mutate(`Verkaufszahlen (%)` = round(Freq / sum(Freq) * 100, 1)) %>% 
  rename(Geschlecht = Var1, Menüinhalt = Var2, Verkaufszahlen = Freq) %>% 
  mutate(Geschlecht = recode(Geschlecht, "F" = "Frauen", "M" = "Männer"))
knitr::kable(table, caption = "Tabelle 1 \n Verkaufszahlen des Menüinhalts nach Geschlecht")

```


*******

### Musterlösung Aufgabe 1.2: $\chi^2$-Test
> Zur eurer Info: dies ist eine spezielle, aber wichtige Anwendung eines $\chi^2$-Tests
> Meine Empfehlung Kapitel "Single factor classification" von [Manny Gimond](https://mgimond.github.io/Stats-in-R/ChiSquare_test.html)


#### Null- und Alternativhypothese
> Beachtet: ungerichtet vs. gerichtete Hypothesen (z. B. Statistik 1, Folie 24)
> Überblick zu Hypothesentestung dazu: https://www.youtube.com/watch?v=F4c0EjsDvzo 
 
$H_0$: Es gibt keine Unterschiede zwischen der Population und der Stichprobe bezüglich Geschlecht und Hochschulzugehörigkeit. 
\par
$H_1$: Es gibt Unterschiede zwischen der Population und der Stichprobe bezüglich Geschlecht und Hochschulzugehörigkeit.

```{r}
# bereitet eure Daten auf
# gruppiert die Variablen und fasst sie 
# nach Geschlecht und Hochschulzugehörigkeit zusammen 
# fügt Information aus der Aufgabenstellung hinzu: absolute Häufigkeiten der Gesamtheit
# für den Chi-Quardrat-Test ist eine Berechnung der relativen Häufigkeiten nötig

df_t <- group_by(nova, gender, member) %>% 
  summarise(stichprobe = n()) %>% 
  ungroup() %>%
  mutate(canteen_member = c("Mitarbeiterinnen", "Studentinnen", "Mitarbeiter", "Studenten"), # Achtung: Reihenfolge muss stimmten vgl. canteen_member
         gesamtheit = c(345, 719, 339, 816), # Achtung: Reihenfolge muss stimmten vgl. oben
         gesamtheit_pct = gesamtheit / sum(gesamtheit),
         stichprobe_pct = stichprobe / sum(stichprobe)) # Berechnung der relativen Häufigkeiten

# berechnet den Chi-Quadrat-Test

chi_sq <- chisq.test(df_t$stichprobe, p = df_t$gesamtheit_pct) # es werden zwei Informationen übergeben, eure beobachteten Werte (stichprobe) und die in der Grundgesamtheit/Population erwarteten Werte (wichtig als relative Häufigkeiten)

chi_sq

```

#### Methodenteil

Ziel war es die NOVANIMAL Stichprobe gemäss Geschlecht und Hochschulzugehörigkeit in der Grundgesamtheit besser einzuordnen. Die Grundgesamtheit definiert sich durch alle aktiven CampusCards. Dafür ist ein einfaktorieller $\chi^2$-Test notwendig (siehe [Manny Gimond](https://mgimond.github.io/Stats-in-R/ChiSquare_test.html)). Dieser sagt uns nämlich, ob die beobachteten Häufigkeiten/Frequenzen aus unser Stichprobe mit einer definierten erwarteten Häufigkeit/Frequenz (hier der Grundgesamtheit) übereinstimmen. 

#### Ergebnisteil

Der $\chi^2$-Test sagt uns, dass die NOVANIMAL-Stichprobe von der Population signifikant unterscheidet ($\chi^2$(`r chi_sq$parameter`) = `r round(chi_sq$statistic[[1]], digits = 3)`, *p* > .001). Demnach ist unsere Stichprobe bezüglich den Variablen Geschlecht und Hochschulzugehörigkeit nicht repräsentativ für die Grundgesamtheit. Es scheint, dass die Studentinnen unter- und die Studenten übervertreten sind (siehe Tabelle 2).

```{r, echo=F}
table <- df_t %>% 
  rename(Hochschulzugehörigkeit = member, `Anzahl Population` = gesamtheit, `Anzahl Stichprobe` = stichprobe, `Anteil Population (%)` = gesamtheit_pct, `Anteil Stichprobe (%)` = stichprobe_pct) %>%
  dplyr::select(Hochschulzugehörigkeit, `Anzahl Population`, `Anteil Population (%)`, `Anzahl Stichprobe`, `Anteil Stichprobe (%)`) %>% 
  mutate(`Anteil Stichprobe (%)` = round(`Anteil Stichprobe (%)`,2)*100, `Anteil Population (%)` = `Anteil Population (%)`*100)
knitr::kable(table, caption = "Tabelle 2 \nAnzahl und Anteil Beobachtungen in der Population und in der Stichprobe")

```

*******

###  Musterlösung Aufgabe 1.3: t-Test
> Meine Empfehlung Kapitel 2 von [Manny Gimond](https://mgimond.github.io/Stats-in-R/z_t_tests.html) 

#### Null- und Alternativhypothese
<>$H_0$: Es gibt keine Unterschiede in den Verkaufszahlen zwischen Basis- und Interventionswochen. <>
\par
$H_1$: Es gibt Unterschiede in den Verkaufszahlen zwischen Basis- und Interventionswochen.


```{r}
# Gemäss Aufgabenstellung müsset die Daten zuerst nach Kalenderwochen "week" und Bedingungen "condition" zusammengefasst werden

df <- nova %>%
    group_by(week, condit) %>%  
    summarise(tot_sold = n()) 

# überprüft die Voraussetzungen für einen t-Test
ggplot(df, aes(x = condit, y= tot_sold)) + # achtung 0 Punkt fehlt
    geom_boxplot(fill = "white", color = "black", size = 1) + 
    labs(x="\nBedingungen", y="Durchschnittlich verkaufte Gerichte pro Woche\n") + 
    mytheme

# Auf den ersten Blick scheint es keine starken Abweichungen zu einer Normalverteilung zu geben resp. es sind keine extremen schiefen Verteilungen ersichtlich (vgl. Statistik 2, Folien 12-21)

```


```{r}

# führt einen t-Tests durch; 
# es wird angenommen, dass die Verkaufszahlen zwischen den Bedingungen unabhängig sind

t_test <- t.test(tot_sold~condit, data=df)

t.test(df[df$condit == "Basis", ]$tot_sold, 
                 df[df$condit == "Intervention", ]$tot_sold) #alternative Formulierung

```

#### Methodenteil

Ziel war es die wöchentlichen Verkaufszahlen zwischen den Interventions- und Basiswochen zu vergleichen. Die Annahme war, dass die wöchentlichen Verkaufszahlen unabhängig sind. Daher können die mittleren Verkaufszahlen pro Woche zwischen den beiden Bedingungen mittels t-Test geprüft werden. Obwohl die visuelle Inspektion keine schwerwiegenden Verletzungen der Modelvoraussetzung zeigte, wurde einen Welch t-Test gerechnet. 

#### Ergebnisteil

In den Basiswochen werden mehr Gerichte pro Woche verkauft als in den Interventionsowochen (siehe Abbildung 1). Die wöchentlichen Verkaufszahlen zwischen den Bedigungen (Basis oder Intervention) unterscheiden sich gemäss Welch t-Test jedoch nicht signifikant (*t*(`r round(t_test$parameter[[1]],digits = 0)`) = `r round(t_test$statistic, digits = 3)` , *p* = `r round(t_test$p.value, digits=3)`).


```{r, purl=F, message=F, echo = F,  fig.cap="Abbildung1. Die wöchentlichen Verkaufszahlen für die Interventions- und Basiswochen unterscheiden sich nicht signifikant."}
# zeigt die Ergebnisse mit einer Abbildung
ggplot(df, aes(x = condit, y= tot_sold)) + 
  stat_boxplot(geom ='errorbar', width = .25) + # erzeugt sogenannte Whiskers mit Strichen
  geom_boxplot(fill = "white", color = "black", size = 1) +
  labs(x="\nBedingungen", y="Durchschnittlich verkaufte Gerichte pro Woche\n") + 
  mytheme

```


*******
##### letztes Update `r Sys.Date()`, gian-andrea.egeler@zhaw.ch & stefan.widmer@zhaw.ch
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:13_Statistik1/solution_stat1.Rmd-->

# Statistik 2 (29.10.2019)

In Statistik 2 lernen die Studierenden die Idee, die Voraussetzungen und die praktische Anwendung „einfacher“ linearer Modelle in R (sowie teilweise ihrer „nicht-parametrischen“ bzw. „robusten“ Äquivalente). Am Anfang steht die Varianzanalyse (ANOVA) als Verallgemeinerung des t-Tests, einschliesslich post-hoc-Tests und mehrfaktorieller ANOVA. Dann geht es um die Voraussetzungen parametrischer (und nicht-parametrischer) Tests und Optionen, wenn diese verletzt sind. Dann beschäftigen wir uns mit Korrelationen, die auf einen linearen Zusammenhang zwischen zwei metrischen Variablen testen, ohne Annahme einer Kausalität. Es folgen einfache lineare Regressionen, die im Prinzip das Gleiche bei klarer Kausalität leisten. Abschliessend besprechen wir, was die grosse Gruppe linearer Modelle (Befehl lm in R) auszeichnet.
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:14_Statistik2/Abstract.Rmd-->


```{r, include=FALSE, purl=F}

knitr::opts_chunk$set(echo = TRUE,include = T, results = "hide",collapse=TRUE)
```

## Demoskript

[Demoscript als Download](14_Statistik2/RFiles/Demo_Tests.R)

 __t-test als ANOVA__ 

```{r}
a<-c(20,19,25,10,8,15,13,18,11,14)
b<-c(12,15,16,7,8,10,12,11,13,10)

blume<-data.frame(cultivar=c(rep("a",10),rep("b",10)),size=c(a,b))

par(mfrow=c(1,1))
boxplot (data=blume, size~cultivar, xlab="Sorte", ylab="Bluetengroesse [cm]")

t.test(size~cultivar, blume, var.equal=T)

aov(size~cultivar,data=blume)
summary(aov(size~cultivar,data=blume))
summary.lm(aov(size~cultivar,data=blume))
```


__Echte ANOVA__

```{r}
c<-c(30,19,31,23,18,25,26,24,17,20)

blume2<-data.frame(cultivar=c(rep("a",10),rep("b",10),rep("c",10)),size=c(a,b,c))

summary(blume2)             
head(blume2)

par(mfrow=c(1,1))
boxplot (data=blume2, size~cultivar, xlab="Sorte", ylab="Blütengrösse [cm]")

aov(size~cultivar,data=blume2)
summary(aov(size~cultivar,data=blume2))
summary.lm(aov(size~cultivar,data=blume2))

aov.1 <- aov(size~cultivar,data=blume2)
summary(aov.1)
summary.lm(aov.1)

#Berechnung Mittelwerte usw. zur Charakterisierung der Gruppen
aggregate(size~cultivar,blume2, function(x) c(Mean = mean(x), SD = sd(x), Min=min(x), Max=max(x)))

lm.1 <- lm(size~cultivar,data=blume2)
summary(lm.1)
```


__Tukeys Posthoc-Test__

```{r eval=FALSE}
if(!require(multcomp)){install.packages("multcomp")}
library(multcomp)
summary(glht(aov(size~cultivar, data=blume2),linfct=mcp(cultivar ="Tukey")))
```


__Beispiel Posthoc-Labels in Plot__

```{r}
anova <- aov(Sepal.Width ~ Species, data=iris)
letters <- cld(glht(anova, linfct=mcp(Species="Tukey")))
boxplot(Sepal.Width ~ Species, data=iris)
mtext(letters$mcletters$Letters, at=1:3)

library(tidyverse)
ggplot(iris, aes(Species, Sepal.Width)) + geom_boxplot(size = 1) +
  annotate("text", y = 5, x = 1:3, label = letters$mcletters$Letters)

```


__Klassische Tests der Modellannahmen (NICHT EMPFOHLEN!!!) __

```{r}
attach(blume2)
shapiro.test(size[cultivar == "a"])

var.test(size[cultivar == "a"],size[cultivar == "b"])

if(!require(car)){install.packages("car")}
library(car)
leveneTest(size[cultivar == "a"],size[cultivar == "b"],center=mean)

wilcox.test(size[cultivar == "a"],size[cultivar == "b"])

detach(blume2)
```


__Nicht-parametrische Alternativen, wenn Modellannahmen der ANVOA massiv verletzt sind__

__Zum Vergleich normale ANOVA noch mal__

```{r}
summary(aov(size~cultivar,data=blume2))
```

__Bei starken Abweichungen von der Normalverteilung, aber ähnlichen Varianzen__

__Kruskal-Wallis-Test__

```{r}
kruskal.test(data=blume2, size~cultivar)
if(!require(FSA)){install.packages("FSA")} 
library(FSA)
dunnTest(data=blume2, size~cultivar, method="bh") #korrigierte p-Werte nach Bejamini-Hochberg
```

__Bei erheblicher Heteroskedastizität, aber relative normal/symmetrisch verteilten Residuen__

__Welch-Test__

```{r}
oneway.test(data=blume2, size~cultivar, var.equal=F)
```

__2-faktorielle ANOVA__

```{r}

d<-c(10,12,11,13,10,25,12,30,26,13)
e<-c(15,13,18,11,14,25,39,38,28,24)
f<-c(10,12,11,13,10,9,2,4,7,13)

blume3<-data.frame(cultivar=c(rep("a",20),rep("b",20),rep("c",20)),
                   house=c(rep(c(rep("yes",10),rep("no",10)),3)),size=c(a,b,c,d,e,f))
blume3

boxplot(size~cultivar+house,data=blume3)

summary(aov(size~cultivar+house,data=blume3))
summary(aov(size~cultivar+house+cultivar:house,data=blume3)) 
summary(aov(size~cultivar*house,data=blume3)) #Kurzschreibweise: "*" bedeutet, dass Interaktion zwischen cultivar und house eingeschlossen wird

summary.lm(aov(size~cultivar+house,data=blume3))


interaction.plot(blume3$cultivar,blume3$house,blume3$size)
interaction.plot(blume3$house,blume3$cultivar,blume3$size)

anova(lm(blume3$size~blume3$cultivar*blume3$house),lm(blume3$size~blume3$cultivar+blume3$house))
anova(lm(blume3$size~blume3$house),lm(blume3$size~blume3$cultivar*blume3$house))
```


__Korrelationen__

```{r}
library(car)

blume<-data.frame(a,b)
scatterplot(a~b,blume)

cor.test(a,b,data = blume, method="pearson")
cor.test(a,b,data = blume, method="spearman")
cor.test(a,b,data = blume, method="kendall") 

#Jetzt als Regression
lm.2 <- lm(b~a)
anova(lm.2)
summary(lm.2)

#Model II-Regression
if(!require(lmodel2)){install.packages("lmodel2")} 
library(lmodel2)
lmodel2(b~a)
```

__Beispiele Modelldiagnostik__
```{r}
par(mfrow=c(2,2)) #4 Plots in einem Fenster
plot(lm(b~a))

if(!require(ggfortify)){install.packages("ggfortify")}
library(ggfortify)
autoplot(lm(b~a))

#Modellstatistik nicht OK
g<-c(20,19,25,10,8,15,13,18,11,14,25,39,38,28,24)
h<-c(12,15,10,7,8,10,12,11,13,10,25,12,30,26,13)
par(mfrow=c(1,1))

plot(h~g,xlim=c(0,40),ylim=c(0,30))
abline(lm(h~g))

par(mfrow=c(2,2))
plot(lm(h~g))

#Modelldiagnostik mit ggplot
df <- data.frame(g,h)
ggplot(df, aes(x = g, y = h)) + 
    # scale_x_continuous(limits = c(0,25)) +
    # scale_y_continuous(limits = c(0,25)) +
    geom_point() +
    geom_smooth( method = "lm", color = "black", size = .5, se = F) + 
    theme_classic()

par(mfrow=c(2,2))
plot(lm(h~g))

autoplot(lm(h~g))


```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:14_Statistik2/Demoskript.Rmd-->

## Übungen 2

Repetition: Abzugeben sind am Ende
    
    a. lauffähiges R-Skript
    b. begründeter Lösungsweg (Kombination aus R-Code, R Output 
       und dessen Interpretation)
    c. ausformulierter Methoden- und Ergebnisteil (für eine wiss.Arbeit).
    
- Bitte **erklärt und begründet die einzelnen Schritte,** die ihr unternehmt, um zu eurem Ergebnis zu kommen. Dazu erstellt bitte ein Word-Dokument, in dem ihr Schritt für Schritt den verwendeten **R-Code**, die dazu      gehörigen **Ausgaben von R**, eure **Interpretation** derselben und die sich ergebenden **Schlussfolgerungen** für das weitere Vorgehen dokumentiert.
  
- Dieser **Ablauf** sollte insbesondere beinhalten:
    - Überprüfen der Datenstruktur nach dem Einlesen, welches sind die abhängige(n) und welches die unabängige(n) Variablen etc.
    - Explorative Datenanalyse, um zu sehen, ob evtl. Dateneingabefehler vorliegen oder Datentransformationen vorgenommen werden sollten
    - Auswahl und Begründung eines statistischen Verfahrens
    - Bestimmung des vollständigen/maximalen Models
    - Selektion des/der besten Models/Modelle
    - Durchführen der Modelldiagnostik für dieses
    - Generieren aller Zahlen, Statistiken und Tabellen, die für eine wiss. Ergebnisdarstellung benötigt werden
    
- Formuliert abschliessend einen **Methoden- und Ergebnisteil** (ggf. incl. adäquaten Abbildungen/Tabellen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (je einen ausformulierten Absatz von ca. 60-100 Worten bzw. 3-8 Sätzen). Alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden.

### Übung 2.1: Regression (NatWis)

**Regressionsanalyse mit [decay.csv](14_Statistik2/data/decay.csv)**

Der Datensatz beschreibt in einem physikalischen Experiment die Zahl der radioaktiven Zerfälle pro Minute in Abhängigkeit vom Zeitpunkt (min nach Start des Experimentes).

- Ladet den Datensatz in R und macht eine explorative Datenanalyse.
- Wählt unter den schon gelernten Methoden der Regressionsanalyse einadäquates Vorgehen zur Analyse dieser Daten und führt diese dann durch.
- Prüft anhand der Residuen, ob die Modellvoraussetzungen erfüllt waren
- Stellt die erhaltenen Ergebnisse angemessen dar (Text, Abbildung und/oder Tabelle).
- Kennt ihr ggf. noch eine andere geeignete Herangehensweise?


### Übung 2.2: Einfaktrielle ANOVA (SozOek)

**ANOVA mit [novanimal.csv](13_Statistik1/data/novanimal.csv)**

Führt mit dem Datensatz novanimal.csv eine einfaktorielle ANOVA durch. Gibt es Unterschiede zwischen der Anzahl verkaufter Gerichte (Buffet, Fleisch oder Vegetarisch) pro Woche?

Hinweise für die Analysen:

- Fasst die vier Inhalte der Gerichte zu drei Inhalten zusammen. Das heisst, dass die pflanzlichen Gerichte neu zu den vegetarischen Gerichten gezählt
werden. Konkret könnt ihr das in R mit einer Umbenennung der Inhalte durchführen (z. B. mit grep()).
- Danach muss der Datensatz gruppiert und zusammengefasst werden.
- Unbekannte Menü-Inhalte können vernachlässigt werden.
- Wie gut sind die Voraussetzungen für eine ANOVA erfüllt? Wären allenfalls auch nicht-parametrische Analysen zulässig?
- Führt anschliessend Post-hoc-Vergleiche durch.
- Fasst die Ergebnisse in einem Satz zusammen.


### Übung 2.3N: Mehrfaktorielle ANOVA (NatWis)

**ANOVA mit [kormoran.csv](14_Statistik2/data/kormoran.csv)**

Der Datensatz enthält 40 Beobachtungen zu Tauchzeiten zweier Kormoranunterarten (C = *Phalocrocorax carbo carbo* und S = *Phalacrocorax carbo sinensis*) aus vier Jahreszeiten (F = Frühling, S = Sommer, H = Herbst, W = Winter).

- Lest den Datensatz nach R ein und führt eine adäquate Analyse durch, um
beantworten zu können, wie Unterart und Jahreszeit die Tauchzeit beeinflussen.
- Stellt eure Ergebnisse dann angemessen dar (Text mit Abbildung und/oder Tabelle).
- Gibt es eine Interaktion?

### Übung 2.3S: Mehrfaktorielle ANOVA mit Interaktion (SozOek)

**ANOVA mit [novanimal.csv](13_Statistik1/data/novanimal.csv)**

Können die Unterschiede in den verkauften Gerichten (Buffet, Fleisch oder
Vegetarisch) durch die beiden Bedingungen (Basis- oder Interventionswochen) erklärt werden?

Hinweise für die Analysen:

- Fasst die vier Inhalte der Gerichte zu drei Inhalten zusammen. Das heisst,
dass die pflanzlichen Gerichte neu zu den vegetarischen Gerichten gezählt
werden. Konkret könnt ihr das in R mit einer Umbenennung der Inhalte
durchführen (z. B. mit grep()).
- Danach muss der Datensatz gruppiert und zusammengefasst werden.
- Unbekannte Menü-Inhalte können vernachlässigt werden.
- Wie gut sind die Voraussetzungen für eine ANOVA erfüllt? Wären allenfalls
auch nicht-parametrische Analysen zulässig?
- Führt anschliessend Post-hoc-Vergleiche durch.
- Stellt eure Ergebnisse dann angemessen dar (Text mit Abbildung und/oder Tabelle).

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:14_Statistik2/assigment_stat2.Rmd-->

---
title: "MSc. Research Methods - Statistikteil Lösungen 2019"
author: "Juergen Dengler"
date: "`r format(Sys.Date(), '%B %Y')`"
output:
  pdf_document: default
  html_document:
    df_print: paged
---


## Musterlösung Aufgabe 2.1: Regression

[RCode als Download](14_Statistik2/RFiles/Loesung_Uebung_2.1_v.02.R)

**Übungsaufgabe (hier so ausführlich formuliert, wie dies auch in der Klausur der Fall sein
wird)**

- Laden Sie den Datensatz decay.csv. Dieser enthält die Zahl radioaktiver Zerfälle pro
Zeiteinheit (amount) für Zeitpunkte (time) nach dem Start des Experimentes.
- **Ermitteln Sie ein statistisches Modell, dass die Zerfallshäufigkeit in Abhängigkeit
von der Zeit beschreibt.**
- Bitte erklären und begründen Sie die einzelnen Schritte, die Sie unternehmen, um zu
diesem Ergebnis zu kommen. Dazu erstellen Sie bitte ein Word-Dokument, in das Sie
Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, Ihre
Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere
Vorgehen dokumentieren.
- Dieser Ablauf sollte insbesondere beinhalten:
    - Überprüfen der Datenstruktur nach dem Einlesen, welches sind die abhängige(n) und welches die             unabängige(n) Variablen
    - Explorative Datenanalyse, um zu sehen, ob evtl. Dateneingabefehler vorliegen oder
      Datentransformationen vorgenommen werden sollten
    - Auswahl und Begründung eines statistischen Verfahrens (es gibt hier mehrere
      statistisch korrekte Möglichkeiten!)
    - Ermittlung eines Modells
    - Durchführen der Modelldiagnostik für das gewählte Modell
    - Generieren aller Zahlen, Statistiken und Tabellen, die für eine wiss.
      Ergebnisdarstellung benötigt werden
    - Formulieren Sie abschliessend einen Methoden- und Ergebnisteil (ggf. incl.
      adäquaten Abbildungen) zu dieser Untersuchung in der Form einer
      wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je
      einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und
      Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige
      Redundanz dagegen vermieden werden.
    - **Abzugeben sind am Ende (a) Ein lauffähiges R-Skript; (b) begründeter Lösungsweg
      (Kombination aus R-Code, R Output und dessen Interpretation) und (c)
      ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit).**
      
      

```{r, include=FALSE, purl=F}
knitr::opts_chunk$set(echo = TRUE,include = T, results = "hide",collapse=TRUE)
```

**Loesung -- Skript**

Uebung 2.1 - Regressionsanalyse

```{r}
decay <-read.csv("14_Statistik2/data/decay.csv")
decay

# Um die Variablen im Dataframe im Folgenden direkt (ohne $ bzw. ohne "data = data") ansprechen zu können
attach(decay)
summary(decay)
str(decay)
```
Man erkennt, dass es 31 Beobachtungen für die Zeit als Integer von Zerfällen gibt, die als rationale Zahlen angegeben werden (dass die Zahl der Zerfälle nicht ganzzahlig ist, deutet darauf hin, dass sie möglicherweise nur in einem Teil des Zeitintervalls oder für einen Teil des betrachteten Raumes gemessen und dann hochgerechnet wurde.

**Explorative Datenanalyse**
```{r}
boxplot(time)
boxplot(amount)
plot(amount~time)
```

Während der Boxplot für time wunderbar symmetrisch ohne Ausreisser ist, zeigt amount eine stark rechtsschiefe (linkssteile) Verteilung mit einem Ausreiser. Das deutet schon an, dass ein einfaches lineares Modell vermutlich die Modellannahmen verletzen wird. Auch der einfache Scatterplot zeigt, dass ein lineares Modell wohl nicht adäquat ist. Wir rechnen aber erst einmal weiter.

**Einfaches lineares Modell**
```{r}
lm.1<-lm(amount~time)
summary(lm.1)
```
Das sieht erst einmal nach einem Supermodell aus, höchstsignifikant und mit einem hohen R² von fast 77%. ABER: wir müssen uns noch die Modelldiagnostik ansehen...

**Modelldiagnostik**
```{r}
par(mfrow=c(2,2))
plot(lm.1)
```
Hier zeigen die wichtigen oberen Plots beide massive Abweichungen vom „Soll“. Der Plot oben links zeigt eine „Banane“ und beim Q-Q-Plot oben rechts weichen die Punkte rechts der Mitte alle stark nach oben von der Solllinie ab. Wir haben unser Modell also offensichtlich falsch spezifiziert.
Um eine Idee zu bekommen, was falsch ist, plotten wir noch, wie das Ergebnis dieses Modells aussähe:


**Ergebnisplot**
```{r}
par(mfrow=c(1,1))
plot(time,amount)
abline(lm(amount~time),col="red")
abline(lm.1,col="red")
```

Die Punkte links liegen alle über der Regressionslinie, die in der Mitte darunter und die ganz rechts wieder systematisch darüber (darum im Diagnostikplot oben die „Banane“). Es liegt also offensichtlich keine lineare Beziehung vor, sondern eine curvilineare.

Um diese korrekt zu analysieren, gibt es im Prinzip drei Möglichkeiten, wovon am zweiten Kurstag nur eine hatten, während die zweite und dritte in Statistik 3 und 4 folgten. Im Folgenden sind alle drei nacheinander dargestellt (in der Klausur würde es aber genügen, eine davon darzustellen, wenn die Aufgabenstellung wie oben lautet).

**Variante (1): Lineares Modell nach Transformation der abhängigen Variablen**
Dass die Verteilung der abhängigen Variable nicht normal ist, haben wir ja schon bei der explorativen Datenanalyse am Anfang gesehen. Da sie stark linkssteil ist, zugleich aber keine Nullwerte enthält, bietet sich eine Logarithmustransformation an, hier z. B. mit dem natürlichen Logarithmus.

**Loesung 1: log-Transformation der abhaengigen Variablen**
```{r} 
par(mfrow=c(1,2))
boxplot(amount)
boxplot(log(amount))
hist(amount)
hist(log(amount))

#Die log-transformierte Variante rechts sieht sowohl im Boxplot als auch im #Histogramm viel symmetrischer/besser normalverteilt aus. Damit ergibt sich #dann folgendes lineares Modell

lm.2<-lm(log(amount)~time)
summary(lm.2)
```
Jetzt ist der R²-Wert noch höher und der p-Wert noch niedriger als im ursprünglichen linearen Modell ohne Transformation. Das erlaubt aber keine Aussage, da wir Äpfel mit Birnen vergleichen, da die abhängige Variable einmal untransformiert und einmal log-transformiert ist. Entscheidend ist die Modelldiagnostik.

**Modelldiagnostik**
```{r}
par(mfrow=c(2,2))
plot(lm.2)
```

Der Q-Q-Plot sieht jetzt exzellent aus, der Plot rechts oben hat kaum noch eine Banane, nur noch einen leichten Keil. Insgesamt deutlich besser und auf jeden Fall ein statistisch korrektes Modell.


Lösungen 2 und 3 greifen auf Methoden von Statistik 3 und 4 zurück, sie sind hier nur zum Vergleich angeführt

**Loesung 2: quadratische Regression (kam erst in Statistik 3;**
_**koente fuer die Datenverteilung passen, entspricht aber nicht der physikalischen**_

**Gesetzmaessigkeit**

```{r}
model.quad<-lm(amount~time+I(time^2))
summary(model.quad)
```
Hier können wir R² mit dem ursprünglichen Modell vergleichen (beide haben amount als abhängige Grösse) und es sieht viel besser aus. Sowohl der lineare als auch der quadratische Term sind hochsignifikant. Sicherheitshalber vergleichen wir die beiden Modelle aber noch mittels ANOVA.

**Vergleich mit dem einfachen Modell mittels ANOVA (es ginge auch AICc)**
```{r}
anova(lm.1,model.quad)
```
In der Tat ist das komplexere Modell (jenes mit dem quadratischen Term) höchstsignifikant besser. Jetzt brauchen wir noch die Modelldiagnostik.


**Modelldiagnostik**
```{r}
par(mfrow=c(2,2))
plot(model.quad)
```

**Loesung 3 (die beste, hatten wir aber am 2. Tag noch nicht; mit Startwerten muss man ggf. ausprobieren)**

_**mit Startwerten muss man ggf. ausprobieren)**_
```{r}
model.nls<-nls(amount~a*exp(-b*time),start=(list(a=100,b=1)))
summary(model.nls)
```

**Modelldiagnostik**
```{r}
if(!require(nlstools)){install.packages("nlstools")}
library(nlstools)
residuals.nls <- nlsResiduals(model.nls)
plot(residuals.nls)
```

Für nls kann man nicht den normalen Plotbefehl für die Residualdiagnostik nehmen, sondern verwendet das Äquivalent aus nlstools. Die beiden entscheidenden Plots sind jetzt links oben und rechts unten. Der QQ-Plot hat im unteren Bereich einen kleinen Schönheitsfehler, aber ansonsten ist alles OK.

Da alle drei Lösungen zumindest statistisch OK waren, sollen jetzt noch die zugehörigen Ergebnisplots erstellt werden.


**Ergebnisplots**
```{r}
par(mfrow=c(1,1))
xv<-seq(0,30,0.1)
```

1. lineares Modell mit log-transformierter Abhaengiger

```{r}
plot(time,amount)
yv1<-exp(predict(lm.2,list(time=xv)))
lines(xv,yv1,col="red")
```

2. quadratisches Modell
```{r}
plot(time,amount)
yv2<-predict(model.quad,list(time=xv))
lines(xv,yv2,col="blue")
```

3. nicht-lineares Modell
```{r}
plot(time,amount)
yv3<-predict(model.nls,list(time=xv))
lines(xv,yv3,col="green")
```
Optisch betrachtet, geben (2) und (3) den empirischen Zusammenhang etwas besser wieder als (1), da sie im linken Bereich die hohen Werte besser treffen. Man könnte sogar meinen, bei Betrachtung der Daten, dass die Werte ab time = 28 wieder leicht ansteigen, was die quadratische Funktion wiedergibt. Wer sich aber mit Physik etwas auskennt, weiss, dass Version (2) physikalisch nicht zutrifft, da die Zerfallsrate mit der Zeit immer weiter abfällt. Aufgrund der kurzen Messreihe wäre eine quadratische Funktion trotzdem eine statistisch korrekte Interpretation. Mit längeren Messreihen würde sich jedoch schnell zeigen, dass sie nicht zutrifft.
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:14_Statistik2/solution_stat2.1.Rmd-->

---
title: "MSc. Research Methods - Statistikteil Lösungen 2019"
author: "Gian-Andrea Egeler"
date: "`r format(Sys.Date(), '%B %Y')`"
output:
  html_document:
    df_print: paged
---



## Musterlösung Aufgabe 2.2: einfaktorielle ANOVA

[RCode als Download](14_Statistik2/RFiles/Loesung_Uebung_2.3N_v.02.R)

```{r, message=FALSE, echo=FALSE, results='hide', warning=FALSE}

library(tidyverse)
library(ggfortify) # zur Testung der Voraussetzungen

## ladet die nötigen Packete und die novanimal.csv Datei in R
nova <- read_delim("13_Statistik1/data/novanimal.csv", delim = ";")

## definiert mytheme für ggplot2 (verwendet dabei theme_classic())
mytheme <- 
  theme_classic() + 
  theme(
    axis.line = element_line(color = "black"), 
    axis.text = element_text(size = 20, color = "black"), 
    axis.title = element_text(size = 20, color = "black"), 
    axis.ticks = element_line(size = 1, color = "black"), 
    axis.ticks.length = unit(.5, "cm")
    )


```



```{r}

df <- nova # klone den originaler Datensatz

# fasst die vier Inhalte der Gerichte zu drei Inhalten zusammen.
df$label_content[grep("Pflanzlich+",df$label_content)] <- "Vegetarisch" 

# gruppiert Daten nach Menü-Inhalt und Woche
df_ <- df %>%
    group_by(label_content, week) %>% 
    summarise(tot_sold = n()) %>%
    drop_na() # lasst die unbekannten Menü-Inhalte weg

# überprüft die Voraussetzungen für eine ANOVA
# Schaut euch die Verteilungen der Mittelwerte an (plus Standardabweichungen)
# Sind Mittelwerte nahe bei Null? Gäbe uns einen weiteren Hinweis auf eine spezielle Binomail-Verteilung (Statistik 4, Folie: XY)
df_  %>% 
  split(.$label_content) %>% # teilt den Datensatz in 3 verschiedene Datensätze auf
  purrr::map(~ psych::describe(.$tot_sold)) # mit map können andere Funktionen auf den Datensatz angewendet werden


# Boxplot
ggplot(df_, aes(x = label_content, y= tot_sold)) + 
  stat_boxplot(geom = "errorbar", width = 0.25) + # Achtung: Reihenfolge spielt hier eine Rolle!
  geom_boxplot(fill="white", color = "black", size = 1, width = .5) +
  labs(x = "\nMenü-Inhalt", y = "Anzahl verkaufte Gerichte pro Woche\n") + 
  mytheme # achtung erster Hinweis einer Varianzheterogenität


# definiert das Modell (Statistik 2: Folien 4-8)
model <- aov(tot_sold ~ label_content, data = df_)

summary.lm(model)

# überprüft die Modelvoraussetzungen
autoplot(model) + mytheme 

```
<br>  
<span style="background-color: #FFFF00">Fazit: Inspektion der Modellvoraussetzung zeigt klare Verletzungen des Residuelplots (zeigt einen "Trichter", siehe Statistik 2: Folie 13-14; 42), somit Voraussetzung der Homoskedastizität verletzt. Mögliche nächste Schritte: Datentransformation oder nicht-parametrischer Test.</span>
<br>
```{r}

# überprüft die Voraussetzungen des Welch-Tests:
# Gibt es eine hohe Varianzheterogenität und ist die relative Verteilung der Residuen gegeben? (siehe Folien Statistik 2: Folie 18)
# Ja Varianzheterogenität ist gegeben, aber die Verteilung der Residuen folgt einem "Trichter", also keiner "normalen/symmetrischen" Verteilung um 0 (siehe Folien Statistik 2: Folie 42)
# Daher ziehe ich eine Transformation der AV dem nicht-parametrischen Test vor
model1 <- aov(log10(tot_sold) ~ label_content, data = df_)

autoplot(model) + mytheme # scheint ok zu sein

TukeyHSD(model1) # (Statistik 2: Folien 9-11)
```

-------

#### Methoden

Ziel war es, die Unterschide in den Verkaufszahlen pro Menü-Inhalt aufzuzeigen. Da die Responsvariable (AV: Verkaufszahlen) metrisch und die Prädiktorvariable (UV: Menü-Inhalt) kategorial sind, wurde eine einfaktorielle ANOVA gerechnet. Die visuelle Inspektion der Voraussetzungen zeigte insbesondere schwere Verletzungen der Homoskedastizität (siehe Statistik 2: Folien 13-14). Der Boxplot bestätigt diesen Befund. Daher wurde in einem weiteren Schritt eine Transformation oder einen nicht-parametrischen Testden Welch-Test für ungleiche Varianzen gerechnet.  

#### Ergebnisse

Die Menü-Inhalte (Fleisch, Vegetarisch und Buffet) unterscheiden sich in den Verkaufszahlen signifikant (*F*(2,15) = `r round(summary(model1)[[1]][["F value"]][1],2)`, *p* < .001). Die Abbildung zeigt die Verkaufszahlen pro Menü-Inhalt.

```{r, echo=F, fig.cap="Die wöchentlichen Verkaufzahlen unterscheiden sich je nach Menü-Inhalt stark.", tidy=T}

# plottet die Ergebnisse

# aufbereitung für die Infos der Signifikanzen => Alternative Lösungen findet ihr in der Musterlösung 2.3S
df1 <- data.frame(a = c(1, 1:3,3), b = c(150, 151, 151, 151, 150)) 
df2 <- data.frame(a = c(1, 1,2, 2), b = c(130, 131, 131, 130))
df3 <- data.frame(a = c(2, 2, 3, 3), b = c(140, 141, 141, 140))


ggplot(df_, aes(x = label_content, y= tot_sold)) +
   stat_boxplot(geom = "errorbar", width = .25) +
   geom_boxplot(fill="white", color = "black", size = 1, width = .5) + 
   geom_line(data = df1, aes(x = a, y = b)) + annotate("text", x = 2, y = 152, label = "***", size = 8) + # aus der Information aus dem Tukey Test von oben: Buffet-Vegetarisch
   geom_line(data = df2, aes(x = a, y = b)) + annotate("text", x = 1.5, y = 132, label = "***", size = 8) + # Buffet - Fleisch
   geom_line(data = df3, aes(x = a, y = b)) + annotate("text", x = 2.5, y = 142, label = "*", size = 8)+ # Fleisch - Vegetarisch
   expand_limits(y = 0) + # nimmt das 0 bei der y-Achse mit ein
   labs(x = "\nMenü-Inhalt", y = "Anzahl verkaufte Gerichte pro Woche\n") + 
   mytheme 

# hier ein paar interessante Links zu anderen R-Packages, die es ermöglichen signifikante Ergebniss in den Plot zu integrieren
# https://www.r-bloggers.com/add-p-values-and-significance-levels-to-ggplots/
# https://cran.r-project.org/web/packages/ggsignif/vignettes/intro.html
  
   
```

--------
letztes Update `r Sys.Date()`, gian-andrea.egeler@zhaw.ch
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:14_Statistik2/solution_stat2.2.Rmd-->

---
output:
  html_document: default
  pdf_document: default
---

## Musterlösung Aufgabe 2.3N: Mehrfaktorielle ANOVA

**Übungsaufgabe (hier so ausführlich formuliert, wie dies auch in der Klausur der Fall sein
wird)**

- Laden Sie den Datensatz kormoran.txt mit read.table. Dieser enthält Tauchzeiten
(hier ohne Einheit) von Kormoranen in Abhängigkeit von Jahreszeit und Unterart.
Unterarten: Phalacrocorax carbo carbo (C) und Phalacrocorax carbo sinensis (S);
Jahreszeiten: F = Frühling, S = Sommer, H = Herbst, W = Winter.
- **Ihre Gesamtaufgabe ist es, aus diesen Daten ein minimal adäquates Modell zu
ermitteln, das diese Abhängigkeit beschreibt.**
- Bitte erklären und begründen Sie die einzelnen Schritte, die Sie unternehmen, um zu
diesem Ergebnis zu kommen. Dazu erstellen Sie bitte ein Word-Dokument, in das Sie
Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, Ihre
Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere
Vorgehen dokumentieren.
- Dieser Ablauf sollte insbesondere beinhalten:
  - Überprüfen der Datenstruktur nach dem Einlesen, welches sind die abhängige(n)
    und welches die unabängige(n) Variablen, welches statistische Verfahren wenden Sie
    an?
  - Explorative Datenanalyse, um zu sehen, ob schon vor dem Start der Analysen
    Transformationen o.ä. vorgenommen werden sollten
  - Definition eines vollen Modelles, das nach statistischen Kritierien zum minimal
    adäquaten Modell reduziert wird
  - Durchführen der Modelldiagnostik, um zu entscheiden, ob das gewählte Vorgehen
    korrekt war oder ggf. angepasst werden muss
  - Generieren aller Zahlen, Statistiken und Tabellen, die für eine wiss.
    Ergebnisdarstellung benötigt werden
- Formulieren Sie abschliessend einen Methoden- und Ergebnisteil (ggf. incl.
  adäquaten Abbildungen) zu dieser Untersuchung in der Form einer
  wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je
  einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und
  Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige
  Redundanz dagegen vermieden werden.
- **Abzugeben sind am Ende (a) Ein lauffähiges R-Skript; (b) begründeter Lösungsweg
  (Kombination aus R-Code, R Output und dessen Interpretation) und (c)
  ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit).**


```{r, include=FALSE, purl=F}

knitr::opts_chunk$set(echo = TRUE, include = T, results = "show",collapse=TRUE)
```

**Loesung -- Skript**

**Uebung 2.3N - Mehrfaktorielle ANOVA**

```{r}

kormoran <-read.delim("14_Statistik2/data/kormoran.csv",sep = ";")

## Ueberpruefen, ob Einlesen richtig funktioniert hat und welche Datenstruktur vorliegt
str(kormoran)
summary(kormoran)

#Man erkennt, dass es sich um einen Dataframe mit einer metrischen (Tauchzeit) und zwei kategorialen (Unterart, Jahreszeit) Variablen handelt.
#Die adäquate Analyse (1 metrische Abhängige vs. 2 kategoriale Unabhängige) ist damit eine zweifaktorielle ANOVA
#Die Sortierung der Jahreszeiten (default: alphabetisch) ist inhaltlich aber nicht sinnvoll und sollte angepasst werden.

# Um die Variablen im Dataframe im Folgenden direkt (ohne $ bzw. ohne "data = kormoran") ansprechen zu koennen
attach(kormoran)

# Umsortieren der Faktoren, damit sie in den Boxplots eine sinnvolle Reihung haben
Jahreszeit<-factor(Jahreszeit,levels=c("F","S","H","W"))

# Explorative Datenanalyse (zeigt uns die Gesamtverteilung)
boxplot(Tauchzeit)

#Das ist noch OK für parametrische Verfahren (Box ziemlich symmetrisch um Median, Whisker etwas asymmetrisch aber nicht kritisch). Wegen der leichten Asymmetrie (Linksschiefe) könnte man eine log-Transformation ausprobieren.

boxplot(log10(Tauchzeit))

#Der Gesamtboxplot für log10 sieht perfekt symmetrisch aus, das spräche also für eine log10-Transformation. De facto kommt es aber nicht auf den Gesamtboxplot an, sondern auf die einzelnen.

# Explorative Datenanalyse (Check auf Normalverteilung der Residuen und Varianzhomogenitaet)
boxplot(Tauchzeit~Jahreszeit*Unterart)
boxplot(log10(Tauchzeit)~Jahreszeit*Unterart)

#Hier sieht mal die Verteilung für die untransformierten Daten, mal für die transformierten besser aus. Da die Transformation keine klare Verbesserung bringt, bleiben wir im Folgenden bei den untransformierten Daten, da diese leichter (direkter) interpretiert werden können

# Vollständiges Modell mit Interaktion
aov.1 <- aov(Tauchzeit~Unterart*Jahreszeit)
aov.1
summary(aov.1)
#p-Wert der Interaktion ist 0.266

#Das volle (maximale) Modell zeigt, dass es keine signifikante Interaktion zwischen Jahreszeit und Unterart gibt. Wir können das Modell also vereinfachen, indem wir die Interaktion herausnehmen (+ statt * in der Modellspezifikation)

#Modellvereinfachung
aov.2 <- aov(Tauchzeit~Unterart+Jahreszeit)
aov.2
summary(aov.2)

#Im so vereinfachten Modell sind alle verbleibenden Terme signifikant, wir sind also beim „minimal adäquaten Modell“ angelangt

#Anderer Weg, um zu pruefen, ob man das komplexere Modell mit Interaktion behalten soll
anova(aov.1,aov.2)
#in diesem Fall bekommen wir den gleichen p-Wert wie oben (0.266)

#Modelldiagnostik
par(mfrow=c(2,2)) #alle vier Abbildungen in einem 2 x 2 Raster
plot(aov.2)
influence.measures(aov.2) # kann man sich zusätzlich zum "plot" ansehen, um herauszufinden, ob es evtl. sehr einflussreiche Werte mit Cook's D von 1 oder grösser gibt

#Links oben ist alles bestens, d. h. keine Hinweise auf Varianzheterogenität („Keil“) oder Nichtlinearität („Banane“)
#Rechts oben ganz gut, allerdings weichen Punkte 1 und 20 deutlich von der optimalen Gerade ab -> aus diesem Grund können wir es doch noch mal mit der log10-Transformation versuchen (s.u.)
#Rechts unten: kein Punkt hat einen problematischen Einfluss (die roten Linien für Cook’s D > 0.5 und > 1 sind noch nicht einmal im Bildausschnitt.

#Alternative mit log10
aov.3 <-aov(log10(Tauchzeit)~Unterart+Jahreszeit)
aov.3
summary(aov.3)
plot(aov.3)

#Rechts oben: Punkt 20 jetzt auf der Linie, aber Punkt 1 weicht umso deutlicher ab -> keine Verbesserung -> wir bleiben bei den untransformierten Daten.

#Ergebnisdarstellung

#Da wir keine Interaktion zwischen Unterart und Jahreszeit festgestellt haben, brauchen wir auch keinen Interaktionsplot (unnötig kompliziert), statt dessen können wir die Ergebnisse am besten mit zwei getrennten Plots für die beiden Faktoren darstellen. Bitte die Achsenbeschriftungen und den Tukey post-hoc-Test nicht vergessen.

par(mfrow=c(1,1)) #Zurückschalten auf Einzelplots
if(!require(multcomp)){install.packages("multcomp")} 
library(multcomp)

#letters<-cld(glht(model2,linfct=mcp(Unterart="Tukey")))
boxplot(Tauchzeit~Unterart,xlab="Unterart",ylab="Tauchzeit")
#mtext(letters$mcletters$Letters,at=1:2)
#genaugenommen braucht man bei nur zwei Kategorien keinen post hoc-Test

letters<-cld(glht(aov.2,linfct=mcp(Jahreszeit="Tukey")))
boxplot(Tauchzeit~Jahreszeit,xlab="Jahreszeit",ylab="Tauchzeit") #Achsenbeschriftung nicht vergessen!
mtext(letters$mcletters$Letters,at=c(1:4))

#Jetzt brauchen wir noch die Mittelwerte bzw. Effektgroessen

#Für den Ergebnistext brauchen wir auch noch Angaben zu den Effektgrössen. Hier sind zwei Möglichkeiten, um an sie zu gelangen.

summary(lm(Tauchzeit~Jahreszeit))
summary(lm(Tauchzeit~Unterart))

aggregate(kormoran[,1],list(Unterart),mean)
aggregate(kormoran[,1],list(Jahreszeit),mean)

```
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:14_Statistik2/solution_stat2.3n.Rmd-->

---
title: "Modul Research Methods HS19"
output:
  html_document:
    df_print: paged
---


## Musterlösung Aufgabe 2.3S: ANOVA mit Interaktion
**Meine Empfehlung** Kapitel 7 von [Manny Gimond](https://mgimond.github.io/Stats-in-R/ANOVA.html)

```{r, message=FALSE, echo=FALSE, results='hide', warning=FALSE}

library(tidyverse)
library(ggfortify) # zur Testung der Voraussetzungen


## ladet die nötigen Packete und die novanimal.csv Datei in R
nova <- read_delim("13_Statistik1/data/novanimal.csv", delim = ";")

## definiert mytheme für ggplot2 (verwendet dabei theme_classic())
mytheme <- 
  theme_classic() + 
  theme(
    axis.line = element_line(color = "black"), 
    axis.text = element_text(size = 20, color = "black"), 
    axis.title = element_text(size = 20, color = "black"), 
    axis.ticks = element_line(size = 1, color = "black"), 
    axis.ticks.length = unit(.5, "cm")
    )


```


```{r}
# klone den originaler Datensatz
df <- nova 

# fasst die vier Inhalte der Gerichte zu drei Inhalten zusammen
df$label_content[grep("Pflanzlich+",df$label_content)] <- "Vegetarisch" # ersetzt beide Pflanzlich und Pflanzlich+
 
# gruppiert Daten gemäss Bedingungen, Menü-Inhalt und Wochen
df_ <- df %>%
    group_by(condit, label_content, week) %>%
    summarise(tot_sold = n()) %>%
    drop_na() # lasst die unbekannten Menü-Inhalte weg


# überprüft Voraussetzungen für eine ANOVA
# Boxplots zeigt klare Varianzheterogenität
ggplot(df_, aes(x = interaction(label_content, condit), y = tot_sold)) +
  stat_boxplot(geom = "errorbar", width = .25) +
  geom_boxplot(fill="white", size = 1, width = .5) + 
  labs(x = "\nMenü-Inhalt", y = "Anzahl verkaufte Gerichte pro Woche\n") +
  mytheme

# definiert das Modell mit Interaktion
model2 <- aov(tot_sold ~ label_content * condit, data = df_)

autoplot(model2) + mytheme  # Inspektion der Modellvoraussetzungen sehen nicht schlecht aus => einzig Normalverteilung Q-Q Plot nicht optimal (vgl. Statistik 2: Folie 42)

summary(model2)

# Alternativ gibt es zwei Möglichkeiten: 1) Transformation der Daten, 2) nicht-parametrischer Test z.B. Kruskal-Wallis-Test (vgl. Statistik 2: Folie 17-18)
model3 <- aov(log10(tot_sold) ~ label_content * condit, data = df_)

autoplot(model3) + mytheme # Ich entscheide mich aufgrund keiner Verbesserung durch Transformation für den nicht-parametrischen Test

```

<span style="color:blue"> Fazit: Die Inspektion des Modells zeigt leichte Verletzungen beim Q-Q Plot, d.h. die Residuen sind nicht normalverteilt. Aufgrund keiner Verbesserung durch eine Transformation der Responsevariable, entscheide ich mich für einen nicht-parametrischer Test, Kruskal-Wallis-Test (Statistik 2: Folie 18)</span>.

```{r, eval=FALSE}
# post-hoc-Tests nach Dunn

summary.lm(model2) # es gibt signifikante Unterschiede in den Interaktionen 

# in einem nächsten Schritt könnt ihr mit Post-hoc Tests diese Unterschiede genauer betrachten
# es gibt zwei Möglochkeiten dunnTest (mit Package FSA)
library(FSA)
dunnTest(df_$tot_sold, df_$condit, method="bh") # Korrektur für Mehrfachvergleiche (vgl. https://mgimond.github.io/Stats-in-R/ANOVA.html#4_identifying_which_levels_are_different)



```
---------

#### Methodenteil

Ziel war es, die Unterschiede in den Verkaufszahlen pro Menü-Inhalt und pro Bedingung aufzuzeigen. Da die Kriteriumsvariable (Verkaufszahlen) metrisch und die beiden Prädiktorvariablen kategorial sind, wurde eine zweifaktorielle ANOVA mit Interaktion gerechnet. Die visuelle Inspektion des Models zeigte keine schwerwiegenden Verletzungen der Voraussetzungen. Um die Einzelvergleiche zu sehen, wurde einen post-hoc-Test nach Tukey durchgeführt (für mehr Informationen [hier]( https://mgimond.github.io/Stats-in-R/ANOVA.html#4_identifying_which_levels_are_different)).  


#### Ergebnisteil
Die Menü-Inhalte (Fleisch, Vegetarisch und Buffet) zwischen den Bedingungen Basis oder Interventionswochen unterscheiden sich in den Verkaufszahlen signifikant
```{r}
#(*F*(5, 12) = r round(summary.lm(model1)$fstatistic[[1]], digit = 3), *p* < .001).
```
Anschliessend durchgeführte post-hoc-Tests (Tukey) zeigen vor allem zwei interessante Ergbenisse: 1) in den Interventionswochen wurden signifikant weniger Fleischgerichte gekauft als in den Basiswochen 2) in den Interventionswochen wurden signifikant mehr vegetarische Gerichte verkauft (siehe Figure 1 oder Figure 2). 


```{r, echo=F, fig.cap="Box-Whisker-Plots der wöchentlichen Verkaufszahlen pro Menü-Inhalte. Kleinbuchstaben bezeichnen homogene Gruppen auf *p* < .05 nach Tukeys post-hoc-Test."}

# zeigt die Ergebnisse anhand eines Boxplots
library(multcomp)
df_$cond_label <- interaction(df_$condit, df_$label_content) # bei Interaktionen gibt es diesen Trick, um bei den multiplen Vergleiche, die richtigen Buchstaben zu bekommen
model1 <- aov(tot_sold ~ cond_label, data = df_)
letters <-cld(glht(model1, linfct=mcp(cond_label="Tukey")))

ggplot(df_, aes(x = cond_label, y= tot_sold)) +
  geom_boxplot(fill="white", color = "black", size = 1) + 
  labs(x = "\nMenü-Inhalt", y = "Anzahl verkaufte Gerichte pro Woche\n") +
  scale_y_continuous(breaks = seq(0, 130,25), limits = c(0, 130)) +
  annotate("text", x = 1:6, y = 130, label = letters$mcletters$Letters, size = 8) +
  mytheme 

``` 



```{r,echo=F, fig.cap="Wöchentliche Verkaufszahlen aggregiert für die drei Menü-Inhalte."}
# eine weitere Möglichkeit die Ergebnisse darzustellen
m_sell <- na.omit(df_) %>% group_by(condit,label_content) %>% summarise(val = mean(tot_sold)) # berechne die durchschnittlichen Verkaufszahlen pro Bedingung

ggplot(df_, aes(x = condit, y = tot_sold, linetype = label_content, shape = label_content)) + 
    geom_point(data = m_sell, aes(y = val), size = 4) +
    geom_line(data = m_sell, aes(y = val, group = label_content), size = 2) + 
    labs(y = "Durchschnittlich verkaufte Gerichte pro Woche", x = "Bedingungen") + 
    guides(linetype = F, shape = guide_legend(title = "Menü-Inhalt"))+
    scale_y_continuous(breaks = seq(0,120,20), limits = c(0,120))+
    mytheme
```

*******
##### letztes Update `r Sys.Date()`, gian-andrea.egeler@zhaw.ch
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:14_Statistik2/solution_stat2.3s.Rmd-->

# Statistik 3 (04.11.2019)

Statistik 3 fassen wir zu Beginn den generellen Ablauf inferenzstatistischer Analysen in einem Flussdiagramm zusammen. Dann wird die ANCOVA als eine Technik vorgestellt, die eine ANOVA mit einer linearen Regression verbindet. Danach geht es um komplexere Versionen linearer Regressionen. Hier betrachten wir polynomiale Regressionen, die z. B. einen Test auf unimodale Beziehungen erlaubt, indem man dieselbe Prädiktorvariable linear und quadriert einspeist. Multiple Regressionen versuchen dagegen, eine abhängige Variable durch zwei oder mehr verschieden Prädiktorvariablen zu erklären. Wir thematisieren verschiedene dabei auftretende Probleme und ihre Lösung, insbesondere den Umgang mit korrelierten Prädiktoren und das Aufspüren des besten unter mehreren möglichen statistischen Modellen. Hieran wird auch der informatian theoretician-Ansatz der Statistik und die multimodel inference eingeführt.
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:15_Statistik3/Abstract.Rmd-->

```{r, include=FALSE, purl=F}

knitr::opts_chunk$set(echo = TRUE,include = T, results = "hide",collapse=TRUE)
```

## Demoskript

[Demoscript als Download](15_Statistik3/RFiles/Demo_Tests.R)
**Datensatz [ipomopsis.csv](15_Statistik3/data/ipomopsis.csv)**
**Datensatz [loyn.csv](15_Statistik3/data/loyn.csv)**

**ANCOVA**
Experiment zur Fruchtproduktion (“Fruit”) von Ipomopsis sp. (“Fruit”) in Abhängigkeit Ungrazedvon der Beweidung (Grazing mit 2 Levels: Grazed, Ungrazed) und korrigiert für die Pflanzengrösse vor der Beweidung (hier ausgedrückt als Durchmesser an der Spitze des Wurzelstock: “Root”)

```{r, eval=FALSE}

compensation<-read.table("data/ipomopsis.csv", header=T,sep=",")
```
```{r, eval=FALSE}
summary(compensation)
attach(compensation)

plot(Fruit~Root)
plot(Fruit~Grazing)

tapply(Fruit,Grazing,mean)

aoc.1<-lm(Fruit~Root*Grazing)
summary.aov(aoc.1)

aoc.2<-lm(Fruit~Grazing*Root)
summary.aov(aoc.2)

aoc.3<-lm(Fruit~Grazing+Root)
summary.lm(aoc.3)

# Plotten der Ergebnisse
plot(Fruit~Root,pch=21,bg=(1+as.numeric(Grazing)))
#legend(locator(1),c("grazed","ungrazed"),col=c(2,3),pch=16) # Position von Legende von Hand setzen

#legend(4.5,110,c("grazed","ungrazed"),col=c(2,3),pch=16) # Position von Legende in Code definieren
#legend("topleft",c("grazed","ungrazed"),col=c(2,3),pch=16) # Alternative position von Legende in Code definieren


abline(-127.829,23.56,col="red")
abline(-127.892+36.103,23.56,col="green")
```


**Polynomische Regression**

```{r, eval=FALSE}
e<-c(20,19,25,10,8,15,13,18,11,14,25,39,38,28,24)
f<-c(12,15,10,7,2,10,12,11,13,10,9,2,4,7,13)

summary(lm(f~e))

par(mfrow=c(1,1))
plot(f~e,xlim=c(0,40),ylim=c(0,30))
abline(lm(f~e))

par(mfrow=c(2,2))
plot(lm(f~e))
plot(lm(f~e+I(e^2)))

summary(lm(f~e+I(e^2)))
```

**Multiple lineare Regression basierend auf Logan, Beispiel 9A** 

```{r, eval=FALSE}
loyn <- read.table("data/loyn.csv", header=T,sep=",")
loyn
library(car)

summary(loyn)

lm.1 <- lm (ABUND ~ YR.ISOL + ALT + GRAZE, data=loyn)
summary(lm.1)

aov(lm.1)
par(mfrow=c(2,2))
plot(lm.1)
influence.measures(lm.1)

cor <- cor(loyn[,2:7])
print(cor, digits=2)

cor[abs(cor)<0.6] <- 0
cor
print(cor, digits=3)

vif(lm.1)
```

**Simulation Overfitting**

```{r, eval=FALSE}
test <- data.frame("x"=c(1,2,3,4,5,6),"y"=c(34,21,70,47,23,45))
attach(test)

plot(x,y)
lm0 <- lm(y~1)
lm1 <- lm(y~x)
lm2 <- lm(y~x+I(x^2))
lm3 <- lm(y~x+I(x^2)+I(x^3))
lm4 <- lm(y~x+I(x^2)+I(x^3)+I(x^4))
lm5 <- lm(y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5))
lm6 <- lm(y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)+I(x^6))
summary(lm0)
summary(lm1)
summary(lm2)
summary(lm3)
summary(lm4)
summary(lm5)

xv<-seq(from=0,to=10,by=0.1)

plot(x,y,cex=2,col="black",lwd=3)
yv<-predict(lm1,list(x=xv))
lines(xv,yv,col="red",lwd=3)
yv<-predict(lm2,list(x=xv))
lines(xv,yv,col="blue",lwd=3)
yv<-predict(lm3,list(x=xv))
lines(xv,yv,col="green",lwd=3)
yv<-predict(lm4,list(x=xv))
lines(xv,yv,col="orange",lwd=3)
yv<-predict(lm5,list(x=xv))
lines(xv,yv,col="black",lwd=3)
```

**Modellvereinfachung (mit Loyn-Datensatz)**

```{r, eval=FALSE}
lm.1 <- lm(ABUND ~ YR.ISOL + ALT + GRAZE, data=loyn)
summary(lm.1)
lm.2 <- update(lm.1,~.-YR.ISOL)

summary(lm.2)

anova(lm.1,lm.2)
```

**Hierarchical partitioning**
```{r, eval=FALSE}
if(!require(hier.part)){install.packages("hier.part")}
library(hier.part)

loyn.preds <-with(loyn, data.frame(YR.ISOL,ALT,GRAZE))
par(mfrow=c(1,1))
hier.part(loyn$ABUND,loyn.preds,gof="Rsqu")
```

**Partial regressions**


```{r, eval=FALSE}
avPlots(lm.1,ask=F)
```

**Multimodel inference**

```{r, eval=FALSE}
if(!require(MuMIn)){install.packages("MuMIn")}
library(MuMIn)

global.model <- lm (ABUND ~ YR.ISOL + ALT + GRAZE, data=loyn)
options(na.action="na.fail")
allmodels <- dredge(global.model)
allmodels
importance(allmodels)

avgmodel<-model.avg(get.models(dredge(global.model,rank="AICc"),subset=TRUE))
summary(avgmodel)
```
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:15_Statistik3/Demoskript.Rmd-->

## Übung 3

### Aufgabe 3.1: Multiple Regression

- **Datensatz [Ukraine_bearbeitet.xlsx](15_Statistik3/data/Ukraine_bearbeitet.xlsx)**
- **Datensatz [Ukraine_bearbeitet.csv](15_Statistik3/data/Ukraine_bearbeitet.csv)**

Artenzahlen von Vegetationsaufnahmen in der Ukraine vs. diverse Umweltparameter (farbig gruppiert nach Kategorien, aus Kuzemko et al. 2016) 

- Bestimmt ein minimal adäquates Modell für die Erklärung der Artenzahlen mit allen notwendigen Arbeitsschritten
- Wahlweise könnt ihr mit AICc und dredge oder mit p-Werten und schrittweiser Vereinfachung eines globalen Models arbeiten


```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:15_Statistik3/Uebungen.Rmd-->

# Statistik 4 (05.11.2019)

Am Anfang von Statistik 4 steht ein Einstieg in nicht-lineare Regressionen, die es erlauben, etwa Potenzgesetze direkt zu modellieren. Danach geht es um generalized linear models (GLMs), die einige wesentliche Limitierungen von linearen Modellen überwinden. Indem sie Fehler- und Varianzstrukturen explizit modellieren, ist man nicht mehr an Normalverteilung der Residuen und Varianzhomogenität gebunden. Bei generalized linear regressions muss man sich zwischen verschiedenen Verteilungen und link-Strukturen entscheiden. Spezifisch werden wir uns die Poisson-Regressionen für Zähldaten und logistische Regression für ja/nein-Daten anschauen. Zum Abschluss gibt es einen Ausblick auf Glättungsverfahren (LOESS) und general additive models (GAMs).

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:16_Statistik4/Abstract.Rmd-->




## Statistik 4 - Demoskript

**(c) Juergen Dengler, 05.11.2019**

[Demoscript als Download](16_Statistik4/RFiles/Demo_Tests.R)

```{r}
##von LMs zu GLMs

temp<-c(10,12,16,20,24,25,30,33,37)
besucher<-c(40,12,50,500,400,900,1500,900,2000)
strand<-data.frame("Temperatur"=temp,"Besucher"=besucher)
attach(strand)

par(mfrow=c(1,1))
plot(besucher~temp)

lm.strand<-lm(Besucher~Temperatur, data=strand)
summary(lm.strand)

par(mfrow=c(2,2))
plot(lm.strand)

par(mfrow=c(1,1))
xv<-rep(0:40,by=.1)
yv<-predict(lm.strand,list(Temperatur=xv))
plot(Temperatur,Besucher,xlim=c(0,40))
lines(xv,yv,lwd=3,col="blue")

glm.gaussian<-glm(Besucher~Temperatur,family=gaussian)
glm.poisson<-glm(Besucher~Temperatur,family=poisson)

summary(glm.gaussian)
summary(glm.poisson)


glm.quasi<-glm(Besucher~Temperatur,family=quasipoisson)
summary(glm.quasi)

par(mfrow=c(2,2))
plot(glm.gaussian)
plot(glm.poisson)
plot(glm.quasi)

par(mfrow=c(1,1))
plot(Temperatur,Besucher,xlim=c(0,40))
xv<-rep(0:40,by=.1)

yv<-predict(lm.strand,list(Temperatur=xv))
lines(xv,yv,lwd=3,col="blue")

yv2<-predict(glm.poisson,list(Temperatur=xv))
lines(xv,exp(yv2),lwd=3,col="red")

yv3<-predict(glm.quasi,list(Temperatur=xv))
lines(xv,exp(yv3),lwd=3,col="green")

```



```{r}

# Test für Overdispersion  
library(AER)
dispersiontest(glm.poisson)


##Logistische Regression

bathing<-data.frame("temperature"=c(1,2,5,9,14,14,15,19,22,24,25,26,27,28,29),
                    "bathing"=c(0,0,0,0,0,1,0,0,1,0,1,1,1,1,1))
plot(bathing~temperature, data=bathing)

glm.1<-glm(bathing~temperature, data=bathing, family="binomial")
summary(glm.1)

#Modeldiagnostik (wenn nicht signifikant, dann OK)
1 - pchisq (glm.1$deviance,glm.1$df.resid)

#Modellgüte (pseudo-R²)
1 - (glm.1$dev / glm.1$null)

#Steilheit der Beziehung (relative Änderung der odds bei x + 1 vs. x)
exp(glm.1$coefficients[2])

#LD50 (also hier: Temperatur, bei der 50% der Touristen baden)
-glm.1$coefficients[1]/glm.1$coefficients[2]

#Vorhersagen
predicted <- predict(glm.1, type="response")

#Konfusionsmatrix
km <- table(bathing$bathing, predicted > 0.5)
km

#Missklassifizierungsrate
1-sum(diag(km)/sum(km))



#Plotting
xs<-seq(0,30,l=1000)
model.predict<-predict(glm.1,type="response",se=T,newdata=data.frame(temperature=xs))
plot(bathing~temperature,data=bathing,xlab="Temperature (°C)",ylab="% Bathing",pch=16, col="red")
points(model.predict$fit ~ xs,type="l")
lines(model.predict$fit+model.predict$se.fit ~ xs, type="l",lty=2)
lines(model.predict$fit-model.predict$se.fit ~ xs, type="l",lty=2)
```

```{r}
##Nicht-lineare Regression

if(!require(AICcmodavg)){install.packages("AICcmodavg")}
if(!require(nlstools)){install.packages("nlstools")}
library(AICcmodavg)
library(nlstools)


loyn <- read.delim("16_Statistik4/data/loyn.csv", sep=",")
attach(loyn)

#Selbstdefinierte Funktion, hier Potenzfunktion
power.model<-nls(ABUND~c*AREA^z,start=(list(c=1,z=0)))
summary(power.model)
AICc(power.model)

#Modeldiagnostik (in nlstools)
plot(nlsResiduals(power.model))

#Vordefinierte "Selbststartfunktionen"#
?selfStart
logistic.model<-nls(ABUND~SSlogis(AREA,Asym,xmid,scal))
summary(logistic.model)
AICc(logistic.model)

#Modeldiagnostik (in nlstools)
plot(nlsResiduals(logistic.model))

#Visualisierung
plot(ABUND~AREA)
par(mfrow=c(1,1))
xv<-seq(0,2000,0.01)

# 1. Potenzfunktion
yv1 <-predict(power.model,list(AREA=xv))
lines(xv,yv1,col="green")

# 2. Logistische Funktion
yv2 <-predict(logistic.model,list(AREA=xv))
lines(xv,yv2,col="blue")

#Visualisierung II
plot(ABUND~log10(AREA))
par(mfrow=c(1,1))

# 1. Potenzfunktion
yv1 <-predict(power.model,list(AREA=xv))
lines(log10(xv),yv1,col="green")

# 2. Logistische Funktion
yv2 <-predict(logistic.model,list(AREA=xv))
lines(log10(xv),yv2,col="blue")

#Model selection among several non-linear models

cand.models<-list()
cand.models[[1]]<-power.model
cand.models[[2]]<-logistic.model

Modnames <- c("Power", "Logistic")

aictab(cand.set = cand.models, modnames = Modnames)


##Smoother

attach(loyn)
log_AREA<-log10(AREA)       
plot(ABUND~log_AREA)
lines(lowess(log_AREA,ABUND,f=0.25),lwd=2,col="red")
lines(lowess(log_AREA,ABUND,f=0.5),lwd=2,col="blue")
lines(lowess(log_AREA,ABUND,f=1),lwd=2,col="green")


#GAMs

if(!require(mgcv)){install.packages("mgcv")}
library(mgcv)

gam.1<-gam(ABUND~s(log_AREA))
gam.1
summary(gam.1)

plot(log_AREA,ABUND,pch=16)
xv<-seq(-1,4,by=0.1)
yv<-predict(gam.1,list(log_AREA=xv))
lines(xv,yv,lwd=2,col="red")

AICc(gam.1)
summary(gam.1)

```
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:16_Statistik4/Demoskript.Rmd-->

## Statistik 4: Übungen

### Übung 4.1: Nicht-lineare Regression (naturwissenschaftlich)

**Datensatz [Curonian_Spit.xlsx](16_Statistik4/data/Curonian_Spit.xlsx)**

Dieser enthält gemittelte Pflanzenartenzahlen (Species.richness) von
geschachtelten Plots (Vegetationsaufnahmen) der Pflanzengesellschaft LolioCynosuretum im Nationalpark Kurische Nehrung (Russland) auf
Flächengrössen (Area) von 0.0001 bis 900 m².

**Ermittelt den funktionellen Zusammenhang (das beste Modell), der die Zunahme der Artenzahl mit der Flächengrösse am besten beschreibt.Berücksichtigt dabei mindestens die Potenzfunktion (power function,**</font>$S=$c$A^{n}$<font color="blue">**), die logarithmische Funktion (logarithmic function,**</font>$S$=$b_{0}$$+$$b_{1}$$\log_{10}$A<font color="blue">**)und eine Funktion mit Sättigung (saturation, asymptote) eurer Wahl.**


### Übung 4.2N: Multiple logistische Regression (naturwissenschaftlich)

**Datensatz [isolation.csv](16_Statistik4/data/isolation.csv)**

Dieser enthält für 50 Inseln die Information, ob eine bestimmte Vogelart dort vorkommt (incidence = 1) oder nicht vorkommt (incidence = 0). Für jede der Inseln sind zudem zwei Umweltvariablen angegeben: area (Fläche in km²) und
isolation (Entfernung vom Festland in km).

**Ermittelt das minimal adäquate statistische Modell, das die Vorkommenswahrscheinlichkeit der Vogelart in Abhängigkeit von Flächengrösse und Entfernung beschreibt.**


### Übung 4.2S: Multiple logistische Regression (sozioökonomisch)

Führt mit dem Datensatz [novanimal.csv](13_Statistik1/data/novanimal.csv) eine logistische Regression durch.Kann der Fleischkonsum durch das Geschlecht, die Hochschulzugehörigkeit und das Alter erklärt werden?

**Hinweise:**

- Generiert eine neue Variable "Fleisch" (0 = kein Fleisch, 1 = Fleisch)
- Entfernt fehlende Werte aus der Variable "Fleisch"
- Lasst für die Analyse den Menü-Inhalt «Buffet» weg
- Definiert das Modell und wendet es auf den Datensatz an
- Berechnet eine Vorhersage des Modells mit predict()
- Eruiert den Modellfit und die Modellgenauigkeit
- Für Motivierte: Berechnet eine Konfusionsmatrix und zieht euer Fazit daraus (vgl. )
- Stellt eure Ergebnisse dann angemessen dar (Text und/oder Tabelle).




```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:16_Statistik4/assigment_stat4.Rmd-->

