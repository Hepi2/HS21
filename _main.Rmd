--- 
title: "Research Methods"
bibliography:
- 00_Admin/book.bib
- 00_Admin/packages.bib
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
documentclass: book
link-citations: yes
description: 'Begleitmaterial zum Modul ''Research Methods'' '
site: bookdown::bookdown_site
biblio-style: apalike
---


# Einleitung


Das Modul „Research Methods“ vermittelt vertiefte Methodenkompetenzen für praxisorientiertes und angewandtes wissenschaftliches Arbeiten im Fachbereich „Umwelt und Natürliche Ressourcen“ auf MSc-Niveau. Die Studierenden erarbeiten sich vertiefte Methodenkompetenzen für die analytische Betrachtung der Zusammenhänge im Gesamtsystem „Umwelt und Natürliche Ressourcen“. Die Studierenden erlernen die methodischen Kompetenzen, auf denen die nachfolgenden Module im MSc Programm UNR aufbauen. Das Modul vermittelt einerseits allgemeine, fächerübergreifende methodische Kompetenzen (z.B. Wissenschaftstheorie, computer-gestützte Datenverar-beitung und Statistik).

Auf dieser Plattform (RStudio Connect) werden die Unterlagen für die R-Übungsteile bereitgestellt. Es werden sukzessive sowohl Demo-Files, Aufgabenstellungen und Lösungen veröffentlicht.




```{r, include=F, message=F}
options(tinytex.verbose = TRUE)
# Set Root Directory / Working directory to Project folder for all Files (if M-K)
knitr::opts_knit$set(root.dir = getwd())
```


```{r, include=F, message=F}
options(tinytex.verbose = TRUE)


grepl_loop <- function(vector,remove){
  for(remove_i in remove){
    vector <- vector[!grepl(remove_i,vector)]
  }
  return(vector)
}




# Allow duplicate Labels so that calling purl() does not create an error
# https://stackoverflow.com/q/36868287/4139249
options(knitr.duplicate.label = 'allow')

# purl all Rmd Documents (with some exceptions) and store them in a Subfolder /RFiles
# Document cannot be knitted if the folder "RFiles" does not exist!
library(stringr)

keywords <- c("ResearchMethods","_Rcode","99_","index","Archive","Admin","main","Abstract")


rmds <- list.files(pattern = ".Rmd",recursive = T)

rmds <- rmds[grepl("PrePro|InfoVis|RaumAn",rmds)]

rmds <- grepl_loop(rmds,keywords)


# 2019-08-15 rata: folgender Teil ist auskommentiert, um die Komplexität des ganzen zu verkleinern
for (file in rmds){
  file_r <- gsub("Rmd","R",file)                          # change fileextension from .rmd to r
  file_r <- str_split_fixed(file_r,"/",Inf)               # split path at /
  file_r <- append(file_r, "RFiles",length(file_r)-1)     # append Foldername "RFiles" in 2nd last pos
  file_r <- paste(file_r,collapse = "/")                  # collapse vector to string
  if(file.exists(file_r)){
    file.remove(file_r)
  }
  knitr::purl(file,documentation = 0,output = file_r)
}

```




```{r include=FALSE, message=F}
options(tinytex.verbose = TRUE)
# automatically create a bib database for R packages
# 2019-08-15 rata: folgender Teil ist auskommentiert, um die Komplexität des ganzen zu verkleinern
# knitr::write_bib(c(
#   .packages(), 'bookdown', 'knitr','forcats','carData', 'rmarkdown','tidyverse','plotly','car','ggfortify','boot','pander','scales','multicomp','ggExtra','lubridate','dplyr','purrr','readr','tidyr','tibble','ggplot2','webshot','bindrcpp','GGally','hier.part','gtools','MuMIn','nlme','lme4','languageR','lmerTest','rms','SparseM','Hmisc','Formula','survival','lattice','Matrix'
# ), '00_Admin/packages.bib')

```
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:index.Rmd-->

# PrePro1 (12.10.2020)

Die Datenkunde 2.0 gibt den Studierenden das Wissen und die Fertigkeiten an die Hand, selbst erhobene und bezogene Daten für Ihre eigenen Analysen vorzubereiten und anzureichern (preprocessing). Die Einheit vermittelt zentrale Datenverarbeitungskompetenzen und thematisiert bekannte Problemzonen der umweltwissenschaftlichen Datenverarbeitung – immer mit einer „hands-on“ Perspektive auf die begleitenden R-Übungen. Die Studierenden lernen die Eigenschaften ihrer Datensätze in der Fachsprache korrekt zu beschreiben. Sie lernen ausserdem Metadaten zu verstehen und die Implikationen derselben für ihre eigenen Analyseprojekte kritisch zu beurteilen. Zentrale Konzepte der Lerneinheit sind Skalenniveaus, Datentypen, Zeitdaten und Typumwandlungen.


```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:09_PrePro1/Abstract.Rmd-->


```{r, include=FALSE, purl=F}

knitr::opts_chunk$set(echo = TRUE,include = T, collapse=TRUE)
```

## Demo: Datentypen, Tabellen

[R-Code als Download](09_PrePro1/RFiles/Demo_Datentypen.R)

### Datentypen 


#### Numerics

Unter die Kategorie `numeric` fallen in R zwei Datentypen:

- `double`: Gleitkommazahl (z.B. 10.3, 7.3)
- `integer`: Ganzzahl (z.B. 10, 7)

##### Doubles

Folgendermassen wird eine Gleitkommazahl einer Variabel zuweisen:

```{r}
x <- 10.3

x

typeof(x)
```



Statt `<-`kann auch `=` verwendet werden. Dies funktioniert aber nicht in allen Situationen, und ist zudem leicht mit `==` zu verwechseln.

```{r}
y = 7.3

y
```



Ohne explizite Zuweisung nimmt R immer den Datentyp `double`an:

```{r}
z <- 42
typeof(z)
is.integer(z)
is.numeric(z)
is.double(z)

```

#### Ganzzahl / Integer 


Erst wenn man eine Zahl explizit als `integer` definiert (mit `as.integer()` oder `L`), wird sie auch als solches abgespeichert.

```{r}
a <- as.integer(z)
is.numeric(a)
is.integer(a)

c <- 8L
is.numeric(c)
is.integer(c)
```




```{r}
typeof(a)

is.numeric(a)
is.integer(a)
```



Mit `c()` können eine Reihe von Werten in einer Variabel zugewiesen werden (als `vector`). Es gibt zudem auch `character vectors`. 

```{r}
vector <- c(10,20,33,42,54,66,77)
vector
vector[5]
vector[2:4]

vector2 <- vector[2:4]
```



Eine Ganzzahl kann explizit mit `as.integer()` definiert werden.

```{r}
a <- as.integer(7)
b <- as.integer(3.14)
a
b
typeof(a)
typeof(b)
is.integer(a)
is.integer(b)

```

Eine Zeichenkette kann als Zahl eingelesen werden.

```{r}
c <- as.integer("3.14")
c
typeof(c)
```


#### Logische Abfragen 

Wird auch auch als boolesch (Eng. **boolean**) bezeichnet.

```{r}
e <- 3
f <- 6
g <- e > f
e
f
g
typeof(g)

```

#### Logische Operationen


```{r}
sonnig <- TRUE
trocken <- FALSE

sonnig & !trocken
```

Oft braucht man auch das Gegenteil / die Negation eines Wertes. Dies wird mittels `!` erreicht

```{r}
u <- TRUE
v <- !u 
v
```



#### Zeichenketten

Zeichenketten (Eng. **character**) stellen Text dar

```{r}
s <- as.character(3.14)
s
typeof(s)
```



Zeichenketten verbinden / zusammenfügen (Eng. **concatenate**)

```{r}
fname <- "Hans"
lname <- "Muster"
paste(fname,lname)

fname2 <- "hans"
fname == fname2
```


#### `Factors`

Mit `Factors` wird in R eine Sammlung von Zeichenketten bezeichnet, die sich wiederholen, z.B. Wochentage (es gibt nur 7 unterschiedliche Werte für "Wochentage").

```{r}
wochentage <- c("Montag","Dienstag","Mittwoch","Donnerstag","Freitag","Samstag","Sonntag",
                "Montag","Dienstag","Mittwoch","Donnerstag","Freitag","Samstag","Sonntag")

typeof(wochentage)

wochentage_fac <- as.factor(wochentage)

wochentage
wochentage_fac


```

Wie man oben sieht, unterscheiden sich `character vectors` und `factors` v.a. dadurch, dass letztere über sogenannte `levels` verfügt. Diese `levels` entsprechen den Eindeutigen (`unique`) Werten.

```{r}
levels(wochentage_fac)

unique(wochentage)
```

Zudem ist fällt auf, dass die Reihenfolge der Wohentag alphabetisch sortiert ist. Wie diese sortiert werden zeigen wir an einem anderen Beispiel:


```{r}
zahlen <- factor(c("null","eins","zwei","drei"))

zahlen
```

Offensichtlich sollten diese `factors` geordnet sein, R weiss davon aber nichts. Eine Ordnung kann man mit dem Befehl `ordered = T` festlegen. 

Beachtet: `ordered = T` kann nur bei der Funktion `factor()` spezifiziert werden, nicht bei `as.factor()`. Ansonsten sind `factor()` und `as.factor()` sehr ähnlich.


```{r}
zahlen <- factor(zahlen,ordered = T)

zahlen
```

Beachtet das "<"-Zeichen zwischen den Levels. Die Zahlen werden nicht in der korrekten Reihenfolge, sondern Alphabetisch geordnet. Die richtige Reihenfolge kann man mit `levels = ` festlegen.

```{r}
zahlen <- factor(zahlen,ordered = T,levels = c("null","eins","zwei","drei","vier"))

zahlen
```

Wie auch schon erwähnt werden `factors` als `character` Vektor dargestellt, aber als Integers gespeichert. Das führt zu einem scheinbaren Wiederspruch wenn man den Datentyp auf unterschiedliche Weise abfragt.
```{r}
typeof(zahlen)

is.integer(zahlen)
```


Mit `typeof()` wird eben diese Form der Speicherung abgefragt und deshalb mit `integer` beantwortet. Da es sich aber nicht um einen eigentlichen Integer Vektor handelt, wird die Frage `is.integer()` mit `FALSE` beantwortet. Das ist etwas verwirrend, beruht aber darauf, dass die beiden Funktionen die Frage von unterschiedlichen Perspektiven beantworten. In diesem Fall schafft `class()` Klarheit:

```{r}
class(zahlen)
```


Wirklich verwirrend wird es, wenn `factors` in numeric umgewandelt werden sollen.

```{r}
zahlen
as.integer(zahlen)
```

Das die Übersetzung der auf Deutsch ausgeschriebenen Nummern in nummerische Zahlen nicht funktionieren würde, war ja klar. Weniger klar ist es jedoch, wenn die `factors` bereits aus nummerischen Zahlen bestehen.

```{r}
zahlen2 <- factor(c("3","2","1","0"))

as.integer(zahlen2)

```

In diesem Fall müssen die `factors` erstmals in `character` umgewandelt werden.

```{r}
zahlen2 <- factor(c("3","2","1","0"))

as.integer(as.character(zahlen2))
```




#### Zeit/Datum

Um in R mit Datum/Zeit Datentypen umzugehen, müssen sie als `POSIXct` eingelesen werden (es gibt alternativ noch `POSIXlt`, aber diese ignorieren wir mal). Anders als Beispielsweise bei Excel, sollten in R Datum und Uhrzeit immer in **einer Spalte** gespeichert werden.

```{r}
datum <- "2017-10-01 13:45:10"

as.POSIXct(datum)

```

Wenn das die Zeichenkette in dem obigen Format (Jahr-Monat-Tag Stunde:Minute:Sekunde) daher kommt, braucht `as.POSIXct`keine weiteren Informationen. Sollte das Format von dem aber Abweichen, muss man der Funktion das genaue Schema jedoch mitteilen. Der Syntax dafür kann via `?strptime` nachgeschlagen werden.

```{r}
datum <- "01.10.2017 13:45"

as.POSIXct(datum,format = "%d.%m.%Y %H:%M")

datum <- as.POSIXct(datum,format = "%d.%m.%Y %H:%M")

```

Beachtet, dass in den den obigen Beispiel R automatisch eine Zeitzone angenommen hat (`CEST`). R geht davon aus, dass die Zeitzone der **System Timezone** (`Sys.timezone()`) entspricht.

```{r}

strftime(datum, format = "%m")
strftime(datum, format = "%b")
strftime(datum, format = "%B")
```



### Data Frames und Conveniance Variabeln

Eine `data.frame` ist die gängigste Art, Tabellarische Daten zu speichern. 

```{r}
df <- data.frame(
  Stadt = c("Zürich","Genf","Basel","Bern","Lausanne"),
  Einwohner = c(396027,194565,175131,140634,135629),
  Ankunft = c("1.1.2017 10:00","1.1.2017 14:00",
              "1.1.2017 13:00","1.1.2017 18:00","1.1.2017 21:00")
)

str(df)

```

In der obigen `data.frame` wurde die Spalte `Einwohner` als Fliesskommazahl abgespeichert. Dies ist zwar nicht tragisch, aber da wir wissen das es sich hier sicher um Ganzzahlen handelt, können wir das korrigieren. Wichtiger ist aber, dass wir die Ankunftszeit (Spalte`Ankunft`) von  einem `Factor` in ein Zeitformat (`POSIXct`) umwandeln. 


```{r}
df$Einwohner <- as.integer(df$Einwohner)

df$Einwohner

df$Ankunft <- as.POSIXct(df$Ankunft, format = "%d.%m.%Y %H:%M")

df$Ankunft
```


Diese Rohdaten können nun helfen, um Hilfsvariablen (**convenience variables**) zu erstellen. Z.B. können wir die Städte einteilen in gross, mittel und klein. 

```{r}
df$Groesse[df$Einwohner > 300000] <- "gross"
df$Groesse[df$Einwohner <= 300000 & df$Einwohner > 150000] <- "mittel"
df$Groesse[df$Einwohner <= 150000] <- "klein"

```



Oder aber, die Ankunftszeit kann von der Spalte `Ankunft`abgeleitet werden. Dazu brauchen wir aber das Package `lubridate`

```{r, message = F}
library(lubridate)
```


```{r}
df$Ankunft_stunde <- hour(df$Ankunft)
```

### Quellen

```{r code=readLines('00_Admin/get_chapter_references.R'), echo=F, eval=T,purl = F, results="asis"}
```




```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:09_PrePro1/Demo_Datentypen.Rmd-->


```{r, include=FALSE, purl = F}
knitr::opts_chunk$set(echo = F, include = T, collapse=TRUE, warning = F)
```


## Uebung A 

### Arbeiten mit RStudio "Project"

Wir empfehlen die Verwendung von "Projects" innerhalb von RStudio. RStudio legt für jedes Projekt dann einen Ordner an, in welches die Projekt-Datei abgelegt wird (Dateiendung .Rproj). Sollen innerhalb des Projekts dann R-Skripts geladen oder erzeugt werden, werden diese dann auch im angelegten Ordner abgelegt. Mehr zu RStudio Projects findet ihr  [hier](https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects).


Das Verwenden von Projects bringt verschiedene Vorteile, wie zum Beispiel:

- Festlegen der Working Directory ohne die Verwendung des expliziten Pfades (`setwd()`). Das ist sinnvoll, da sich dieser Pfad ändern kann (Zusammenarbeit mit anderen Usern, Ausführung des Scripts zu einem späteren Zeitpunkt) 
- Automatisches Zwischenspeichern geöffneter Scripts und Wiederherstellung der geöffneten Scripts bei der nächsten Session
- Festlegen verschiedener projektspezifischer Optionen
- Verwendung von Versionsverwaltungssystemen (git oder SVN)


### Arbeiten mit Libraries / Packages

R ist ohne Zusatzpackete nicht mehr denkbar. Die allermeisten Packages werden auf [CRAN](https://cran.r-project.org/) gehostet und können leicht mittels `install.packages()` installiert werden. Eine sehr wichtige Sammlung von Packages wird von RStudio entwickelt. Unter dem Namen [Tidyverse](https://www.tidyverse.org/) werden eine Reihe von Packages angeboten, den R-Alltag enorm erleichtert. Wir werden später näher auf das "Tidy"-Universum eingehen, an dieser Stelle können wir die Sammlung einfach mal installieren.

```
install.packages("tidyverse")
```

Um ein `package` in R verwenden zu können, gibt es zwei Möglichkeiten: 

- entweder man lädt es zu Beginn der R-session mittles `library(tidyverse)` (ohne Anführungs- und Schlusszeichen). 
- oder man ruft eine `function` mit vorangestelltem Packetname sowie zwei Doppelpunkten auf. `dplyr::filter()` ruft die Funktion `filter()` des Packets `dplyr` auf. 

Letztere Notation ist vor allem dann sinnvoll, wenn sich zwei unterschiedliche Funktionen mit dem gleichen namen in verschiedenen pacakges existieren. `filter()` existiert als Funktion einersits im package `dplyr` sowie in  `stats`. Dieses Phänomen nennt man "masking". 


Zu beginn laden wir die nötigen Pakete:


```{r,message = F}
library(tidyverse)
# Im Unterschied zu `install.packages()` werden bei `library()` keine Anführungs- 
# und Schlusszeichen gesetzt.


library(lubridate)
# Im Unterschied zu install.packages("tidyverse") wird bei library(tidyverse) 
# das package lubridate nicht berücksichtigt
```


<!-- Aus oben beschriebenen Grund ist es auch problematisch, dass `data.frame()` sowie alle `read.*` Funktionen (`read.table`, `read.csv` etc) immer davon ausgehen, dass `strings` als `factors` interpretiert werden sollten. Es gibt in Base R einige Funktionen, welche Annahmen treffen die problematisch sein können. Ein weiteres Beispiel ist die Annahme der Zeitzone und Verwendung von Sommerzeit bei `as.POSIXct()`. -->

Tidyverse liefert viele Funktionen, für die es in der normalen R-Umgebung ("base R") keine wirkliche Alternative gibt. Andere Funktionen sind alternativen zu Base-R Funktionen:

- `data_frame()` statt `data.frame()` 
- `read_*` statt `read.*`
- `parse_datetime` statt `as.POSIXct()`

Diese verhalten sich leicht anders als Base-R Funktionen: Sie treffen weniger Annahmen und sind etwas restriktiver. Wir verwenden oft Tidyverse Funktionen, ihr könnt aber selber entscheiden welche Version ihr benutzt.

### Aufgabe 1

Erstelle eine `data.frame` mit nachstehenden Daten.

Tipps:

- Eine leere `data.frame` zu erstellen ist schwieriger als wenn erstellen und befüllen der `data.frame` in einem Schritt erfolgt
- R ist dafür gedacht, Spalte für Spalte zu arbeiten ([warum?](http://www.noamross.net/blog/2014/4/16/vectorization-in-r--why.html)), nicht Reihe für Reihe. Versuche dich an dieses Schema zu halten.

```{r}

# Lösung Aufgabe 1

df <- data.frame(
  Tierart = c("Fuchs","Bär","Hase","Elch"),
  Anzahl = c(2,5,1,3),
  Gewicht = c(4.4, 40.3,1.1,120),
  Geschlecht = c("m","f","m","m"),
  Beschreibung = c("Rötlich","Braun, gross", "klein, mit langen Ohren","Lange Beine, Schaufelgeweih")
  )

```


```{r, echo = F, purl=F}
knitr::kable(df)
```



### Aufgabe 2

Was für Datentypen wurden (in Aufgabe 1) von R automatisch angenommen? Sind diese sinnvoll? 

Tipp: Nutze dazu `str()`

```{r}
# Lösung Aufgabe 2

str(df)

# Anzahl wurde als `double` interpretiert, ist aber eigentlich ein `integer`. 
# Mit data.frame() wurde Beschreibung wurde als `factor` interpretiert, ist 
# aber eigentlich `character`
```


```{r}


typeof(df$Anzahl)

df$Anzahl <- as.integer(df$Anzahl)
df$Beschreibung <- as.character(df$Beschreibung)

```


### Aufgabe 3


Nutze die Spalte `Gewicht` um die Tiere in 3 Gewichtskategorien einzuteilen: 

- leicht: < 5kg
- mittel: 5 - 100 kg
- schwer: > 100kg


```{r}

# Lösung Aufgabe 3

df$Gewichtsklasse[df$Gewicht > 100] <- "schwer"
df$Gewichtsklasse[df$Gewicht <= 100 & df$Gewicht > 5] <- "mittel"
df$Gewichtsklasse[df$Gewicht <= 5] <- "leicht"

```


```{r, purl=F}
knitr::kable(df)
```




### Aufgabe 4

Importiere den Datensatz [order_52252_data.txt](09_PrePro1/data/order_52252_data.txt). Es handelt sich dabei um die stündlich gemittelten Temperaturdaten an verschiedenen Standorten in der Schweiz im Zeitraum 2000 - 2005. Wir empfehlen `read_table()`^[@wickham2017, Kapitel 8 bzw. http://r4ds.had.co.nz/data-import.html)] anstelle von `read.table()`.

```{r, message = F}
# Lösung Aufgabe 4

wetter <- readr::read_table("09_PrePro1/data/order_52252_data.txt")
```




```{r, purl=F}
knitr::kable(head(wetter,10))
```


### Aufgabe 5

Schau dir die Rückmeldung von `read_table()`an. Sind die Daten korrekt interpretiert worden?


```{r}
# Lösung Aufgabe 5
# Die Spalte 'time' wurde als 'integer' interpretiert. Dabei handelt es
# sich offensichtlich um Zeitangaben.
```



### Aufgabe 6

Die Spalte `time` ist eine Datum/Zeitangabe im Format JJJJMMTTHH (siehe [meta.txt](09_PrePro1/data/meta.txt)). Damit R dies als Datum-/Zeitangabe erkennt, müssen wir die Spalte in einem R-Format (`POSIXct`) einlesen und dabei R mitteilen, wie sie aktuell formatiert ist. Lies die Spalte mit `as.POSIXct()` (oder `parse_datetime`) ein und spezifiziere sowohl `format` wie auch `tz`. 

Tipps: 

- Wenn keine Zeitzone festgelegt wird, trifft `as.POSIXct()` eine Annahme (basierend auf `Sys.timezone()`). In unserem Fall handelt es sich aber um Werte in UTC (siehe [meta.txt](09_PrePro1/data/meta.txt))
- `as.POSIXct`erwartet `character`: Wenn du eine Fehlermeldung hast die `'origin' must be supplied` (o.ä) heisst, hast du der Funktion vermutlich einen `Numeric` übergeben.

```{r}
# Lösung Aufgabe 6

# mit readr
parse_datetime(as.character(wetter$time), format = "%Y%m%d%H")


# mit as.POSIXct()
wetter$time <- as.POSIXct(as.character(wetter$time), format = "%Y%m%d%H",tz = "UTC")

```


```{r, purl=F}
knitr::kable(head(wetter,10))
```




### Aufgabe 7


Erstelle zwei neue Spalten mit Wochentag (Montag, Dienstag, etc) und Kalenderwoche. Verwende dazu die neu erstellte `POSIXct`-Spalte


```{r}

# Lösung Aufgabe 7

wetter$wochentag <- wday(wetter$time,label = T)
wetter$kw <- week(wetter$time)

```


```{r, purl=F}
knitr::kable(head(wetter,10))
```





### Aufgabe 8


Erstelle eine neue Spalte basierend auf die Temperaturwerte mit der Einteilung "kalt" (Unter Null Grad) und "warm" (über Null Grad)

```{r}

# Lösung Aufgabe 8

wetter$temp_kat[wetter$tre200h0>0] <- "warm"
wetter$temp_kat[wetter$tre200h0<=0] <- "kalt"
```


```{r, purl=F}
knitr::kable(head(wetter,10))
```


```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:09_PrePro1/Uebung_A.Rmd-->

## Uebung A Loesung

[R-Script als Download](09_PrePro1/RFiles/Uebung_A.R)

```{r code=readLines('09_PrePro1/RFiles/Uebung_A.R',encoding = "UTF-8"), echo=T, eval=F}
```


```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:09_PrePro1/Uebung_A_loesung.Rmd-->


```{r, include=FALSE, purl = F}

knitr::opts_chunk$set(echo = F, include = T, collapse=TRUE)
# knitr::opts_knit$set(root.dir = "09_PrePro1") 

```



## Uebung B 




```{r, message = F}
library(tidyverse)
```



Fahre mit dem Datensatz `wetter` aus Übung A fort. 
```{r, purl=F}
wetter <- read_table("09_PrePro1/data/order_52252_data.txt",
                  col_types = list(
                    col_factor(levels = NULL),    
                    col_datetime(format = "%Y%m%d%H"),
                    col_double()
                    )
                  )
```


### Aufgabe 1

Nutze `plot()` um die Temparaturkurve zu visualisieren. Verwende aber vorher `filter()` um dich auf eine Station (z.B. "`ABO`") zu beschränken (es handelt sich sonst um zuviele Datenpunkte).



```{r}
# Lösung Aufgabe 1

wetter_fil <- dplyr::filter(wetter, stn == "ABO")

plot(wetter_fil$time,wetter_fil$tre200h0, type = "l")

```


Nun schauen wir uns das plotten mit `ggplot2` an. Ein simpler Plot wie der in der vorherigen Aufgabe ist in `ggplot2` zugegebenermassen *etwas* komplizierter. `ggplot2` wird aber rasch einfacher, wenn die Grafiken komplexer werden. Wir empfehlen deshalb stark, `ggplot2` zu verwenden.

Schau dir ein paar online Tutorials zu `ggplot2` an (siehe ^[@wickham2017, Kapitel 1 bzw. [http://r4ds.had.co.nz/data-visualisation.html](http://r4ds.had.co.nz/data-visualisation.html) oder hier ein sehr schönes Video: [Learn R: An Introduction to ggplot2](https://youtu.be/YxKr2a-Y1WE?t=1m40s)]) 
und reproduziere den obigen Plot mit `ggplot2`


```{r, echo = TRUE}

p <- ggplot(wetter_fil, aes(time,tre200h0)) +
  geom_line()

p
```



### Aufgabe 2

Spiele mit Hilfe der erwähnten Tutorials mit dem Plot etwas rum. Versuche die x-/y-Achsen zu beschriften sowie einen Titel hinzu zu fügen.

```{r}
# Lösung Aufgabe 2
p <- p +
  labs(x = "Datum", y = "Temperatur", title = "Stündlich gemittelte Temperaturwerte")

p
```


### Aufgabe 3

Reduziere den x-Achsenausschnitt auf einen kleineren Zeitraum, beispielsweise einn beliebigen Monat. Verwende dazu `lims()` zusammen mit `as.POSIXct()` oder mache ein Subset von deinem Datensatz mit einer convenience-Variabel und `filter()`.

```{r}
# Lösung Aufgabe 3

limits <- as.POSIXct(c("2002-01-01 00:00:00","2002-02-01 00:00:00"),tz = "UTC")

p +
  lims(x = limits)
```



```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:09_PrePro1/Uebung_B.Rmd-->

## Uebung B Loesung

[R-Code als Download](09_PrePro1/Uebung_B.R)

```{r code=readLines('09_PrePro1/RFiles/Uebung_B.R',encoding = "UTF-8"), echo=T, eval=F}
```

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:09_PrePro1/Uebung_B_loesung.Rmd-->

# PrePro2 (13.10.2020)

Die Lerneinheit vermittelt zentralste Fertigkeiten zur Vorverarbeitung von strukturierten Daten in der umweltwissenschaftlichen Forschung: Datensätze verbinden (Joins) und umformen („reshape“, „split-apply-combine“). Im Anwendungskontext haben Daten selten von Anfang an diejenige Struktur, welche für die statistische Auswertung oder für die Informationsvisualisierung erforderlich wäre. In dieser Lerneinheit lernen die Studierenden die für diese oft zeitraubenden Preprocessing-Schritte notwendigen Konzepte und R-Werkzeuge kennen und kompetent anzuwenden.
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:10_PrePro2/Abstract.Rmd-->


```{r, include=F, purl=F}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,include = TRUE, collapse=TRUE)
```



## Demo: `tidyverse`

[Demoscript als Download](10_PrePro2/RFiles/Demo_Tidyverse.R)


Hier möchten wir euch mit einer Sammlung von Tools vertraut machen, die spezifisch für das Daten prozessieren in Data Science entwickelt wurden. Der Prozess und das Modell ist hier^[http://r4ds.had.co.nz/introduction.html#] schön beschrieben.
Die Sammlung von Tools wird unter dem Namen [tidyverse](https://www.tidyverse.org/) vertrieben, welches wir ja schon zu Beginn der ersten Übung installiert und geladen haben. Die Tools erleichtern den Umgang mit Daten ungeheuer und haben sich mittlerweile zu einem "must have" im Umgang mit Daten in R entwickelt. 

Wir können Euch nicht sämtliche Möglichkeiten von tidyverse zeigen. Wir fokussieren uns deshalb auf einzelne Komponenten^[`dplyr, ggplot2, tidyr, stringr, magrittr, lubridate`] und zeigen ein paar Funktionalitäten, die wir oft verwenden und Euch ggf. noch nicht bekannt sind. Wer sich vertieft mit dem Thema auseinandersetzen möchte, der sollte sich unbedingt das Buch @wickham2017 beschaffen. Eine umfangreiche, aber nicht ganz vollständige Version gibt es online^[http://r4ds.had.co.nz/], das vollständige eBook kann über die Bibliothek bezogen werden^[https://ebookcentral.proquest.com/lib/zhaw/detail.action?docID=4770093].



### Split-Apply-Combine

#### Packete laden

```{r,message=F}
library(tidyverse)
```

Mit `library(tidyverse)` werden nicht alle Packete geladen, die mit `install.packages(tidyverse)` intalliert wurden ([warum?](https://community.rstudio.com/t/which-packages-get-loaded/298)). Unter anderem muss `lubridate` noch separat geladen werden:

```{r, message=F}
library(lubridate) 
```



#### Daten Laden

Wir laden die Wetterdaten von der letzten Übung.

```{r}

wetter <- read_table("09_PrePro1/data/order_52252_data.txt",
                  col_types = list(
                    col_factor(levels = NULL),    
                    col_datetime(format = "%Y%m%d%H"),
                    col_double()
                    )
                  )

```


#### Kennwerte berechnen
Wir möchten den Mittelwert aller gemessenen Temperaturwerte berechnen. Dazu könnten wir folgenden Befehl verwenden:

```{r}
mean(wetter$tre200h0, na.rm = TRUE) 
```

Die Option `na.rm = T` bedeutet, dass NA Werte von der Berechnung ausgeschlossen werden sollen. 

Mit der selben Herangehensweise können diverse Werte berechnet werden (z.B. das Maximum (`max()`), Minimum (`min()`), Median (`median()`) u.v.m.). 

Diese Herangehensweise funktioniert nur dann gut, wenn wir die Kennwerte über *alle* Beobachtungen (Zeilen) für eine Variable (Spalte) berechnen wollen. Sobald wir die Beobachtungen gruppieren wollen, wird es schwierig. Zum Beispiel, wenn wir die durchschnittliche Temperatur *pro Jahr* berechnen wollen.


#### Convenience Variablen

Um diese Aufgabe zu lösen, muss zuerst das "Jahr" berechne werden (das Jahr ist die *convenience variabel*).   Hierfür brauchen wir die Funktion `year()` (von `lubridate`). 

Nun kann kann die **convenience Variable** "Jahr" erstellt werden. Ohne `dpylr` wird eine neue Spalte wird folgendermassen hinzugefügt. 
```{r}
wetter$year <- year(wetter$time)
```


Mit `dplyr` (siehe ^[@wickham2017, Kapitel 10 / http://r4ds.had.co.nz/transform.html]) sieht der gleiche Befehl folgendermassen aus:
```{r}
wetter <- mutate(wetter,year = year(time))
```

Der grosse Vorteil von `dplyr` ist an dieser Stelle noch nicht ersichtlich. Dieser wird aber später klar.


#### Kennwerte nach Gruppen berechnen

Jetzt kann man die `data.frame` mithilfe der Spalte "Jahr" filtern. 
```{r}
mean(wetter$tre200h0[wetter$year == 2000], na.rm = TRUE)
```

Dies müssen wir pro Jahr wiederholen, was natürlich sehr umständlich ist, v.a. wenn man eine Vielzahl an Gruppen hat (z.B. Kalenderwochen statt Jahre). Deshalb nutzen wir das package `dplyr`. Damit geht die Aufgabe (Temperaturmittel pro Jahr berechnen) folgendermassen:


```{r}
summarise(group_by(wetter,year),temp_mittel = mean(tre200h0, na.rm = TRUE))
```


#### Verketten vs. verschachteln

Auf Deutsch übersetzt heisst die obige Operation folgendermassen: 

1) nimm den Datensatz `wetter`
2) Bilde Gruppen pro Jahr  (`group_by(wetter,year)`) 
3) Berechne das Temperaturmittel (`mean(tre200h0)`)

Diese Übersetzung `R`-> Deutsch unterscheidet sich vor allem darin, dass die Operation auf Deutsch *verkettet* ausgesprochen wird (Operation 1->2->3) während der Computer *verschachtelt* liest 3(2(1)). Um `R` näher an die gesprochene Sprache zu bringen, kann man den `%>%`-Operator verwenden  (siehe ^[@wickham2017, Kapitel 14 / http://r4ds.had.co.nz/pipes.html]). 
```{r, eval = F}

summarise(group_by(wetter,year),temp_mittel = mean(tre200h0))

# wird zu:

wetter %>%                                #1) nimm den Datensatz "wetter"
  group_by(year) %>%                      #2) Bilde Gruppen pro Jahr
  summarise(temp_mittel = mean(tre200h0)) #3) berechne das Temperaturmittel 

```


Dieses Verketten mittels `%>%` macht den Code einiges schreib- und leserfreundlicher, und wir werden ihn in den nachfolgenden Übungen verwenden. Dabei handelt es sich um das package `magrittr`, welches mit `tidyverse` mitgeliefert wird. 

Zu `dplyr` und `magrittr`gibt es etliche Tutorials online (siehe^[@wickham2017, Kapitel 10 / http://r4ds.had.co.nz/transform.html, oder [Hands-on dplyr tutorial..](https://youtu.be/jWjqLW-u3hc)]), deshalb werden wir diese Tools nicht in allen Details erläutern. Nur noch folgenden wichtigen Unterschied zu zwei wichtigen Funktionen in `dpylr`: `mutate()` und `summarise()`.

- `summarise()` fasst einen Datensatz zusammen. Dabei reduziert sich die Anzahl Beobachtungen (Zeilen) auf die Anzahl Gruppen (z.B. eine zusammengefasste Beobachtung (Zeile) pro Jahr). Zudem reduziert sich die Anzahl Variablen (Spalten) auf diejenigen, die in der "summarise" Funktion spezifiziert wurde (z.B. `temp_mittel`).
- mit `mutate` wird ein `data.frame` vom Umfang her belassen, es werden lediglich *zusätzliche* Variablen (Spalten) hinzugefügt (siehe Beispiel unten).

```{r, eval=T}
# Maximal und minimal Temperatur pro Kalenderwoche
wetter %>%                              #1) nimm den Datensatz "wetter"
  filter(stn == "ABO") %>%              #2) filter auf Station namnes "ABO"
  mutate(kw = week(time)) %>%       #3) erstelle eine neue Spalte "kw"
  group_by(kw) %>%                      #4) Nutze die neue Spalte um Guppen zu bilden
  summarise(
    temp_max = max(tre200h0, na.rm = TRUE),#5) Berechne das Maximum 
    temp_min = min(tre200h0, na.rm = TRUE) #6) Berechne das Minimum
    )   
```


#### Resultate plotten

Mit diesen Tools können wir nun auch eine neue Grafik plotten, ähnlich wie in der Übung 1. Dafür müssen wir die ganzen Operationen aber zuerst in einer Variabel speichern (bis jetzt hat R zwar alles schön berechnet, aber uns nur auf die Konsole ausgegeben).

```{r, warning = F}

wetter_sry <- wetter %>%                              
  mutate(
    kw = week(time)
    ) %>%
  filter(stn == "ABO") %>%
  group_by(kw) %>%                      
  summarise(
    temp_max = max(tre200h0),               
    temp_min = min(tre200h0),
    temp_mean = mean(tre200h0)
    )  
```

Dieses Mal plotten wir nur mit `ggplot2` (siehe ^[@wickham2017, Kapitel 1 / http://r4ds.had.co.nz/data-visualisation.html oder hier ein sehr schönes Video: [Learn R: An Introduction to ggplot2](https://youtu.be/YxKr2a-Y1WE?t=1m40s)]) 
 
```{r}

ggplot() +
  geom_line(data = wetter_sry, aes(kw,temp_max), colour = "yellow") +
  geom_line(data = wetter_sry, aes(kw,temp_mean), colour = "pink") +
  geom_line(data = wetter_sry, aes(kw,temp_min), colour = "black") +
  labs(y = "temp")

```


Das sieht schon mal gut aus. Nur, wir mussten pro Linie einen eigene Zeile schreiben (`geom_line()`) und dieser eine Farbe zuweisen. Bei drei Werten ist das ja ok, aber wie sieht es denn aus wenn es Hunderte sind? Da hat ggplot natürlich eine Lösung, dafür müssen aber alle Werte in *einer* Spalte daher kommen. Das ist ein häufiges Problem: Wir haben eine *breite* Tabelle (viele Spalten), bräuchten aber eine *lange* Tabelle (viele Zeilen).


### Reshaping data

#### Breit -> lang

Da kommt `tidyverse` wieder ins Spiel. Die Umformung von Tabellen *breit*->*lang* erfolgt mittels `tidyr`(siehe ^[https://r4ds.had.co.nz/tidy-data.html#pivoting]). Auch dieses package funktioniert wunderbar mit piping (`%>%`). 

```{r}

wetter_sry %>%
  pivot_longer(c(temp_max,temp_min,temp_mean))
```

Im Befehl `pivot_longer()` müssen wir festlegen, welche Spalten zusammengefasst werden sollen (hier: `temp_max`,`temp_min`,`temp_mean`). Alternativ (und in diesem Fall auch einfacher), können wir angeben, welche Spalten wir *nicht* zusammenfassen wollen:

```{r}
wetter_sry %>%
  pivot_longer(-kw)
```


Wenn wir die Namen neuen Spalten festlegen wollen (anstelle von `name` und `value`) erreichen wir dies mit `names_to` bzw. `values_to`:

```{r}
wetter_sry_long <- wetter_sry %>%
  pivot_longer(-kw, names_to = "Messtyp", values_to = "Messwert")
```

Die ersten 6 Zeilen von `wetter_sry_long`:
```{r, echo = F, purl=F}
kable(head(arrange(wetter_sry_long,kw),6))
```


Die ersten 6 Zeilen von `wetter_sry`:
```{r, echo = F, purl = F}
kable(head(wetter_sry,6))
```


Beachte: `wetter_sry_long` umfasst 159 Beobachtungen (Zeilen), das sind 3 mal soviel wie `wetter_sry`, da wir ja drei Spalten zusammengefasst haben.
```{r}
nrow(wetter_sry)
nrow(wetter_sry_long)
```


```{r}
ggplot(wetter_sry_long, aes(kw,Messwert, colour = Messtyp)) +
  geom_line()
```

Beachtet, dass wir gegenüber dem letzten Plot `colour` nun *innerhalb* von `aes()` festlegen und nicht mit einem expliziten Farbwert, sondern mit dem Verweis auf die Spalte `key`.


#### Lang -> breit

Das Gegenstück zu `pivot_longer` ist `pivot_wider`. Mit dieser Funktion können wir eine *lange* Tabelle in eine *breite* überführen. Dazu müssen wir in `names_from` angeben, aus welcher Spalte die neuen Spaltennamen erstellt werden sollen (`names_from`) und aus welcher Spalte die Werte entstammen sollen (`values_from`):

```{r}
wetter_sry_long %>%
  pivot_wider(names_from = Messtyp, values_from = Messwert)
```


### Quellen

```{r code=readLines('00_Admin/get_chapter_references.R'), echo=F, eval=T,purl = F, results="asis"}
```

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:10_PrePro2/Demo_Tidyverse.Rmd-->

```{r include=FALSE, purl=F}
knitr::opts_chunk$set(echo = TRUE, include = F, collapse=TRUE)
# knitr::opts_knit$set(root.dir = "10_PrePro2") 

select <- dplyr::select


```

## Uebung A


```{r,message=F}
library(tidyverse)
library(lubridate)
library(stringr)
```


### Aufgabe 1

Lade die Wetterdaten aus der letzten Übung.

```{r}
# Lösung Aufgabe 1

wetter <- read_table("09_PrePro1/data/order_52252_data.txt",
                  col_types = list(
                    col_factor(levels = NULL),    
                    col_datetime(format = "%Y%m%d%H"),
                    col_double()
                    )
                  )
```


### Aufgabe 2

Bereinige den Datensatz. Entferne z.B. alle Zeilen, bei dem der Stationsnahme oder Temperaturwerte fehlen 

```{r}
# Lösung Aufgabe 2

wetter <- wetter %>%
  filter(!is.na(stn)) %>%
  filter(!is.na(tre200h0))

```



### Aufgabe 3

Überführe die **lange** Tabelle über in eine breite. Dabei sollte jede Station eine eigene Spalte enthalten (`names_from`), gefüllt mit den Temperaturwerten (`values_from`).  Speichere diese Tabelle in einer neuen Variabel.

```{r}

# Lösung Aufgabe 3

wetter_wide <- pivot_wider(wetter, names_from = stn, values_from = tre200h0)

```

```{r, echo = TRUE}
# Hier ein Ausschnitt der df, wie es aussehen sollte:
wetter_wide[1:5, 1:7]
```


### Aufgabe 4



Importiere die Datei [order_52252_legend.csv](09_PrePro1/data/order_52252_legend.csv) (z.B. mit `read_delim`).

Hinweis: Wenn Umlaute und Sonderzeichen nicht korrekt dargestellt werden (z.B. in Gen*è*ve), hat das vermutlich mit der [Zeichencodierung](https://de.wikipedia.org/wiki/Zeichenkodierung) zu tun. Das File ist aktuell in 'ANSI' Codiert, welche für gewisse Betriebssysteme / R-Versionen ein Problem darstellt. Um das Problem zu umgehen muss man das File mit einem Editor öffnen (Windows 'Editor' oder 'Notepad++', Mac: 'TextEdit') und mit einer neuen Codierung (z.B 'UTF-8') abspeichern. Danach kann die Codierung spezifitiert werden (bei `read_delim(): mit `locale = locale(encoding = "UTF-8")`)

```{r}

# Lösung Aufgabe 4

wetter_legende <- read_delim("09_PrePro1/data/order_52252_legend.csv",
                             delim = ";", 
                             locale = locale(encoding = "UTF-8"))

```



### Aufgabe 5


Die x-/y-Koordinaten sind aktuell in einer Spalte erfasst. Um mit den Koordinaten sinnvoll arbeiten zu können, brauchen wir die Koordinaten getrennt. Trenne die `x` und `y` Koordinaten aus der Spalte `Koordinaten` (Tipp: nutze dafür `tidyr::separate()`).

```{r}

# Lösung Aufgabe 5

# Variante mit str_split_fixed()
koordinaten <- str_split_fixed(wetter_legende$Koordinaten, "/", 2)

colnames(koordinaten) <- c("x","y")

cbind(wetter_legende,koordinaten)



# Variante mit tidyr::separate
# ich lösche die Spalten wieder, damit ich die tidyr lösung zeigen kann

wetter_legende <- wetter_legende %>%
  separate(Koordinaten,c("x","y"),"/")

```


### Aufgabe 6

Nun wollen wir den Datensatz `wetter`mit den Informationen aus `wetter_legende`anreichern. Uns interessiert aber nur das Stationskürzel, der Name, die x/y Koordinaten sowie die Meereshöhe. Lösche die nicht benötigten Spalten (oder selektiere die benötigten Spalten).

Tipp: Nutze `select()` von `dplyr`

```{r, message=F}

# Lösung Aufgabe 6

wetter_legende <- dplyr::select(wetter_legende, stn, Name, x,y,Meereshoehe)
```


### Aufgabe 7

Nun ist der Datensatz `wetter_legende`genügend vorbereitet. Jetzt kann er mit dem Datensatz `wetter` verbunden werden. Überlege dir, welcher Join dafür sinnvoll ist und mit welchem Attribut wir "joinen" können.

Nutze die Join-Möglichkeiten von `dplyr` (Hilfe via `?dplyr::join`)  um die Datensätze `wetter` und `wetter_legende`zu verbinden.

```{r}

# Lösung Aufgabe 7

wetter <- left_join(wetter,wetter_legende,by = "stn")

# Jointyp: Left-Join auf 'wetter', da uns nur die Stationen im Datensatz 'wetter' interessieren.
# Attribut: "stn"
```

### Aufgabe 8

Berechne die Durchschnittstemperatur pro Station. Nutze dabei `dplyr::summarise()` und wenn möglich `%>%`. Speichere das Resultat in einer neuen Variabel.


```{r,warning=F}

# Lösung Aufgabe 8

wetter_sry <- wetter %>%
  group_by(stn) %>%
  summarise(temp_mean = mean(tre200h0))
```

### Aufgabe 9

Nun wollen wir das Resultat aus Aufgabe 7 nutzen, um die Durchschnittstemperatur der Meereshöhe gegenüber zu stellen. Dummerweise ging das Attribut `Meereshoehe` bei der `summarise()` Operation verloren (da bei `summarise()` alle Spalten weg fallen, die **nicht** in `group_by()` definiert wurden). Um die Spalte `Meereshoehe` beizubehalten, muss sie also unter `group_by()` aufgelistet werden. 

Wiederhole Übung 7 und siehe zu, dass die Meereshöhe beibehalten wird. Stelle danach in einem Scatterplot (wenn möglich mit `ggplot()`) die Meereshöhe der Durchschnittstemperatur gegenüber.

```{r}
# Lösung Aufgabe 9

wetter_sry <- wetter %>%
  group_by(stn,Meereshoehe) %>%
  summarise(temp_mean = mean(tre200h0))

# Achtung: wenn mehrere Argumente in group_by() definiert werden führt das 
# üblicherweise zu Untergruppen. In unserem Fall hat jede Station nur EINE 
# Meereshöhe, deshalb wird die Zahl der Gruppen nicht erhöht.
```


```{r}

ggplot(wetter_sry, aes(temp_mean,Meereshoehe)) +
  geom_point()
```


```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:10_PrePro2/Uebung_A.Rmd-->

## Uebung A: Loesung

[R-Code als Download](10_PrePro2/RFiles/Uebung_A.R)


```{r code=readLines('10_PrePro2/RFiles/Uebung_A.R',encoding = "UTF-8"), echo=T, eval=F, include = T}
```

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:10_PrePro2/Uebung_A_loesung.Rmd-->

```{r, include=FALSE, purl = F}
knitr::opts_chunk$set(collapse=TRUE)
# knitr::opts_knit$set(root.dir = "10_PrePro2")

```

## Uebung B


```{r,message=F}
library(tidyverse)
library(lubridate)
library(stringr)
```



### Aufgabe 1

Gegeben sind die Daten von drei Sensoren ([sensor1.csv](10_PrePro2/data/sensor1.csv), [sensor2.csv](10_PrePro2/data/sensor2.csv), [sensor3.csv](10_PrePro2/data/sensor3.csv)). Lade die Datensätze runter und lese sie ein.



```{r, message=F}
# Lösung Aufgabe 1

sensor1 <- read_delim("10_PrePro2/data/sensor1.csv",";")
sensor2 <- read_delim("10_PrePro2/data/sensor2.csv",";")
sensor3 <- read_delim("10_PrePro2/data/sensor3.csv",";")

```

### Aufgabe 2

Erstelle aus den 3 Dataframes eine einzige Dataframe, die aussieht wie unten dargestellt. Dafür musst du: 

1. jedem Dataframe eine neue Spalte "Sensor" hinzufügen wo der jeweilige Sensor vermerkt ist
2. die drei Dataframes zu **einer** mittels `rbind()` zusammenführen
3. die Spalte `Datetime` in ein `POSIXct`-Format konvertiren (das ursprüngliche Format lautet:`DDMMYYYY_HHMM`)
4. die Tabelle von long zu wide mittels `pivot_wider` überführen




```{r}

# Lösung Aufgabe 2

sensor1$sensor <- "sensor1"
sensor2$sensor <- "sensor2"
sensor3$sensor <- "sensor3"

sensor_all <- rbind(sensor1,sensor2,sensor3)

sensor_all <- sensor_all %>%
  mutate(
    Datetime = as.POSIXct(Datetime,format = "%d%m%Y_%H%M")
  ) %>%
  pivot_wider(names_from = sensor, values_from = Temp)

```


```{r,  echo = F, eval = T, purl=F}
knitr::kable(sensor_all)
```









### Aufgabe 3

Importiere die Datei [sensor_1_fail.csv](10_PrePro2/data/sensor_fail.csv) in `R`.


```{r, message = F}

# Lösung Aufgabe 3

sensor_fail <- read_delim("10_PrePro2/data/sensor_fail.csv", delim = ";")

```


```{r,  echo = F, eval = T, purl=F}
knitr::kable(sensor_fail)
```


`sensor_fail.csv` hat eine Variabel `SensorStatus`: `1` bedeutet der Sensor misst, `0` bedeutet der Sensor miss nicht. Fälschlicherweise wurde auch dann der Messwert `Temp = 0` erfasst, wenn `Sensorstatus = 0`. Richtig wäre hier `NA` (not available). Korrigiere den Datensatz entsprechend.


```{r}

# Lösungsweg 1
sensor_fail$Datetime <- as.POSIXct(sensor_fail$Datetime,format = "%d%m%Y_%H%M")

sensor_fail$`Hum_%`[sensor_fail$SensorStatus == 0] <- NA
sensor_fail$Temp[sensor_fail$SensorStatus == 0] <- NA
```


### Aufgabe 4


Warum spielt das es eine Rolle, ob `0` oder `NA` erfasst wird? Berechne die Mittlere der Temperatur / Feuchtigkeit nach der Korrektur. 

```{r}

# Lösung Aufgabe 4

# Mittelwerte der korrigierten Sensordaten: mit na.rm = TRUE werden NA-Werte aus der Berechnung entfernt. 
# Ansonsten würden sie als 0 in die Berechnung einfliessen und diese verfälschen.
mean(sensor_fail$Temp, na.rm = TRUE)
mean(sensor_fail$`Hum_%`, na.rm = TRUE)

```





```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:10_PrePro2/Uebung_B.Rmd-->

## Uebung B: Loesung

[R-Code als Download](10_PrePro2/RFiles/Uebung_B.R)


```{r code=readLines('10_PrePro2/RFiles/Uebung_B.R',encoding = "UTF-8"), echo=T, eval=F, include = T}
```

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:10_PrePro2/Uebung_B_loesung.Rmd-->

# InfoVis1 (19.10.2020)

Die konventionelle schliessende Statistik arbeitet in der Regel konfirmatorisch, sprich aus der bestehenden Theorie heraus werden Hypothesen formuliert, welche sodann durch Experimente geprüft und akzeptiert oder verworfen werden. Die Explorative Datenanalyse (EDA) nimmt dazu eine antagonistische Analyseperspektive ein und will in den Daten zunächst Zusammenhänge aufdecken, welche dann wiederum zur Formulierung von prüfbaren Hypothesen führen kann. Die Einheit stellt dazu den klassischen 5-stufigen EDA-Prozess nach Tukey (1980!) vor. Abschliessend wird dann noch die Brücke geschlagen zur modernen Umsetzung der EDA in Form von Visual Analytics.
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:11_InfoVis1/Abstract.Rmd-->


```{r, include=F, purl=F}

knitr::opts_chunk$set(echo = T,include = T,message = F, collapse=TRUE) # 
# knitr::opts_knit$set(root.dir = "11_InfoVis1") 

```

```{r, message = F}
library(tidyverse)
library(lubridate)

```

## Demo: `ggplot2`

[Demoscript als Download](11_InfoVis1/RFiles/Demo_ggplot.R)

Als erstes laden wir den Wetterdatensatz von der Übung Prepro1 ein.

```{r}
wetter <- read_table("09_PrePro1/data/order_52252_data.txt",
                  col_types = list(
                    col_factor(levels = NULL),    
                    col_datetime(format = "%Y%m%d%H"),
                    col_double()
                    )
                  )
```


```{r, echo = F, eval = T, purl=F}

knitr::kable(head(wetter))

```

Der Datensatz hat `r nrow(wetter)` Zeilen. Bevor wir mit plotten beginnen, müssen wir den Datensatz etwas filtern da die Plots ansonsten zu schwerfällig werden. Wir filtern deshalb auf Januar 2000.

```{r}
wetter_fil <- wetter %>%
  mutate(
    year = year(time),
    month = month(time)
    ) %>%
  filter(year == 2000 & month == 1)
```

Um in "base-R" einen Scatterplot zu erstellen wo Datum der Temperatur gegenübersteht, gehen wir wie folgt vor:

```{r}
plot(wetter_fil$time, wetter_fil$tre200h0)
```

In `ggplot` sieht das etwas anders und auf den ersten Blick etwas komplizierter aus: Ein ggplot wird durch den Befehl `ggplot()` initiiert. Hier wird einerseits der Datensatz festgelegt, auf dem der Plot beruht (`data = `), sowie die Variablen innerhalb des Datensatzes, die Einfluss auf den Plot ausüben (`mapping = aes()`). 

```{r}
# Datensatz: "wetter_fil" | Beeinflussende Variabeln: "time" und "tre200h0"
ggplot(data = wetter_fil, mapping = aes(time,tre200h0))             
```

Weiter braucht es *mindestens* ein "Layer" der beschreibt, wie die Daten dargestellt werden sollen (z.B. `geom_point()`).
Anders als bei "Piping" (`%>%`) wird ein Layer mit `+` hinzugefügt.

```{r}
ggplot(data = wetter_fil, mapping = aes(time,tre200h0)) +
  # Layer: "geom_point" entspricht Punkten in einem Scatterplot 
  geom_point()                                    
```


Da ggplot die Eingaben in der Reihenfolge `data = ` und dann `mapping = `erwartet, können wir diese Spezifizierungen auch weglassen.

```{r, eval=F}

ggplot(wetter_fil, aes(time,tre200h0)) +
  geom_point()

```

Nun wollen wir die unterschiedlichen Stationen unterschiedlich einfärben. Da wir Variablen definieren wollen, welche Einfluss auf die Grafik haben sollen, gehört diese Information in `aes()`.

```{r}
ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) +
  geom_point()
```

Wir können noch einen Layer mit Linien hinzufügen:

```{r}
ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) +
  geom_point() +
  geom_line()

```

Weiter können wir die Achsen beschriften und einen Titel hinzufügen. Zudem lasse ich die Punkte (`geom_point()`) nun weg, da mir diese nicht gefallen.

```{r}
ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) +
  geom_line() +
  labs(x = "Woche",
       y = "Temperatur in Grad C°", 
       title = "Temperaturdaten Schweiz",
       subtitle = "Januar 2000")
```

Man kann auch Einfluss auf die x-/y-Achsen nehmen. Dabei muss man zuerst festlegen, was für ein Achsentyp der Plot hat (vorher hat `ggplot` eine Annahme auf der Basis der Daten getroffen). 

Bei unserer y-Achse handelt es sich um numerische Daten, `ggplot` nennt diese: `scale_y_continuous()`. Unter [ggplot2.tidyverse.org](http://ggplot2.tidyverse.org/reference/#section-scales) findet man noch andere x/y-Achsentypen (`scale_x_irgenwas` bzw. `scale_y_irgendwas`).

```{r}
ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) +
  geom_line() +
  labs(x = "Woche",
       y = "Temperatur in Grad C°", 
       title = "Temperaturdaten Schweiz",
       subtitle = "Januar 2000") +    
  scale_y_continuous(limits = c(-30,30))    # y-Achsenabschnitt bestimmen

```


Das gleiche Spiel kann man für die y-Achse betreiben. Bei unserer y-Achse handelt es sich ja um unsere `POSIXct` Daten. `ggplot` nennt diese: `scale_x_datetime()`. 
```{r}
ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) +
  geom_line() +
  labs(x = "Woche",
       y = "Temperatur in Grad C°", 
       title = "Temperaturdaten Schweiz",
       subtitle = "Januar 2000") +    
  scale_y_continuous(limits = c(-30,30)) +
  scale_x_datetime(date_breaks = "1 week", 
                   date_minor_breaks = "1 day", 
                   date_labels = "KW%W")

```


Mit `theme` verändert man das allgmeine Layout der Plots. Beispielsweise kann man mit `theme_classic()` `ggplot`-Grafiken etwas weniger "Poppig" erscheinen lassen: so sind sie besser für Bachelor- / Masterarbeiten sowie Publikationen geeignet. `theme_classic()` kann man indiviudell pro Plot anwenden, oder für die aktuelle Session global setzen (s.u.)

Individuell pro Plot:
```{r}
ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) +
  geom_line() +
  labs(x = "Woche",
       y = "Temperatur in Grad C°", 
       title = "Temperaturdaten Schweiz",
       subtitle = "Januar 2000") +    
  scale_y_continuous(limits = c(-30,30)) +
  scale_x_datetime(date_breaks = "1 week", 
                   date_minor_breaks = "1 day", 
                   date_labels = "KW%W") +
  theme_classic()
```

Global (für alle nachfolgenden Plots der aktuellen Session):

```{r}
theme_set(theme_classic())
```


Sehr praktisch sind auch die Funktionen für "Small multiples". Dies erreicht man mit `facet_wrap()` (oder `facet_grid()`, mehr dazu später). Man muss mit einem Tilde-Symbol "`~`" nur festlegen, welche *Variable* für das Aufteilen des Plots in kleinere Subplots verantwortlich sein soll. 

```{r, fig.width=8,fig.height=10}
ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) +
  geom_line() +
  labs(x = "Woche",
       y = "Temperatur in Grad C°", 
       title = "Temperaturdaten Schweiz",
       subtitle = "Januar 2000") +    
  scale_y_continuous(limits = c(-30,30)) +
  scale_x_datetime(date_breaks = "2 weeks", 
                   date_minor_breaks = "1 day", 
                   date_labels = "KW%W") +
  facet_wrap(~stn)

```


Auch `facet_wrap` kann man auf seine Bedürfnisse anpassen. Da wir 24 Stationen haben möchte ich lieber 3 pro Zeile, damit es schön aufgeht. Dies erreiche ich mit `ncol = 3`.

Zudem brauchen wir die Legende nicht mehr, da der Stationsnamen über jedem Facet steht. Ich setze deshalb `theme(legend.position="none")` 

```{r, fig.width=8,fig.height=10}
ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) +
  geom_line() +
  labs(x = "Woche",
       y = "Temperatur in Grad C°", 
       title = "Temperaturdaten Schweiz",
       subtitle = "Januar 2000") +  
  scale_y_continuous(limits = c(-30,30)) +
  scale_x_datetime(date_breaks = "1 week", date_minor_breaks = "1 day", date_labels = "KW%W") +
  facet_wrap(~stn,ncol = 3) +
  theme(legend.position="none")
```


Genau wie `data.frames` und andere Objekte, kann man einen ganzen Plot auch in einer Variabel speichern. Dies kann nützlich sein um einen Plot zu exportieren (als png, jpg usw.) oder sukzessive erweitern wie in diesem Beispiel.

```{r, message = F}
p <- ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) +
  geom_line() +
  labs(x = "Woche",
       y = "Temperatur in Grad C°", 
       title = "Temperaturdaten Schweiz",
       subtitle = "Januar 2000") +
  scale_y_continuous(limits = c(-30,30)) +
  scale_x_datetime(date_breaks = "1 week", date_minor_breaks = "1 day", date_labels = "KW%W") +
  facet_wrap(~stn,ncol = 3)
  # ich habe an dieser Stelle theme(legend.position="none") entfernt



```

Folgendermassen kann ich den Plot als png-File abspeichern (ohne Angabe von "plot = " wird einfach der letzte Plot gespeichert)

```{r, eval = F}
ggsave(filename = "11_InfoVis1/plot.png",plot = p)
```

.. und so kann ich einen bestehenden Plot (in einer Variabel) mit einem Layer / einer Option erweitern

```{r, eval = F}
p +
  theme(legend.position="none")

```


Wie üblich wurde diese Änderung nicht gespeichert, sondern nur das Resultat davon ausgeben. Wenn die Änderung in meinem Plot (in der Variabel) abspeichern will, muss ich die Variabel überschreiben:

```{r}
p <- p +
  theme(legend.position="none")
```


Mit `geom_smooth()` kann `ggplot` eine Trendlinie auf der Baiss von Punktdaten berechnen. Die zugrunde liegende statistische Methode kann selbst gewählt werden. Wenn nichts angegeben wird verwendet `ggplot` bei weniger als 1'000 Messungen, die Methode `loess` (local smooths).


```{r, fig.width=8,fig.height=10}
p <- p +
  geom_smooth(colour = "black")

p
```


```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:11_InfoVis1/Demo_ggplot.Rmd-->

```{r, include=F, purl=F}

knitr::opts_chunk$set(echo = T,include = T,message = F, collapse=TRUE)
# knitr::opts_knit$set(root.dir = "11_InfoVis1") 

```




## EDA Beispiel Vorlesung

[Demoscript als Download](11_InfoVis1/RFiles/Demo_EDA.R)


```{r,message=F}


library(tidyverse)
library(scales)

# create some data about age and height of people
people <- data.frame(
  ID = c(1:30),
  
  age = c(5.0, 7.0, 6.5 ,9.0, 8.0, 5.0, 8.6, 7.5, 9.0, 6.0,
          63.5 ,65.7, 57.6, 98.6, 76.5, 78.0, 93.4, 77.5, 256.6, 512.3,
          15.5, 18.6, 18.5, 22.8, 28.5, 39.5, 55.9, 50.3, 31.9, 41.3),
  
  height = c(0.85, 0.93, 1.1, 1.25, 1.33, 1.17, 1.32, 0.82, 0.89, 1.13,
             1.62, 1.87, 1.67, 1.76, 1.56, 1.71, 1.65, 1.55, 1.87, 1.69,
             1.49, 1.68, 1.41, 1.55, 1.84, 1.69, 0.85, 1.65, 1.94, 1.80),
  
  weight = c(45.5, 54.3, 76.5, 60.4, 43.4, 36.4, 50.3, 27.8, 34.7, 47.6,
             84.3, 90.4, 76.5, 55.6, 54.3, 83.2, 80.7, 55.6, 87.6, 69.5,
             48.0, 55.6, 47.6, 60.5, 54.3, 59.5, 34.5, 55.4, 100.4, 110.3)
)



# build a scatterplot for a first inspection
ggplot(people, aes(x=age, y=height)) + geom_point() + scale_y_continuous(limits=c(0.75, 2.0))

# Go to help page: http://docs.ggplot2.org/current/ -> Search for icon of fit-line
# http://docs.ggplot2.org/current/geom_smooth.html

# build a scatterplot for a first inspection, with regression line
ggplot(people, aes(x=age, y=height)) + 
  geom_point() + 
  scale_y_continuous(limits=c(0, 2.0)) +
  geom_smooth(method="loess", fill='lightblue', size=0.5, alpha=0.5)

?stem

# stem and leaf plot
stem(people$height)
stem(people$height, scale=2)

# explore the two variables with box-whiskerplots
summary(people$age)
boxplot(people$age)


summary(people$height)
boxplot(people$height)

# boxplot(people$height) why twice the same


# explore data with a histgram
ggplot(people, aes(x=age)) + 
  geom_histogram(stat="bin", fill='green', binwidth=20)  

density(x = people$height)


# re-expression: use log or sqrt axes
#
# Find here guideline about scaling axes 
# http://www.cookbook-r.com/Graphs/Axes_(ggplot2)/
# http://docs.ggplot2.org/0.9.3.1/scale_continuous.html


# logarithmic axis: respond to skewness in the data, e.g. log10 
ggplot(people, aes(x=age, y=height)) + 
  geom_point() + 
  scale_y_continuous(limits=c(0, 2.0)) +
  geom_smooth(method="lm", fill='lightblue', size=0.5, alpha=0.5) +
  scale_x_log10()

# logarithmic axis: show multiplicative factors, e.g. log2
ggplot(people, aes(x=age, y=height)) + 
  geom_point() + 
  scale_y_continuous(limits=c(0, 2.0)) +
  geom_smooth(method="lm", fill='lightblue', size=0.5, alpha=0.5) +
  scale_x_continuous(trans = log2_trans(),
                   breaks = trans_breaks("log2", function(x) 2^x),
                   labels = trans_format("log2", math_format(2^.x)))


# outliers: Remove very small and very old people
peopleTemp <- filter(people, ID != 27) # Diese Person war zu klein.
peopleClean <- filter(peopleTemp, age < 100) # Fehler in der Erhebung des Alters

# re-explore cleaned data with a histogram
ggplot(peopleClean, aes(x=age)) + 
  geom_histogram(stat="bin", fill='#6baed6', binwidth=10)

ggplot(peopleClean, aes(x=age, y=height)) + 
  geom_point() + 
  scale_y_continuous(limits=c(0, 2.0)) +
  geom_smooth(method="lm", fill='lightblue', size=0.5, alpha=0.5)

# with custom binwidth
ggplot(peopleClean, aes(x=age)) + 
  geom_histogram(stat="bin", fill='#6baed6', binwidth=10) + 
  theme_bw() # specifying the theme



# quadratic axis
ggplot(peopleClean, aes(x=age, y=height)) + 
  geom_point() + scale_y_continuous(limits=c(0, 2.0)) +
  geom_smooth(method="lm", fill='lightblue', size=0.5, alpha=0.5) + scale_x_sqrt()




# filter "teenies": No trend
kids <- filter(peopleClean, age < 15)

ggplot(kids, aes(x=age, y=height)) + 
  geom_point() + 
  scale_y_continuous(limits=c(0, 2.0)) +
  geom_smooth(method="lm", fill='lightblue', size=0.5, alpha=0.5)

# filter "teenies": No trend
oldies <- filter(peopleClean, age > 55)

ggplot(oldies, aes(x=age, y=height)) + 
  geom_point() + 
  scale_y_continuous(limits=c(0, 2.0)) +
  geom_smooth(method="lm", fill='lightblue', size=0.5, alpha=0.5)


# Onwards towards multidimensional data

# Finally, make a scatterplot matrix
pairs(peopleClean[,2:4], panel=panel.smooth)

pairs(peopleClean[,2:4], panel=panel.smooth)

# Or as a bubble chart
peopleClean$radius <- sqrt( peopleClean$weight/ pi )
symbols(peopleClean$age, peopleClean$height, circles=peopleClean$radius)

symbols(peopleClean$age, peopleClean$height, circles=peopleClean$radius)

```

### Quellen

```{r code=readLines('00_Admin/get_chapter_references.R'), echo=F, eval=T,purl = F, results="asis"}
```
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:11_InfoVis1/Demo_EDA.Rmd-->

```{r, include=F, purl=F}
library(knitr)

knitr::opts_chunk$set(echo = FALSE,include = TRUE,message = FALSE, collapse=TRUE) 


```

## Uebung

In dieser Übung geht es darum, die Grafiken aus dem Blog-post von Marko Kovic ([blog.tagesanzeiger.ch](https://blog.tagesanzeiger.ch/datenblog/index.php/668/je-weniger-auslaender-desto-mehr-ja-stimmen-wirklich)) zu rekonstruieren. Schau dir die Grafiken in dem Blogpost durch. Freundlicherweise hat Herr Kovic meist die `ggplot2` Standardeinstellungen benutzt, was die Rekonstruktion relativ einfach macht. 

Die Links im Text verweisen auf die Originalgrafik, die eingebetteten Plots sind meine eigenen Rekonstruktionen. Importiere als erstes den Datensatz [initiative_masseneinwanderung_kanton.csv](11_InfoVis1/data/initiative_masseneinwanderung_kanton.csv) (auf der Blog-Seite erhältlich).


```{r, message=F}

library(tidyverse)
library(ggplot2)
library(stringr)
library(janitor) # um die Spaltennamen zu säubern

```


```{r, eval=F}
# Es kann sein, dass man die Codierung des Files spezifizieren muss. Mit `readr::read_delim()` 
# läuft dies mit der Option locale = locale(encoding = "UTF-8") wobei anstelle von UTF-8 die 
# entsprechende Codierung angegeben wird. 
# Tipp: Excel speichert CSV oft in ANSI, welches für den Import in R nicht sonderlich geeignet 
# ist. Falls Probleme auftreten muss das File mittels einer geeigneter Software (Widows: "Editor" 
# oder "Notepad++", Mac: "TextEdit")  und mit einer neuen Codierung (z.B. `UTF-8`) abgespeichert 
# werden.
```

```{r, echo = TRUE}
kanton <- read_delim("11_InfoVis1/data/initiative_masseneinwanderung_kanton.csv",",",locale = locale(encoding = "UTF-8")) %>%
  janitor::clean_names() # säubert die Spaltennamen
```



### Aufgabe 1

Rekonstruiere [Grafik 1](https://blog.tagesanzeiger.ch/datenblog/wp-content/uploads/sites/32/2014/03/Kantone-2.png) von Kovic. Erstelle dazu einen Scatterplot wo der Ausländeranteil der Kantone dem Ja-Anteil gegenüber gestellt wird. Speichere den Plot einer Variabel `plot1`.

- nutze `ggplot(kanton, aes(auslanderanteil, ja_anteil))` um den ggplot zu initiieren.Füge danach ein einen Punkte Layer hinzu (`geom_point()`)
- nutze `coord_fixed()` um die beiden Achsen in ein fixes Verhältnis zu setzen (1:1).
- Optional: 
  - setze die Achsen Start- und Endwerte mittels `lims()` oder `scale_y_continuous`bzw. `scale_x_continuous`.
  - Setze analog Kovic die `breaks` (`0.0`, `0.1`...`0.7`) manuell

Rekonstruktion:

```{r}

# Lösung zu Aufgabe 1

plot1 <- ggplot(kanton, aes(auslanderanteil, ja_anteil)) +
  geom_point() +
  coord_fixed(1) +
  scale_y_continuous(breaks = c(0,0.1,0.3,0.5,0.7),limits =  c(0,0.7)) +
  scale_x_continuous(breaks = c(0,0.1,0.3,0.5,0.7),limits =  c(0,0.7)) +
  labs(y = "Anteil Ja-Stimmen")

plot1
```


### Aufgabe 2

Rekonstruiere [Grafik 2](https://blog.tagesanzeiger.ch/datenblog/wp-content/uploads/sites/32/2014/03/Kantone-LOESS-986x923.png). Erweitere dazu `plot1` mit einer Trendlinie.

```{r}
# Lösung zu Aufgabe 2

plot1 +
  geom_smooth()
```



### Aufgabe 3


Importiere die Gemeindedaten [initiative_masseneinwanderung_gemeinde.csv](11_InfoVis1/data/initiative_masseneinwanderung_gemeinde.csv):

```{r, echo = T}
gemeinde <- read_delim("11_InfoVis1/data/initiative_masseneinwanderung_gemeinde.csv",",",locale = locale(encoding = "UTF-8")) %>%
  janitor::clean_names() # säubert die Spaltennamen
```


Rekonstruiere [Grafik 3](https://blog.tagesanzeiger.ch/datenblog/wp-content/uploads/sites/32/2014/03/Gemeinden-2-986x939.png). Stelle dazu den Ausländeranteil aller Gemeinden dem Ja-Stimmen-Anteil gegenüber. Speichere den Plot als `plot2`

```{r}
# Lösung zu Aufgabe 3

plot2 <- ggplot(gemeinde, aes(anteil_ausl, anteil_ja)) +
  geom_point() +
  labs(x = "Ausländeranteil",y = "Anteil Ja-Stimmen") +
  coord_fixed(1) +
  lims(x = c(0,1), y = c(0,1))

plot2
```


### Aufgabe 4

Rekonstruiere [Grafik 4](https://blog.tagesanzeiger.ch/datenblog/wp-content/uploads/sites/32/2014/03/Gemeinden-GAM-2-986x939.png) indem `plot2` mit einer Trendlinie erweitert wird.

```{r}
# Lösung zu Aufgabe 4

plot2 +
  geom_smooth()
```


### Aufgabe 5

Rekonstruiere [Grafik 5](https://blog.tagesanzeiger.ch/datenblog/wp-content/uploads/sites/32/2014/03/Gemeinden-x-Kantone-2-986x857.png) indem `plot2` mit `facetting` erweitert wird. Die Facets sollen die einzelnen Kantone sein. Speichere den Plot als `plot3`.

```{r}

# Lösung zu Aufgabe 5

plot3 <- plot2 +
  facet_wrap(~kanton)
plot3
```


### Aufgabe 6

Rekonstruiere [Grafik 6](https://blog.tagesanzeiger.ch/datenblog/wp-content/uploads/sites/32/2014/03/Gemeinden-x-Kantone-LOESS-2-986x857.png) indem `plot3` mit einer Trendlinie erweitert wird.

Rekonstruktion:

```{r, warning=F}

# Lösung zu Aufgabe 6

plot3 +
  geom_smooth()
```


### Aufgabe 7

Rekonstruiere [Grafik 7](https://blog.tagesanzeiger.ch/datenblog/wp-content/uploads/sites/32/2014/03/Gemeinden-x-Quantile-2-986x637.png) indem `plot2`mit `facetting` erweitert wird. Die Facets sollen nun den Grössen-Quantilen entsprechen. Speichere den Plot unter `plot4`.

Rekonstruktion:

```{r}

# Lösung zu Aufgabe 7

plot4 <- plot2 +
  facet_wrap(~quantile)
plot4
```


### Aufgabe 8

Rekonstruiere [Grafik 8](https://blog.tagesanzeiger.ch/datenblog/wp-content/uploads/sites/32/2014/03/Gemeinden-x-Quantile-LOESS-2-986x637.png) indem `plot4` mit einer Trendlinie ausgestattet wird.

```{r}

# Lösung zu Aufgabe 8

plot4 +
  geom_smooth()
```


### Aufgabe 9 (Optional, fortgeschritten)

Rekonstruiere die [Korrelationstabelle](https://tagi_dwpro.s3.amazonaws.com/UMvkt/2/fs.html).

Tipp: 
- Nutze `group_by()` und `summarise()`
- Nutze `cor.test()` um den Korrelationskoeffizienten sowie den p-Wert zu erhalten. 
- Mit `$estimate` und `$p.value` können die entsprechenden Werte direkt angesprochen werden

Hinweis: aus bisher unerklärlichen Gründen weiche gewisse meiner Werte leicht von den Berechnungen des Herrn Kovics ab.

```{r}

# Lösung zu Aufgabe 9

korr_tab <- gemeinde %>%
  group_by(kanton) %>%
  summarise(
    Korr.Koeffizient = cor.test(anteil_ja,anteil_ausl,method = "pearson")$estimate,
    Signifikanz_val = cor.test(anteil_ja,anteil_ausl,method = "pearson")$p.value,
    Signifikanz = ifelse(Signifikanz_val < 0.001,"***",ifelse(Signifikanz_val<0.01,"**",ifelse(Signifikanz_val<0.05,"*","-")))
  ) %>%
  select(-Signifikanz_val)

```



```{r,echo=F, purl=F}
knitr::kable(korr_tab)
```


```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:11_InfoVis1/Uebung.Rmd-->

## Loesung

[RCode als Download](11_InfoVis1/RFiles/Uebung.R)


```{r code=readLines('11_InfoVis1/RFiles/Uebung.R', encoding = "UTF-8"), echo=T, eval=F}
```
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:11_InfoVis1/Uebung_loesung.Rmd-->

# InfoVis2 (22.10.2019)

Die Informationsvisualisierung ist eine vielseitige, effektive und effiziente Methode für die explorative Datenanalyse. Während Scatterplots und Histogramme weitherum bekannt sind, bieten weniger bekannte Informationsvisualisierungs-Typen wie etwa Parallelkoordinatenplots, TreeMaps oder Chorddiagramme originelle alternative Darstellungsformen zur visuellen Analyse von Datensätze, welche stets grösser und komplexer werden. Die Studierenden lernen in dieser Lerneinheit eine Reihe von Informationsvisualisierungstypen kennen, lernen diese zielführend zu gestalten und selber zu erstellen.
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:12_InfoVis2/Abstract.Rmd-->

---
output:
  pdf_document: default
  html_document: default
---

```{r, include=F, purl = F}
library(knitr)
knitr::opts_chunk$set(echo = F,include = T,message = F, warning = F, collapse=TRUE)
# knitr::opts_knit$set(root.dir = "12_InfoVis2") 

```



## Uebung A

```{r, message = F, echo = T}
library(tidyverse)
library(lubridate)
```


Laden den `wetter`-Datensatz, bereinige ihn wenn nötig (`NA`-Werte entfernen) und importiere auch den Datensatz `order_52252_legend.csv` und verbinde die Datensätze mit einem join via dem Stationskürzel.



```{r, echo = T, message = F}

wetter <- read_table("09_PrePro1/data/order_52252_data.txt",
                     col_types = list(
                       col_character(),    
                       col_datetime(format = "%Y%m%d%H"),
                       col_double()
                       )
                     )

wetter <- wetter %>%
  filter(!is.na(stn)) %>%
  filter(!is.na(time))

station_meta <- read_delim("09_PrePro1/data/order_52252_legend.csv",";")

wetter <- left_join(wetter,station_meta,by = "stn")

```




### Aufgabe 1

Erstelle zwei Hilfsspalten (convenience variables) "Jahr" und "Monat". Filtere auf ein beliebiges Jahr und zwei beliebige Monate. Speichere den gefilterten Datensatz in einer neuen Variablen ab. Verwende diesen Datensatz für alle folgenden Übungen.

```{r}

# Lösung Aufgabe 1

wetter_fil <- wetter %>%
  mutate(
    year = year(time),
    month = month(time)
  ) %>%
  filter(year == 2000 & month < 3)
```


### Aufgabe 2

Erstelle ein Scatterplot (`time` vs. `tre200h0`) wobei die Punkte aufgrund ihrer Meereshöhe eingefärbt werden sollen. Tiefe Werte sollen dabei blau eingefärbt werden und hohe Werte rot. Verkleinere die Punkte um übermässiges Überplotten der Punkten zu vermeiden. Weiter sollen im Abstand von zwei Wochen die Kalenderwochen auf der Achse erscheinen. 

Speichere den Plot in einer Variabel `p` ab.

```{r}

# Lösung Aufgabe 2

p <- ggplot(wetter_fil, aes(time,tre200h0, colour = Meereshoehe)) +
  geom_point(size = 0.5) +
  labs(x = "Kalenderwoche", y = "Temperatur in ° Celsius") +
  scale_color_continuous(low = "blue", high = "red") +
  scale_x_datetime(date_breaks = "2 week", date_labels = "KW%W") 

p 

```



### Aufgabe 3

Füge am obigen Plot (gespeichert als Variabel `p`) eine schwarze, gestrichelte Trendlinie hinzu und aktualisiere `p` (`p <- p + ...`).

```{r, message=F}

# Lösung Aufgabe 3

p <- p +
  stat_smooth(colour = "black",lty = 2)

p
```


### Aufgabe 4

Positioniere die Legende oberhalb des Plots und lege sie quer (nutze dazu `theme()` mit `legend.direction` und `legend.position`). Speichere diese Änderungen in `p`.

```{r, message=F}

# Lösung Aufgabe 4


p <- p + 
  theme(legend.direction = "horizontal",legend.position = "top")

p
    
```



### Aufgabe 5 (optional, fortgeschritten)

Füge den Temperaturwerten auf der y-Ache ein `°C` hinzu (siehe unten und studiere [diesen Tipp](https://stackoverflow.com/a/35967126/4139249) zur Hilfe). Speichere den plot in `p2`.

```{r, message=F}

# Lösung Aufgabe 5

p +
  scale_y_continuous(labels = function(x)paste0(x,"°C")) +
  labs(x = "Kalenderwoche", y = "Temperatur")


```


### Aufgabe 6 (optional, fortgeschritten)

Füge dem Plot eine zweite, korrekt ausgerichtete Achse mit Kelvin oder Farenheit hinzu (siehe `sec_axis`). Wenn du es vorherigen Übung schon geschafft hast, setze auch hier die Einheit (`K` oder `°F`) hinter die Werte auf der Achse. 

$$ K = °C + 273,15$$
$$°F = °C × \frac{9}{5} + 32$$

```{r, message=F}
# Lösung Aufgabe 6

p <- p +
  labs(x = "Kalenderwoche", y = "Temperatur") +
  scale_y_continuous(labels = function(x)paste0(x,"°C"),sec.axis = sec_axis(~.*(9/5)+32,name = "Temperatur",labels = function(x)paste0(x,"° F")))


p
```




### Aufgabe 7

Jetzt verlassen wir den scatterplot und machen einen Boxplot mit den Temperaturdaten. Färbe die Boxplots wieder in Abhängigkeit der Meereshöhe ein. 

- Beachte den Unterschied zwischen `colour =` und `fill =`
- Beachte den Unterschied zwischen `facet_wrap()` und `facet_grid()`
- `facet_grid()` braucht übrigens noch einen Punkt (`.`) zur Tilde (`~`). 
- Beachte den Unterschied zwischen "`.~`" und "`~.`" bei `facet_grid()`
- verschiebe nach Bedarf die Legende

```{r}

# Lösung Aufgabe 7

wetter_fil <- mutate(wetter_fil,monat = month(time,label = T,abbr = F))


ggplot(wetter_fil, aes(stn,tre200h0, fill = Meereshoehe)) +
  geom_boxplot() +
  facet_grid(monat~.) +
  labs(x = "Station", y = "Temperatur") +
  theme(legend.direction = "horizontal",legend.position = "top")

```


### Aufgabe 8

Teile die Stationen in verschiedene Höhenlagen ein (Tieflage [< 450 m], Mittellage [450 - 1000 m] und Hochlage [> 1'000 m]). Vergleiche die Verteilung der Temperaturwerte in den verschiedenen Lagen.  

- Nutze dazu `facet_grid` um die Höhenlage dem Monat gegenüber zu stellen (`Monat~Lage`)
- Passe `scales =` an damit keine leeren Stellen auf der x-Achse entstehen
- Optional: Verwende den vollen Stationsnamen anstelle des Kürzels und drehe diese ab damit sie sich gegenseitig nicht überschreiben

```{r, warning=F}

# Lösung Aufgabe 8

wetter_fil$Lage[wetter_fil$Meereshoehe < 450] <- "Tieflage" 
wetter_fil$Lage[wetter_fil$Meereshoehe >= 450 & wetter_fil$Meereshoehe <1000] <- "Mittellage" 
wetter_fil$Lage[wetter_fil$Meereshoehe >= 1000] <- "Hochlage" 


ggplot(wetter_fil, aes(Name,tre200h0)) +
  geom_boxplot() +
  facet_grid(monat~Lage, scales = "free_x") +
  labs(x = "Lage", y = "Temperatur") +
  theme(axis.text.x = element_text(angle = 45,hjust = 1))

```



### Aufgabe 9


Als letzter wichtiger Plottyp noch zwei Übungen zum Histogramm. Erstelle ein Histogramm `geom_histogram()` mit den Temperaturwerten. Färbe Säulen aufgrund ihrer Höhenlage ein und die Begrenzungslinie weiss. Setze die Klassenbreite auf 1 Grad.

```{r}

# Lösung Aufgabe 9


h <- ggplot(wetter_fil,aes(tre200h0, fill = Lage)) +
  geom_histogram(binwidth = 1, colour = "white") +
  labs(x = "Temperatur in °C", y = "Anzahl")

h
```


### Aufgabe 10

Erstelle `facets` aufgrund der Höhenlage. Setze noch eine Vertikale linie beim Nullpunkt und stelle den x-Achsenabschnit symmetrisch ein (z.B -30 bis + 30°C).

```{r}

# Lösung Aufgabe 10


h + 
  geom_vline(xintercept = 0, lty = 2, alpha = 0.5) +
  facet_wrap(~Lage) +
  lims(x = c(-30,30)) +
  theme(legend.position = "none")

```





```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:12_InfoVis2/Uebung_A.Rmd-->

## Uebung A: Loesung

[RCode als Download](12_InfoVis2/RFiles/Uebung_A.R)

```{r code=readLines('12_InfoVis2/RFiles/Uebung_A.R',encoding = "UTF-8"), echo=T, eval=F}
```


```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:12_InfoVis2/Uebung_A_loesung.Rmd-->


```{r, include=F, purl = F}
library(knitr)
knitr::opts_chunk$set(echo = F,include = T,message = F, warning = F, collapse=TRUE)
# knitr::opts_knit$set(root.dir = "12_InfoVis2") 


output <- knitr::opts_knit$get("rmarkdown.pandoc.to") # html / latex


run_plotly_image = F # set to "TRUE" in order to create static images via plotly_api (max 100/day)
show_static_image = T # set to "TRUE" in order to show the image / "FALSE" to show error message
default_error <- "In der PDF Version kann die interaktive Grafik bis auf weiteres nicht dargestellt werden."

```

## Uebung B (Optional)

In dieser Übung bauen wir einige etwas unübliche Plots aus der Vorlesung nach. Dafür verwenden wir Datensätze, die in R bereits integriert sind. Eine Liste dieser Datensätze findet man [hier](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/00Index.html) oder mit der Hilfe `?datasets`.


Dazu verwenden wir vor allem das Package `plotly` welches im Gegensatz zu `ggplot2` ein paar zusätzliche Plot-Typen kennt und zudem noch interaktiv ist.  Leider scheinen gewisse Browsers (z.B. Firefox) sowie der Viewer Pane mit `plotly` Mühe zu haben. Deshalb empfehlen wir folgendes:

- [Übungsunterlagen für InfoVis2](http://oyster.zhaw.ch:3939/ResearchMethodsUebungen/) in Chrome zu öffnen
- Falls ihr auf dem [RStudio Server](http://oyster.zhaw.ch:8787) arbeitet: hier ebenfalls in Chrome arbeiten
- Falls ihr lokal mit RStudio arbeitet: Mit der Option `options(viewer=NULL)` werden Plots mit dem Standart Browser. 



```{r, echo = F,message=F}

library(tidyverse)
library(plotly)
library(pander)
library(webshot)

```


```{r, echo = F, purl= F}
Sys.setenv("plotly_username" = "rata_zhaw")
Sys.setenv("plotly_api_key" = "ae8Fn1ltcjUGvWYX957J")
```



### Aufgabe 1: Parallel coordinate plots

Erstelle einen [parallel coordinate plot](https://en.wikipedia.org/wiki/Parallel_coordinates). Dafür eignet sich der integrierte Datensatz [`mtcars`](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html):

```{r, echo = F, purl=F}

knitr::kable(head(mtcars))

```

```{r, echo = T, eval=F}

# Nur nötig, wenn ihr mit einer lokalen Installation von RStudio arbeitet
# (also nicht auf dem Server).
options(viewer=NULL)

```


Parallel Coordinates lassen sich mit nativem `ggplot2` nicht herstellen. Es braucht dazu entweder Erweiterungen oder "standalone" Tools. Als "standalone" Tool kann ich `plotly` stark empfehlen. `Plotly` verfügt zwar über eine etwas eigenwillige Syntax, bietet dafür über sehr vielseitige zusätzliche Möglichkeiten. Vor allem aber sind sämtliche `plotly` Grafiken webbasiert und interaktiv. 

Hier findet ihr eine Anleitung zur Herstellung eines Parallel Coordinates Plot mit `plotly`: https://plot.ly/r/parallel-coordinates-plot/

So sieht der fertige Plot aus:

```{r}
# Lösung Aufgabe 1

p <- mtcars %>%
  plot_ly(type = 'parcoords',
          line = list(color = ~mpg,
                      colorscale = list(c(0,'red'),c(1,'blue'))),
          dimensions = list(
            list(label = 'mpg', values = ~mpg),
            list(label = 'disp', values = ~disp),
            list(label = 'hp', values = ~hp),
            list(label = 'drat', values = ~drat),
            list(label = 'wt', values = ~wt),
            list(label = 'qsec', values = ~qsec),
            list(label = 'vs', values = ~vs),
            list(label = 'am', values = ~am),
            list(label = 'gear', values = ~gear),
            list(label = 'carb', values = ~carb)
          )
  )
```


```{r, results="asis", purl = F, echo=F}
aufgabe <- "aufgabe_1"
filename <- paste0("12_InfoVis2/",aufgabe,".png")

if(output == "latex"){
  if(run_plotly_image){plotly_IMAGE(p, format = "png", out_file = filename)} 
  if(show_static_image){pander::pandoc.image(filename)} else{pandoc.strong(default_error)}
} else(p)

```

### Aufgabe 2: Polar Plot mit Biber Daten

Polar Plots (welche man ebenfalls mit Plotly erstellen kann) eignen sich unter anderem für Daten, die zyklischer Natur sind, wie zum Beispiel zeitlich geprägte Daten (Tages-, Wochen-, oder Jahresrhythmen). Aus den Beispiels-Datensätzen  habe ich zwei Datensätze gefunden, die zeitlich geprägt sind:

- [`beaver1` und `beaver2`](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/beavers.html)
[`AirPassenger`](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/AirPassengers.html)

Beide Datensätze müssen noch etwas umgeformt werden, bevor wir sie für einen Radialplot verwenden können. In Aufgabe 2 verwenden wir die Biber-Datensätze, in der nächsten Aufgabe (3) die Passagier-Daten.

Wenn wir die Daten von beiden Bibern verwenden wollen, müssen wir diese noch zusammenfügen:
```{r, echo = T}


beaver1_new <- beaver1 %>%
  mutate(beaver = "nr1")

beaver2_new <- beaver2 %>%
  mutate(beaver = "nr2")

beaver_new <- rbind(beaver1_new,beaver2_new)

```

Zudem müssen wir die Zeitangabe noch anpassen: Gemäss der [Datenbeschreibung](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/beavers.html) handelt es sich bei der Zeitangabe um ein sehr programmier-unfreundliches Format. 3:30 wird als "0330" notiert. Wir müssen diese Zeitangabe, noch in ein Dezimalsystem umwandeln:
```{r, echo = T}
beaver_new <- beaver_new %>%
  mutate(
    hour_dec = (time/100)%/%1,         # Ganze Stunden (mittels ganzzaliger Division)
    min_dec = (time/100)%%1/0.6,       # Dezimalminuten (15 min wird zu 0.25, via Modulo)
    hour_min_dec = hour_dec+min_dec    # Dezimal-Zeitangabe (03:30 wird zu 3.5)
    ) 
```



Der Datensatz: 
```{r, echo = F, purl = F}
knitr::kable(head(beaver_new))
#  formatRound(c("min_dec","hour_min_dec"), 2)
```

So sieht der fertige Plot aus. Rekonstruiere dies mit `plotly`:

```{r}

# Lösung Aufgabe 2

p <- beaver_new %>%
  plot_ly(r = ~temp, t = ~hour_min_dec, color = ~beaver,mode = "lines", type = "scatter") %>%
  layout(
    radialaxis = list(range = c(35,39)),
    angularaxis = list(range = c(0,24)),
    orientation = 270,
    showlegend = F
    )
```

```{r, results="asis", purl = F, echo=F}
aufgabe <- "aufgabe_2"
filename <- paste0("12_InfoVis2/",aufgabe,".png")

if(output == "latex"){
  if(run_plotly_image){plotly_IMAGE(p, format = "png", out_file = filename)} 
  if(show_static_image){pander::pandoc.image(filename)} else{pandoc.strong(default_error)}
} else(p)

```




### Aufgabe 3: Polar Plot mit Passagier-Daten


Analog Aufgabe 2, dieses Mal mit dem Datensatz [`AirPassanger`](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/AirPassengers.html)

`AirPassengers` kommt in einem Format daher, das ich selbst noch gar nicht kannte. Es sieht zwar aus wie ein `data.frame` oder eine `matrix`, ist aber von der Klasse [`ts`](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/ts.html).

```{r, echo = T}
AirPassengers

class(AirPassengers)
```


Damit wir den Datensatz verwenden können, müssen wir ihn zuerst in eine `matrix` umwandeln. Wie das geht habe ich [hier](https://stackoverflow.com/a/5332664/4139249) erfahren.
```{r, echo = T}
AirPassengers2 <- tapply(AirPassengers, list(year = floor(time(AirPassengers)), month = month.abb[cycle(AirPassengers)]), c)
```

Aus der `matrix` muss noch ein Dataframe her, zudem müssen wir aus der breiten Tabelle eine lange Tabelle machen:

```{r, echo = T}


AirPassengers3 <- AirPassengers2 %>%
  as.data.frame() %>%
  rownames_to_column("year") %>%
  gather(month,n,-year) %>%
  mutate(
    # ich nutze einen billigen Trick um ausgeschriebene Monate in Nummern umzuwandeln [1]
    month = factor(month, levels = month.abb,ordered = T),
    month_numb = as.integer(month),
    year = factor(year, ordered = T)
  )


# [1] beachtet an dieser Stelle das Verhalten von as.integer() wenn es sich um factors() handelt. Hier wird das Verhalten genutzt, andersweitig kann es einem zum Verhngnis werden. Das Verhalten wir auch hier verdeutlicht:
# as.integer(as.character("500"))
# as.integer(as.factor("500"))

```


Hier der fertige Plot. Rekonstruiere dies mit `plotly`:
```{r}

# Lösung Aufgabe 3

p <- AirPassengers3 %>%
  plot_ly(r = ~n, t = ~month_numb, color = ~year, mode = "markers", type = "scatter") %>%
  layout(
    showlegend = T,
    angularaxis = list(range = c(0,12)),
    orientation = 270,
    legend = list(traceorder = "reversed")
) 

```



```{r, results="asis", purl = F, echo=F}
aufgabe <- "aufgabe_3"
filename <- paste0("12_InfoVis2/",aufgabe,".png")

if(output == "latex"){
  if(run_plotly_image){plotly_IMAGE(p, format = "png", out_file = filename)} 
  if(show_static_image){pander::pandoc.image(filename)} else{pandoc.strong(default_error)}
} else(p)
```

### Aufgabe 4: 3D Scatterplot

Erstelle einen 3D Scatterplot, ebenfalls mit `plotly`. Nutze dazu den Datensatz `trees`. Ein Beispiel für einen 3D Scatterplot findet ihr [hier](https://plot.ly/r/3d-scatter-plots/).


```{r}

# Lösung Aufgabe 4

  p <- trees %>%
  plot_ly(x = ~Girth, y = ~Height, z = ~Volume)
```



```{r, results="asis", purl = F, echo=F}
aufgabe <- "aufgabe_4"
filename <- paste0("12_InfoVis2/",aufgabe,".png")

if(output == "latex"){
  if(run_plotly_image){plotly_IMAGE(p, format = "png", out_file = filename)} 
  if(show_static_image){pander::pandoc.image(filename)} else{pandoc.strong(default_error)}
} else(p)
```





```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:12_InfoVis2/Uebung_B.Rmd-->

## Uebung B: Loesung

[RCode als Download](12_InfoVis2/RFiles/Uebung_B.R)

```{r code=readLines('12_InfoVis2/RFiles/Uebung_B.R',encoding = "UTF-8"), echo=T, eval=F}
```

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:12_InfoVis2/Uebung_B_loesung.Rmd-->

# Statistik 1 (26.10.2020)

In Statistik 1 lernen die Studierenden, was (Inferenz-) Statistik im Kern leistet und warum sie für wissenschaftliche Erkenntnis (in den meisten Disziplinen) unentbehrlich ist. Nach einer Wiederholung der Rolle von Hypothesen wird erläutert, wie Hypothesentests in der frequentist-Statistik umgesetzt werden, einschliesslich p-Werten und Signifikanz-Levels. Die praktische Statistik beginnt mit den beiden einfachsten Fällen, dem Chi-Quadrat-Test für die Assoziation zwischen zwei kategorialen Variablen und dem t-Test auf Unterschiede in Mittelwerten zwischen zwei Gruppen. Abschliessend beschäftigen wir uns damit, wie man Ergebnisse statistischer Analysen am besten in Abbildungen, Tabellen und Text darstellt.


<!-- TODO: -->
<!-- Referenzen passt noch nicht -->
<!-- hierarchien noch kontrollieren und mit anderen blöcke abgleichen -->
<!-- Beschreibung forschungsprojekte: tabelle besser als csv -->
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:13_Statistik1/Abstract.Rmd-->

```{r, echo = F, purl = F}
knitr::opts_chunk$set(echo = T, collapse=TRUE)
library(knitr)
```


## Demo: Statistik 1

[Demoscript als Download](13_Statistik1/RFiles/Statistik_1_Demo_v.05.R)

### Chi-Quadrat-Test & Fishers Test

Ermitteln des kritischen Wertes
```{r}
qchisq(0.95,1)
```

Direkter Test in R (dazu Werte als Matrix nötig)
```{r}
count<-matrix(c(38,14,11,51),nrow=2)
count
chisq.test(count)
fisher.test(count)
```



### t-Test

```{r}
a<-c(20,19,25,10,8,15,13,18,11,14)
b<-c(12,15,16,7,8,10,12,11,13,10)
blume<-data.frame(a,b)
blume
summary(blume)
boxplot(blume$a,blume$b)
boxplot(blume)
hist(blume$a)
hist(blume$b)
```

zweiseitiger t-Test
```{r}
t.test(blume$a,blume$b)
```

einseitiger t-Test

```{r}
t.test(blume$a,blume$b, alternative="greater") #einseitig
t.test(blume$a,blume$b, alternative="less") #einseitig
```

klassischer t-Test vs. Welch Test

```{r}
t.test(blume$a,blume$b, var.equal=T) #Varianzen gleich, klassischer t-Test
t.test(blume$a,blume$b, var.equal=F) #Varianzen ungleich, Welch's t-Test, ist auch default, d.h. wenn var.equal nicht                        # definiert wird, wird ein Welch's t-Test ausgeführt. 
```

gepaarter t-Test

```{r}
t.test(blume$a,blume$b, paired=T)
t.test(blume$a,blume$b, paired=T,alternative="greater")
```

Wilcoxon-Vorzeichen-Rang-Test
```{r}
shapiro.test(blume$b)
var.test(blume$a,blume$b)
if(!require(car)){install.packages("car")} # installiert das Zusatzpacket car (wenn nicht bereits installiert)
library(car)
leveneTest(blume$a,blume$b,center=mean)
wilcox.test(blume$a,blume$b)
```

Das gleiche mit einem “long table”


```{r}
cultivar<-c(rep("a",10),rep("b",10))
size<-c(a,b)
blume.long<-data.frame(cultivar,size)

rm(size) #Befehl rm entfernt die nicht mehr benötitgten Objekte aus dem Workspace
rm(cultivar)


rm(size) #Befehl rm entfernt die nicht mehr benötitgten Objekte aus dem Workspace
rm(cultivar)
```


Das gleiche in einer Zeile

```{r}
blume.long<-data.frame(cultivar=c(rep("a",10),rep("b",10)),size=c(a,b))
summary(blume.long)             
head(blume.long)

boxplot(size~cultivar, data=blume.long)

t.test(size~cultivar, blume.long, var.equal=T)
t.test(size~cultivar, blume.long, var.equal=F)
```


### Base R vs. ggplot2


```{r}

library(tidyverse)
ggplot(blume.long, aes(cultivar,size)) + geom_boxplot()
ggplot(blume.long, aes(cultivar,size)) + geom_boxplot()+theme_classic()
ggplot(blume.long, aes(cultivar,size)) + geom_boxplot(size=1) + theme_classic()+
theme(axis.line = element_line(size=1))+theme(axis.title = element_text(size=14))+
theme(axis.text = element_text(size=14))
ggplot(blume.long, aes(cultivar,size)) + geom_boxplot(size=1) + theme_classic()+
  theme(axis.line = element_line(size=1), axis.ticks = element_line(size=1), 
       axis.text = element_text(size = 20), axis.title = element_text(size = 20))
```


Definieren von mytheme mit allen gewünschten Settings, das man zu Beginn einer Sitzung einmal laden und dann immer wieder ausführen kann (statt des langen Codes)

```{r}
mytheme <- theme_classic() + 
  theme(axis.line = element_line(color = "black", size=1), 
        axis.text = element_text(size = 20, color = "black"), 
        axis.title = element_text(size = 20, color = "black"), 
        axis.ticks = element_line(size = 1, color = "black"), 
        axis.ticks.length = unit(.5, "cm"))
```

```{r}
ggplot(blume.long, aes(cultivar,size)) + 
  geom_boxplot(size=1) +
  mytheme

t_test <- t.test(size~cultivar, blume.long)

ggplot(blume.long, aes(cultivar,size)) + 
  geom_boxplot(size=1) + 
  mytheme +
  annotate("text", x = "b", y = 24, label = paste0("italic(p) == ", round(t_test$p.value, 3)), parse = TRUE, size = 8)

ggplot (blume.long, aes(cultivar,size)) + 
  geom_boxplot(size=1) + 
  mytheme +
  labs(x="Cultivar",y="Size (cm)")

```

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:13_Statistik1/Demo_Statistik_1.Rmd-->

# Statistik 2 (27.10.2019)

In Statistik 2 lernen die Studierenden die Idee, die Voraussetzungen und die praktische Anwendung „einfacher“ linearer Modelle in R (sowie teilweise ihrer „nicht-parametrischen“ bzw. „robusten“ Äquivalente). Am Anfang steht die Varianzanalyse (ANOVA) als Verallgemeinerung des t-Tests, einschliesslich post-hoc-Tests und mehrfaktorieller ANOVA. Dann geht es um die Voraussetzungen parametrischer (und nicht-parametrischer) Tests und Optionen, wenn diese verletzt sind. Dann beschäftigen wir uns mit Korrelationen, die auf einen linearen Zusammenhang zwischen zwei metrischen Variablen testen, ohne Annahme einer Kausalität. Es folgen einfache lineare Regressionen, die im Prinzip das Gleiche bei klarer Kausalität leisten. Abschliessend besprechen wir, was die grosse Gruppe linearer Modelle (Befehl lm in R) auszeichnet.
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:14_Statistik2/Abstract.Rmd-->


```{r, include=FALSE, purl=F}

knitr::opts_chunk$set(echo = TRUE,include = T, results = "hide",collapse=TRUE)
```

## Demoskript

[Demoscript als Download](14_Statistik2/RFiles/Statistik_2_Demo_v.05.R)

 __t-test als ANOVA__ 

```{r}
a<-c(20,19,25,10,8,15,13,18,11,14)
b<-c(12,15,16,7,8,10,12,11,13,10)

blume<-data.frame(cultivar=c(rep("a",10),rep("b",10)),size=c(a,b))

par(mfrow=c(1,1))
boxplot (data=blume, size~cultivar, xlab="Sorte", ylab="Bluetengroesse [cm]")

t.test(size~cultivar, blume, var.equal=T)

aov(size~cultivar,data=blume)
summary(aov(size~cultivar,data=blume))
summary.lm(aov(size~cultivar,data=blume))
```

__Echte ANOVA__

```{r}
c<-c(30,19,31,23,18,25,26,24,17,20)

blume2<-data.frame(cultivar=c(rep("a",10),rep("b",10),rep("c",10)),size=c(a,b,c))
blume2$cultivar<-as.factor(blume2$cultivar)

summary(blume2)             
head(blume2)

par(mfrow=c(1,1))
boxplot (data=blume2, size~cultivar, xlab="Sorte", ylab="Blütengrösse [cm]")

aov(size~cultivar,data=blume2)
summary(aov(size~cultivar,data=blume2))
summary.lm(aov(size~cultivar,data=blume2))

aov.1 <- aov(size~cultivar,data=blume2)
summary(aov.1)
summary.lm(aov.1)

#Berechnung Mittelwerte usw. zur Charakterisierung der Gruppen
aggregate(size~cultivar,blume2, function(x) c(Mean = mean(x), SD = sd(x), Min=min(x), Max=max(x)))

lm.1 <- lm(size~cultivar,data=blume2)
summary(lm.1)
```


__Tukeys Posthoc-Test__

```{r}
if(!require(multcomp)){install.packages("multcomp")}
library(multcomp)
summary(glht(aov(size~cultivar, data=blume2),linfct=mcp(cultivar ="Tukey")))
```


__Beispiel Posthoc-Labels in Plot__

```{r}
anova <- aov(Sepal.Width ~ Species, data=iris)
letters <- cld(glht(anova, linfct=mcp(Species="Tukey")))
boxplot(Sepal.Width ~ Species, data=iris)
mtext(letters$mcletters$Letters, at=1:3)

library(tidyverse)
ggplot(iris, aes(Species, Sepal.Width)) + geom_boxplot(size = 1) +
  annotate("text", y = 5, x = 1:3, label = letters$mcletters$Letters)

```


__Klassische Tests der Modellannahmen (NICHT EMPFOHLEN!!!) __

```{r}
shapiro.test(blume2$size[blume2$cultivar == "a"])
var.test(blume2$size[blume2$cultivar == "a"],blume2$size[blume2$cultivar == "b"])

if(!require(car)){install.packages("car")}
library(car)
leveneTest(blume2$size[blume2$cultivar == "a"],blume2$size[blume2$cultivar == "b"],center=mean)

wilcox.test(blume2$size[blume2$cultivar == "a"],blume2$size[blume2$cultivar == "b"])
```


__Nicht-parametrische Alternativen, wenn Modellannahmen der ANVOA massiv verletzt sind__

__Zum Vergleich normale ANOVA noch mal__

```{r}
summary(aov(size~cultivar,data=blume2))
```

__Bei starken Abweichungen von der Normalverteilung, aber ähnlichen Varianzen__

__Kruskal-Wallis-Test__

```{r}
kruskal.test(data=blume2, size~cultivar)
if(!require(FSA)){install.packages("FSA")} 
library(FSA)
dunnTest(data=blume2, size~cultivar, method="bh") #korrigierte p-Werte nach Bejamini-Hochberg
```

__Bei erheblicher Heteroskedastizität, aber relative normal/symmetrisch verteilten Residuen__

__Welch-Test__

```{r}
oneway.test(data=blume2, size~cultivar, var.equal=F)
```

__2-faktorielle ANOVA__

```{r}
d<-c(10,12,11,13,10,25,12,30,26,13)
e<-c(15,13,18,11,14,25,39,38,28,24)
f<-c(10,12,11,13,10,9,2,4,7,13)

blume3<-data.frame(cultivar=c(rep("a",20),rep("b",20),rep("c",20)),
                   house=c(rep(c(rep("yes",10),rep("no",10)),3)),size=c(a,b,c,d,e,f))
blume3

boxplot(size~cultivar+house,data=blume3)

summary(aov(size~cultivar+house,data=blume3))
summary(aov(size~cultivar+house+cultivar:house,data=blume3)) 
summary(aov(size~cultivar*house,data=blume3)) #Kurzschreibweise: "*" bedeutet, dass Interaktion zwischen cultivar und house eingeschlossen wird

summary.lm(aov(size~cultivar+house,data=blume3))


interaction.plot(blume3$cultivar,blume3$house,blume3$size)
interaction.plot(blume3$house,blume3$cultivar,blume3$size)

anova(lm(blume3$size~blume3$cultivar*blume3$house),lm(blume3$size~blume3$cultivar+blume3$house))
anova(lm(blume3$size~blume3$house),lm(blume3$size~blume3$cultivar*blume3$house))
```


__Korrelationen__

```{r}
library(car)

blume<-data.frame(a,b)
scatterplot(a~b,blume)

cor.test(a,b,data = blume, method="pearson")
cor.test(a,b,data = blume, method="spearman")
cor.test(a,b,data = blume, method="kendall") 

#Jetzt als Regression
lm.2 <- lm(b~a)
anova(lm.2)
summary(lm.2)

#Model II-Regression
if(!require(lmodel2)){install.packages("lmodel2")} 
library(lmodel2)
lmodel2(b~a)
```

__Beispiele Modelldiagnostik__
```{r}
par(mfrow=c(2,2)) #4 Plots in einem Fenster
plot(lm(b~a))

if(!require(ggfortify)){install.packages("ggfortify")}
library(ggfortify)
autoplot(lm(b~a))

#Modellstatistik nicht OK
g<-c(20,19,25,10,8,15,13,18,11,14,25,39,38,28,24)
h<-c(12,15,10,7,8,10,12,11,13,10,25,12,30,26,13)
par(mfrow=c(1,1))

plot(h~g,xlim=c(0,40),ylim=c(0,30))
abline(lm(h~g))

par(mfrow=c(2,2))
plot(lm(h~g))

#Modelldiagnostik mit ggplot
df <- data.frame(g,h)
ggplot(df, aes(x = g, y = h)) + 
    # scale_x_continuous(limits = c(0,25)) +
    # scale_y_continuous(limits = c(0,25)) +
    geom_point() +
    geom_smooth( method = "lm", color = "black", size = .5, se = F) + 
    theme_classic()

par(mfrow=c(2,2))
plot(lm(h~g))

autoplot(lm(h~g))


```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:14_Statistik2/Demo_Statistik_2.Rmd-->

