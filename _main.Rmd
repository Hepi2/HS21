--- 
title: "Research Methods"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
#output: bookdown::gitbook
documentclass: book
bibliography: [00_Admin/book.bib, 00_Admin/packages.bib]
biblio-style: apalike
link-citations: yes
description: "Begleitmaterial zum Modul 'Research Methods' "
---

# Einleitung


Das Modul „Research Methods“ vermittelt vertiefte Methodenkompetenzen für praxisorientiertes und angewandtes wissenschaftliches Arbeiten im Fachbereich „Umwelt und Natürliche Ressourcen“ auf MSc-Niveau. Die Studierenden erarbeiten sich vertiefte Methodenkompetenzen für die analytische Betrachtung der Zusammenhänge im Gesamtsystem „Umwelt und Natürliche Ressourcen“. Die Studierenden erlernen die methodischen Kompetenzen, auf denen die nachfolgenden Module im MSc Programm UNR aufbauen. Das Modul vermittelt einerseits allgemeine, fächerübergreifende methodische Kompetenzen (z.B. Wissenschaftstheorie, computer-gestützte Datenverar-beitung und Statistik).

Auf dieser Plattform (RStudio Connect) werden die Unterlagen für die R-Übungsteile bereitgestellt. Es werden sukzessive sowohl Demo-Files, Aufgabenstellungen und Lösungen veröffentlicht.




```{r, include=F, message=F}
# Set Root Directory / Working directory to Project folder for all Files (if M-K)
knitr::opts_knit$set(root.dir = getwd())
```


```{r, include=F, message=F}


grepl_loop <- function(vector,remove){
  for(remove_i in remove){
    vector <- vector[!grepl(remove_i,vector)]
  }
  return(vector)
}




# Allow duplicate Labels so that calling purl() does not create an error
# https://stackoverflow.com/q/36868287/4139249
options(knitr.duplicate.label = 'allow')

# purl all Rmd Documents (with some exceptions) and store them in a Subfolder /RFiles
# Document cannot be knitted if the folder "RFiles" does not exist!
library(stringr)

keywords <- c("ResearchMethods","_Rcode","99_","index","Archive","Admin","main","Abstract")


rmds <- list.files(pattern = ".Rmd",recursive = T)

rmds <- grepl_loop(rmds,keywords)


for (file in rmds){
  file_r <- gsub("Rmd","R",file)                          # change fileextension from .rmd to r
  file_r <- str_split_fixed(file_r,"/",Inf)               # split path at /
  file_r <- append(file_r, "RFiles",length(file_r)-1)     # append Foldername "RFiles" in 2nd last pos
  file_r <- paste(file_r,collapse = "/")                  # collapse vector to string
  if(file.exists(file_r)){
    file.remove(file_r)
  }
  knitr::purl(file,documentation = 0,output = file_r)
}

```




```{r include=FALSE, message=F}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr','forcats','carData', 'rmarkdown','tidyverse','plotly','car','ggfortify','boot','pander','scales','multicomp','ggExtra','lubridate','dplyr','purrr','readr','tidyr','tibble','ggplot2','webshot','bindrcpp','GGally','hier.part','gtools','MuMIn','nlme','lme4','languageR','lmerTest','rms','SparseM','Hmisc','Formula','survival','lattice','Matrix'
), '00_Admin/packages.bib')

```
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:index.Rmd-->

# PrePro1 (15.10.2018)

Die Datenkunde 2.0 gibt den Studierenden das Wissen und die Fertigkeiten an die Hand, selbst erhobene und bezogene Daten für Ihre eigenen Analysen vorzubereiten und anzureichern (preprocessing). Die Einheit vermittelt zentrale Datenverarbeitungskompetenzen und thematisiert bekannte Problemzonen der umweltwissenschaftlichen Datenverarbeitung – immer mit einer „hands-on“ Perspektive auf die begleitenden R-Übungen. Die Studierenden lernen die Eigenschaften ihrer Datensätze in der Fachsprache korrekt zu beschreiben. Sie lernen ausserdem Metadaten zu verstehen und die Implikationen derselben für ihre eigenen Analyseprojekte kritisch zu beurteilen. Zentrale Konzepte der Lerneinheit sind Skalenniveaus, Datentypen, Zeitdaten und Typumwandlungen.
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:09_PrePro1/Abstract.Rmd-->


```{r, include=FALSE, purl=F}

knitr::opts_chunk$set(echo = TRUE,include = T, collapse=TRUE)
```

## Demo: Datentypen, Tabellen

[R-Code als Download](09_PrePro1/RFiles/Demo_Datentypen.R)

### Datentypen 


#### Numerics

Unter die Kategorie `numeric` fallen in R zwei Datentypen:

- `double`: Gleitkommazahl (z.B. 10.3, 7.3)
- `integer`: Ganzzahl (z.B. 10, 7)

##### Doubles

Folgendermassen wird eine Gleitkommazahl einer Variabel zuweisen:

```{r}
x <- 10.3

x

typeof(x)
```



Statt `<-`kann auch `=` verwendet werden. Dies funktioniert aber nicht in allen Situationen, und ist zudem leicht mit `==` zu verwechseln.

```{r}
y = 7.3

y
```



Ohne explizite Zuweisung nimmt R immer den Datentyp `double`an:

```{r}
z <- 42
typeof(z)
is.integer(z)
is.numeric(z)
is.double(z)

```

#### Ganzzahl / Integer 


Erst wenn man eine Zahl explizit als `integer` definiert (mit `as.integer()` oder `L`), wird sie auch als solches abgespeichert.

```{r}
a <- as.integer(z)
is.numeric(a)
is.integer(a)

c <- 8L
is.numeric(c)
is.integer(c)
```




```{r}
typeof(a)

is.numeric(a)
is.integer(a)
```



Mit `c()` können eine Reihe von Werten in einer Variabel zugewiesen werden (als `vector`). Es gibt zudem auch `character verctors`. 

```{r}
vector <- c(10,20,33,42,54,66,77)
vector
vector[5]
vector[2:4]

vector2 <- vector[2:4]
```



Eine Ganzzahl kann explizit mit `as.integer()` definiert werden.

```{r}
a <- as.integer(7)
b <- as.integer(3.14)
a
b
typeof(a)
typeof(b)
is.integer(a)
is.integer(b)

```

Eine Zeichenkette kann als Zahl eingelesen werden.

```{r}
c <- as.integer("3.14")
c
typeof(c)
```


#### Logische Abfragen 

Wird auch auch als boolesch (Eng. **boolean**) bezeichnet.

```{r}
e <- 3
f <- 6
g <- e > f
e
f
g
typeof(g)

```

#### Logische Operationen


```{r}
sonnig <- TRUE
trocken <- FALSE

sonnig & !trocken
```

Oft braucht man auch das Gegenteil / die Negation eines Wertes. Dies wird mittels `!` erreicht

```{r}
u <- TRUE
v <- !u 
v
```



#### Zeichenketten

Zeichenketten (Eng. **character**) stellen Text dar

```{r}
s <- as.character(3.14)
s
typeof(s)
```



Zeichenketten verbinden / zusammenfügen (Eng. **concatenate**)

```{r}
fname <- "Hans"
lname <- "Muster"
paste(fname,lname)

fname2 <- "hans"
fname == fname2
```


#### `Factors`

Mit `Factors` wird in R eine Sammlung von Zeichenketten bezeichnet, die sich wiederholen, z.B. Wochentage (es gibt nur 7 unterschiedliche Werte für "Wochentage").

```{r}
wochentage <- c("Montag","Dienstag","Mittwoch","Donnerstag","Freitag","Samstag","Sonntag",
                "Montag","Dienstag","Mittwoch","Donnerstag","Freitag","Samstag","Sonntag")

typeof(wochentage)

wochentage_fac <- as.factor(wochentage)

wochentage
wochentage_fac


```

Wie man oben sieht, unterscheiden sich `character vectors` und `factors` v.a. dadurch, dass letztere über sogenannte `levels` verfügt. Diese `levels` entsprechen den Eindeutigen (`unique`) Werten.

```{r}
levels(wochentage_fac)

unique(wochentage)
```



#### Zeit/Datum

Um in R mit Datum/Zeit Datentypen umzugehen, müssen sie als `POSIXct` eingelesen werden (es gibt alternativ noch `POSIXlt`, aber diese ignorieren wir mal). Anders als Beispielsweise bei Excel, sollten in R Datum und Uhrzeit immer in **einer Spalte** gespeichert werden.

```{r}
datum <- "2017-10-01 13:45:10"

as.POSIXct(datum)

```

Wenn das die Zeichenkette in dem obigen Format (Jahr-Monat-Tag Stunde:Minute:Sekunde) daher kommt, braucht `as.POSIXct`keine weiteren Informationen. Sollte das Format von dem aber Abweichen, muss man der Funktion das genaue Schema jedoch mitteilen. Der Syntax dafür kann via `?strptime` nachgeschlagen werden.

```{r}
datum <- "01.10.2017 13:45"

as.POSIXct(datum,format = "%d.%m.%Y %H:%M")

```

Beachtet, dass in den den obigen Beispiel R automatisch eine Zeitzone angenommen hat (`CEST`). R geht davon aus, dass die Zeitzone der **System Timezone** (`Sys.timezone()`) entspricht.


### Data Frames und Conveniance Variabeln

Eine `data.frame` ist die gängigste Art, Tabellarische Daten zu speichern. 

```{r}
df <- data.frame(
  Stadt = c("Zürich","Genf","Basel","Bern","Lausanne"),
  Einwohner = c(396027,194565,175131,140634,135629),
  Ankunft = c("1.1.2017 10:00","1.1.2017 14:00",
              "1.1.2017 13:00","1.1.2017 18:00","1.1.2017 21:00")
)

str(df)

```

In der obigen `data.frame` wurde die Spalte `Einwohner` als Fliesskommazahl abgespeichert. Dies ist zwar nicht tragisch, aber da wir wissen das es sich hier sicher um Ganzzahlen handelt, können wir das korrigieren. Wichtiger ist aber, dass wir die Ankunftszeit (Spalte`Ankunft`) von  einem `Factor` in ein Zeitformat (`POSIXct`) umwandeln. 


```{r}
df$Einwohner <- as.integer(df$Einwohner)

df$Einwohner

df$Ankunft <- as.POSIXct(df$Ankunft, format = "%d.%m.%Y %H:%M")

df$Ankunft
```


Diese Rohdaten können nun helfen, um Hilfsvariablen (**convenience variables**) zu erstellen. Z.B. können wir die Städte einteilen in gross, mittel und klein. 

```{r}
df$Groesse[df$Einwohner > 300000] <- "gross"
df$Groesse[df$Einwohner <= 300000 & df$Einwohner > 150000] <- "mittel"
df$Groesse[df$Einwohner <= 150000] <- "klein"

```



Oder aber, die Ankunftszeit kann von der Spalte `Ankunft`abgeleitet werden. Dazu brauchen wir aber das Package `lubridate`

```{r, message = F}
library(lubridate)
```


```{r}
df$Ankunft_stunde <- hour(df$Ankunft)
```

### Quellen

```{r code=readLines('00_Admin/get_chapter_references.R'), echo=F, eval=T,purl = F, results="asis"}
```




```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:09_PrePro1/Demo_Datentypen.Rmd-->


```{r, include=FALSE, purl = F}
knitr::opts_chunk$set(echo = F, include = T, collapse=TRUE, warning = F)
```


## Übung A

R ist ohne Zusatzpackete nicht mehr denkbar. Die allermeisten Packages werden auf [CRAN](https://cran.r-project.org/) gehostet und können leicht mittels `install.packages()` installiert werden. Eine sehr wichtige Sammlung von Packages wird von RStudio entwickelt. Unter dem Namen [Tidyverse](https://www.tidyverse.org/) werden eine Reihe von Packages angeboten, den R-Alltag enorm erleichtert. Wir werden später näher auf das "Tidy"-Universum eingehen, an dieser Stelle können wir die Sammlung einfach mal installieren.

```
install.packages("tidyverse")
```

Um ein `package` in R verwenden zu können, gibt es zwei Möglichkeiten: 

- entweder man lädt es zu Beginn der R-session mittles `library()`. 
- oder man ruft eine `function` mit vorangestelltem Packetname sowie zwei Doppelpunkten auf. `dplyr::filter()` ruft die Funktion `filter()` des Packets `dplyr` auf. 

Letztere Notation ist vor allem dann sinnvoll, wenn sich zwei unterschiedliche Funktionen mit dem gleichen namen in verschiedenen pacakges existieren. `filter()` existiert als Funktion einersits im package `dplyr` sowie in  `stats`. Dieses Phänomen nennt man "masking". 


Zu beginn laden wir die nötigen Pakete:


```{r,message = F}
library(tidyverse)
# Im Unterschied zu `install.packages()` werden bei `library()` keine Anführungs- 
# und Schlusszeichen gesetzt.


library(lubridate)
# Im Unterschied zu install.packages("tidyverse") wird bei library(tidyverse) 
# das package lubridate nicht berücksichtigt
```


### Aufgabe 1

Erstelle eine `data.frame` mit nachstehenden Daten.

Tipps:

- Eine leere `data.frame` zu erstellen ist schwieriger als wenn erstellen und befüllen der `data.frame` in einem Schritt erfolgt
- R ist dafür gedacht, Spalte für Spalte zu arbeiten ([warum?](http://www.noamross.net/blog/2014/4/16/vectorization-in-r--why.html)), nicht Reihe für Reihe. Versuche dich an dieses Schema zu halten.

```{r}

# Lösung Aufgabe 1

df <- data_frame(
  Tierart = c("Fuchs","Bär","Hase","Elch"),
  Anzahl = c(2,5,1,3),
  Gewicht = c(4.4, 40.3,1.1,120),
  Geschlecht = c("m","f","m","m"),
  Beschreibung = c("Rötlich","Braun, gross", "klein, mit langen Ohren","Lange Beine, Schaufelgeweih")
  )

```


```{r, echo = F, purl=F}
knitr::kable(df)
```



### Aufgabe 2

Was für Datentypen wurden (in Aufgabe 1) von R automatisch angenommen? Sind diese sinnvoll? 

Tipp: Nutze dazu `str()`

```{r}
# Lösung Aufgabe 2

str(df)

# Anzahl wurde als `double` interpretiert, ist aber eigentlich ein `integer`. 
# Mit data.frame() wurde Beschreibung wurde als `factor` interpretiert, ist 
# aber eigentlich `character`
```


```{r}


typeof(df$Anzahl)

df$Anzahl <- as.integer(df$Anzahl)
df$Beschreibung <- as.character(df$Beschreibung)

```


### Aufgabe 3


Nutze die Spalte `Gewicht` um die Tiere in 3 Gewichtskategorien einzuteilen: 

- leicht: < 5kg
- mittel: 5 - 100 kg
- schwer: > 100kg


```{r}

# Lösung Aufgabe 3

df$Gewichtsklasse[df$Gewicht > 100] <- "schwer"
df$Gewichtsklasse[df$Gewicht <= 100 & df$Gewicht > 5] <- "mittel"
df$Gewichtsklasse[df$Gewicht <= 5] <- "leicht"

```


```{r, purl=F}
knitr::kable(df)
```




### Aufgabe 4

Importiere den Datensatz [order_52252_data.txt](09_PrePro1/data/order_52252_data.txt). Es handelt sich dabei um die stündlich gemittelten Temperaturdaten an verschiedenen Standorten in der Schweiz im Zeitraum 2000 - 2005. Wir empfehlen `read_table()`^[@wickham2017, Kapitel 8 bzw. http://r4ds.had.co.nz/data-import.html)] anstelle von `read.table()`.

```{r, message = F}
# Lösung Aufgabe 4

wetter <- readr::read_table("09_PrePro1/data/order_52252_data.txt")
```




```{r, purl=F}
knitr::kable(head(wetter,10))
```


### Aufgabe 5

Schau dir die Rückmeldung von `read_table()`an. Sind die Daten korrekt interpretiert worden?


```{r}
# Lösung Aufgabe 5
# Die Spalte 'time' wurde als 'integer' interpretiert. Dabei handelt es
# sich offensichtlich um Zeitangaben.
```



### Aufgabe 6

Die Spalte `time` ist eine Datum/Zeitangabe im Format JJJJMMTTHH (siehe [meta.txt](09_PrePro1/data/meta.txt)). Damit R dies als Datum-/Zeitangabe erkennt, müssen wir die Spalte in einem R-Format (`POSIXct`) einlesen und dabei R mitteilen, wie sie aktuell formatiert ist. Lies die Spalte mit `as.POSIXct()` (oder `parse_datetime`) ein und spezifiziere sowohl `format` wie auch `tz`. 

Tipps: 

- Wenn keine Zeitzone festgelegt wird, trifft `as.POSIXct()` eine Annahme (basierend auf `Sys.timezone()`). In unserem Fall handelt es sich aber um Werte in UTC (siehe [meta.txt](09_PrePro1/data/meta.txt))
- `as.POSIXct`erwartet `character`

```{r}
# Lösung Aufgabe 6

# mit readr
parse_datetime(as.character(wetter$time[1:10]), format = "%Y%m%d%H")


# mit as.POSIXct()
wetter$time <- as.POSIXct(as.character(wetter$time), format = "%Y%m%d%H",tz = "UTC")

```


```{r, purl=F}
knitr::kable(head(wetter,10))
```




### Aufgabe 7


Erstelle zwei neue Spalten mit Wochentag (Montag, Dienstag, etc) und Kalenderwoche. Verwende dazu die neu erstellte `POSIXct`-Spalte


```{r}

# Lösung Aufgabe 7

wetter$wochentag <- wday(wetter$time,label = T)
wetter$kw <- week(wetter$time)

```


```{r, purl=F}
knitr::kable(head(wetter,10))
```





### Aufgabe 8


Erstelle eine neue Spalte basierend auf die Temperaturwerte mit der Einteilung "kalt" (Unter Null Grad) und "warm" (über Null Grad)

```{r}

# Lösung Aufgabe 8

wetter$temp_kat[wetter$tre200h0>0] <- "warm"
wetter$temp_kat[wetter$tre200h0<=0] <- "kalt"
```


```{r, purl=F}
knitr::kable(head(wetter,10))
```


```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:09_PrePro1/Uebung_A.Rmd-->

## Übung A Lösung

[R-Script als Download](09_PrePro1/RFiles/Uebung_A.R)

```{r code=readLines('09_PrePro1/RFiles/Uebung_A.R'), echo=T, eval=F}
```

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:09_PrePro1/Uebung_A_loesung.Rmd-->


```{r, include=FALSE, purl = F}

knitr::opts_chunk$set(echo = F, include = T, collapse=TRUE)
# knitr::opts_knit$set(root.dir = "09_PrePro1") 

```



## Übung B 




```{r, message = F}
library(tidyverse)
```



Fahre mit dem Datensatz `wetter` aus Übung A fort. 
```{r, purl=F}
wetter <- read_table("09_PrePro1/data/order_52252_data.txt",
                  col_types = list(
                    col_factor(levels = NULL),    
                    col_datetime(format = "%Y%m%d%H"),
                    col_double()
                    )
                  )
```


### Aufgabe 1

Nutze `plot()` um die Temparaturkurve zu visualisieren. Verwende aber vorher `filter()` um dich auf eine Station (z.B. "`ABO`") zu beschränken (es handelt sich sonst um zuviele Datenpunkte).



```{r}
# Lösung Aufgabe 1

wetter_fil <- dplyr::filter(wetter, stn == "ABO")

plot(wetter_fil$time,wetter_fil$tre200h0, type = "l")

```


Nun schauen wir uns das plotten mit `ggplot2` an. Ein simpler Plot wie der in der vorherigen Aufgabe ist in `ggplot2` zugegebenermassen *etwas* komplizierter. `ggplot2` wird aber rasch einfacher, wenn die Grafiken komplexer werden. Wir empfehlen deshalb stark, `ggplot2` zu verwenden.

Schau dir ein paar online Tutorials zu `ggplot2` an (siehe ^[@wickham2017, Kapitel 1 bzw. [http://r4ds.had.co.nz/data-visualisation.html](http://r4ds.had.co.nz/data-visualisation.html) oder hier ein sehr schönes Video: [Learn R: An Introduction to ggplot2](https://youtu.be/YxKr2a-Y1WE?t=1m40s)]) 
und reproduziere den obigen Plot mit `ggplot2`


```{r}

p <- ggplot(wetter_fil, aes(time,tre200h0)) +
  geom_line()

p
```



### Aufgabe 2

Spiele mit Hilfe der erwähnten Tutorials mit dem Plot etwas rum. Versuche die x-/y-Achsen zu beschriften sowie einen Titel hinzu zu fügen.

```{r}
# Lösung Aufgabe 2
p <- p +
  labs(x = "Datum", y = "Temperatur", title = "Stündlich gemittelte Temperaturwerte")

p
```


### Aufgabe 3

Reduziere den x-Achsenausschnitt auf einen kleineren Zeitraum, beispielsweise einn beliebigen Monat. Verwende dazu `lims()` zusammen mit `as.POSIXct()` oder mache ein Subset von deinem Datensatz mit einer convenience-Variabel und `filter()`.

```{r}
# Lösung Aufgabe 3

limits <- as.POSIXct(c("2002-01-01 00:00:00","2002-02-01 00:00:00"),tz = "UTC")

p +
  lims(x = limits)
```



```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:09_PrePro1/Uebung_B.Rmd-->

## Übung B Lösung

[R-Code als Download](09_PrePro1/Uebung_B.R)

```{r code=readLines('09_PrePro1/RFiles/Uebung_B.R'), echo=T, eval=F}
```

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:09_PrePro1/Uebung_B_loesung.Rmd-->

# PrePro2 (16.10.2018)

Die Lerneinheit vermittelt zentralste Fertigkeiten zur Vorverarbeitung von strukturierten Daten in der umweltwissenschaftlichen Forschung: Datensätze verbinden (Joins) und umformen („reshape“, „split-apply-combine“). Im Anwendungskontext haben Daten selten von Anfang an diejenige Struktur, welche für die statistische Auswertung oder für die Informationsvisualisierung erforderlich wäre. In dieser Lerneinheit lernen die Studierenden die für diese oft zeitraubenden Preprocessing-Schritte notwendigen Konzepte und R-Werkzeuge kennen und kompetent anzuwenden.
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:10_PrePro2/Abstract.Rmd-->

```{r, include=F, purl=F}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,include = T, collapse=TRUE)
```

## Ergänzungen zu PrePro 1

### Integer mit "L"

In `R` kann eine Zahl mit dem Suffix "L" explizit als Integer spezifiziert werden. 

```{r, parse = F}
typeof(42)
typeof(42L)
```


Warum dazu der Buchstabe "L" verwendet wird ist nirgends offiziell Dokumentiert (zumindest haben wir nichts gefunden). Die gängigste Meinung, die auch [von renommierten R-Profis vertreten wird](https://hypatia.math.ethz.ch/pipermail/r-devel/2017-June/074467.html
) ist, dass damit `Long integer` abgekürzt wird.



### Arbeiten mit RStudio "Project"

Wir empfehlen die Verwendung von "Projects" innerhalb von RStudio. RStudio legt für jedes Projekt dann einen Ordner an, in welches die Projekt-Datei abgelegt wird (Dateiendung .Rproj). Sollen innerhalb des Projekts dann R-Skripts geladen oder erzeugt werden, werden diese dann auch im angelegten Ordner abgelegt. Mehr zu RStudio Projects findet ihr  [hier](https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects).


Das Verwenden von Projects bringt verschiedene Vorteile, wie zum Beispiel:

- Festlegen der Working Directory ohne die Verwendung des expliziten Pfades (`setwd()`). Das ist sinnvoll, da sich dieser Pfad ändern kann (Zusammenarbeit mit anderen Usern, Ausführung des Scripts zu einem späteren Zeitpunkt) 
- Automatisches Zwischenspeichern geöffneter Scripts und Wiederherstellung der geöffneten Scripts bei der nächsten Session
- Festlegen verschiedener projektspezifischer Optionen
- Verwendung von Versionsverwaltungssystemen (Github oder SVN)



### Arbeiten mit `factors`

Wie bereits angedeutet, ist das Arbeiten mit `factors` etwas gewöhnungsbedürftig. Wir gehen hier auf ein paar Stolpersteine ein.

```{r}
zahlen <- factor(c("null","eins","zwei","drei"))

zahlen
```

Offensichtlich sollten diese `factors` geordnet sein, R weiss davon aber nichts. Eine Ordnung kann man mit dem Befehl `ordered = T` festlegen. 

Beachtet: `ordered = T` kann nur bei der Funktion `factor()` spezifiziert werden, nicht bei `as.factor()`. Ansonsten sind `factor()` und `as.factor()` sehr ähnlich.


```{r}
zahlen <- factor(zahlen,ordered = T)

zahlen
```

Beachtet das "<"-Zeichen zwischen den Levels. Die Zahlen werden nicht in der korrekten Reihenfolge, sondern Alphabetisch geordnet. Die richtige Reihenfolge kann man mit `levels = ` festlegen.

```{r}
zahlen <- factor(zahlen,ordered = T,levels = c("null","eins","zwei","drei","vier"))

zahlen
```

Wie auch schon erwähnt werden `factors` als `character` Vektor dargestellt, aber als Integers gespeichert. Das führt zu einem scheinbaren Wiederspruch wenn man den Datentyp auf unterschiedliche Weise abfragt.
```{r}
typeof(zahlen)

is.integer(zahlen)
```


Mit `typeof()` wird eben diese Form der Speicherung abgefragt und deshalb mit `integer` beantwortet. Da es sich aber nicht um einen eigentlichen Integer Vektor handelt, wird die Frage `is.integer()` mit `FALSE` beantwortet. Das ist etwas verwirrend, beruht aber darauf, dass die beiden Funktionen die Frage von unterschiedlichen Perspektiven beantworten. In diesem Fall schafft `class()` Klarheit:

```{r}
class(zahlen)
```


Wirklich verwirrend wird es, wenn `factors` in numeric umgewandelt werden sollen.

```{r}
zahlen
as.integer(zahlen)
```

Das die Übersetzung der auf Deutsch ausgeschriebenen Nummern in nummerische Zahlen nicht funktionieren würde, war ja klar. Weniger klar ist es jedoch, wenn die `factors` bereits aus nummerischen Zahlen bestehen.

```{r}
zahlen2 <- factor(c("3","2","1","0"))

as.integer(zahlen2)

```

In diesem Fall müssen die `factors` erstmals in `character` umgewandelt werden.

```{r}
zahlen2 <- factor(c("3","2","1","0"))

as.integer(as.character(zahlen2))
```




### Heikle Annahmen - bessere Alternativen

Aus oben beschriebenen Grund ist es auch problematisch, dass `data.frame()` sowie alle `read.*` Funktionen (`read.table`, `read.csv` etc) immer davon ausgehen, dass `Strings` als `factors` interpretiert werden sollten. Es gibt in Base R einige Funktionen, welche Annahmen treffen die problematisch sein können. Ein weiteres Beispiel ist die Annahme der Zeitzone und Verwendung von Sommerzeit bei `as.POSIXct()`.

Oft gibt es dafür im Tidyverse alternative Funktionen, in denen diese Probleme besser gelöst sind. Wir empfehlen, wenn immer Möglich die Tidyverse-Alternativen zu verwenden. Beispiele:

- `data_frame()` statt `data.frame()` 
- `read_*` statt `read.*`
- `parse_datetime` statt `as.POSIXct()`


Beim Import von Daten kann es sinnvoll sein, die Datentypen der Spalten bereits _im Importbefehl_ zu spezifizieren. So vermeidet man die anschliessende  Typumwandlung und die damit verbundenen Fehlerquellen. Zudem wird der Importprozess beschleunigt, da R keine Zeit daran verschwenden muss die Datentypen (aufgrund der ersten 1000 Zeilen) zu erraten.

```{r,message=F}

library(tidyverse)

df1 <- read_table("09_PrePro1/data/order_52252_data.txt",
                  col_types = list(
                    col_character(),                  # Macht aus der 1.Spalte ein character
                    col_datetime(format = "%Y%m%d%H"),# Macht aus der 2.Spalte ein POSIXct
                    col_double()                      # Macht aus der 3.Spalte ein double
                    )
                  )

df1

df1 <- read_table("09_PrePro1/data/order_52252_data.txt",
                  col_types = list(
                    col_factor(levels = NULL),        # Macht aus der 1.Spalte ein factor
                    col_datetime(format = "%Y%m%d%H"),# Macht aus der 2.Spalte ein POSIXct
                    col_double()                      # Macht aus der 3.Spalte ein double
                    )
                  )


df1

```


```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:10_PrePro2/Nachtrag.Rmd-->


```{r, include=F, purl=F}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,include = T, collapse=TRUE)
```



## Demo: `tidyverse`

[Demoscript als Download](10_PrePro2/RFiles/Demo_Tidyverse.R)


Hier möchten wir euch mit einer Sammlung von Tools vertraut machen, die spezifisch für das Daten prozessieren in Data Science entwickelt wurden. Der Prozess und das Modell ist hier^[http://r4ds.had.co.nz/introduction.html#] schön beschrieben.
Die Sammlung von Tools wird unter dem namen [tidyverse](https://www.tidyverse.org/) vertrieben, welches wir ja schon zu Beginn der ersten Übung installiert und geladen haben. Die Tools erleichtern den Umgang mit Daten ungeheuer und haben sich mittlerweile zu einem "must have" im Umgang mit Daten in R entwickelt. 

Wir können euch nicht sämtliche Möglichkeiten von tidyverse zeigen. Wir fokussieren uns deshalb auf einzelne Komponenten^[`dplyr, ggplot2, tidyr, stringr, magrittr, lubridate`] und zeigen ein paar Funktionalitäten, die wir oft verwenden und euch ggf. noch nicht bekannt sind. Wer sich vertieft mit dem Thema auseinandersetzen möchte, der sollte sich unbedingt das Buch @wickham2017 beschaffen. Eine umfangreiche, aber nicht ganz vollständige Version gibt es online^[http://r4ds.had.co.nz/], das vollständige eBook kann über die Bibliothek bezogen werden^[https://ebookcentral.proquest.com/lib/zhaw/detail.action?docID=4770093].



### Split-Apply-Combine

#### Packete laden

```{r,message=F}
library(tidyverse)
```

Mit `library(tidyverse)` werden nicht alle Packete geladen, die mit `install.packages(tidyverse)` intalliert wurden ([warum?](https://community.rstudio.com/t/which-packages-get-loaded/298)). Unter anderem muss `lubridate` noch separat geladen werden:

```{r, message=F}
library(lubridate) 
```



#### Daten Laden

Wir Laden die Wetterdaten von der letzten Übung.

```{r}

wetter <- read_table("09_PrePro1/data/order_52252_data.txt",
                  col_types = list(
                    col_factor(levels = NULL),    
                    col_datetime(format = "%Y%m%d%H"),
                    col_double()
                    )
                  )

```


#### Kennwerte berechnen
Wir möchten den Mittelwert aller gemessenen Temperaturwerte berechnen. Dazu könnten wir folgenden Befehl verwenden:

```{r}
mean(wetter$tre200h0, na.rm = T) 
```

Die option `na.rm = T` bedeutet, dass NA Werte von der Berechnung ausgeschlossen werden sollen. 

Mit der selben Herangehensweise können diverse Werte berechnet werden (z.B. das Maximum (`max()`), Minimum (`min()`), Median (`median()`) u.v.m.). 

Diese Herangehensweise funktioniert nur dann gut, wenn wir die Kennwerte über *alle* Beobachtungen (Zeilen) für eine Variable (Spalte) berechnen wollen. Sobald wir die Beobachtungen gruppieren wollen, wird es schwierig. Zum Beispiel, wenn wir die durchschnittliche Temperatur *pro Jahr* berechnen wollen.


#### Convenience Variablen

Um diese Aufgabe zu lösen, muss zuerst das "Jahr" berechnen können (das Jahr ist die *convenience variabel*).   Hierfür brauchen wir die Funktion `year()` (von `lubridate`). 

Nun kann kann die **convenience Variable** "Jahr" erstellt werden. Ohne `dpylr` wird eine neue Spalte wird folgendermassen hinzugefügt. 
```{r}
wetter$year <- year(wetter$time)
```


Mit `dplyr` (siehe ^[@wickham2017, Kapitel 10 / http://r4ds.had.co.nz/transform.html]) sieht der gleiche Befehl folgendermassen aus:
```{r}
wetter <- mutate(wetter,year = year(time))
```

Der grosse Vorteil von `dplyr` ist an dieser Stelle noch nicht ersichtlich. Dieser wird aber später klar.


#### Kennwerte nach Gruppen berechnen

Jetzt kann man die `data.frame` mithilfe der Spalte "Jahr" filtern. 
```{r}
mean(wetter$tre200h0[wetter$year == 2000], na.rm = T)
```

Dies müssen wir pro Jahr wiederholen, was natürlich sehr umständlich ist, v.a. wenn man eine Vielzahl an Gruppen hat (z.B. Kalenderwochen statt Jahre). Deshalb nutzen wir das package `dplyr`. Damit geht die Aufgabe (Temperaturmittel pro Jahr berechnen) folgendermassen:


```{r}
summarise(group_by(wetter,year),temp_mittel = mean(tre200h0, na.rm = T))
```


#### Verketten vs. verschachteln

Auf Deutsch übersetzt heisst die obige Operation folgendermassen: 

1) nimm den Datensatz `wetter`
2) Bilde Gruppen pro Jahr  (`group_by(wetter,year)`) 
3) Berechne das Temperaturmittel (`mean(tre200h0)`)

Diese Übersetzung `R`-> Deutsch unterscheidet sich vor allem darin, dass die Operation auf Deutsch *verkettet* ausgesprochen wird (Operation 1->2->3) während der Computer *verschachtelt* liest 3(2(1)). Um `R` näher an die gesprochene Sprache zu bringen, kann man den `%>%`-Operator verwenden  (siehe ^[@wickham2017, Kapitel 14 / http://r4ds.had.co.nz/pipes.html]). 
```{r, eval = F}

summarise(group_by(wetter,year),temp_mittel = mean(tre200h0))

# wird zu:

wetter %>%                                #1) nimm den Datensatz "wetter"
  group_by(year) %>%                      #2) Bilde Gruppen pro Jahr
  summarise(temp_mittel = mean(tre200h0)) #3) berechne das Temperaturmittel 

```


Dieses Verketten mittels `%>%` macht ein Code einiges schreib- und leserfreundlicher, und wir werden ihn in den nachfolgenden Übungen verwenden. Dabei handelt es sich um das package `magrittr`, welches mit dem `tidyverse` mitgeliefert wird. 

Zu `dplyr` und `magrittr`gibt es etliche Tutorials online (siehe^[@wickham2017, Kapitel 10 / http://r4ds.had.co.nz/transform.html, oder [Hands-on dplyr tutorial..](https://youtu.be/jWjqLW-u3hc)]), deshalb werden wir diese Tools nicht in allen Details erläutern. Nur noch folgenden wichtigen Unterschied zu zwei wichtigen Funktionen in `dpylr`: `mutate()` und `summarise()`.

- `summarise()` fasst einen Datensatz zusammen. Dabei reduziert sich die Anzahl Beobachtungen (Zeilen) auf die Anzahl Gruppen (z.B. eine zusammengefasste Beobachtung (Zeile) pro Jahr). Zudem reduziert sich die Anzahl Variablen (Spalten) auf diejenigen, die in der "summarise" Funktion spezifiziert wurde (z.B. `temp_mittel`)
- mit `mutate` wird eine `data.frame` vom Umfang her belassen, es werden lediglich *zusätzliche* Variablen (Spalten) hinzugefügt (siehe Beispiel unten)

```{r, eval=T}
# Maximal und minimal Temperatur pro Kalenderwoche
wetter %>%                              #1) nimm den Datensatz "wetter"
  filter(stn == "ABO") %>%              #2) filter auf Station namnes "ABO"
  mutate(kw = week(time)) %>%       #3) erstelle eine neue Spalte "kw"
  group_by(kw) %>%                      #4) Nutze die neue Spalte um Guppen zu bilden
  summarise(
    temp_max = max(tre200h0, na.rm = T),#5) Berechne das Maximum 
    temp_min = min(tre200h0, na.rm = T) #6) Berechne das Minimum
    )   
```


#### Resultate plotten

Mit diesen Tools können wir nun auch eine neue Grafik plotten, ähnlich wie in der Übung 1. Dafür müssen wir die ganzen Operationen aber zuerst in einer Variabel speichern (bis jetzt hat R zwar alles schön berechnet, aber uns nur auf die Konsole ausgegeben).

```{r, warning = F}

wetter_sry <- wetter %>%                              
  mutate(
    kw = week(time)
    ) %>%
  filter(stn == "ABO") %>%
  group_by(kw) %>%                      
  summarise(
    temp_max = max(tre200h0),               
    temp_min = min(tre200h0),
    temp_mean = mean(tre200h0)
    )  
```

Dieses mal plotten wir nur mit `ggplot2` (siehe ^[@wickham2017, Kapitel 1 / http://r4ds.had.co.nz/data-visualisation.html oder hier ein sehr schönes Video: [Learn R: An Introduction to ggplot2](https://youtu.be/YxKr2a-Y1WE?t=1m40s)]) 
 
```{r}

ggplot() +
  geom_line(data = wetter_sry, aes(kw,temp_max), colour = "yellow") +
  geom_line(data = wetter_sry, aes(kw,temp_mean), colour = "pink") +
  geom_line(data = wetter_sry, aes(kw,temp_min), colour = "black") +
  labs(y = "temp")

```


Ok das sieht schon mal gut aus. Nur, wir mussten pro Linie einen eigene Zeile schreiben (`geom_line()`) und dieser eine Farbe zuweisen. Bei drei Werten ist das ja ok, aber wie sieht es den aus wenn es Hunderte sind? Da hat ggplot natürlich eine Lösung, dafür müssen aber alle Werte in *einer* Spalte daher kommen. Das ist ein häufiges Problem: Wir haben eine *breite* Tabelle (viele Spalten), bräuchten aber eine *lange* Tabelle (viele Zeilen).


### Reshaping data

#### Breit -> lang

Da kommt `tidyverse` wieder ins Spiel. Die Umformung von Tabellen *breit*->*lang* erfolgt mittels `tidyr`(siehe ^[http://r4ds.had.co.nz/tidy-data.html#gathering]). Auch dieses package funktioniert wunderbar mit piping (`%>%`). 

```{r}
wetter_sry_long <- wetter_sry %>%
  gather(Key, Value, c(temp_max,temp_min,temp_mean))

```

Im Befehl `gather()` braucht es drei Werte:

- beliebiger Name der neuen Variablen (Spalte) für die *Schlüssel*: "temp_mean", "temp_min"... (ich verwenden den Namen: `Key`)
- beliebiger Name der neuen Variablen (Spalte) für die effektiven *Werte*: 5°C, 10°C (ich verwenden den Namen: `Value`)
- Name der (bestehenden) Variablen (Spalten), die zusammen gefasst werden sollten: `(hier: temp_max,temp_min,temp_mean`)


Die ersten 6 Zeilen von `wetter_sry`:
```{r, echo = F, purl = F}
kable(head(wetter_sry,6))
```

Die ersten 6 Zeilen von `wetter_sry_long`:
```{r, echo = F, purl=F}
kable(head(arrange(wetter_sry_long,kw),6))

```

Beachte: `wetter_sry_long` umfasst 159 Beobachtungen (Zeilen), das sind 3 mal soviel wie `wetter_sry`, da wir ja drei Spalten zusammengefasst haben.
```{r}
nrow(wetter_sry)
nrow(wetter_sry_long)
```


Statt die Variablen (Spalten) zu benennen, die zusammengefasst werden sollten, wäre es in unserem Fall einfacher, die Variablen (Spalten) zu benennen die *nicht* zusammengefasst werden sollen (`kw`):

```{r,}
wetter_sry_long <- wetter_sry %>%
  gather(Key, Value, -kw)
```

Nun können wir den obigen Plot viel einfacher erstellen:

```{r}
ggplot(wetter_sry_long, aes(kw,Value, colour = Key)) +
  geom_line()
```

Beachtet, dass wir gegenüber dem letzten Plot `colour` nun *innerhalb* von `aes()` festlegen und nicht mit einem expliziten Farbwert, sondern mit dem Verweis auf die Spalte `key`.


#### Lang -> breit

Um unsere *lange* Tabelle wieder zurück in eine *breite* zu überführen, brauchen wir lediglich ein Befehl (`spread`):

```{r}
wetter_sry_long %>%
  spread(Key,Value)
```


### Quellen

```{r code=readLines('00_Admin/get_chapter_references.R'), echo=F, eval=T,purl = F, results="asis"}
```

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:10_PrePro2/Demo_Tidyverse.Rmd-->

```{r include=FALSE, purl=F}
knitr::opts_chunk$set(echo = TRUE, include = F, collapse=TRUE)
# knitr::opts_knit$set(root.dir = "10_PrePro2") 

select <- dplyr::select


```

## Übung A


```{r,message=F}
library(tidyverse)
library(lubridate)
library(stringr)
```


### Aufgabe 1

Lade die Wetterdaten aus der letzten Übung.

```{r}
# Lösung Aufgabe 1

wetter <- read_table("09_PrePro1/data/order_52252_data.txt",
                  col_types = list(
                    col_factor(levels = NULL),    
                    col_datetime(format = "%Y%m%d%H"),
                    col_double()
                    )
                  )
```


### Aufgabe 2

Bereinige den Datensatz. Entferne z.B. alle Zeilen, bei dem der Stationsnahme oder Temperaturwerte fehlen 

```{r}
# Lösung Aufgabe 2

wetter <- wetter %>%
  filter(!is.na(stn)) %>%
  filter(!is.na(tre200h0))

```



### Aufgabe 3

Überführe die **lange** Tabelle über in eine breite. Dabei sollte jede Station eine eigene Spalte enthalten (`key`), gefüllt mit den Temperaturwerten (`value`).  Speichere diese Tabelle in einer neuen Variabel.

```{r}

# Lösung Aufgabe 3

wetter_spread <- spread(wetter, stn,tre200h0)


```



### Aufgabe 4



Importiere die Datei [order_52252_legend.csv](09_PrePro1/data/order_52252_legend.csv) (z.B. mit `read_delim`).

Hinweis: Wenn Umlaute und Sonderzeichen nicht korrekt dargestellt werden (z.B. in Gen*è*ve), hat das vermutlich mit der [Zeichencodierung](https://de.wikipedia.org/wiki/Zeichenkodierung) zu tun. Das File ist aktuell in 'ANSI' Codiert, welche für gewisse Betriebssysteme / R-Versionen ein Problem darstellt. Um das Problem zu umgehen muss man das File mit einem Editor öffnen (Windows 'Editor' oder 'Notepad++', Mac: 'TextEdit') und mit einer neuen Codierung (z.B 'UTF-8') abspeichern. Danach kann die Codierung spezifitiert werden (bei `read_delim(): mit `locale = locale(encoding = "UTF-8")`)

```{r}

# Lösung Aufgabe 4

wetter_legende <- read_delim("09_PrePro1/data/order_52252_legend.csv",delim = ";", locale = locale(encoding = "UTF-8"))

```



### Aufgabe 5


Die x-/y-Koordinaten sind aktuell in einer Spalte erfasst. Um mit den Koordinaten sinnvoll arbeiten zu können, brauchen wir die Koordinaten getrennt. Trenne die x und y Koordinaten aus der Spalte `Koordinaten` (Tipp: nutze Excel oder `str_split_fixed()` aus `stringr`).

```{r}

# Lösung Aufgabe 5

koordinaten <- str_split_fixed(wetter_legende$Koordinaten, "/", 2)

colnames(koordinaten) <- c("x","y")

wetter_legende <- cbind(wetter_legende,koordinaten)
```


### Aufgabe 6

Nun wollen wir den Datensatz `wetter`mit den Informationen aus `wetter_legende`anreichern. Uns interessiert aber nur das Stationskürzel, der Name, die x/y Koordinaten sowie die Meereshöhe. Lösche die nicht benötigten Spalten (oder selektiere die benötigten Spalten).

Tipp: Nutze `select()` von `dplyr`

```{r, message=F}

# Lösung Aufgabe 6

wetter_legende <- dplyr::select(wetter_legende, stn, Name, x,y,Meereshoehe)
```


### Aufgabe 7

Nun ist der Datensatz `wetter_legende`genügend vorbereitet. Jetzt kann er mit dem Datensatz `wetter` verbunden werden. Überlege dir, welcher Join dafür sinnvoll ist und mit welchem Attribut wir "joinen" können.

Nutze die Join-Möglichkeiten von `dplyr` (Hilfe via `?dplyr::join`)  um die Datensätze `wetter` und `wetter_legende`zu verbinden.

```{r}

# Lösung Aufgabe 7

wetter <- left_join(wetter,wetter_legende,by = "stn")

# Jointyp: Left-Join auf 'wetter', da uns nur die Stationen im Datensatz 'wetter' interessieren.
# Attribut: "stn"
```

### Aufgabe 8

Berechne die Durchschnittstemperatur pro Station. Nutze dabei `dplyr::summarise()` und wenn möglich `%>%`. Speichere das Resultat in einer neuen Variabel.


```{r,warning=F}

# Lösung Aufgabe 8

wetter_sry <- wetter %>%
  group_by(stn) %>%
  summarise(temp_mean = mean(tre200h0))
```

### Aufgabe 9

Nun wollen wir das Resultat aus Aufgabe 7 nutzen, um die Durchschnittstemperatur der Meereshöhe gegenüber zu stellen. Dummerweise ging das Attribut `Meereshoehe` bei der `summarise()` Operation verloren (da bei `summarise()` alle Spalten weg fallen, die **nicht** in `group_by()` definiert wurden). Um die Spalte `Meereshoehe` beizubehalten, muss sie also unter `group_by()` aufgelistet werden. 

Wiederhole Übung 7 und siehe zu, dass die Meereshöhe beibehalten wird. Stelle danach in einem Scatterplot (wenn möglich mit `ggplot()`) die Meereshöhe der Durchschnittstemperatur gegenüber.

```{r}
# Lösung Aufgabe 9

wetter_sry <- wetter %>%
  group_by(stn,Meereshoehe) %>%
  summarise(temp_mean = mean(tre200h0))

# Achtung: wenn mehrere Argumente in group_by() definiert werden führt das 
# üblicherweise zu Untergruppen. In unserem Fall hat jede Station nur EINE 
# Meereshöhe, deshalb wird die Zahl der Gruppen nicht erhöht.
```


```{r}

ggplot(wetter_sry, aes(temp_mean,Meereshoehe)) +
  geom_point()
```


```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:10_PrePro2/Uebung_A.Rmd-->

## Übung A: Lösung

[R-Code als Download](10_PrePro2/RFiles/Uebung_A.R)


```{r code=readLines('10_PrePro2/RFiles/Uebung_A.R'), echo=T, eval=F, include = T}
```

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:10_PrePro2/Uebung_A_loesung.Rmd-->

```{r, include=FALSE, purl = F}
knitr::opts_chunk$set(collapse=TRUE)
# knitr::opts_knit$set(root.dir = "10_PrePro2")

```

## Übung B


```{r,message=F}
library(tidyverse)
library(lubridate)
library(stringr)
```



### Aufgabe 1

Gegeben sind die Daten von drei Sensoren ([sensor1.csv](10_PrePro2/data/sensor1.csv), [sensor2.csv](10_PrePro2/data/sensor2.csv), [sensor3.csv](10_PrePro2/data/sensor3.csv)). Lade die Datensätze runter und lese sie ein.



```{r, message=F}
# Lösung Aufgabe 1

sensor1 <- read_delim("10_PrePro2/data/sensor1.csv",";")
sensor2 <- read_delim("10_PrePro2/data/sensor2.csv",";")
sensor3 <- read_delim("10_PrePro2/data/sensor3.csv",";")

```

### Aufgabe 2



Füge die drei Tabellen zu **einer** zusammen. Dazu kannst du entweder die  Spalten (Variablen) mittels `join()` oder die Zeilen (Beobachtungen) mittels `rbind()` zusammen "kleben".  Überführe zudem die Spalte `Datetime` in ein `POSIXct`-Format. Das ursprüngliche Format lautet:`DDMMYYYY_HHMM`


```{r}

# Lösung Aufgabe 2 (Var 1: Spalten [Variabeln] zusammen 'kleben')
sensor_all <- sensor1 %>%
  rename(sensor1 = Temp) %>%              # Spalte "Temp" in "sensor1" umbenennen
  full_join(sensor2,by = "Datetime") %>%    
  rename(sensor2 = Temp) %>%
  full_join(sensor3, by = "Datetime") %>%
  rename(sensor3 = Temp) %>%
  mutate(Datetime = as.POSIXct(Datetime,format = "%d%m%Y_%H%M"))
```




```{r}

# Lösung Aufgabe 2 (Var 2: Zeilen [Beobachtungen] zusammen 'kleben)

sensor1$sensor <- "sensor1"
sensor2$sensor <- "sensor2"
sensor3$sensor <- "sensor3"

sensor_all <- rbind(sensor1,sensor2,sensor3)

sensor_all <- sensor_all %>%
  mutate(
    Datetime = as.POSIXct(Datetime,format = "%d%m%Y_%H%M")
  ) %>%
  spread(sensor, Temp)

```


```{r,  echo = F, eval = T, purl=F}
# Die neue Tabelle sollte folgendermassen aussehen:
knitr::kable(sensor_all)
```



### Aufgabe 3

Importiere die Datei [sensor_1_fail.csv](10_PrePro2/data/sensor_fail.csv) in `R`.


```{r, message = F}

# Lösung Aufgabe 3

sensor_fail <- read_delim("10_PrePro2/data/sensor_fail.csv", delim = ";")

```


```{r,  echo = F, eval = T, purl=F}
knitr::kable(sensor_fail)
```


`sensor_fail.csv` hat eine Variabel `SensorStatus`: `1` bedeutet der Sensor misst, `0` bedeutet der Sensor miss nicht. Fälschlicherweise wurde auch dann der Messwert `Temp = 0` erfasst, wenn `Sensorstatus = 0`. Richtig wäre hier `NA` (not available). Korrigiere den Datensatz entsprechend.


```{r}

# Lösungsweg 1
sensor_fail$Datetime <- as.POSIXct(sensor_fail$Datetime,format = "%d%m%Y_%H%M")

sensor_fail$`Hum_%`[sensor_fail$SensorStatus == 0] <- NA
sensor_fail$Temp[sensor_fail$SensorStatus == 0] <- NA
```


```{r, message = F}

# Lösungsweg 2

sensor_fail <- read_delim("10_PrePro2/data/sensor_fail.csv", delim = ";")


sensor_fail_corr <- sensor_fail %>%
  mutate(
    Datetime = as.POSIXct(Datetime,format = "%d%m%Y_%H%M")
  ) %>%
  rename(Humidity = `Hum_%`) %>%         # Weil R "%" in Headers nicht mag
  gather(key,val, c(Temp, Humidity)) %>%
  mutate(
    val = ifelse(SensorStatus == 0,NA,val)
  ) %>%
  spread(key,val)
  
```


### Aufgabe 4


Warum spielt das es eine Rolle, ob `0` oder `NA` erfasst wird? Vergleiche dazu die Mittlere Temperatur / Feuchtigkeit vor und nach der Korrektur. 

```{r}

# Lösung Aufgabe 4

# Mittelwerte der unkorrigierten Sensordaten (`NA` als `0`)
mean(sensor_fail$Temp)
mean(sensor_fail$`Hum_%`)

```



```{r}
# Mittelwerte der korrigierten Sensordaten (`NA` als `NA`). Hier müssen wir die Option 
# `na.rm = T` (Remove NA = T) wählen, denn `mean()` (und ähnliche Funktionen) retourieren 
# immer `NA`, sobald ein **einzelner** Wert in der Reihe `NA`ist.
mean(sensor_fail_corr$Temp, na.rm = T)
mean(sensor_fail_corr$Humidity, na.rm = T)

```



```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:10_PrePro2/Uebung_B.Rmd-->

## Übung B: Lösung

[R-Code als Download](10_PrePro2/RFiles/Uebung_B.R)


```{r code=readLines('10_PrePro2/RFiles/Uebung_B.R'), echo=T, eval=F, include = T}
```

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:10_PrePro2/Uebung_B_loesung.Rmd-->

# InfoVis1 (22.10.2018)

Die konventionelle schliessende Statistik arbeitet in der Regel konfirmatorisch, sprich aus der bestehenden Theorie heraus werden Hypothesen formuliert, welche sodann durch Experimente geprüft und akzeptiert oder verworfen werden. Die Explorative Datenanalyse (EDA) nimmt dazu eine antagonistische Analyseperspektive ein und will in den Daten zunächst Zusammenhänge aufdecken, welche dann wiederum zur Formulierung von prüfbaren Hypothesen führen kann. Die Einheit stellt dazu den klassischen 5-stufigen EDA-Prozess nach Tukey (1980!) vor. Abschliessend wird dann noch die Brücke geschlagen zur modernen Umsetzung der EDA in Form von Visual Analytics.
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:11_InfoVis1/Abstract.Rmd-->

```{r, include=F, purl=F}

knitr::opts_chunk$set(echo = T,include = T,message = F, collapse=TRUE)
# knitr::opts_knit$set(root.dir = "11_InfoVis1") 

```




## EDA Beispiel Vorlesung

[Demoscript als Download](11_InfoVis1/RFiles/Demo_EDA.R)


```{r,message=F}


library(tidyverse)
library(scales)

# create some data about age and height of people
people <- data.frame(
  ID = c(1:30),
  
  age = c(5.0, 7.0, 6.5 ,9.0, 8.0, 5.0, 8.6, 7.5, 9.0, 6.0,
          63.5 ,65.7, 57.6, 98.6, 76.5, 78.0, 93.4, 77.5, 256.6, 512.3,
          15.5, 18.6, 18.5, 22.8, 28.5, 39.5, 55.9, 50.3, 31.9, 41.3),
  
  height = c(0.85, 0.93, 1.1, 1.25, 1.33, 1.17, 1.32, 0.82, 0.89, 1.13,
             1.62, 1.87, 1.67, 1.76, 1.56, 1.71, 1.65, 1.55, 1.87, 1.69,
             1.49, 1.68, 1.41, 1.55, 1.84, 1.69, 0.85, 1.65, 1.94, 1.80),
  
  weight = c(45.5, 54.3, 76.5, 60.4, 43.4, 36.4, 50.3, 27.8, 34.7, 47.6,
             84.3, 90.4, 76.5, 55.6, 54.3, 83.2, 80.7, 55.6, 87.6, 69.5,
             48.0, 55.6, 47.6, 60.5, 54.3, 59.5, 34.5, 55.4, 100.4, 110.3)
)



# build a scatterplot for a first inspection
ggplot(people, aes(x=age, y=height)) + 
  geom_point() + 
  scale_y_continuous(limits=c(0.75, 2.0)) +
  theme(axis.text=element_text(size=12), axis.title=element_text(size=20,face="bold"))

# Go to help page: http://docs.ggplot2.org/current/ -> Search for icon of fit-line
# http://docs.ggplot2.org/current/geom_smooth.html

# build a scatterplot for a first inspection, with regression line
ggplot(people, aes(x=age, y=height)) + 
  geom_point() + 
  scale_y_continuous(limits=c(0, 2.0)) +
  geom_smooth(method="loess", fill='lightblue', size=0.5, alpha=0.5) +
  theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face="bold"))

?stem

# stem and leaf plot
stem(people$height)
stem(people$height, scale=2)

# explore the two variables with box-whiskerplots
summary(people$age)
boxplot(people$age)

boxplot(people$age)

summary(people$height)
boxplot(people$height)

boxplot(people$height)


# explore data with a histgram
ggplot(people, aes(x=age)) + 
  geom_histogram(stat="bin", fill='green', binwidth=20) + 
  theme_bw() + labs(x = '\nage', y = 'count\n') +
  theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face="bold")) 

density(x = people$height)


# re-expression: use log or sqrt axes
#
# Find here guideline about scaling axes 
# http://www.cookbook-r.com/Graphs/Axes_(ggplot2)/
# http://docs.ggplot2.org/0.9.3.1/scale_continuous.html


# logarithmic axis: respond to skewness in the data, e.g. log10 
ggplot(people, aes(x=age, y=height)) + 
  geom_point() + 
  scale_y_continuous(limits=c(0, 2.0)) +
  geom_smooth(method="lm", fill='lightblue', size=0.5, alpha=0.5) +
  theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face="bold")) +
  scale_x_log10()

# logarithmic axis: show multiplicative factors, e.g. log2
ggplot(people, aes(x=age, y=height)) + 
  geom_point() + 
  scale_y_continuous(limits=c(0, 2.0)) +
  geom_smooth(method="lm", fill='lightblue', size=0.5, alpha=0.5) +
  theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face="bold")) +
  scale_x_continuous(trans = log2_trans(),
                   breaks = trans_breaks("log2", function(x) 2^x),
                   labels = trans_format("log2", math_format(2^.x)))


# outliers: Remove very small and very old people
peopleTemp <- subset(people, ID != 27) # Diese Person war zu klein.
peopleClean <- subset(peopleTemp, age < 100) # Fehler in der Erhebung des Alters

# re-explore cleaned data with a histgram
ggplot(peopleClean, aes(x=age)) + 
  geom_histogram(stat="bin", fill='#6baed6', binwidth=10) + 
  theme_bw() + labs(x = '\nage', y = 'count\n') +
  theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face="bold"))

ggplot(peopleClean, aes(x=age, y=height)) + 
  geom_point() + 
  scale_y_continuous(limits=c(0, 2.0)) +
  geom_smooth(method="lm", fill='lightblue', size=0.5, alpha=0.5) +
  theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face="bold"))

# with custom binwidth
ggplot(peopleClean, aes(x=age)) + 
  geom_histogram(stat="bin", fill='#6baed6', binwidth=10) + 
  theme_bw() + labs(x = '\nAlter', y = 'Anzahl\n')



# quadratic axis
ggplot(peopleClean, aes(x=age, y=height)) + 
  geom_point() + scale_y_continuous(limits=c(0, 2.0)) +
  geom_smooth(method="lm", fill='lightblue', size=0.5, alpha=0.5) +
  theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face="bold")) +
  scale_x_sqrt()




# subset "teenies": No trend
kids <- subset(peopleClean, age < 15)

ggplot(kids, aes(x=age, y=height)) + 
  geom_point() + 
  scale_y_continuous(limits=c(0, 2.0)) +
  geom_smooth(method="lm", fill='lightblue', size=0.5, alpha=0.5) +
  theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face="bold"))


# subset "teenies": No trend
oldies <- subset(peopleClean, age > 55)

ggplot(oldies, aes(x=age, y=height)) + 
  geom_point() + 
  scale_y_continuous(limits=c(0, 2.0)) +
  geom_smooth(method="lm", fill='lightblue', size=0.5, alpha=0.5) +
  theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face="bold"))


# Onwards towards multidimensional data

# Finally, make a scatterplot matrix
pairs(peopleClean[,2:4], panel=panel.smooth)

pairs(peopleClean[,2:4], panel=panel.smooth)

# Or as a bubble chart
peopleClean$radius <- sqrt( peopleClean$weight/ pi )
symbols(peopleClean$age, peopleClean$height, circles=peopleClean$radius)

symbols(peopleClean$age, peopleClean$height, circles=peopleClean$radius)

```

### Quellen

```{r code=readLines('00_Admin/get_chapter_references.R'), echo=F, eval=T,purl = F, results="asis"}
```
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:11_InfoVis1/Demo_EDA.Rmd-->


```{r, include=F, purl=F}

knitr::opts_chunk$set(echo = T,include = T,message = F, collapse=TRUE) # 
# knitr::opts_knit$set(root.dir = "11_InfoVis1") 

```

```{r, message = F}
library(tidyverse)
library(lubridate)

```

## Demo: `ggplot2`

[Demoscript als Download](11_InfoVis1/RFiles/Demo_ggplot.R)

Als erstes laden wir den Wetterdatensatz von der Übung Prepro1 ein.

```{r}
wetter <- read_table("09_PrePro1/data/order_52252_data.txt",
                  col_types = list(
                    col_factor(levels = NULL),    
                    col_datetime(format = "%Y%m%d%H"),
                    col_double()
                    )
                  )
```


```{r, echo = F, eval = T, purl=F}

knitr::kable(head(wetter))

```

Der Datensatz hat `r nrow(wetter)` Zeilen. Bevor wir mit plotten beginnen, müssen wir den Datensatz etwas filtern da die Plots ansonsten zu schwerfällig werden. Wir filtern deshalb auf Januar 2000.

```{r}
wetter_fil <- wetter %>%
  mutate(
    year = year(time),
    month = month(time)
    ) %>%
  filter(year == 2000 & month == 1)
```


Ein ggplot wird durch den Befehl `ggplot()` initiiert. Hier wird einerseits der Datensatz festgelegt, auf dem der Plot beruht (`data = `), sowie die Variablen innerhalb des Datensatzes, die Einfluss auf den Plot ausüben (`mapping = aes()`). 

Weiter braucht es *mindestens* ein "Layer" der beschreibt, wie die Daten dargestellt werden sollen (z.B. `geom_point()`).

Anders als bei "Piping" (`%>%`) wird ein Layer mit `+` hinzugefügt.

```{r}
# Datensatz: "wetter_fil" | Beeinflussende Variabeln: "time" und "tre200h0"
ggplot(data = wetter_fil, mapping = aes(time,tre200h0)) +
  # Layer: "geom_point" entspricht Punkten in einem Scatterplot 
  geom_point()                                                 

```


Da ggplot die Eingaben in der Reihenfolge `data = ` und dann `mapping = `erwartet, können wir diese Spezifizierungen auch weglassen.

```{r, eval=F}

ggplot(wetter_fil, aes(time,tre200h0)) +
  geom_point()

```

Nun wollen wir die unterschiedlichen Stationen unterschiedlich einfärben. Da wir Variablen definieren wollen, welche Einfluss auf die Grafik haben sollen, gehört diese Information in `aes()`.

```{r}
ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) +
  geom_point()
```

Wir können noch einen Layer mit Linien hinzufügen:

```{r}
ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) +
  geom_point() +
  geom_line()

```

Weiter können wir die Achsen beschriften und einen Titel hinzufügen. Zudem lasse ich die Punkte (`geom_point()`) nun weg, da mir diese nicht gefallen.

```{r}
ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) +
  geom_line() +
  labs(x = "Woche",
       y = "Temperatur in Grad C°", 
       title = "Temperaturdaten Schweiz",
       subtitle = "Januar 2000")
```

Man kann auch Einfluss auf die x-/y-Achsen nehmen. Dabei muss man zuerst festlegen, was für ein Achsentyp der Plot hat (vorher hat `ggplot` eine Annahme auf der Basis der Daten getroffen). 




Bei unserer y-Achse handelt es sich um numerische Daten, `ggplot` nennt diese: `scale_y_continuous()`. Unter [ggplot2.tidyverse.org](http://ggplot2.tidyverse.org/reference/#section-scales) findet man noch andere x/y-Achsentypen (`scale_x_irgenwas` bzw. `scale_y_irgendwas`).

```{r}
ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) +
  geom_line() +
  labs(x = "Woche",
       y = "Temperatur in Grad C°", 
       title = "Temperaturdaten Schweiz",
       subtitle = "Januar 2000") +    
  scale_y_continuous(limits = c(-30,30))    # y-Achsenabschnitt bestimmen

```


Das gleiche Spiel kann man für die y-Achse betreiben. Bei unserer y-Achse handelt es sich ja um unsere `POSIXct` Daten. `ggplot` nennt diese: `scale_x_datetime()`. 
```{r}
ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) +
  geom_line() +
  labs(x = "Woche",
       y = "Temperatur in Grad C°", 
       title = "Temperaturdaten Schweiz",
       subtitle = "Januar 2000") +    
  scale_y_continuous(limits = c(-30,30)) +
  scale_x_datetime(date_breaks = "1 week", 
                   date_minor_breaks = "1 day", 
                   date_labels = "KW%W")

```


Mit `theme` verändert man das allgmeine Layout der Plots. Beispielsweise kann man mit `theme_classic()` `ggplot`-Grafiken etwas weniger "Poppig" erscheinen lassen: so sind sie besser für Bachelor- / Masterarbeiten sowie Publikationen geeignet. `theme_classic()` kann man indiviudell pro Plot anwenden, oder für die aktuelle Session global setzen (s.u.)

Individuell pro Plot:
```{r}
ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) +
  geom_line() +
  labs(x = "Woche",
       y = "Temperatur in Grad C°", 
       title = "Temperaturdaten Schweiz",
       subtitle = "Januar 2000") +    
  scale_y_continuous(limits = c(-30,30)) +
  scale_x_datetime(date_breaks = "1 week", 
                   date_minor_breaks = "1 day", 
                   date_labels = "KW%W") +
  theme_classic()
```

Global (für alle nachfolgenden Plots der aktuellen Session):

```{r}
theme_set(theme_classic())
```


Sehr praktisch sind auch die Funktionen für "Small multiples". Dies erreicht man mit `facet_wrap()` (oder `facet_grid()`, mehr dazu später). Man muss mit einem Tilde-Symbol "`~`" nur festlegen, welche *Variable* für das Aufteilen des Plots in kleinere Subplots verantwortlich sein soll. 

```{r, fig.width=8,fig.height=10}
ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) +
  geom_line() +
  labs(x = "Woche",
       y = "Temperatur in Grad C°", 
       title = "Temperaturdaten Schweiz",
       subtitle = "Januar 2000") +    
  scale_y_continuous(limits = c(-30,30)) +
  scale_x_datetime(date_breaks = "2 weeks", 
                   date_minor_breaks = "1 day", 
                   date_labels = "KW%W") +
  facet_wrap(~stn)

```


Auch `facet_wrap` kann man auf seine Bedürfnisse anpassen. Da wir 24 Stationen haben möchte ich lieber 3 pro Zeile, damit es schön aufgeht. Dies erreiche ich mit `ncol = 3`.

Zudem brauchen wir die Legende nicht mehr, da der Stationsnamen über jedem Facet steht. Ich setze deshalb `theme(legend.position="none")` 

```{r, fig.width=8,fig.height=10}
ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) +
  geom_line() +
  labs(x = "Woche",
       y = "Temperatur in Grad C°", 
       title = "Temperaturdaten Schweiz",
       subtitle = "Januar 2000") +  
  scale_y_continuous(limits = c(-30,30)) +
  scale_x_datetime(date_breaks = "1 week", date_minor_breaks = "1 day", date_labels = "KW%W") +
  facet_wrap(~stn,ncol = 3) +
  theme(legend.position="none")
```


Genau wie `data.frames` und andere Objekte, kann man einen ganzen Plot auch in einer Variabel speichern. Dies kann nützlich sein um einen Plot zu exportieren (als png, jpg usw.) oder sukzessive erweitern wie in diesem Beispiel.

```{r, message = F}
p <- ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) +
  geom_line() +
  labs(x = "Woche",
       y = "Temperatur in Grad C°", 
       title = "Temperaturdaten Schweiz",
       subtitle = "Januar 2000") +
  scale_y_continuous(limits = c(-30,30)) +
  scale_x_datetime(date_breaks = "1 week", date_minor_breaks = "1 day", date_labels = "KW%W") +
  facet_wrap(~stn,ncol = 3)
  # ich habe an dieser Stelle theme(legend.position="none") entfernt



```

Folgendermassen kann ich den Plot als png-File abspeichern (ohne Angabe von "plot = " wird einfach der letzte Plot gespeichert)

```{r, eval = F}
ggsave(filename = "11_InfoVis1/plot.png",plot = p)
```

.. und so kann ich einen bestehenden Plot (in einer Variabel) mit einem Layer / einer Option erweitern

```{r, eval = F}
p +
  theme(legend.position="none")

```


Wie üblich wurde diese Änderung nicht gespeichert, sondern nur das Resultat davon ausgeben. Wenn die Änderung in meinem Plot (in der Variabel) abspeichern will, muss ich die Variabel überschreiben:

```{r}
p <- p +
  theme(legend.position="none")
```


Mit `geom_smooth()` kann `ggplot` eine Trendlinie auf der Baiss von Punktdaten berechnen. Die zugrunde liegende statistische Methode kann selbst gewählt werden. Wenn nichts angegeben wird verwendet `ggplot` bei weniger als 1'000 Messungen, die Methode `loess` (local smooths).


```{r, fig.width=8,fig.height=10}
p <- p +
  geom_smooth(colour = "black")

p
```

### Quellen

```{r code=readLines('00_Admin/get_chapter_references.R'), echo=F, eval=T,purl = F, results="asis"}
```

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:11_InfoVis1/Demo_ggplot.Rmd-->

```{r, include=F, purl=F}
library(knitr)

knitr::opts_chunk$set(echo = F,include = T,message = F, collapse=TRUE) # 
# knitr::opts_knit$set(root.dir = "11_InfoVis1") 


```

## Übung

In dieser Übung geht es darum, die Grafiken aus dem Blog-post von Marko Kovic ([blog.tagesanzeiger.ch](https://blog.tagesanzeiger.ch/datenblog/index.php/668/je-weniger-auslaender-desto-mehr-ja-stimmen-wirklich)) zu rekonstruieren. Freundlicherweise hat Herr Kovic meist die `ggplot2` Standardeinstellungen benutzt, was die Rekonstruktion relativ einfach macht. 

Die Links im Text verweisen auf die Originalgrafik, die eingebetteten Plots sind meine eigenen Rekonstruktionen. Importiere als erstes den Datensatz [initiative_masseneinwanderung_kanton.csv](11_InfoVis1/data/initiative_masseneinwanderung_kanton.csv) (auf der Blog-Seite erhältlich).


```{r, message=F}

library(tidyverse)
library(ggplot2)
library(stringr)

```


```{r, eval=F}
# Es kann sein, dass man die Codierung des Files spezifizieren muss. Mit `readr::read_delim()` 
# läuft dies mit der Option locale = locale(encoding = "UTF-8") wobei anstelle von UTF-8 die 
# entsprechende Codierung angegeben wird. 
# Tipp: Excel speichert CSV oft in ANSI, welches für den Import in R nicht sonderlich geeignet 
# ist. Falls Probleme auftreten muss das File mittels einer geeigneter Software (Widows: "Editor" 
# oder "Notepad++", Mac: "TextEdit")  und mit einer neuen Codierung (z.B. `UTF-8`) abgespeichert 
# werden.
```

```{r}
kanton <- read_delim("11_InfoVis1/data/initiative_masseneinwanderung_kanton.csv",",",locale = locale(encoding = "UTF-8"))
```



### Aufgabe 1

Rekonstruiere [Grafik 1](https://blog.tagesanzeiger.ch/datenblog/wp-content/uploads/sites/32/2014/03/Kantone-2.png) von Kovic. Erstelle dazu einen Scatterplot wo der Ausländeranteil der Kantone dem Ja-Anteil gegenüber gestellt wird. Speichere den Plot einer Variabel `plot1`.

- nutze `coord_fixed()` um die beiden Achsen in ein fixes Verhältnis zu setzen (1:1).
- setze die Achsen Start- und Endwerte mittels `lims()` oder `scale_y_continuous`bzw. `scale_x_continuous`.
- Optional: Setze analog Kovic die `breaks` (`0.0`, `0.1`...`0.7`) manuell

Rekonstruktion:

```{r}

# Lösung zu Aufgabe 1

# da die Spalten in Kovic's Daten Umlaute und Sonderzeichen enthalten, müssen diese in R mit Graviszeichen 
# angesprochen werden. Dieses Zeichen wirder Schweizer Tastatur [1]  mit 
# Shitft + Gravis (Links von der Backspace taste) + Leerschlag erstellt
# [1] https://de.wikipedia.org/wiki/Tastaturbelegung#Schweiz


# Alternativ können die Spalten im Originalfile oder mit dplyr::rename() umbenannt werden


plot1 <- ggplot(kanton, aes(`Ausländeranteil`, `Ja-Anteil`)) +
  geom_point() +
  coord_fixed(1) +
  scale_y_continuous(breaks = c(0,0.1,0.3,0.5,0.7),limits =  c(0,0.7)) +
  scale_x_continuous(breaks = c(0,0.1,0.3,0.5,0.7),limits =  c(0,0.7)) +
  labs(y = "Anteil Ja-Stimmen")

plot1
```


### Aufgabe 2

Rekonstruiere [Grafik 2](https://blog.tagesanzeiger.ch/datenblog/wp-content/uploads/sites/32/2014/03/Kantone-LOESS-986x923.png). Erweitere dazu `plot1` mit einer Trendlinie.

```{r}
# Lösung zu Aufgabe 2

plot1 +
  geom_smooth()
```



### Aufgabe 3


Importiere die Gemeindedaten [initiative_masseneinwanderung_gemeinde.csv](11_InfoVis1/data/initiative_masseneinwanderung_gemeinde.csv):

```{r, echo = T}
gemeinde <- read_delim("11_InfoVis1/data/initiative_masseneinwanderung_gemeinde.csv",",",locale = locale(encoding = "UTF-8"))
```


Rekonstruiere [Grafik 3](https://blog.tagesanzeiger.ch/datenblog/wp-content/uploads/sites/32/2014/03/Gemeinden-2-986x939.png). Stelle dazu den Ausländeranteil aller Gemeinden dem Ja-Stimmen-Anteil gegenüber. Speichere den Plot als `plot2`

```{r}
# Lösung zu Aufgabe 3

plot2 <- ggplot(gemeinde, aes(`Anteil Ausl`, `Anteil Ja`)) +
  geom_point() +
  labs(x = "Ausländeranteil",y = "Anteil Ja-Stimmen") +
  coord_fixed(1) +
  lims(x = c(0,1), y = c(0,1))

plot2
```


### Aufgabe 4

Rekonstruiere [Grafik 4](https://blog.tagesanzeiger.ch/datenblog/wp-content/uploads/sites/32/2014/03/Gemeinden-GAM-2-986x939.png) indem `plot2` mit einer Trendlinie erweitert wird.

```{r}
# Lösung zu Aufgabe 4

plot2 +
  geom_smooth()
```


### Aufgabe 5

Rekonstruiere [Grafik 5](https://blog.tagesanzeiger.ch/datenblog/wp-content/uploads/sites/32/2014/03/Gemeinden-x-Kantone-2-986x857.png) indem `plot2` mit `facetting` erweitert wird. Die Facets sollen die einzelnen Kantone sein. Speichere den Plot als `plot3`.

```{r}

# Lösung zu Aufgabe 5

plot3 <- plot2 +
  facet_wrap(~Kanton)
plot3
```


### Aufgabe 6

Rekonstruiere [Grafik 6](https://blog.tagesanzeiger.ch/datenblog/wp-content/uploads/sites/32/2014/03/Gemeinden-x-Kantone-LOESS-2-986x857.png) indem `plot3` mit einer Trendlinie erweitert wird.

Rekonstruktion:

```{r, warning=F}

# Lösung zu Aufgabe 6

plot3 +
  geom_smooth()
```


### Aufgabe 7

Rekonstruiere [Grafik 7](https://blog.tagesanzeiger.ch/datenblog/wp-content/uploads/sites/32/2014/03/Gemeinden-x-Quantile-2-986x637.png) indem `plot2`mit `facetting` erweitert wird. Die Facets sollen nun den Grössen-Quantilen entsprechen. Speichere den Plot unter `plot4`.

Rekonstruktion:

```{r}

# Lösung zu Aufgabe 7

plot4 <- plot2 +
  facet_wrap(~Quantile)
plot4
```


### Aufgabe 8

Rekonstruiere [Grafik 8](https://blog.tagesanzeiger.ch/datenblog/wp-content/uploads/sites/32/2014/03/Gemeinden-x-Quantile-LOESS-2-986x637.png) indem `plot4` mit einer Trendlinie ausgestattet wird.

```{r}

# Lösung zu Aufgabe 8

plot4 +
  geom_smooth()
```


### Aufgabe 9 (Fortgeschritten)

Rekonstruiere die [Korrelationstabelle](https://tagi_dwpro.s3.amazonaws.com/UMvkt/2/fs.html).

Tipp: 
- Nutze `group_by()` und `summarise()`
- Nutze `cor.test()` um den Korrelationskoeffizienten sowie den p-Wert zu erhalten. 
- Mit `$estimate` und `$p.value` können die entsprechenden Werte direkt angesprochen werden

Hinweis: aus bisher unerklärlichen Gründen weiche gewisse meiner Werte leicht von den Berechnungen des Herrn Kovics ab.

```{r}

# Lösung zu Aufgabe 9

korr_tab <- gemeinde %>%
  group_by(Kanton) %>%
  summarise(
    Korr.Koeffizient = cor.test(`Anteil Ja`,`Anteil Ausl`,method = "pearson")$estimate,
    Signifikanz_val = cor.test(`Anteil Ja`,`Anteil Ausl`,method = "pearson")$p.value,
    Signifikanz = ifelse(Signifikanz_val < 0.001,"***",ifelse(Signifikanz_val<0.01,"**",ifelse(Signifikanz_val<0.05,"*","-")))
  ) %>%
  select(-Signifikanz_val)

```



```{r,echo=F, purl=F}
knitr::kable(korr_tab)
```


```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:11_InfoVis1/Uebung.Rmd-->

## Lösung

[RCode als Download](11_InfoVis1/RFiles/Uebung.R)


```{r code=readLines('11_InfoVis1/RFiles/Uebung.R'), echo=T, eval=F}
```
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:11_InfoVis1/Uebung_loesung.Rmd-->

# InfoVis2 (23.10.2018)

Die Informationsvisualisierung ist eine vielseitige, effektive und effiziente Methode für die explorative Datenanalyse. Während Scatterplots und Histogramme weitherum bekannt sind, bieten weniger bekannte Informationsvisualisierungs-Typen wie etwa Parallelkoordinatenplots, TreeMaps oder Chorddiagramme originelle alternative Darstellungsformen zur visuellen Analyse von Datensätze, welche stets grösser und komplexer werden. Die Studierenden lernen in dieser Lerneinheit eine Reihe von Informationsvisualisierungstypen kennen, lernen diese zielführend zu gestalten und selber zu erstellen.
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:12_InfoVis2/Abstract.Rmd-->


```{r, include=F, purl = F}
library(knitr)
knitr::opts_chunk$set(echo = F,include = T,message = F, warning = F, collapse=TRUE)
# knitr::opts_knit$set(root.dir = "12_InfoVis2") 

```



## Übung A

```{r, message = F, echo = T}
library(tidyverse)
library(lubridate)
```


Laden den `wetter`-Datensatz, bereinige ihn wenn nötig (`NA`-Werte entfernen) und importiere auch den Datensatz `order_52252_legend.csv` und verbinde die Datensätze mit einem join via dem Stationskürzel.



```{r, echo = T, message = F}

wetter <- read_table("09_PrePro1/data/order_52252_data.txt",
                     col_types = list(
                       col_character(),    
                       col_datetime(format = "%Y%m%d%H"),
                       col_double()
                       )
                     )

wetter <- wetter %>%
  filter(!is.na(stn)) %>%
  filter(!is.na(time))

station_meta <- read_delim("09_PrePro1/data/order_52252_legend.csv",";")

wetter <- left_join(wetter,station_meta,by = "stn")

```




### Aufgabe 1

Erstelle zwei Hilfsspalten (convenience variables) "Jahr" und "Monat". Filtere auf ein beliebiges Jahr und zwei beliebige Monate. Speichere den gefilterten Datensatz in einer neuen Variablen ab. Verwende diesen Datensatz für alle folgenden Übungen.

```{r}

# Lösung Aufgabe 1

wetter_fil <- wetter %>%
  mutate(
    year = year(time),
    month = month(time)
  ) %>%
  filter(year == 2000 & month < 3)
```


### Aufgabe 2

Erstelle ein Scatterplot (`time` vs. `tre200h0`) wobei die Punkte aufgrund ihrer Meereshöhe eingefärbt werden sollen. Tiefe Werte sollen dabei blau eingefärbt werden und hohe Werte rot. Verkleinere die Punkte um übermässiges Überplotten der Punkten zu vermeiden. Weiter sollen im Abstand von zwei Wochen die Kalenderwochen auf der Achse erscheinen. 

Speichere den Plot in einer Variabel `p` ab.

```{r}

# Lösung Aufgabe 2

p <- ggplot(wetter_fil, aes(time,tre200h0, colour = Meereshoehe)) +
  geom_point(size = 0.5) +
  labs(x = "Kalenderwoche", y = "Temperatur in ° Celsius") +
  scale_color_continuous(low = "blue", high = "red") +
  scale_x_datetime(date_breaks = "2 week", date_labels = "KW%W") 

p 

```



### Aufgabe 3

Füge am obigen Plot (gespeichert als Variabel `p`) eine schwarze, gestrichelte Trendlinie hinzu und aktualisiere `p` (`p <- p + ...`).

```{r, message=F}

# Lösung Aufgabe 3

p <- p +
  stat_smooth(colour = "black",lty = 2)

p
```


### Aufgabe 4

Positioniere die Legende oberhalb des Plots und lege sie quer (nutze dazu `theme()` mit `legend.direction` und `legend.position`). Speichere diese Änderungen in `p`.

```{r, message=F}

# Lösung Aufgabe 4


p <- p + 
  theme(legend.direction = "horizontal",legend.position = "top")

p
    
```



### Aufgabe 5 (für ambitionierte)

Füge den Temperaturwerten auf der y-Ache ein `°C` hinzu (siehe unten und studiere [diesen Tipp](https://stackoverflow.com/a/35967126/4139249) zur Hilfe). Aktualisiere `p` an dieser Stelle noch nicht.

```{r, message=F}

# Lösung Aufgabe 5

p +
  scale_y_continuous(labels = function(x)paste0(x,"°C")) +
  labs(x = "Kalenderwoche", y = "Temperatur")


```


### Aufgabe 6 (für *noch* ambitioniertere)

Füge dem Plot eine zweite, korrekt ausgerichtete Achse mit Kelvin oder Farenheit hinzu (siehe `sec_axis`). Wenn du es vorherigen Übung schon geschafft hast, setze auch hier die Einheit (`K` rep. `°F`) hinter die Werte auf der Achse. 

$$ K = °C + 273,15$$
$$°F = °C × \frac{9}{5} + 32$$

```{r, message=F}
# Lösung Aufgabe 6

p <- p +
  labs(x = "Kalenderwoche", y = "Temperatur") +
  scale_y_continuous(labels = function(x)paste0(x,"°C"),sec.axis = sec_axis(~.*(9/5)+32,name = "Temperatur",labels = function(x)paste0(x,"° F")))


p
```




### Aufgabe 7

Jetzt verlassen wir den scatterplot und machen einen Boxplot mit den Temperaturdaten. Färbe die Boxplots wieder in Abhängigkeit der Meereshöhe ein. 

- Beachte den Unterschied zwischen `colour =` und `fill =`
- Beachte den Unterschied zwischen `facet_wrap()` und `facet_grid()`
- `facet_grid()` braucht übrigens noch einen Punkt (`.`) zur Tilde (`~`). 
- Beachte den Unterschied zwischen "`.~`" und "`~.`" bei `facet_grid()`
- verschiebe nach Bedarf die Legende

```{r}

# Lösung Aufgabe 7

wetter_fil <- mutate(wetter_fil,monat = month(time,label = T,abbr = F))


ggplot(wetter_fil, aes(stn,tre200h0, fill = Meereshoehe)) +
  geom_boxplot() +
  facet_grid(monat~.) +
  labs(x = "Station", y = "Temperatur") +
  theme(legend.direction = "horizontal",legend.position = "top")

```


### Aufgabe 8

Teile die Stationen in verschiedene Höhenlagen ein (Tieflage [< 450 m], Mittellage [450 - 1000 m] und Hochlage [> 1'000 m]). Vergleiche die Verteilung der Temperaturwerte in den verschiedenen Lagen.  

- Nutze dazu `facet_grid` um die Höhenlage dem Monat gegenüber zu stellen (`Monat~Lage`)
- Passe `scales =` an damit keine leeren Stellen auf der x-Achse entstehen
- Optional: Verwende den vollen Stationsnamen anstelle des Kürzels und drehe diese ab damit sie sich gegenseitig nicht überschreiben

```{r, warning=F}

# Lösung Aufgabe 8

wetter_fil$Lage[wetter_fil$Meereshoehe < 450] <- "Tieflage" 
wetter_fil$Lage[wetter_fil$Meereshoehe >= 450 & wetter_fil$Meereshoehe <1000] <- "Mittellage" 
wetter_fil$Lage[wetter_fil$Meereshoehe >= 1000] <- "Hochlage" 


ggplot(wetter_fil, aes(Name,tre200h0)) +
  geom_boxplot() +
  facet_grid(monat~Lage, scales = "free_x") +
  labs(x = "Lage", y = "Temperatur") +
  theme(axis.text.x = element_text(angle = 45,hjust = 1))

```



### Aufgabe 9


Als letzter wichtiger Plottyp noch zwei Übungen zum Histogramm. Erstelle ein Histogramm `geom_histogram()` mit den Temperaturwerten. Färbe Säulen aufgrund ihrer Höhenlage ein und die Begrenzungslinie weiss. Setze die Klassenbreite auf 1 Grad.

```{r}

# Lösung Aufgabe 9


h <- ggplot(wetter_fil,aes(tre200h0, fill = Lage)) +
  geom_histogram(binwidth = 1, colour = "white") +
  labs(x = "Temperatur in °C", y = "Anzahl")

h
```


### Aufgabe 10

Erstelle `facets` aufgrund der Höhenlage. Setze noch eine Vertikale linie beim Nullpunkt und stelle den x-Achsenabschnit symmetrisch ein (z.B -30 bis + 30°C).

```{r}

# Lösung Aufgabe 10


h + 
  geom_vline(xintercept = 0, lty = 2, alpha = 0.5) +
  facet_wrap(~Lage) +
  lims(x = c(-30,30)) +
  theme(legend.position = "none")

```





```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:12_InfoVis2/Uebung_A.Rmd-->

## Übung A: Lösung

[RCode als Download](12_InfoVis2/RFiles/Uebung_A.R)

```{r code=readLines('12_InfoVis2/RFiles/Uebung_A.R'), echo=T, eval=F}
```


```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:12_InfoVis2/Uebung_A_loesung.Rmd-->


```{r, include=F, purl = F}
library(knitr)
knitr::opts_chunk$set(echo = F,include = T,message = F, warning = F, collapse=TRUE)
# knitr::opts_knit$set(root.dir = "12_InfoVis2") 


output <- knitr::opts_knit$get("rmarkdown.pandoc.to") # html / latex


run_plotly_image = F # set to "TRUE" in order to create static images via plotly_api (max 100/day)
show_static_image = T # set to "TRUE" in order to show the image / "FALSE" to show error message
default_error <- "In der PDF Version kann die interaktive Grafik bis auf weiteres nicht dargestellt werden."

```

## Übung B

In dieser Übung bauen wir einige etwas unübliche Plots aus der Vorlesung nach. Dafür verwenden wir Datensätze, die in R bereits integriert sind. Eine Liste dieser Datensätze findet man [hier](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/00Index.html) oder mit der Hilfe `?datasets`.


Dazu verwenden wir vor allem das Package `plotly` welches im Gegensatz zu `ggplot2` ein paar zusätzliche Plot-Typen kennt und zudem noch interaktiv ist.  Leider scheinen gewisse Browsers (z.B. Firefox) sowie der Viewer Pane mit `plotly` Mühe zu haben. Deshalb empfehlen wir folgendes:

- [Übungsunterlagen für InfoVis2](http://oyster.zhaw.ch:3939/ResearchMethodsUebungen/) in Chrome zu öffnen
- Falls ihr auf dem [RStudio Server](http://oyster.zhaw.ch:8787) arbeitet: hier ebenfalls in Chrome arbeiten
- Falls ihr lokal mit RStudio arbeitet: Mit der Option `options(viewer=NULL)` werden Plots mit dem Standart Browser. 



```{r, echo = F,message=F}

library(tidyverse)
library(plotly)
library(pander)
library(webshot)

```


```{r, echo = F, purl= F}
Sys.setenv("plotly_username" = "rata_zhaw")
Sys.setenv("plotly_api_key" = "ae8Fn1ltcjUGvWYX957J")
```



### Aufgabe 1: Parallel coordinate plots

Erstelle einen [parallel coordinate plot](https://en.wikipedia.org/wiki/Parallel_coordinates). Dafür eignet sich der integrierte Datensatz [`mtcars`](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html):

```{r, echo = F, purl=F}

knitr::kable(head(mtcars))

```

```{r, echo = T, eval=F}

# Nur nötig, wenn ihr mit einer lokalen Installation von RStudio arbeitet
# (also nicht auf dem Server).
options(viewer=NULL)

```


Parallel Coordinates lassen sich mit nativem `ggplot2` nicht herstellen. Es braucht dazu entweder Erweiterungen oder "standalone" Tools. Als "standalone" Tool kann ich `plotly` stark empfehlen. `Plotly` verfügt zwar über eine etwas eigenwillige Syntax, bietet dafür über sehr vielseitige zusätzliche Möglichkeiten. Vor allem aber sind sämtliche `plotly` Grafiken webbasiert und interaktiv. 

Hier findet ihr eine Anleitung zur Herstellung eines Parallel Coordinates Plot mit `plotly`: https://plot.ly/r/parallel-coordinates-plot/

So sieht der fertige Plot aus:

```{r}
# Lösung Aufgabe 1

p <- mtcars %>%
  plot_ly(type = 'parcoords',
          line = list(color = ~mpg,
                      colorscale = list(c(0,'red'),c(1,'blue'))),
          dimensions = list(
            list(label = 'mpg', values = ~mpg),
            list(label = 'disp', values = ~disp),
            list(label = 'hp', values = ~hp),
            list(label = 'drat', values = ~drat),
            list(label = 'wt', values = ~wt),
            list(label = 'qsec', values = ~qsec),
            list(label = 'vs', values = ~vs),
            list(label = 'am', values = ~am),
            list(label = 'gear', values = ~gear),
            list(label = 'carb', values = ~carb)
          )
  )
```


```{r, results="asis", purl = F, echo=F}
aufgabe <- "aufgabe_1"
filename <- paste0("12_InfoVis2/",aufgabe,".png")

if(output == "latex"){
  if(run_plotly_image){plotly_IMAGE(p, format = "png", out_file = filename)} 
  if(show_static_image){pander::pandoc.image(filename)} else{pandoc.strong(default_error)}
} else(p)

```

### Aufgabe 2: Polar Plot mit Biber Daten

Polar Plots (welche man ebenfalls mit Plotly erstellen kann) eignen sich unter anderem für Daten, die zyklischer Natur sind, wie zum Beispiel zeitlich geprägte Daten (Tages-, Wochen-, oder Jahresrhythmen). Aus den Beispiels-Datensätzen  habe ich zwei Datensätze gefunden, die zeitlich geprägt sind:

- [`beaver1` und `beaver2`](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/beavers.html)
[`AirPassenger`](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/AirPassengers.html)

Beide Datensätze müssen noch etwas umgeformt werden, bevor wir sie für einen Radialplot verwenden können. In Aufgabe 2 verwenden wir die Biber-Datensätze, in der nächsten Aufgabe (3) die Passagier-Daten.

Wenn wir die Daten von beiden Bibern verwenden wollen, müssen wir diese noch zusammenfügen:
```{r, echo = T}


beaver1_new <- beaver1 %>%
  mutate(beaver = "nr1")

beaver2_new <- beaver2 %>%
  mutate(beaver = "nr2")

beaver_new <- rbind(beaver1_new,beaver2_new)

```

Zudem müssen wir die Zeitangabe noch anpassen: Gemäss der [Datenbeschreibung](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/beavers.html) handelt es sich bei der Zeitangabe um ein sehr programmier-unfreundliches Format. 3:30 wird als "0330" notiert. Wir müssen diese Zeitangabe, noch in ein Dezimalsystem umwandeln:
```{r, echo = T}
beaver_new <- beaver_new %>%
  mutate(
    hour_dec = (time/100)%/%1,         # Ganze Stunden (mittels ganzzaliger Division)
    min_dec = (time/100)%%1/0.6,       # Dezimalminuten (15 min wird zu 0.25, via Modulo)
    hour_min_dec = hour_dec+min_dec    # Dezimal-Zeitangabe (03:30 wird zu 3.5)
    ) 
```



Der Datensatz: 
```{r, echo = F, purl = F}
knitr::kable(head(beaver_new))
#  formatRound(c("min_dec","hour_min_dec"), 2)
```

So sieht der fertige Plot aus. Rekonstruiere dies mit `plotly`:

```{r}

# Lösung Aufgabe 2

p <- beaver_new %>%
  plot_ly(r = ~temp, t = ~hour_min_dec, color = ~beaver,mode = "lines", type = "scatter") %>%
  layout(
    radialaxis = list(range = c(35,39)),
    angularaxis = list(range = c(0,24)),
    orientation = 270,
    showlegend = F
    )
```

```{r, results="asis", purl = F, echo=F}
aufgabe <- "aufgabe_2"
filename <- paste0("12_InfoVis2/",aufgabe,".png")

if(output == "latex"){
  if(run_plotly_image){plotly_IMAGE(p, format = "png", out_file = filename)} 
  if(show_static_image){pander::pandoc.image(filename)} else{pandoc.strong(default_error)}
} else(p)

```




### Aufgabe 3: Polar Plot mit Passagier-Daten


Analog Aufgabe 2, dieses Mal mit dem Datensatz [`AirPassanger`](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/AirPassengers.html)

`AirPassengers` kommt in einem Format daher, das ich selbst noch gar nicht kannte. Es sieht zwar aus wie ein `data.frame` oder eine `matrix`, ist aber von der Klasse [`ts`](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/ts.html).

```{r, echo = T}
AirPassengers

class(AirPassengers)
```


Damit wir den Datensatz verwenden können, müssen wir ihn zuerst in eine `matrix` umwandeln. Wie das geht habe ich [hier](https://stackoverflow.com/a/5332664/4139249) erfahren.
```{r, echo = T}
AirPassengers2 <- tapply(AirPassengers, list(year = floor(time(AirPassengers)), month = month.abb[cycle(AirPassengers)]), c)
```

Aus der `matrix` muss noch ein Dataframe her, zudem müssen wir aus der breiten Tabelle eine lange Tabelle machen:

```{r, echo = T}


AirPassengers3 <- AirPassengers2 %>%
  as.data.frame() %>%
  rownames_to_column("year") %>%
  gather(month,n,-year) %>%
  mutate(
    # ich nutze einen billigen Trick um ausgeschriebene Monate in Nummern umzuwandeln [1]
    month = factor(month, levels = month.abb,ordered = T),
    month_numb = as.integer(month),
    year = factor(year, ordered = T)
  )


# [1] beachtet an dieser Stelle das Verhalten von as.integer() wenn es sich um factors() handelt. Hier wird das Verhalten genutzt, andersweitig kann es einem zum Verhngnis werden. Das Verhalten wir auch hier verdeutlicht:
# as.integer(as.character("500"))
# as.integer(as.factor("500"))

```


Hier der fertige Plot. Rekonstruiere dies mit `plotly`:
```{r}

# Lösung Aufgabe 3

p <- AirPassengers3 %>%
  plot_ly(r = ~n, t = ~month_numb, color = ~year, mode = "markers", type = "scatter") %>%
  layout(
    showlegend = T,
    angularaxis = list(range = c(0,12)),
    orientation = 270,
    legend = list(traceorder = "reversed")
) 

```



```{r, results="asis", purl = F, echo=F}
aufgabe <- "aufgabe_3"
filename <- paste0("12_InfoVis2/",aufgabe,".png")

if(output == "latex"){
  if(run_plotly_image){plotly_IMAGE(p, format = "png", out_file = filename)} 
  if(show_static_image){pander::pandoc.image(filename)} else{pandoc.strong(default_error)}
} else(p)
```

### Aufgabe 4: 3D Scatterplot

Erstelle einen 3D Scatterplot, ebenfalls mit `plotly`. Nutze dazu den Datensatz `trees`. Ein Beispiel für einen 3D Scatterplot findet ihr [hier](https://plot.ly/r/3d-scatter-plots/).


```{r}

# Lösung Aufgabe 4

  p <- trees %>%
  plot_ly(x = ~Girth, y = ~Height, z = ~Volume)
```



```{r, results="asis", purl = F, echo=F}
aufgabe <- "aufgabe_4"
filename <- paste0("12_InfoVis2/",aufgabe,".png")

if(output == "latex"){
  if(run_plotly_image){plotly_IMAGE(p, format = "png", out_file = filename)} 
  if(show_static_image){pander::pandoc.image(filename)} else{pandoc.strong(default_error)}
} else(p)
```





```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:12_InfoVis2/Uebung_B.Rmd-->

## Übung B: Lösung

[RCode als Download](12_InfoVis2/RFiles/Uebung_B.R)

```{r code=readLines('12_InfoVis2/RFiles/Uebung_B.R'), echo=T, eval=F}
```

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:12_InfoVis2/Uebung_B_loesung.Rmd-->

# Statistik 1 (29.1.2018)

In Statistik 1 lernen die Studierenden, was (Inferenz-) Statistik im Kern leistet und warum sie für wissenschaftliche Erkenntnis (in den meisten Disziplinen) unentbehrlich ist. Nach einer Wiederholung der Rolle von Hypothesen wird erläutert, wie Hypothesentests in der frequentist-Statistik umgesetzt werden, einschliesslich p-Werten und Signifikanz-Levels. Die praktische Statistik beginnt mit den beiden einfachsten Fällen, dem Chi-Quadrat-Test für die Assoziation zwischen zwei kategorialen Variablen und dem t-Test auf Unterschiede in Mittelwerten zwischen zwei Gruppen. Dann geht es um die Voraussetzungen parametrischer (und nicht-parametrischer) Tests und Optionen, wenn diese verletzt sind. Abschliessend gibt es einen ersten Einstieg in die Varianzanalyse (ANOVA).


<!-- TODO: -->
<!-- Referenzen passt noch nicht -->
<!-- hierarchien noch kontrollieren und mit anderen blöcke abgleichen -->
<!-- Beschreibung forschungsprojekte: tabelle besser als csv -->
```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:13_Statistik1/Abstract.Rmd-->

```{r, echo = F, purl = F}
knitr::opts_chunk$set(echo = T, collapse=TRUE)
# knitr::opts_knit$set(root.dir = "13_Statistik1") 
library(knitr)

```





## Demo: Stastische Tests

[Demoscript als Download](13_Statistik1/RFiles/Demo_Tests.R)

### Assoziationstests

```{r, message = F}
library(car)
library(tidyverse)

library(ggfortify)

select <- dplyr::select

```




```{r}

# Chi-Quadrat-Test

# Ermitteln des kritischen Wertes für 95 perzentile und 1 FG
qchisq(0.95,1)


```

```{r}

# Datensatz von Folie 28: Test auf Assoziation zwischen zwei kategorialen Variablen
# Frage: Wie hängen zwei Eigenschaften des gleichen Objektes zusammen?

count <- matrix(c(38,14,11,51),nrow=2)

count

dimnames(count) <- list(c("helle_haare","hunkle_haare"),c("blaue_augen","braune_augen"))

count

chisq.test(count)

chisq.test(count,correct = F)

fisher.test(count)
```




###	t-Test


```{r}

# Datensatz "Blumen" erstellen für t-Test

blume <-data.frame(
  sorte_a = c(20,19,25,10,8,15,13,18,11,14),
  sorte_b = c(12,15,16,7,8,10,12,11,13,10)
  )

blume
summary(blume)

```


#### Tests mit einer *breiten* (wide) Tabelle
```{r}



boxplot(blume$sorte_a,blume$sorte_b)

hist(blume$sorte_a)

hist(blume$sorte_b)

t.test(blume$sorte_a,blume$sorte_b) #zweiseitig
t.test(blume$sorte_a,blume$sorte_b, alternative="greater") #einseitig
t.test(blume$sorte_a,blume$sorte_b, alternative="less") #einseitig
t.test(blume$sorte_a,blume$sorte_b, var.equal=T) #Varianzen gleich, klassischer t-Test
t.test(blume$sorte_a,blume$sorte_b, var.equal=F) #Varianzen ungleich, Welch's t-Test, ist auch default
t.test(blume$sorte_a,blume$sorte_b, paired=T) #gepaarter t-Test 
t.test(blume$sorte_a,blume$sorte_b, paired=T,alternative="greater") #gepaarter t-Test 

shapiro.test(blume$sorte_b)

wilcox.test(blume$sorte_a,blume$sorte_b)

```


#### Tests mit einer *langen* (long) Tabelle
```{r}
# T-Tests mit einer langen (long) Tabelle

# breit zu long mit tidyr::gather

blume_long <- gather(blume,sorte,groesse)

# Für eine Long table können wir auch gut ggplot verwenden

ggplot(blume_long, aes(sorte, groesse)) +
  geom_boxplot()

ggplot(blume_long, aes(groesse)) +
  geom_histogram(binwidth = 1, colour = "white")

ggplot(blume_long, aes(groesse)) +
  geom_histogram(binwidth = 2, colour = "white")+
  facet_grid(sorte~.)

ggplot(blume_long, aes(groesse, fill = sorte)) +
  geom_density(alpha = 0.5)




t.test(groesse~sorte, blume_long)

# Optionen "alternative" und "var.equal" werden angepasst analog der "wide" table

```





### Randomisierung

Datensatz: [beetle.csv](13_Statistik1/data/beetle.csv)

```{r}
# Randomisierung

beetles <- read_delim("13_Statistik1/data/beetle.csv", ",")


ggplot(beetles, aes(SIZE, BEETLES)) +
  geom_boxplot()


ggplot(beetles, aes(SIZE, sqrt(BEETLES))) +
  geom_boxplot()


ggplot(beetles, aes(SIZE, BEETLES)) +
  geom_boxplot() +
  scale_y_sqrt()


# extract specific inidies from t-test
stat <- function(data, indices) {
  t.test <- t.test(BEETLES~SIZE, data)$"stat"
  t.test
}


# Random generator auf der Basis eines Datensatzes
rand.gen <- function(data,mle) {
  out <- data
  out$SIZE <- sample(out$SIZE, replace=F)
  out
}


library(boot)

# boot(): bootstrap resampling

beetles.boot <- boot(data = beetles,
                     statistic = stat, 
                     R=5000, 
                     sim="parametric", 
                     ran.gen=rand.gen)


print(beetles.boot)



plot(beetles.boot)



tval <- length(beetles.boot[beetles.boot$t >= abs(beetles.boot$t0)])+1

tval

tval/(beetles.boot$R + 1)

```


###	Voraussetzungen parametrischer Verfahren


#### F-Test
```{r}

# F-Test

var.test(blume$sorte_a,blume$sorte_b)
var.test(groesse~sorte, blume_long)

```


#### Levene Test
```{r}

# Levene Test 

library(car)
leveneTest(blume$sorte_a,blume$sorte_b,center=mean)
leveneTest(groesse~sorte, blume_long)

```

#### Visuelle Inspektion / Transformation

```{r}

# Visuelle Inspektion

sleep <- msleep %>%
  filter(vore %in% c("carni","herbi")) %>%
  dplyr::select(name,vore,bodywt)


ggplot(sleep, aes(vore, bodywt, group = vore)) +
  geom_boxplot()


sleep <- sleep %>%
  mutate(
    bodywt_log10 = log10(bodywt),
    bodywt_sqrt = sqrt(bodywt),
    bodywt_4throot = bodywt^0.25
  ) %>%
  gather(key,val, -c(name,vore))

sleep


ggplot(sleep, aes(vore, val, group = vore)) +
  geom_boxplot() +
  facet_wrap(~key, scales = "free_y")




```


###	Einstieg ANOVA

#### ANOVA mit "Blumen"-Daten

```{r}

# ANOVA mit "Blumen"-Daten

ggplot(blume_long, aes(sorte, groesse)) +
  geom_boxplot()

t.test(groesse~sorte, blume_long, var.equal=T)

#now as ANOVA

aov(groesse~sorte,blume_long)
summary(aov(groesse~sorte,blume_long)) #F-value = 4.325
summary.lm(aov(groesse~sorte,blume_long))

lm(groesse~sorte,blume_long)
summary(lm(groesse~sorte,blume_long))

# Compare the above results of t.test, aov and lm and how the relevant data are displayed

# Now check with the F-distribution:
# What would have been the critical value for a significant result at p < 0.05?

qf(0.95,1,18)

# which is the p-value associated with the obtained F-value?

1-pf(4.325,1,18) #probability of obtaining F=4,325 or greater

```


```{r}
## Optional für Demo

#overall mean and residuals

blume_long$index <- 1:20

blume_long <- blume_long %>%
  group_by(sorte) %>%
  mutate(
    mean = mean(groesse)
  )

ggplot(blume_long, aes(index,groesse)) +
  geom_point() +
  geom_hline(aes(yintercept =  mean(groesse))) +
  geom_segment(aes(x = index,y = groesse,xend = index,yend = mean(groesse))) +
  labs(x = "Order",y = expression(Size~(cm^2)))

#group means and residuals

ggplot(blume_long, aes(index,groesse, colour = sorte)) +
  geom_point() +
  geom_hline(aes(yintercept = mean)) +
  geom_segment(aes(x = index,y = groesse,xend = index,yend = mean)) +
  labs(x = "Order",y = expression(Size~(cm^2))) +
  facet_grid(~sorte)



library(ggfortify)

autoplot(aov(groesse~sorte, blume_long))
```



#### ANOVA mit Gewässerdaten

Aus @logan2010 [S. 265]: Script an Tools/Packages des Moduls Research Methods angepasst

Medley and Clements (1998) investigated the impact of zinc contamination (and other heavy metals) on the diversity of diatom species in the USA Rocky Mountains (from Box 8.1 of Quinn and Keough (2002)). The diversity of diatoms (number of species) and degree of zinc contamination (categorized as either of high, medium, low or natural background level) were recorded from between four and six sampling stations within each of six streams known to be polluted. These data were used to test the null hypothesis that there were no differences the diversity of diatoms between different zinc levels ($$H_0 = \mu_M = \mu_L = \mu_V = \mu; \alpha_i = 0$$)

The linear effects model would be:

$$\gamma_{ij} = \mu + \alpha_i + \epsilon_{ij}$$
$$\text{diatom species diversity} = \text{overal mean} + \text{effect of zinc level} + \text{error}$$

##### Step 1

Import the Medley and Clements (1998) data set [medley.csv](13_Statistik1/data/medley.csv)
```{r}

# impact of zinc contamination (and other heavy metals) on the diversity of diatom species in the USA Rocky Mountains

medley <- read_delim("13_Statistik1/data/medley.csv", ",")

```


##### Step 2

Reorganize the levels of the categorical factor into a more logical order

```{r}

# Reorganize the levels of the categorical factor into a more logical order

medley$ZINC <- factor(medley$ZINC, levels=c("BACK", "LOW", "MED","HIGH"), ordered=T)

```

##### Step 3

Assess normality/homogeneity of variance using boxplot of species diversity against zinc group

```{r}

# Assess normality/homogeneity of variance using boxplot of species diversity against zinc group

ggplot(medley, aes(ZINC, DIVERSITY)) +
  geom_boxplot()
```

##### Step 4

Assess homogeneity of variance assumption with a table and/or plot of mean vs variance
```{r}

# Assess homogeneity of variance assumption with a table and/or plot of mean vs variance

medley_sry <- medley %>%
  group_by(ZINC) %>%
  summarise(
    mean = mean(DIVERSITY),
    var = var(DIVERSITY),
    sd = sd(DIVERSITY)
  )

ggplot(medley_sry, aes(mean,var)) +
  geom_point()
```



### Libraries

```{r code=readLines('00_Admin/get_chapter_references.R'), echo=F, eval=T,purl = F, results="asis"}
```

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:13_Statistik1/Demo_Tests.Rmd-->


## Beschreibung Forschungsprojekt NOVANIMAL (NFP69)

Im Forschungsprojekt NOVANIMAL wird u.a. der Frage nachgegangen, was es braucht, damit Menschen freiwillig weniger tierische Produkte konsumieren? Ein interessanter Ansatzpunkt ist die Ausser-Haus-Verpflegung. Gemäss der ersten in den Jahren 2014/2015 durchgeführten nationalen Ernährungserhebung menuCH essen 70 % der Bevölkerung zwischen 18 und 75 Jahren am Mittag auswärts (Bochud et al. 2017). Daher rückt die Gastronomie als zentraler Akteur einer innovativen und nachhaltigen Ernährungswirtschaft ins Blickfeld. Welche Innovationen in der Gastronomie könnten dazu beitragen, den Pro-Kopf-Verbrauch an tierischen Nahrungsmitteln zu senken? 

Dazu wurde u.a. ein Experiment in zwei Hochschulmensen durchgeführt. Forschungsleitend war die Frage, wie die Gäste dazu bewogen werden können, häufiger vegetarische oder vegane Gerichte zu wählen. Konkret wurde untersucht, wie die Gäste auf ein verändertes Menü-Angebot mit einem höheren Anteil an vegetarischen und veganen Gerichten reagieren. Das Experiment fand während 12 Wochen statt und bestand aus zwei Mensazyklen à 6 Wochen. Über den gesamten Untersuchungszeitraum werden insgesamt 90 verschiedene Gerichte angeboten. In den 6 Referenz- bzw. Basiswochen wurden zwei fleisch- oder fischhaltige Menüs und ein vegetarisches Menü angeboten. In den 6 Interventionswochen wurde das Verhältnis umgekehrt und es wurden ein veganes, ein vegetarisches und ein fleisch- oder fischhaltiges Gericht angeboten. Basis- und Interventionsangebote wechselten wöchentlich ab. Während der gesamten 12 Wochen konnten die Gäste jeweils auf ein Buffet ausweichen und ihre Mahlzeit aus warmen und kalten Komponenten selber zusammenstellen. Die Gerichte wurden über drei vorgegebene Menülinien  (F, K, W) randomisiert angeboten.

![Die Abbildung zeigt das Versuchsdesign der ersten 6 Experimentalwochen (Kalenderwoche 40 bis 45).](13_Statistik1/design_experiment.png)

Mehr Informationen über das Forschungsprojekt NOVANIMAL findet ihr auf dieser [Webpage](https://www.novanimal.ch)


<!-- @ Egel: Diesen Teil habe ich aus der Aufgabenstellung rausgenommen. Könnnen wir den hier platzieren?  -->
### Weitere Erläuterungen zum Datensatz



Der Datensatz beinhaltet knapp 1100 Einträge mit 18 Variablen. Die Daten stammen aus dem Kassensystem des Catering-Unternehmen und stellen eine repräsentative Stichprobe des originalen Datensatzes dar.

Folgende Variablen sind im Datensatz:


```{r, echo = F}
library(tidyverse)

variablen <- read_delim("13_Statistik1/novanimal_variabeln.csv",";")

knitr::kable(variablen)
  
```


1. Locals (Local F, Local K, Local W) sind nebst den drei "normalen Menü-Linien" zusätzlich angebotene Gerichte
2. hier werden Gerichte mit Fisch & Geflügel als Fleisch zusammengefasst.
3. Vegane Gerichte enthalten ausschliesslich pflanzliche Zutaten. Im Exeriment wurde zwischen Gerichte mit pflanzlichen Fleischsubstituten und authentischen, eigenständigen veganen Gerichten unterschieden
4. Vegetarisch bedeutet ovo-lakto-vegetarisch, d.h. die Gerichte enthalten Eier und/oder Milchprodukte 

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:13_Statistik1/Intro_Daten_egel.Rmd-->

## Übung A (NatWis)

###	Aufgabe 1

Datenerhebung für einen Assoziationstest zweier kategorialer Variablen, Dateneingabe und Durchführung von Chi-Quadrat- sowie Fishers exaktem Test


###	Aufgabe 2

Überlegen Sie eine Hypothese, die mit einem t-Test geprüft werden kann, erheben Sie dazu die Daten, geben Sie diese ein, visualisieren Sie diese, testen auf evtl. Verletzungen der Modellannahmen, führen dann den geeigneten t-Test sowie eine nicht-parametrische Alternative durch und fassen die Ergebnisse in einem Satz zusammen


###	Aufgabe 3: ANOVA

Führt mit dem Datensatz [competition.csv](13_Statistik1/data/competition.csv) einen einfaktoriellen ANOVA selbst durch (Aufbereitung der Daten, visuelle Inspektion der Modellvoraussetzung, Berechnung des Modells, Darstellung und Interpretation der Ergebnisse)



```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:13_Statistik1/Uebung_A.Rmd-->

## Übung A: Lösung

###  Musterlösung Aufgabe 3
[R-script als Download](13_Statistik1/RFiles/Uebung_A_loesung.R)


```{r,warning=F}
library(tidyverse)
```



Aus @crawley2015 [S. 162] (Scripts angepasst)

There are two traditional ways of plotting the results of ANOVA:

- box-and-whisker plots
- barplots with error bars

We have an experiment on plant competition where the response variable is biomass and we have one factor with five levels. The factor is called cl ipping and the levels are control (i.e. unclipped), two intensities of shoot pruning and two intensities of root pruning:

```{r}
comp <- read_delim("13_Statistik1/data/competition.csv",",")

ggplot(comp, aes(clipping, biomass)) +
  geom_boxplot(fill = "lightgrey") +
  labs(x = "Competition treatment", y = "Biomass")


```

The box-and-whisker plot is good at showing the nature of the variation within each treatment, and also whether there is skew within each treatment (e.g. for the control plots, there is a wider range of values between the median and upper quartile than between the lower quartile and median). No outliers are shown above the whiskers, so the tops and bottoms of the bars are the maxima and minima within each treatment. The medians for the competition treatments are all higher than the upper quartile of the controls, suggesting that they may be significantly different from the controls, but there is little to suggest that any of the competition treatments are significantly different from one another (see below for the analysis).

Barplots with error bars are the style preferred by many journal editors, and some people think that they make hypothesis testing easier. We shall see.
```{r}
comp_summary <- comp %>% 
  group_by(clipping) %>%
  summarise(
    mean = mean(biomass)
  )
```


Now draw the barplot, making sure that the y axis is long enough to accommodate the tops of the error bars that we intend to add later:


```{r}

p <- ggplot(comp_summary, aes(clipping, mean)) +
  geom_bar(stat = "identity")

p
```

This is fine as far as it goes, but it gives us no impression of the uncertainty associated with the estimated heights of the bars. 

Let us use one standard error of the mean based on the pooled error variance from the ANOVA, then return to a discussion of the pros and cons of different kinds of error bars later. Here is the one-way ANOVA:

```{r}
t <- aov(biomass~clipping, comp)

summary(t)

```

There was equal replication (which makes life easier), and each mean was based on six replicates, so the standard error of the mean is $\sqrt{\frac{s^2}{n}} = \sqrt{\frac{4961}{6}} = 28.75$

We shall draw an error bar up 28.75 from each mean and down by the same distance, so we need five values, one for each bar, each of 28.75:

```{r}
comp_summary <- comp %>% 
  group_by(clipping) %>%
  summarise(
    mean = mean(biomass),
    se = sqrt(4691/n())
  )

comp_summary

```
Now we can use the new function to add the error bars to the plot:
```{r}
ggplot(comp_summary, aes(clipping, mean)) +
  geom_bar(stat = "identity") +
  geom_errorbar(aes(ymin = mean-se, ymax = mean+se),width = 0.2)


```

We do not get the same feel for the distribution of the values within each treatment as was obtained by the box-and-whisker plot, but we can certainly see clearly which means are not significantly different. If, as here, we use ± 1 standard error as the length of the error bars, then when the bars overlap this implies that the two means are not significantly different. Remember the rule of thumb for t: significance requires 2 or more standard errors, and if the bars overlap it means that the difference between the means is less than 2 standard errors.This shows clearly that none of the means for the clipped plants (n25, n50, r10 or r5) is significantly different from any other (the top of the bar for n25 overlaps the bottom of the bar for r10).

There is another issue, too. For comparing means, we should be using the standard error of the difference between two means (not the standard error of one mean) in our tests (see p. 91); these bars would be about 1.4 times as long as the bars we have drawn here. So while we can be sure that the pruning treatments are not significantly different from one another, we cannot conclude from this plot that the controls have significantly lower biomass than the rest (because the error bars are not the correct length for testing differences between means).

An alternative graphical method is to use 95% confidence intervals for the lengths of the bars, rather than standard errors of means. This is easy to do: we multiply our standard errors by Student’s t, qt ( .975,5) = 2.570582, to get the lengths of the confidence intervals:

```{r}






comp_summary <- comp %>% 
  group_by(clipping) %>%
  summarise(
    mean = mean(biomass),
    se = sqrt(4691/n())
  ) %>%
  mutate(
    ci = se*qt(0.975,5)
  )

ggplot(comp_summary, aes(clipping, mean)) +
  geom_bar(stat = "identity") +
  geom_errorbar(aes(ymin = mean-ci, ymax = mean+ci),width = 0.2)



comp_summary <- comp %>% 
  group_by(clipping) %>%
  summarise(
    mean = mean(biomass),
    se = sqrt(4691/n())
  ) %>%
  mutate(
    ci = se*qt(0.975,5),
    lsd = (qt(0.975,10)*sqrt(2*4961/6))/2
  )


ggplot(comp_summary, aes(clipping, mean)) +
  geom_bar(stat = "identity") +
  geom_errorbar(aes(ymin = mean-lsd, ymax = mean+lsd),width = 0.2)
```


```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:13_Statistik1/Uebung_A_loesung.Rmd-->

## Übung B (SozOek)


###	Aufgabe 1:

Unterscheidet sich die novanimal-Stichprobe von der gesamten Population bezüglich Geschlecht und Hochschulzugehörigkeit? 


* Es wird angenommen, dass 15 Prozent aller Mitarbeitenden mit CampusCard weiblich sind. 16 Prozent der Population (N = 2138) sind Mitarbeiter und 37 Prozent Studenten.
* Definiert die Null- ($H_0$) und Alternativhypothese ($H_1$)
* Führt einen $\chi^2$-Test und anschliessend einen exakten Fishers-Test mit dem Datensatz novanimal.csv durch
* Fasst die Ergebnisse in einem Satz zusammen


###	Aufgabe 2: t-Test

Werden in den Basis- und Interventionswochen unterschiedlich viele Gerichte verkauft?

* Definiert die Null- ($H_0$) und Alternativhypothese ($H_1$)
* Führt einen t-Test mit dem Datensatz novanimal.csv durch
* Welche Form von t-Test müssen Sie anwenden? einseitig/zweiseitig? gepaart/ungepaart?
* Überprüft die Voraussetzungen für einen t-Test (verwendet dafür das classic_theme())
* Fasst die Ergebnisse in einem Satz zusammen





```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:13_Statistik1/Uebung_B.Rmd-->

## Übung B: Lösung


###  Musterlösung Aufgabe 1
[R-script als Download](13_Statistik1/RFiles/Uebung_B_loesung.R)

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width = 20, fig.height = 12, warning = F, message = F, error = F, include = T)
#knitr::opts_chunk$get("root.dir")
```


```{r}
## Lade Datei

library(tidyverse)
nova <- read_delim("13_Statistik1/data/novanimal.csv", delim = ";")

## definiere theme für die Plots

mytheme <- theme_classic() + 
    theme(axis.line = element_line(color = "black"), axis.text = element_text(size = 25, color = "black"), axis.title = element_text(size = 25, color = "black"), axis.ticks = element_line(size = 1, color = "black"), axis.ticks.length = unit(.5, "cm"))

```

$H_0$: Es gibt keine Unterschiede zwischen der Population und Stichprobe bezüglich Geschlecht und Hochschulzugehörigkeit. 
$H_1$: Es gibt Unterschiede zwischen der Population und Stichprobe bezüglich Geschlecht und Hochschulzugehörigkeit.

```{r}
# Gruppiere und fasse die Variablen nach Geschlecht und Hochschulzugehörigkeit zusammen 
canteen <- group_by(nova, gender, member) %>% 
  summarise(tot = n()) %>% 
  ungroup() %>%
  mutate(canteen_member = c("Mitarbeiterinnen", "Studentinnen", "Mitarbeiter", "Studenten")) # Fasse die ersten beiden Variablen zusammen

# Definiere einen Vektor mit erwarteten Häufigkeiten, beachte dabei die Reihenfolge 

population_exp <- c(.15,.32,.16,.37) # erwartete Verteilung der Population (Achtung die Summe muss 1 ergeben)
  
# Berechne den Chi-Quadrat-Test

chi_sq <- chisq.test(canteen$tot, p = population_exp)
chi_sq

# Führe einen exakten Fisher Test durch
# erstelle dafür einen neuen Datensatz. tibble(), ist eine ähnliche Funktion wie data.frame() vom Hause tidyverse

fisher_t <- tibble(member = c("Mitarbeiterinnen", "Studentinnen", "Mitarbeiter", "Studenten"),
  population = population_exp * 2138, # absolute Zahlen der Population Mitarbeiterinnen, Studentinnen, Mitarbeiter, Studenten => Fisher exakt test kann nicht mit erwarteten Wahrscheinlichkeiten rechnen
  stichprobe = canteen$tot,
  population_pct = population_exp,
  canteen_pct = canteen$tot / sum(canteen$tot)) 

fish <- fisher.test(fisher_t[ ,2:3]) # was sind die Unterschiede zwischen den beiden Tests (Chisquare und Fisher exakt Test)? Wieso gibt es keinen OR aus?
fish
```

Der $\chi^2$-Test sagt uns, dass die NOVANIMAL-Stichprobe von der Population signifikant unterscheidet ($\chi^2$(`r chi_sq$parameter`) = `r round(chi_sq$statistic[[1]], digits = 3)`, *p* > .0001). Auch der exakte Fisher-Test zeigt einen ähnlichen p-Wert (*p* = `r round(fish$p.value, digits = 3)`). Demnach ist unsere Stichprobe im Geschlecht und in der Hochschuzugehörigkeit nicht für die Population repräsentativ. Es scheint, dass die Studentinnen unter- und die Studenten übervertreten sind.


###  Musterlösung Aufgabe 2

$H_0$: Es gibt keine Unterschiede in den Verkaufszahlen zwischen Basis- und Interventionswochen. 
$H_1$: Es gibt Unterschiede in den Verkaufszahlen zwischen Basis- und Interventionswochen.


```{r}
# Daten müssen zuerst nach "week" und "condition" zusammengefasst werden

df <- nova %>%
    group_by(date, condit) %>%  
    summarise(tot_sold = n()) %>% # zählt alle Beobachtungen gemäss dem group_by zusammen
    mutate(day = format(date, format = "%d.%m")) # erstelle neue Variable, damit beim Historgamm die X-Achse besser leserlich wird

# Testen der Voraussetzungen
ggplot(df, aes(x = condit, y= tot_sold)) + 
    geom_boxplot(fill = "white", color = "black") + 
    scale_y_continuous(breaks = seq(0,60,10), limits = c(0,60)) +
    labs(x="\nBedingungen", y="Anzahl verkaufte Gerichte pro Tag\n") + 
    mytheme
# Histogramme für die Bedingungen Basis
df %>% filter(condit == "Basis") %>%
ggplot(aes(x = as.factor(day), y= tot_sold)) + 
    geom_bar(stat = "identity", fill = "lightgrey", width = .6) + 
    scale_y_continuous(breaks = seq(0,60,10), limits = c(0,60)) +
    labs(x="Tage", y="Anzahl verkaufte Gerichte") +
    mytheme
  
# Histogramme für die Bedingungen Intervention
df %>% filter(condit == "Intervention") %>%
ggplot(aes(x = as.factor(day), y= tot_sold)) + 
    geom_bar(stat = "identity", fill = "lightgrey", width = .6) + 
    scale_y_continuous(breaks = seq(0,60,10), limits = c(0,60)) + 
    labs(x="Tage", y="Anzahl verkaufte Gerichte") +
    mytheme

```


```{r}

# Durchführung eines t-Tests
t_test <- t.test(df[df$condit == "Basis", ]$tot_sold, df[df$condit == "Intervention", ]$tot_sold, var.equal = F) # siehe ungerichtete Hypothese
t_test
```

In den Basiswochen werden mehr Gerichte als in den Interventionsowchen verkauft. Die wöchentlichen Verkaufszahlen zwischen den Bedigungen (Basis oder Intervention) unterscheiden sich jedoch nicht signifikant (*t*(`r round(t_test$parameter[[1]],digits = 2`) = `r round(t_test$statistic, digits = 4)` , *p* = `r round(t_test$p.value, digits=3)`.

```{r include=FALSE, cache=FALSE}





# detach packages
packages = sapply(sessionInfo()$otherPkgs, function(x) x$Package)
packages = packages[packages != "bookdown"]
packages = sapply(packages, function(p) paste0("package:", p))
lapply(packages, detach, character.only = TRUE, unload = TRUE)
# clear environment
rm(list = ls())
```

<!--chapter:end:13_Statistik1/Uebung_B_loesung.Rmd-->

