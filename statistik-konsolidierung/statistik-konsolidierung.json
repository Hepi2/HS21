[
  {
    "path": "statistik-konsolidierung/Statistik_Konsolidierung1_Demo_assoziationen/",
    "title": "Konsolidierung 1: Demo Assoziationen",
    "description": {},
    "author": [
      {
        "name": "Gian-Andrea Egeler",
        "url": {}
      }
    ],
    "date": "2021-11-12",
    "categories": [
      "Statistik_Konsolidierung1"
    ],
    "contents": "\nKonsolidierung 1: Demo Assoziationen\n\nDownload R-Skript\n\n\n\n#lade Packages\n\nlibrary(tidyverse)\n\n\n#mytheme\nmytheme <- \n  theme_classic() + \n  theme(\n    axis.line = element_line(color = \"black\"), \n    axis.text = element_text(size = 20, color = \"black\"), \n    axis.title = element_text(size = 20, color = \"black\"), \n    axis.ticks = element_line(size = 1, color = \"black\"), \n    axis.ticks.length = unit(.5, \"cm\")\n  )\n\n\n\n\n\n\n#lade Daten\n# mehr Info darüber: https://cran.r-project.org/web/packages/explore/vignettes/explore_mtcars.html\ncars <- mtcars\n\n#neue kategoriale variable\ncars %<>% \n  as_tibble() %>% # da \"nur\" data frame kann glimplse nichts damit anfangen \n  mutate(vs_cat = if_else(.$vs == 0, \"normal\", \"v-type\")) %>% \n  mutate(am_cat = if_else(am == 0, \"automatic\", \"manual\"))\n\n# betrachte die Daten\nsummary(cars)\n\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb          vs_cat         \n Min.   :0.0000   Min.   :3.000   Min.   :1.000   Length:32         \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000   Class :character  \n Median :0.0000   Median :4.000   Median :2.000   Mode  :character  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812                     \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000                     \n Max.   :1.0000   Max.   :5.000   Max.   :8.000                     \n    am_cat         \n Length:32         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\nglimpse(cars)\n\n\nRows: 32\nColumns: 13\n$ mpg    <dbl> 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8,~\n$ cyl    <dbl> 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4,~\n$ disp   <dbl> 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.~\n$ hp     <dbl> 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 1~\n$ drat   <dbl> 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92,~\n$ wt     <dbl> 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.19~\n$ qsec   <dbl> 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.0~\n$ vs     <dbl> 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,~\n$ am     <dbl> 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,~\n$ gear   <dbl> 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4,~\n$ carb   <dbl> 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1,~\n$ vs_cat <chr> \"normal\", \"normal\", \"v-type\", \"v-type\", \"normal\", \"v-~\n$ am_cat <chr> \"manual\", \"manual\", \"manual\", \"automatic\", \"automatic~\n\n\n#Assoziation zwischen Anzahl Zylinder und Motorentyp ()\ntable(cars$vs_cat, cars$am_cat) # Achtung: sieht aus, als gäbe es weniger V-Motoren bei den handgeschalteten Autos\n\n\n        \n         automatic manual\n  normal        12      6\n  v-type         7      7\n\n\n#lass und das überprüfen\n#achtung: bei chi-square test kommt es sehr auf das format drauf an (er erwartet entweder vektoren oder eine matrix!)\n\n#exkurs um in es in ein matrix form zu bringen\nchi_sq_matrix <- xtabs(~ vs_cat + am_cat, data = as.data.frame(cars)) # in diesem Spezialfall haben wir keine Kriteriumsvariable\n\n#1.version\nchisq.test(chi_sq_matrix)\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  chi_sq_matrix\nX-squared = 0.34754, df = 1, p-value = 0.5555\n\n\n#2. version\nchi_sq <- chisq.test(cars$am_cat, cars$vs_cat)\n\n#resp. fisher exacter test verwenden, da 2x2 table\nfisher.test(chi_sq_matrix)\n\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  chi_sq_matrix\np-value = 0.4727\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.3825342 10.5916087\nsample estimates:\nodds ratio \n  1.956055 \n\n\n#fisher exakter test\nfisher.test(cars$am_cat, cars$vs_cat)\n\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  cars$am_cat and cars$vs_cat\np-value = 0.4727\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.3825342 10.5916087\nsample estimates:\nodds ratio \n  1.956055 \n\n\n\n#visualisieren: kudos goes to https://mgimond.github.io/Stats-in-R/ChiSquare_test.html#3_two_factor_classification\nOP <- par(mfrow=c(1,2), \"mar\"=c(1,1,3,1))\nmosaicplot(chi_sq$observed, cex.axis =1 , main = \"Observed counts\")\nmosaicplot(chi_sq$expected, cex.axis =1 , main = \"Expected counts\\n(if class had no influence)\")\n\n\n\npar(OP)\n\n\n\n\nmöglicher Text für Ergebnisse\nDer \\(\\chi^2\\)-Test sagt uns, dass das Art des Motors und Art des Fahrwerks statistisch nicht zusammenhängen. Es gibt keine signifikante Unterscheide zwischen den Variablen “VS” und “AM - Transmission” (\\(\\chi^2\\)(1) = 0.348, p = 0.556. Der Fisher exacter Test bestätigt diesen Befund. Die Odds Ratio (OR) sagt uns hingegen - unter der Prämisse, dass “normale” Motoren eher mit automatischen und V-Motoren eher mit handgeschalteten Fahrwerken ausgestattet sind - dass die Chance doppelt so hoch ist, dass ein Auto mit “normalem” Motor automatisch geschaltet ist, als dies bei einem Auto mit V-Motor der Fall wäre\n\n\n#define dataset\ncars <- mtcars\n\n#neue kategoriale variable\ncars %<>% \n  as_tibble() %>% # da \"nur\" data frame kann glimplse nichts damit anfangen \n  mutate(vs_cat = if_else(.$vs == 0, \"normal\", \"v-type\")) %>% \n  mutate(am_cat = if_else(am == 0, \"automatic\", \"manual\"))\n\n\n# bei t-Test immer zuerst visualisieren: in diesem Fall Boxplot mit Variablen Getriebe (v- vs. s-motor) und Anzahl Pferdestärke\nggplot2::ggplot(cars, aes(y = hp, x = vs_cat)) +\n  # stat_boxplot(geom ='errorbar', width = .25) +\n  # geom_boxplot() +\n  geom_violin()+\n  labs(x = \"\\nBauform Motor\", y = \"Pferdestärke (PS)\\n\") +\n  mytheme\n\n\n\n  \n#alternativ     \nboxplot(cars$hp ~ cars$vs_cat) # sieht ganz ok aus, jedoch weist die variable \"normale Motoren\" deutlich eine grössere Streuung aus -> siehe aov.1 und deren Modelgüte-Plots\n\n\n\n\n# Definiere Model: t-Test, wobei die AV metrisch (in unserem Fall eine Zählvariable) sein muss\nttest <- t.test(cars$hp ~ cars$vs_cat)\naov.1 <- aov(cars$hp ~ cars$vs_cat)\n\n\n#schaue Modellgüte an\npar(mfrow = c(2,2))\nplot(aov.1)\n\n\n\n\n#zeige resultate\nttest\n\n\n\n    Welch Two Sample t-test\n\ndata:  cars$hp by cars$vs_cat\nt = 6.2908, df = 23.561, p-value = 1.82e-06\nalternative hypothesis: true difference in means between group normal and group v-type is not equal to 0\n95 percent confidence interval:\n  66.06161 130.66854\nsample estimates:\nmean in group normal mean in group v-type \n           189.72222             91.35714 \n\nsummary.lm(aov.1)\n\n\n\nCall:\naov(formula = cars$hp ~ cars$vs_cat)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-98.72 -25.61  -4.04  22.55 145.28 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         189.72      11.35  16.720  < 2e-16 ***\ncars$vs_catv-type   -98.37      17.16  -5.734 2.94e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 48.14 on 30 degrees of freedom\nMultiple R-squared:  0.5229,    Adjusted R-squared:  0.507 \nF-statistic: 32.88 on 1 and 30 DF,  p-value: 2.941e-06\n\n\n#wie würdet ihr nun die Ergebnisse darstellen?\n\n\n\n\n\n# für mehr infos here: https://cran.r-project.org/web/packages/datasauRus/vignettes/Datasaurus.html\n\nlibrary(datasauRus)\nif(requireNamespace(\"dplyr\")){\n  suppressPackageStartupMessages(library(dplyr))\n  dt <- datasaurus_dozen %>% \n    group_by(dataset) %>% \n    summarize(\n      mean_x    = mean(x),\n      mean_y    = mean(y),\n      std_dev_x = sd(x),\n      std_dev_y = sd(y),\n      corr_x_y  = cor(x, y)\n    )\n}\n\n# check data structure\nglimpse(dt)\n\n\n# plot two examples  \nif(requireNamespace(\"ggplot2\")){\n  library(ggplot2)\n  \n  dt = filter(datasaurus_dozen, dataset == \"dino\" | dataset == \"slant_up\")\n  \n  ggplot(dt, aes(x=x, y=y, colour=dataset))+\n    geom_point()+\n    theme_bw() +\n    theme(legend.position = \"none\") +\n    facet_wrap(~dataset) +\n    geom_smooth(method = \"lm\", se = FALSE)\n  \n}\n\n\n\n\n\n\n\n",
    "preview": "statistik-konsolidierung/Statistik_Konsolidierung1_Demo_assoziationen/distill-preview.png",
    "last_modified": "2021-11-15T09:44:10+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "statistik-konsolidierung/Statistik_Konsolidierung1_open_datasets/",
    "title": "Open Datasets",
    "description": {},
    "author": [
      {
        "name": "Gian-Andrea Egeler",
        "url": {}
      }
    ],
    "date": "2021-11-12",
    "categories": [
      "Statistik_Konsolidierung1"
    ],
    "contents": "\n\n\n\nVerschiedene Datensätze\nIn diesem Dokument findet ihr verschiedene Wege und Quellen, um an Datensätze zu gelangen.\nin R\nIn R gibt es vordefinierte Datensätze, welche gut abrufbar sind. Beispiele sind:\nsleep\nUSAccDeaths\nUSArrests\n…\n\n\ndata() # erzeugt eine Liste mit den Datensätzen, welche in R verfügbaren sind\nhead(chickwts)\n\n\n  weight      feed\n1    179 horsebean\n2    160 horsebean\n3    136 horsebean\n4    227 horsebean\n5    217 horsebean\n6    168 horsebean\n\nstr(chickwts)\n\n\n'data.frame':   71 obs. of  2 variables:\n $ weight: num  179 160 136 227 217 168 108 124 143 140 ...\n $ feed  : Factor w/ 6 levels \"casein\",\"horsebean\",..: 2 2 2 2 2 2 2 2 2 2 ...\n\nKaggle\nAuf Kaggle findet ihr öffentlich zugängliche Datensätze. Einzig was ihr tun müsst, ist euch registrieren. Beispiele sind:\n911\nfoodPreferences\nS.F. salaries\n…\n\n\n# Load packages and data\ndata_911 <- read_delim(\"911.csv\", delim = \",\")\nstr(data_911)\n\n\nspec_tbl_df [99,492 x 9] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ lat      : num [1:99492] 40.3 40.3 40.1 40.1 40.3 ...\n $ lng      : num [1:99492] -75.6 -75.3 -75.4 -75.3 -75.6 ...\n $ desc     : chr [1:99492] \"REINDEER CT & DEAD END;  NEW HANOVER; Station 332; 2015-12-10 @ 17:10:52;\" \"BRIAR PATH & WHITEMARSH LN;  HATFIELD TOWNSHIP; Station 345; 2015-12-10 @ 17:29:21;\" \"HAWS AVE; NORRISTOWN; 2015-12-10 @ 14:39:21-Station:STA27;\" \"AIRY ST & SWEDE ST;  NORRISTOWN; Station 308A; 2015-12-10 @ 16:47:36;\" ...\n $ zip      : num [1:99492] 19525 19446 19401 19401 NA ...\n $ title    : chr [1:99492] \"EMS: BACK PAINS/INJURY\" \"EMS: DIABETIC EMERGENCY\" \"Fire: GAS-ODOR/LEAK\" \"EMS: CARDIAC EMERGENCY\" ...\n $ timeStamp: POSIXct[1:99492], format: \"2015-12-10 17:40:00\" ...\n $ twp      : chr [1:99492] \"NEW HANOVER\" \"HATFIELD TOWNSHIP\" \"NORRISTOWN\" \"NORRISTOWN\" ...\n $ addr     : chr [1:99492] \"REINDEER CT & DEAD END\" \"BRIAR PATH & WHITEMARSH LN\" \"HAWS AVE\" \"AIRY ST & SWEDE ST\" ...\n $ e        : num [1:99492] 1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   lat = col_double(),\n  ..   lng = col_double(),\n  ..   desc = col_character(),\n  ..   zip = col_double(),\n  ..   title = col_character(),\n  ..   timeStamp = col_datetime(format = \"\"),\n  ..   twp = col_character(),\n  ..   addr = col_character(),\n  ..   e = col_double()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n\nTidytuesday\nTidytuesday ist eine Plattform, in der wöchentlich - jeden Dienstag - einen öffentlich zugänglichen Datensatz publiziert. Dieses Projekt ist aus der R4DS Online Learning Community und dem R for Data Science Lehrbuch hervorgegangen. Beispiele sind:\nWomen in the Workplace\nDairy production & Consumption\nStar Wars Survey\nGlobal Coffee Chains\nMalaria Deaths\n…\nDownload via Github - 1. Möglichkeit\nGeht zum File, welches ihr herunterladen wollt\nKlickt auf das File (.csv, .xlsx etc.), um den Inhalt innerhalb der GitHub Benutzeroberfläche anzuzeigen \nKlickt mit der rechten Maustaste auf den Knopf “raw” \n(Ziel) Speichern unter…\nDownload via Github - 2. Möglichkeit\n\n\n# Beachtet dabei, dass ihr die URL zum originalen (raw) Datensatz habt \nstar_wars <- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-05-14/week7_starwars.csv\", locale = readr::locale(encoding = \"latin1\")) #not working yet \n\n\n\nopendata.swiss\nAuf opendata.swiss sind offene, frei verfügbare Daten der Schweizerischen Behörden zu finden. opendata.swiss ist ein gemeinsames Projekt von Bund, Kantonen, Gemeinden und weiteren Organisationen mit einem staatlichen Auftrag. Beispiele sind:\nStatistik der Schweizer Städte\nVerpflegungsbetriebe nach Jahr und Stadtquartier\nAltpapiermengen\n…\nOpen Data Katalog Stadt Zürich\nAuf der Seite der Stadt Zürich Open Data findet ihr verschiedene Datensätze der Stadt Zürich. Spannend daran ist, dass die veröffentlichten Daten kostenlos und zur freien - auch kommerziellen - Weiterverwendung zur Verfügung. Beispiele sind:\nBevölkerung nach Bildungsstand, Jahr, Alter und Geschlecht seit 1970\nLuftqualitätsmessungen\nHäufigste Hauptsprachen\n…\n\n\n# lade die Datei \"Häufigste Sprachen\"\nurlfile = \"https://data.stadt-zuerich.ch/dataset/bfs_ste_bev_hauptsprachen_top50_od3011/download/BEV301OD3011.csv\"\n\ndat_lang <- read_delim(url(urlfile), delim = \",\", col_names = T)\nhead(dat_lang)\n\n\n# A tibble: 6 x 7\n  Sprache  AntBev AnzBev untAntBevKI obAntBevKI untAnzBevKI obAnzBevKI\n  <chr>     <dbl>  <dbl>       <dbl>      <dbl>       <dbl>      <dbl>\n1 Deutsch    75.6 259680        75.2       76.1      258000     261360\n2 Englisch   12.9  44190        12.5       13.3       42880      45490\n3 Italien~    6.1  21040         5.9        6.4       20120      21950\n4 Französ~    4.8  16460         4.6        5         15650      17280\n5 Spanisch    4.3  14710         4.1        4.5       13920      15500\n6 Serbokr~    3.4  11560         3.2        3.6       10860      12260\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-15T09:44:10+00:00",
    "input_file": {}
  },
  {
    "path": "statistik-konsolidierung/Statistik_Konsolidierung1_suggest_datasets/",
    "title": "Vorgeschlagene Datensätze",
    "description": {},
    "author": [
      {
        "name": "Gian-Andrea Egeler",
        "url": {}
      }
    ],
    "date": "2021-11-12",
    "categories": [
      "Statistik_Konsolidierung1"
    ],
    "contents": "\nVorgeschlagene Datensätze zur Repetition der Übungen\nR data sets\nchickwts\niris\nTitanic (achtung hat das “table” Datenformat)\nstarwars (dplyr package)\nResearch Methods data sets\nNOVANIMAL (Kassendaten oder Gästebefragung)\nUkraine (Demoskript 3)\nIpomopsis (Demoskript 3)\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-15T09:44:10+00:00",
    "input_file": {}
  },
  {
    "path": "statistik-konsolidierung/Statistik_Konsolidierung2_Demo_ordinationen/",
    "title": "Konsolidierung 2: Ordinationen",
    "description": {},
    "author": [
      {
        "name": "Gian-Andrea Egeler Jürgen Dengler",
        "url": {}
      }
    ],
    "date": "2021-11-12",
    "categories": [
      "Statistik_Konsolidierung2"
    ],
    "contents": "\n\n\n\n\nDownload R-Skript\n\nDemo Ordinationen (PCA)\n\n\n\n#Beispiel inspiriert von Luke Hayden: https://www.datacamp.com/community/tutorials/pca-analysis-r\n\n#Ausgangslage: viel zusammenhängende Variablen\n#Ziel: Reduktion der Variablenkomplexität\n#WICHTIG hier: Datenformat muss Wide sein! Damit die Matrixmultiplikation gemacht werden kann\n\n# lade Datei\ncars <- mtcars\n\n# Korrelationen\ncor<- cor(cars[,c(1:7,10,11)])\ncor[abs(cor)<.7] <- 0\ncor\n\n# pca\n# achtung unterschiedliche messeinheiten, wichtig es muss noch einheitlich transfomiert werden\nlibrary(FactoMineR) # siehe Beispiel hier: https://www.youtube.com/watch?v=vP4korRby0Q\no.pca <- PCA(mtcars[,c(1:7,10,11)], scale.unit = TRUE) # entweder korrelations oder covarianzmatrix\n\n\n\n\n# schaue output an\nsummary(o.pca) # generiert auch automatische plots\n\n\n# plote das ganze\nlibrary(ggbiplot)\nggbiplot(o.pca,choices = c(1,2))\n\n\n\n\n# nehme noch die autonamen hinzu\nggbiplot(o.pca, labels=rownames(mtcars), choices = c(1,2)) # + mytheme # choice gibt die axen an\n\n\n\n\n\n\n\nlibrary(vegan)\n\n# ebenfalls mit transformierten daten\no.ca<-vegan::cca(cars)\no.ca1 <- FactoMineR::CA(cars) #blau: auots, rot: variablen\n\n\n\n\n# plotten (schwarz: autos, rot: variablen)\nplot(o.ca)\n\n\n\nsummary(o.ca)\nsummary(o.ca1)\n\n#Nur autos plotten; wieso?\nx<-o.ca$CA$u[,1]\ny<-o.ca$CA$u[,2]\nplot(x,y)\n\n\n\n\n#Anteilige Varianz, die durch die ersten beiden Achsen erklaert wird\no.ca$CA$eig[1:63]/sum(o.ca$CA$eig)\n\n\n\n\n\n\n#Distanzmatrix als Start erzeugen\nlibrary(MASS)\n\nmde <-vegan::vegdist(cars,method=\"euclidean\")\nmdm <-vegan::vegdist(cars,method=\"manhattan\")\n\n#Zwei verschiedene NMDS-Methoden\nset.seed(1) #macht man, wenn man bei einer Wiederholung exakt die gleichen Ergebnisse will\no.imds<-MASS::isoMDS(mde, k=2) # mit K = Dimensionen\nset.seed(1)\no.mmds<-vegan::metaMDS(mde,k=2) # scheint nicht mit 2 Dimensionen zu konvergieren\n\nplot(o.imds$points)\n\n\n\n\n\n#Stress =  Abweichung der zweidimensionalen NMDS-Loesung von der originalen Distanzmatrix\nstressplot(o.imds,mde)\n\n\n\n\n\n\n\n#Mit Beispieldaten aus Wildi (2013, 2017)\nlibrary(labdsv)\nlibrary(dave) # lade package für Daten sveg\n# head(sveg)\n\n\n#PCA-----------\n#Deckungen Wurzeltransformiert, cor=T erzwingt Nutzung der Korrelationsmatrix\no.pca <- labdsv::pca(sveg^0.25,cor=T)\no.pca2 <- stats::prcomp(sveg^0.25)\n\n#Koordinaten im Ordinationsraum => Y\nhead(o.pca$scores)\nhead(o.pca2$x)\n\n#Korrelationen der Variablen mit den Ordinationsachsen\nhead(o.pca$loadings)\nhead(o.pca2$rotation)\n\n#Erklaerte Varianz der Achsen (sdev ist die Wurzel daraus)\n# früher gabs den Befehl summary()\n# jetzt von hand: standardabweichung im quadrat/totale varianz * 100 (um prozentwerte zu bekommen)\nE<-o.pca$sdev^2/o.pca$totdev*100\nE[1:5] # erste fünf PCA\n\n\n#PCA-Plot der Lage der Beobachtungen im Ordinationsraum\nplot(o.pca$scores[,1],o.pca$scores[,2],type=\"n\", asp=1, xlab=\"PC1\", ylab=\"PC2\")\npoints(o.pca$scores[,1],o.pca$scores[,2],pch=18)\n\n\n\n\nplot(o.pca$scores[,1],o.pca$scores[,3],type=\"n\", asp=1, xlab=\"PC1\", ylab=\"PC3\")\npoints(o.pca$scores[,1],o.pca$scores[,3],pch=18)\n\n\n\n\n#Subjektive Auswahl von Arten zur Darstellung\nsel.sp <- c(3,11,23,39,46,72,77,96, 101, 119)\nsnames <- names(sveg[ , sel.sp])\nsnames\n\n#PCA-Plot der Korrelationen der Variablen (hier Arten) mit den Achsen (h)\nx <- o.pca$loadings[,1]\ny <- o.pca$loadings[,2]\nplot(x,y,type=\"n\",asp=1)\narrows(0,0,x[sel.sp],y[sel.sp],length=0.08)\ntext(x[sel.sp],y[sel.sp],snames,pos=1,cex=0.6)\n\n\n\n\n# hier gehts noch zu weiteren Beispielen zu PCA's:\n# https://stats.stackexchange.com/questions/102882/steps-done-in-factor-analysis-compared-to-steps-done-in-pca/102999#102999\n# https://stats.stackexchange.com/questions/222/what-are-principal-component-scores\n# https://stats.stackexchange.com/questions/102882/steps-done-in-factor-analysis-compared-to-steps-done-in-pca/102999#102999\n\n\n\n\n\n#Idee von Ordinationen aus Wildi p. 73-74\nlibrary(labdsv)\n\n#Für Ordinationen benötigen wir Matrizen, nicht Data.frames\n#Generieren von Daten\nraw <- matrix(c(1,2,2.5,2.5,1,0.5,0,1,2,4,3,1), nrow=6)\ncolnames(raw) <- c(\"spec.1\", \"spec.2\")\nrownames(raw) <- c(\"r1\",\"r2\",\"r3\",\"r4\",\"r5\",\"r6\")\nraw\n\n#originale Daten im zweidimensionalen Raum\nx1 <- raw[,1]\ny1 <- raw[,2]\nz <- c(rep(1:6))\n\n\n#Plot Abhängigkeit der Arten vom Umweltgradienten\nplot(c(x1, y1)~c(z,z), type=\"n\", axes=T, bty=\"l\", las=1, xlim=c(1,6), ylim=c(0,5),\n     xlab=\"Umweltgradient\",ylab=\"Deckung der Arten\")\npoints(x1~z, pch=21, type=\"b\")\npoints(y1~z, pch=16, type=\"b\")\n\n\n\n\n#zentrierte Daten\ncent <- scale(raw, scale=F)\nx2 <- cent[,1]\ny2 <- cent[,2]\n\n#rotierte Daten\no.pca <- pca(raw)\nx3 <- o.pca$scores[,1]\ny3 <- o.pca$scores[,2]\n\n\n#Visualisierung der Schritte im Ordinationsraum\nplot(c(y1,y2,y3)~c(x1,x2,x3), type=\"n\", axes=T, bty=\"l\", las=1, xlim=c(-4,4), \n     ylim=c(-4,4), xlab=\"Art 1\", ylab=\"Art 2\")\npoints(y1~x1, pch=21, type=\"b\", col=\"green\", lwd=2)\npoints(y2~x2, pch=16, type=\"b\",col=\"red\", lwd=2)\npoints(y3~x3, pch=17, type=\"b\", col=\"blue\", lwd=2)\n\n\n\n\n#Durchführung der PCA\npca <- pca(raw)\n\n#Koordinaten im Ordinationsraum\npca$scores\n\n#Korrelationen der Variablen mit den Ordinationsachsen\npca$loadings\n\n#Erklärte Varianz der Achsen in Prozent\nE <- pca$sdev^2/pca$totdev*100\nE\n\n#mit prcomp, ein weiteres Package für Ordinationen\npca.2 <- prcomp(raw, scale=F)\nsummary(pca.2)\nplot(pca.2)\n\n\n\nbiplot(pca.2)\n\n\n\n\n#mit vegan, ein anderes Package für Ordinationen\nlibrary(\"vegan\")\npca.3 <- rda(raw, scale=FALSE) #Die Funktion rda führt ein PCA aus an wenn nicht Umwelt und Artdaten definiert werden\n#scores(pca.3,display=c(\"sites\"))\n#scores(pca.3,display=c(\"species\"))\nsummary(pca.3, axes=0)\nbiplot(pca.3, scaling=2)\nbiplot(pca.3, scaling=\"species\")#scaling=species macht das selbe wie scaling=2\n\n\n\n\n\n\n#PCA: Deckungen Wurzeltransformiert, cor=T erzwingt Nutzung der Korrelationsmatrix\npca.5 <- pca(sveg^0.25, cor=T)\n\n#Koordinaten im Ordinationsraum\npca.5$scores\n\n#Korrelationen der Variablen mit den Ordinationsachsen\npca.5$loadings\n\n#Erklärte Varianz der Achsen in Prozent (sdev ist die Wurzel daraus)\nE<-pca.5$sdev^2/pca.5$totdev*100\nE\nE[1:5]\n\n#PCA-Plot der Lage der Beobachtungen im Ordinationsraum\nplot(pca.5$scores[,1], pca.5$scores[,2], type=\"n\", asp=1, xlab=\"PC1\", ylab=\"PC2\")\npoints(pca.5$scores[,1], pca.5$scores[,2], pch=18)\n\n\n\n\n#Subjektive Auswahl von Arten zur Darstellung\nsel.sp <- c(3,11,23,39,46,72,77,96)\nsnames <- names(sveg[,sel.sp])\nsnames\n\n#PCA-Plot der Korrelationen der Variablen (hier Arten) mit den Achsen (h)\nx <- pca.5$loadings[,1]\ny <- pca.5$loadings[,2]\nplot(x,y,type=\"n\", asp=1)\narrows(0,0, x[sel.sp], y[sel.sp], length=0.08)\ntext(x[sel.sp], y[sel.sp], snames,pos=1, cex=0.6)\n\n\n\n\n\n# Mit vegan\npca.6 <- rda(sveg^0.25, scale=TRUE)\n#Erklärte Varianz der Achsen\nsummary(pca.6, axes=0)\n#PCA-Plot der Lage der Beobachtungen im Ordinationsraum\nbiplot(pca.6, display = \"sites\", type = \"points\", scaling=1)\n\n\n\n#Subjektive Auswahl von Arten zur Darstellung\nsel.sp <- c(3,11,23,39,46,72,77,96)\nsnames <- names(sveg[,sel.sp])\nsnames\n#PCA-Plot der Korrelationen der Variablen (hier Arten) mit den Achsen (h)\nscores <- scores(pca.6, display=\"species\")\nx <- scores[,1]\ny <- scores[,2]\nplot(x, y, type=\"n\", asp=1)\narrows(0,0, x[sel.sp], y[sel.sp], length=0.08)\ntext(x[sel.sp], y[sel.sp], snames,pos=1,cex=0.6)\n\n\n\nplot(x, y, type=\"n\", asp=1, xlim=c(-1, 1), ylim=c(-0.6, 0.6)) # angepasste Achsen\n\n\n\n\n\n\nlibrary(vegan)\nlibrary(dave) #for the dataset sveg\nlibrary(FactoMineR)# siehe Beispiel hier: https://www.youtube.com/watch?v=vP4korRby0Q\n\n# ebenfalls mit transformierten daten\no.ca<-cca(sveg^0.5) #package vegan\no.ca1 <- CA(sveg^0.5) #package FactoMineR\n\n\n\n\n#Arten (o) und Communities (+) plotten\nplot(o.ca)\n\n\n\n\nsummary(o.ca1)\n\n\n#Nur Arten plotten\nx<-o.ca$CA$u[,1]\ny<-o.ca$CA$u[,2]\nplot(x,y)\n\n\n\n\n#Anteilige Varianz, die durch die ersten beiden Achsen erklaert wird\no.ca$CA$eig[1:63]/sum(o.ca$CA$eig)\n\n\n\n\n\n#NMDS----------\n\n#Distanzmatrix als Start erzeugen\nlibrary(MASS)\nlibrary(vegan)\n\nmde <-vegdist(sveg,method=\"euclidean\")\nmdm <-vegdist(sveg,method=\"manhattan\")\n\n#Zwei verschiedene NMDS-Methoden\nset.seed(1) #macht man, wenn man bei einer Wiederholung exakt die gleichen Ergebnisse will\no.imds<-isoMDS(mde, k=2) # mit K = Dimensionen\nset.seed(1)\no.mmds<-metaMDS(mde,k=3) # scheint nicht mit 2 Dimensionen zu konvergieren\n\nplot(o.imds$points)\nplot(o.mmds$points)\n\n#Stress =  Abweichung der zweidimensionalen NMDS-Loesung von der originalen Distanzmatrix\nstressplot(o.imds,mde)\nstressplot(o.mmds,mde)\n\n\n\n\n\n\n\n",
    "preview": "statistik-konsolidierung/Statistik_Konsolidierung2_Demo_ordinationen/distill-preview.png",
    "last_modified": "2021-11-15T09:44:10+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
