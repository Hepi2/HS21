{
  "articles": [
    {
      "path": "index.html",
      "title": "InfoVis1",
      "author": [],
      "contents": "\nDas Modul „Research Methods“ vermittelt vertiefte Methodenkompetenzen für praxisorientiertes und angewandtes wissenschaftliches Arbeiten im Fachbereich „Umwelt und Natürliche Ressourcen“ auf MSc-Niveau. Die Studierenden erarbeiten sich vertiefte Methodenkompetenzen für die analytische Betrachtung der Zusammenhänge im Gesamtsystem „Umwelt und Natürliche Ressourcen“. Die Studierenden erlernen die methodischen Kompetenzen, auf denen die nachfolgenden Module im MSc Programm UNR aufbauen. Das Modul vermittelt einerseits allgemeine, fächerübergreifende methodische Kompetenzen (z.B. Wissenschaftstheorie, computer-gestützte Datenverar-beitung und Statistik).\nHier werden die Unterlagen für die R-Übungsteile bereitgestellt. Es werden sukzessive sowohl Demo-Files, Aufgabenstellungen und Lösungen veröffentlicht.\n\n\n\n",
      "last_modified": "2021-08-25T09:08:14+02:00"
    },
    {
      "path": "index.html",
      "title": "Modul Research Methods",
      "author": [],
      "contents": "\nDas Modul „Research Methods“ vermittelt vertiefte Methodenkompetenzen für praxisorientiertes und angewandtes wissenschaftliches Arbeiten im Fachbereich „Umwelt und Natürliche Ressourcen“ auf MSc-Niveau. Die Studierenden erarbeiten sich vertiefte Methodenkompetenzen für die analytische Betrachtung der Zusammenhänge im Gesamtsystem „Umwelt und Natürliche Ressourcen“. Die Studierenden erlernen die methodischen Kompetenzen, auf denen die nachfolgenden Module im MSc Programm UNR aufbauen. Das Modul vermittelt einerseits allgemeine, fächerübergreifende methodische Kompetenzen (z.B. Wissenschaftstheorie, computer-gestützte Datenverar-beitung und Statistik).\nHier werden die Unterlagen für die R-Übungsteile bereitgestellt. Es werden sukzessive sowohl Demo-Files, Aufgabenstellungen und Lösungen veröffentlicht.\n\n\n\n",
      "last_modified": "2021-08-25T09:08:14+02:00"
    },
    {
      "path": "InfoVis_abstract.html",
      "title": "Informations Visualisierung",
      "author": [],
      "contents": "\n\nContents\nInfovis 1\nInfovis 2\n\nInfovis 1\nDie konventionelle schliessende Statistik arbeitet in der Regel konfirmatorisch, sprich aus der bestehenden Theorie heraus werden Hypothesen formuliert, welche sodann durch Experimente geprüft und akzeptiert oder verworfen werden. Die Explorative Datenanalyse (EDA) nimmt dazu eine antagonistische Analyseperspektive ein und will in den Daten zunächst Zusammenhänge aufdecken, welche dann wiederum zur Formulierung von prüfbaren Hypothesen führen kann. Die Einheit stellt dazu den klassischen 5-stufigen EDA-Prozess nach Tukey (1980!) vor. Abschliessend wird dann noch die Brücke geschlagen zur modernen Umsetzung der EDA in Form von Visual Analytics.\nInfovis 2\nDie Informationsvisualisierung ist eine vielseitige, effektive und effiziente Methode für die explorative Datenanalyse. Während Scatterplots und Histogramme weitherum bekannt sind, bieten weniger bekannte Informationsvisualisierungs-Typen wie etwa Parallelkoordinatenplots, TreeMaps oder Chorddiagramme originelle alternative Darstellungsformen zur visuellen Analyse von Datensätze, welche stets grösser und komplexer werden. Die Studierenden lernen in dieser Lerneinheit eine Reihe von Informationsvisualisierungstypen kennen, lernen diese zielführend zu gestalten und selber zu erstellen.\n\n\n\n",
      "last_modified": "2021-08-25T09:08:14+02:00"
    },
    {
      "path": "InfoVis.html",
      "title": "InfoVis",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2021-08-25T09:08:14+02:00"
    },
    {
      "path": "PrePro_abstract.html",
      "title": "Preprocessing: Die Vorverarbeitung von Daten",
      "author": [],
      "contents": "\nDie Datenkunde 2.0 gibt den Studierenden das Wissen und die Fertigkeiten an die Hand, selbst erhobene und bezogene Daten für Ihre eigenen Analysen vorzubereiten und anzureichern (preprocessing). Die Einheit vermittelt zentrale Datenverarbeitungskompetenzen und thematisiert bekannte Problemzonen der umweltwissenschaftlichen Datenverarbeitung – immer mit einer „hands-on“ Perspektive auf die begleitenden R-Übungen. Die Studierenden lernen die Eigenschaften ihrer Datensätze in der Fachsprache korrekt zu beschreiben. Sie lernen ausserdem Metadaten zu verstehen und die Implikationen derselben für ihre eigenen Analyseprojekte kritisch zu beurteilen. Zentrale Konzepte der Lerneinheit sind Skalenniveaus, Datentypen, Zeitdaten und Typumwandlungen.\nDie Lerneinheit vermittelt zentralste Fertigkeiten zur Vorverarbeitung von strukturierten Daten in der umweltwissenschaftlichen Forschung: Datensätze verbinden (Joins) und umformen („reshape“, „split-apply-combine“). Im Anwendungskontext haben Daten selten von Anfang an diejenige Struktur, welche für die statistische Auswertung oder für die Informationsvisualisierung erforderlich wäre. In dieser Lerneinheit lernen die Studierenden die für diese oft zeitraubenden Preprocessing-Schritte notwendigen Konzepte und R-Werkzeuge kennen und kompetent anzuwenden.\n\n\n\n",
      "last_modified": "2021-08-25T09:08:15+02:00"
    },
    {
      "path": "PrePro.html",
      "title": "Daten Preprocessing",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2021-08-25T09:08:15+02:00"
    },
    {
      "path": "RaumAn_abstract.html",
      "title": "Räumliche Analyse",
      "author": [],
      "contents": "\n\nContents\nTeil 1\nTeil 2\nTeil 3\n\nTeil 1\nDie erste Übung zur Raumanalyse illustriert das einfache Laden und Anzeigen von Geodaten im Vektor- und Raster-Datenformat. Zusätzlich veranschaulicht die Übung den Umgang mit Koordinatensystemen sowie die Vektor-Raster-Konvertierung. Einfach erste Analysen umfassen den Spatial Join (Annotieren von Punkten mit Attributen von die Punkte einbettenden Vektordaten) sowie Puffer-Operationen. Zum Abschluss thematisiert die Übung die Aggregationsabhängigkeit räumlicher Daten durch die Illustration des Modifiable Areal Unit Problem (MAUP). Inhaltlich orientiert sich die Übung an Bodeneigenschaften für den Untersuchungsraum Schweiz.\nTeil 2\nIn dieser zweiten Übung wirst Du wiederum Geodatensätze verarbeiten und darstellen. Wir starten mit einem Punktdatensatz zu einem Messnetz zur Erhebung der Luftqualität in der Schweiz (Stickstoffdioxid NO2 um genau zu sein). Im Gegensatz zum Punktdatensatz zur Wasserverfügbarkeit aus der vorherigen Übung, sind die Messstellen des Messnetzes zur Luftqualität sehr unregelmässig im Raum verteilt. Trotzdem möchten wir versuchen ein kontinuierliches Raster von Luftqualitätswerten für die ganze Schweiz zu interpolieren. Wir starten mit der einfachen Interpolations-Methode Inverse Distance Weighting IDW. Danach wollen wir für den gleichen Datensatz nach dem Ansatz der nächsten Nachbarn die Thiessen Polygone konstruieren. Im zweiten Teil der Übung wollen wir Dichteverteilung untersuchen. Dabei untersuchen wir einen Datensatz mit Bewegungsdaten eines Rotmilans in der Schweiz. Mittels einer Kernel Density Estimation (KDE) berechnen wir eine kontinuierliche Dichteverteilung, über die wir eine Annäherung an das Habitat des untersuchten Greifvogels berechnen können. Bevor wir aber starten, schauen wir uns die Punktdatensätze genauer an indem wir die G-Function berechnen und plotten.\nTeil 3\nZum Abschluss des Themenblockes Spatial Data Science berechnen wir mit dem Moran’s I einen Index zur Berechnung der räumlichen Autokorrelation einer Choroplethenkarte. Wir verwenden nochmals die aggregierten Choroplethenkarten zur Wasserverfügbarkeit aus der ersten Übung und schauen uns an, wie stark die Werte für die Kantone und die Bezirke autokorreliert sind. Anstatt einfach eine Funktion zur Berechnung von Moran’s I aufzurufen und diese dann wie eine Black Box anzuwenden, wollen wir Formel für die Berechnung des Index in Ihre Bausteine zerlegen und diese Schritt für Schritt selber nachrechnen. So seht Ihr, wie Moran’s I wirklich funktioniert und könnte dabei erst noch die zuvor gelernten Data Science Techniken repetieren.\nAusserdem zeigen wir Euch ganz einfache Verfahren, um die bereits erstellten Karten interaktiv zu machen.\nAuf geht’s!\n\n\n\n",
      "last_modified": "2021-08-25T09:08:16+02:00"
    },
    {
      "path": "RaumAn.html",
      "title": "Räumliche Analyse: 'Spatial Data Science'",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2021-08-25T09:08:16+02:00"
    },
    {
      "path": "Readme.html",
      "title": "Anleitung für Admins",
      "author": [],
      "contents": "\n\nContents\nDependencies\n\nDependencies\n\nPackage\nVersion\nAER\n1.2-9\nAICcmodavg\n2.3-1\nAlgDesign\n1.2.0\nAmelia\n1.8.0\nDBI\n1.1.1\nDT\n0.18\nDataEditR\n0.1.3\nFNN\n1.1.3\nFSA\n0.9.1\nFactoMineR\n2.4\nFormula\n1.2-4\nGGally\n2.1.2\nHmisc\n4.5-0\nKernSmooth\n2.23-20\nLearnBayes\n2.15.1\nMASS\n7.3-54\nMatrix\n1.3-4\nMatrixModels\n0.5-0\nMuMIn\n1.43.17\nR.cache\n0.15.0\nR.methodsS3\n1.8.1\nR.oo\n1.24.0\nR.utils\n2.10.1\nR6\n2.5.1\nRColorBrewer\n1.1-2\nRNeXML\n2.4.5\nRcpp\n1.0.7\nRcppArmadillo\n0.10.6.0.0\nRcppEigen\n0.3.3.9.1\nRtsne\n0.15\nSparseM\n1.81\nTH.data\n1.0-10\nTMB\n1.7.20\nVGAM\n1.1-5\nXML\n3.99-0.7\nabind\n1.4-5\nade4\n1.7-17\nadegenet\n2.1.4\nadegraphics\n1.0-15\nadephylo\n1.1-11\nadespatial\n0.3-14\nagricolae\n1.3-5\nape\n5.5\naskpass\n1.1\nassertthat\n0.2.1\nbackports\n1.2.1\nbase64enc\n0.1-3\nbetareg\n3.1-4\nbiscale\n0.2.0\nbit\n4.0.4\nbit64\n4.0.5\nblob\n1.2.2\nbookdown\n0.23\nboot\n1.3-28\nbrio\n1.1.2\nbroom\n0.7.9\nbslib\n0.2.5.1\ncachem\n1.0.6\ncallr\n3.7.0\ncar\n3.0-11\ncarData\n3.0-4\ncellranger\n1.1.0\ncheckmate\n2.0.0\nclass\n7.3-19\nclassInt\n0.4-3\ncli\n3.0.1\nclipr\n0.7.1\ncluster\n2.1.2\ncocorresp\n0.4-3\ncoda\n0.19-4\ncodetools\n0.2-18\ncolorspace\n2.0-2\ncolourpicker\n1.1.0\ncombinat\n0.0-8\ncommonmark\n1.7\nconquer\n1.0.2\ncowplot\n1.1.1\ncpp11\n0.3.1\ncrayon\n1.4.1\ncrosstalk\n1.1.1\ncurl\n4.3.2\ndata.table\n1.14.0\ndave\n2.0\ndbplyr\n2.1.1\ndeldir\n0.2-10\ndichromat\n2.0-0\ndiffr\n0.1\ndigest\n0.6.27\ndistill\n1.2.4\ndownlit\n0.2.1\ndplyr\n1.0.7\ndtplyr\n1.1.0\ndunn.test\n1.3.5\ne1071\n1.7-8\nellipse\n0.4.2\nellipsis\n0.3.2\nevaluate\n0.14\nexpm\n0.999-6\nfansi\n0.5.0\nfarver\n2.1.0\nfasterize\n1.0.3\nfastmap\n1.1.0\nflashClust\n1.01-2\nflexmix\n2.3-17\nforcats\n0.5.1\nforeign\n0.8-81\nfs\n1.5.0\ngargle\n1.2.0\ngclus\n1.3.2\ngdata\n2.18.0\ngenerics\n0.1.0\ngeojsonsf\n2.0.1\ngeometries\n0.2.0\ngeosphere\n1.5-10\nggExtra\n0.9\nggfortify\n0.4.12\nggplot2\n3.3.5\nggrepel\n0.9.1\nglmmML\n1.1.1\nglue\n1.4.2\ngmodels\n2.18.1\ngoogledrive\n2.0.0\ngooglesheets4\n1.0.0\ngridExtra\n2.3\ngstat\n2.0-7\ngtable\n0.3.0\ngtools\n3.9.2\nhaven\n2.4.3\nhere\n1.0.1\nhier.part\n1.0-6\nhighr\n0.9\nhms\n1.1.0\nhtmlTable\n2.2.1\nhtmltools\n0.5.1.1\nhtmlwidgets\n1.5.3\nhttpuv\n1.6.2\nhttr\n1.4.2\nids\n1.0.1\nigraph\n1.2.6\nintervals\n0.15.2\nisoband\n0.2.5\njanitor\n2.1.0\njpeg\n0.1-9\njquerylib\n0.1.4\njsonify\n1.2.1\njsonlite\n1.7.2\njtools\n2.1.3\nklaR\n0.6-15\nknitr\n1.33\nlabdsv\n2.0-1\nlabeling\n0.4.2\nlabelled\n2.8.0\nlanguageR\n1.5.0\nlater\n1.3.0\nlattice\n0.20-44\nlatticeExtra\n0.6-29\nlazyeval\n0.2.2\nleafem\n0.1.6\nleaflet\n2.0.4.1\nleaflet.providers\n1.9.0\nleafsync\n0.1.0\nleaps\n3.1\nlifecycle\n1.0.0\nlme4\n1.1-27.1\nlmerTest\n3.1-3\nlmodel2\n1.7-3\nlmtest\n0.9-38\nlubridate\n1.7.10\nlwgeom\n0.2-7\nmagrittr\n2.0.1\nmaptools\n1.1-1\nmarkdown\n1.1\nmatrixStats\n0.60.1\nmemoise\n2.0.0\nmgcv\n1.8-36\nmime\n0.11\nminiUI\n0.1.1.1\nminqa\n1.2.4\nmnormt\n2.0.2\nmodelr\n0.1.8\nmodeltools\n0.2-23\nmove\n4.0.6\nmultcomp\n1.4-17\nmultcompView\n0.1-8\nmunsell\n0.5.0\nmvtnorm\n1.1-2\nnlme\n3.1-152\nnloptr\n1.2.2.2\nnlstools\n1.0-2\nnnet\n7.3-16\nnumDeriv\n2016.8-1.1\nopenssl\n1.4.4\nopenxlsx\n4.2.4\npackrat\n0.7.0\npander\n0.6.4\npbkrtest\n0.5.1\npermute\n0.9-5\nphylobase\n0.8.10\npillar\n1.6.2\npixmap\n0.4-12\npkgconfig\n2.0.3\nplotly\n4.9.4.1\nplotrix\n3.8-1\nplyr\n1.8.6\npng\n0.1-7\npolspline\n1.1.19\nprettyunits\n1.1.1\nprocessx\n3.5.2\nprogress\n1.2.2\npromises\n1.2.0.1\nproxy\n0.4-26\nps\n1.6.0\npscl\n1.5.5\npsych\n2.1.6\npurrr\n0.3.4\nquantreg\n5.86\nquestionr\n0.7.4\nrapidjsonr\n1.2.0\nrappdirs\n0.3.3\nraster\n3.4-13\nreadr\n2.0.1\nreadxl\n1.3.1\nrematch\n1.0.1\nrematch2\n2.1.2\nrenv\n0.14.0\nreprex\n2.0.1\nreshape\n0.8.8\nreshape2\n1.4.4\nrgdal\n1.5-23\nrgl\n0.107.14\nrhandsontable\n0.3.8\nrio\n0.5.27\nrlang\n0.4.11\nrmarkdown\n2.10\nrms\n6.2-0\nrncl\n0.8.4\nrpart\n4.1-15\nrprojroot\n2.0.2\nrsconnect\n0.8.24\nrstudioapi\n0.13\nrvest\n1.0.1\ns2\n1.0.6\nsandwich\n3.0-1\nsass\n0.4.0\nscales\n1.1.1\nscatterplot3d\n0.3-41\nsciplot\n1.2-0\nsegmented\n1.3-4\nselectr\n0.4-2\nseqinr\n4.2-8\nsf\n1.0-2\nsfheaders\n0.4.0\nshiny\n1.6.0\nshinyBS\n0.61\nshinyWidgets\n0.6.0\nshinyjs\n2.0.0\nshinythemes\n1.2.0\nsnakecase\n0.11.0\nsourcetools\n0.1.7\nsp\n1.4-5\nspData\n0.3.10\nspacetime\n1.2-5\nspdep\n1.1-8\nstars\n0.5-3\nstringi\n1.7.3\nstringr\n1.4.0\nstyler\n1.5.1\nsurvival\n3.2-13\nsys\n3.4\ntibble\n3.1.3\ntidyr\n1.1.3\ntidyselect\n1.1.1\ntidyverse\n1.3.1\ntinytex\n0.33\ntmap\n3.3-2\ntmaptools\n3.1-1\ntmvnsim\n1.0-2\ntree\n1.0-41\ntzdb\n0.1.2\nunits\n0.7-2\nunmarked\n1.1.1\nutf8\n1.2.2\nuuid\n0.1-4\nvctrs\n0.3.8\nvegan\n2.5-7\nvegan3d\n1.1-2\nviridis\n0.6.1\nviridisLite\n0.4.0\nvroom\n1.5.4\nwebshot\n0.5.2\nwesanderson\n0.3.6\nwhisker\n0.4\nwidgetframe\n0.3.1\nwithr\n2.4.2\nwk\n0.5.0\nxfun\n0.25\nxml2\n1.3.2\nxtable\n1.8-4\nxts\n0.12.1\nyaml\n2.2.1\nzip\n2.2.0\nzoo\n1.8-9\n\n\n\n\n",
      "last_modified": "2021-08-25T09:08:17+02:00"
    },
    {
      "path": "Statistik_abstract.html",
      "title": "Statistik",
      "author": [],
      "contents": "\n\nContents\nStatistik 1\nStatistik 2\nStatistik 3\nStatistik 4\nStatistik 5\nStatistik 6\nStatistik 7\nStatistik 8\n\nStatistik 1\nIn Statistik 1 lernen die Studierenden, was (Inferenz-) Statistik im Kern leistet und warum sie für wissenschaftliche Erkenntnis (in den meisten Disziplinen) unentbehrlich ist. Nach einer Wiederholung der Rolle von Hypothesen wird erläutert, wie Hypothesentests in der frequentist-Statistik umgesetzt werden, einschliesslich p-Werten und Signifikanz-Levels. Die praktische Statistik beginnt mit den beiden einfachsten Fällen, dem Chi-Quadrat-Test für die Assoziation zwischen zwei kategorialen Variablen und dem t-Test auf Unterschiede in Mittelwerten zwischen zwei Gruppen. Abschliessend beschäftigen wir uns damit, wie man Ergebnisse statistischer Analysen am besten in Abbildungen, Tabellen und Text darstellt.\n\n\n\n\nStatistik 2\nIn Statistik 2 lernen die Studierenden die Idee, die Voraussetzungen und die praktische Anwendung „einfacher“ linearer Modelle in R (sowie teilweise ihrer „nicht-parametrischen“ bzw. „robusten“ Äquivalente). Am Anfang steht die Varianzanalyse (ANOVA) als Verallgemeinerung des t-Tests, einschliesslich post-hoc-Tests und mehrfaktorieller ANOVA. Dann geht es um die Voraussetzungen parametrischer (und nicht-parametrischer) Tests und Optionen, wenn diese verletzt sind. Dann beschäftigen wir uns mit Korrelationen, die auf einen linearen Zusammenhang zwischen zwei metrischen Variablen testen, ohne Annahme einer Kausalität. Es folgen einfache lineare Regressionen, die im Prinzip das Gleiche bei klarer Kausalität leisten. Abschliessend besprechen wir, was die grosse Gruppe linearer Modelle (Befehl lm in R) auszeichnet.\n\n\nStatistik 3\nStatistik 3 fassen wir zu Beginn den generellen Ablauf inferenzstatistischer Analysen in einem Flussdiagramm zusammen. Dann wird die ANCOVA als eine Technik vorgestellt, die eine ANOVA mit einer linearen Regression verbindet. Danach geht es um komplexere Versionen linearer Regressionen. Hier betrachten wir polynomiale Regressionen, die z. B. einen Test auf unimodale Beziehungen erlaubt, indem man dieselbe Prädiktorvariable linear und quadriert einspeist. Multiple Regressionen versuchen dagegen, eine abhängige Variable durch zwei oder mehr verschieden Prädiktorvariablen zu erklären. Wir thematisieren verschiedene dabei auftretende Probleme und ihre Lösung, insbesondere den Umgang mit korrelierten Prädiktoren und das Aufspüren des besten unter mehreren möglichen statistischen Modellen. Hieran wird auch der informatian theoretician-Ansatz der Statistik und die multimodel inference eingeführt.\nStatistik 4\nHeute geht es hauptsächlich um generalized linear models (GLMs), die einige wesentliche Limitierungen von linearen Modellen überwinden. Indem sie Fehler- und Varianzstrukturen explizit modellieren, ist man nicht mehr an Normalverteilung der Residuen und Varianzhomogenität gebunden. Bei generalized linear regressions muss man sich zwischen verschiedenen Verteilungen und link-Strukturen entscheiden. Spezifisch werden wir uns die Poisson-Regressionen für Zähldaten und die logistische Regression für ja/nein-Daten anschauen. Danach folgt ein Einstieg in nicht-lineare Regressionen, die es erlauben, etwa Potenzgesetze oder Sättigungsfunktionen direkt zu modellieren. Zum Abschluss gibt es einen Ausblick auf Glättungsverfahren (LOWESS) und general additive models (GAMs).\nStatistik 5\nIn Statistik 5 lernen die Studierenden Lösungen kennen, welche die diversen Limitierungen von linearen Modellen überwinden. Während generalized linear models (GLMs) aus Statistik 4 bekannt sind, geht es jetzt um linear mixed effect models (LMMs und generalized linear mixed effect models (GLMMs). Dabei bezeichnet generalized die explizite Modellierung anderer Fehler- und Varianzstrukturen und mixed die Berücksichtigung von Abhängigkeiten bzw. Schachtelungen unter den Beobachtungen. Einfachere Fälle von LMMs, wie split-plot und repeated-measures ANOVAs, lassen sich noch mit dem aov-Befehl in Base R bewältigen, für komplexere Versuchsdesigns/Analysen gibt es spezielle R packages. Abschliessend gibt es eine kurze Einführung in GLMMs, die eine Analyse komplexerer Beobachtungsdaten z. B. mit räumlichen Abhängigkeiten, erlauben.\nStatistik 6\nStatistik 6 führt in multivariat-deskriptive Methoden ein, die dazu dienen Datensätze mit multiplen abhängigen und multiplen unabhängigen Variablen effektiv zu analysieren. Dabei betonen Ordinationen kontinuierliche Gradienten und fokussieren auf zusammengehörende Variablen, während Cluster-Analysen Diskontinuitäten betonen und auf zusammengehörende Beobachtungen fokussieren. Es folgt eine konzeptionelle Einführung in die Idee von Ordinationen als einer Technik der deskriptiven Statistik, die Strukturen in multivariaten Datensätzen via Dimensionsreduktion visualisiert. Das Prinzip und die praktische Implementierung wird detailliert am Beispiel der Hauptkomponentenanalyse (PCA) erklärt. Danach folgen kurze Einführungen in weitere Ordinationstechniken für besondere Fälle, welche bestimmte Limitierungen der PCA überwinden, namentlich CA, DCA und NMDS.\nStatistik 7\nIn Statistik 7 beschäftigen wir uns zunächst damit, wie wir Ordinationsdiagramme informativer gestalten können, etwa durch die Beschriftung der Beobachtunge, post-hoc-Projektion der Prädiktorvariablen oder Response surfaces. Während wir bislang mit «unconstrained» Ordinationen gearbeitet haben, welche die Gesamtvariabilität in den Beobachtungen visualisieren, beschränken die jeweiligen «constrained»-Varianten derselben Ordinationsmethoden die Betrachtung auf den Teil der Variabilität, welcher durch eine Linearkombination der berücksichtigen Prädiktoren erklärt werden kann. Wir beschäftigen uns im Detail mit der Redundanz-Analyse (RDA), der «constrained»-Variante der PCA und gehen einen kompletten analytischen Ablauf mit Aufbereitung, Interpretation und Visualisierung der Ergebnisse am Beispiel eines gemeinschaftsökologischen Datensatzes (Fischgesellschaften und Umweltfaktoren im Jura-Fluss Doubs) durch\nStatistik 8\nIn Statistik 8 lernen die Studierenden Clusteranalysen/Klassifikationen als eine den Ordinationen komplementäre Technik der deskriptiven Statistik multivariater Datensätze kennen. Es gibt Partitionierungen (ohne Hierarchie), divisive und agglomerative Clusteranalysen (die jeweils eine Hierarchie produzieren). Etwas genauer gehen wir auf die k-means Clusteranalyse (eine Partitionierung) und eine Reihe von agglomerativen Clusterverfahren ein. Hierbei hat das gewählte Distanzmass und der Modus für die sukzessive Fusion von Clustern einen grossen Einfluss auf das Endergebnis. Wir besprechen ferner, wie man die Ergebnisse von Clusteranalysen adäquat visualisieren und mit anderen statistischen Prozeduren kombinieren kann. Im Abschluss von Statistik 8 werden wir dann die an den acht Statistiktagen behandelten Verfahren noch einmal rückblickend betrachten und thematisieren, welches Verfahren wann gewählt werden sollte. Ebenfalls ist Platz, um den adäquaten Ablauf statistischer Analysen vom Einlesen der Daten bis zur Verschriftlichung der Ergebnisse, einschliesslich der verschiedenen zu treffenden Entscheidungen, zu thematisieren.\n\n\n\n",
      "last_modified": "2021-08-25T09:08:17+02:00"
    },
    {
      "path": "Statistik-Konsolidierung_abstract.html",
      "title": "Statistik Konsolidierung",
      "author": [],
      "contents": "\nUm was geht es hier eigentlich?\n\n\n\n",
      "last_modified": "2021-08-25T09:08:18+02:00"
    },
    {
      "path": "Statistik-Konsolidierung.html",
      "title": "Statistik Konsolidierung",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2021-08-25T09:08:18+02:00"
    },
    {
      "path": "Statistik.html",
      "title": "Statistik",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2021-08-25T09:08:19+02:00"
    }
  ],
  "collections": ["infovis/infovis.json", "prepro/prepro.json", "rauman/rauman.json", "statistik-konsolidierung/statistik-konsolidierung.json", "statistik/statistik.json"]
}
