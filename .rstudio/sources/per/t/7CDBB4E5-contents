## Übung A: Lösung

###  Musterlösung Aufgabe 1
```{r, message=F}
library(tidyverse)
library(ggfortify) # um autoplot() für lm/aov nutzen zu können

decay <- read_csv("14_Statistik2/data/decay.csv")

```

##### Lösungsweg 1: $log_{10}$-Transformation

We start by fitting a straight line through the scatterplot, using fitline with a linear model:

```{r}
ggplot(decay, aes(time, amount)) +
  geom_point() +
  geom_smooth(method = "lm")

```

This draws attention to the pronounced curvature in the data. Most of the residuals at low values of t ime are positive, most of the residuals for intermediate values of t ime are negative, and most of the residuals at high values of t ime are positive. This is clearly not a good model for these data. There is a very important point here. If, instead of looking at the fit of the model to the data using plot , we had simply done the statistics, then we might easily have come to the opposite conclusion. Here is a summary of the linear model applied to these data:

```{r}
summary(lm(amount~time, decay))
```


The model explains more than 76% of the variation in the response (a very high value of $r$-squared) and the $p$ value is vanishingly small. The moral is that $p$ values and $r$-squared are *not* good measures of model adequacy.

Because the data relate to a decay process, it might be that an exponential function $y = ae^{-bx}$ describes the data better. If we can linearize this equation, then we can estimate the parameter values using a linear model. Let us try taking logs of both sides

$$y = ae^{-bx}$$
$$log(y) = log(a) - bx$$


If we replace log(y) by Y and log(a) by A, you can see that we have a linear model:
$$Y = A - bx$$

The intercept of this linear model is A and the slope is $-b$. To fit the model we have the *untransformed values* of time on the x axis and the $log$ of amount on the y axis:

```{r}
ggplot(decay, aes(time, log(amount))) +
  geom_point() +
  geom_smooth(method = "lm")

```


The fit to the model is greatly improved. There is a new issue, however, in that the variance appears to increase with time and, as you will recall, non-constant variance is a potentially serious problem. Let us estimate the parameter values of this exponential model and then check its assumptions using plot (model ).

```{r}
model <- lm(log(amount)~time, decay)
summary(model)
```

The slope of the straight line is -0.068528 and its standard error is 0.005743. The value of $r~2$ is even higher following transformation (83%) and the $p$ value is even lower. The intercept of 4.547386 with its standard error of 0.100295 is for A, not the value we need for our exponential equation, but a is the antilog of A. When we back-transform, the standard errors become asymmetric up and down. It may take a moment for you to see why this is the case. Let us add one standard error to the intercept and subtract one standard error from it to get upper and lower intervals.

```{r}
upper <- 4.547386 + 0.100295
lower <- 4.547386 - 0.100295
```

Now we return to the original scale of measurement by taking antilogs using exp:

```{r}
exp(upper)
exp(lower)
```

so the intercept on the original axis is between 85.38 and 104.34, but the best estimate for the intercept is
```{r}
exp(4.547386)
```

which means that the interval above the intercept is 9.957 but the interval below it is 9.007. Beginners often find it disconcerting that the two unreliability measures are different sizes.

Now we check the assumptions of the model using `autoplot(model)`:


```{r}
autoplot(model)
```

The good news is that the normality of errors assumption looks good (the top right plot is reasonably straight). As we guessed by looking at the transformed data, however, the variance does show strong signs of non-constancy (the top left and bottom left plots). The bottom right plot shows that data points 30 and 31 have high leverage and point number 28 has a large residual. We shall see how deal with these issues later, but for the moment, we want to plot the curved line through the scatterplot on the original scale of measurement.


```{r}

fun.1 <- function(x){94.38536 * exp(-0.068528 * x)}


ggplot(decay, aes(time, amount)) +
  geom_point() +
  stat_function(fun = fun.1)
```

As you can see, our model is a good description of the data for intermediate values of time, but the model is poor at predicting amount for time = 0 and for time > 28. Clearly, more work is required to understand what is going on at the extremes, but exponential decay describes the central part of the data reasonably well.

