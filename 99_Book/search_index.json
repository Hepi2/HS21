[
["index.html", "Research Methods Kapitel 1 Einleitung", " Research Methods 2019-10-28 Kapitel 1 Einleitung Das Modul „Research Methods“ vermittelt vertiefte Methodenkompetenzen für praxisorientiertes und angewandtes wissenschaftliches Arbeiten im Fachbereich „Umwelt und Natürliche Ressourcen“ auf MSc-Niveau. Die Studierenden erarbeiten sich vertiefte Methodenkompetenzen für die analytische Betrachtung der Zusammenhänge im Gesamtsystem „Umwelt und Natürliche Ressourcen“. Die Studierenden erlernen die methodischen Kompetenzen, auf denen die nachfolgenden Module im MSc Programm UNR aufbauen. Das Modul vermittelt einerseits allgemeine, fächerübergreifende methodische Kompetenzen (z.B. Wissenschaftstheorie, computer-gestützte Datenverar-beitung und Statistik). Auf dieser Plattform (RStudio Connect) werden die Unterlagen für die R-Übungsteile bereitgestellt. Es werden sukzessive sowohl Demo-Files, Aufgabenstellungen und Lösungen veröffentlicht. "],
["2-prepro1-14-10-2019.html", "Kapitel 2 PrePro1 (14.10.2019)", " Kapitel 2 PrePro1 (14.10.2019) Die Datenkunde 2.0 gibt den Studierenden das Wissen und die Fertigkeiten an die Hand, selbst erhobene und bezogene Daten für Ihre eigenen Analysen vorzubereiten und anzureichern (preprocessing). Die Einheit vermittelt zentrale Datenverarbeitungskompetenzen und thematisiert bekannte Problemzonen der umweltwissenschaftlichen Datenverarbeitung – immer mit einer „hands-on“ Perspektive auf die begleitenden R-Übungen. Die Studierenden lernen die Eigenschaften ihrer Datensätze in der Fachsprache korrekt zu beschreiben. Sie lernen ausserdem Metadaten zu verstehen und die Implikationen derselben für ihre eigenen Analyseprojekte kritisch zu beurteilen. Zentrale Konzepte der Lerneinheit sind Skalenniveaus, Datentypen, Zeitdaten und Typumwandlungen. "],
["2-1-demo-datentypen-tabellen.html", "2.1 Demo: Datentypen, Tabellen", " 2.1 Demo: Datentypen, Tabellen R-Code als Download 2.1.1 Datentypen 2.1.1.1 Numerics Unter die Kategorie numeric fallen in R zwei Datentypen: double: Gleitkommazahl (z.B. 10.3, 7.3) integer: Ganzzahl (z.B. 10, 7) 2.1.1.1.1 Doubles Folgendermassen wird eine Gleitkommazahl einer Variabel zuweisen: x &lt;- 10.3 x ## [1] 10.3 typeof(x) ## [1] &quot;double&quot; Statt &lt;-kann auch = verwendet werden. Dies funktioniert aber nicht in allen Situationen, und ist zudem leicht mit == zu verwechseln. y = 7.3 y ## [1] 7.3 Ohne explizite Zuweisung nimmt R immer den Datentyp doublean: z &lt;- 42 typeof(z) ## [1] &quot;double&quot; is.integer(z) ## [1] FALSE is.numeric(z) ## [1] TRUE is.double(z) ## [1] TRUE 2.1.1.2 Ganzzahl / Integer Erst wenn man eine Zahl explizit als integer definiert (mit as.integer() oder L), wird sie auch als solches abgespeichert. a &lt;- as.integer(z) is.numeric(a) ## [1] TRUE is.integer(a) ## [1] TRUE c &lt;- 8L is.numeric(c) ## [1] TRUE is.integer(c) ## [1] TRUE typeof(a) ## [1] &quot;integer&quot; is.numeric(a) ## [1] TRUE is.integer(a) ## [1] TRUE Mit c() können eine Reihe von Werten in einer Variabel zugewiesen werden (als vector). Es gibt zudem auch character vectors. vector &lt;- c(10,20,33,42,54,66,77) vector ## [1] 10 20 33 42 54 66 77 vector[5] ## [1] 54 vector[2:4] ## [1] 20 33 42 vector2 &lt;- vector[2:4] Eine Ganzzahl kann explizit mit as.integer() definiert werden. a &lt;- as.integer(7) b &lt;- as.integer(3.14) a ## [1] 7 b ## [1] 3 typeof(a) ## [1] &quot;integer&quot; typeof(b) ## [1] &quot;integer&quot; is.integer(a) ## [1] TRUE is.integer(b) ## [1] TRUE Eine Zeichenkette kann als Zahl eingelesen werden. c &lt;- as.integer(&quot;3.14&quot;) c ## [1] 3 typeof(c) ## [1] &quot;integer&quot; 2.1.1.3 Logische Abfragen Wird auch auch als boolesch (Eng. boolean) bezeichnet. e &lt;- 3 f &lt;- 6 g &lt;- e &gt; f e ## [1] 3 f ## [1] 6 g ## [1] FALSE typeof(g) ## [1] &quot;logical&quot; 2.1.1.4 Logische Operationen sonnig &lt;- TRUE trocken &lt;- FALSE sonnig &amp; !trocken ## [1] TRUE Oft braucht man auch das Gegenteil / die Negation eines Wertes. Dies wird mittels ! erreicht u &lt;- TRUE v &lt;- !u v ## [1] FALSE 2.1.1.5 Zeichenketten Zeichenketten (Eng. character) stellen Text dar s &lt;- as.character(3.14) s ## [1] &quot;3.14&quot; typeof(s) ## [1] &quot;character&quot; Zeichenketten verbinden / zusammenfügen (Eng. concatenate) fname &lt;- &quot;Hans&quot; lname &lt;- &quot;Muster&quot; paste(fname,lname) ## [1] &quot;Hans Muster&quot; fname2 &lt;- &quot;hans&quot; fname == fname2 ## [1] FALSE 2.1.1.6 Factors Mit Factors wird in R eine Sammlung von Zeichenketten bezeichnet, die sich wiederholen, z.B. Wochentage (es gibt nur 7 unterschiedliche Werte für “Wochentage”). wochentage &lt;- c(&quot;Montag&quot;,&quot;Dienstag&quot;,&quot;Mittwoch&quot;,&quot;Donnerstag&quot;,&quot;Freitag&quot;,&quot;Samstag&quot;,&quot;Sonntag&quot;, &quot;Montag&quot;,&quot;Dienstag&quot;,&quot;Mittwoch&quot;,&quot;Donnerstag&quot;,&quot;Freitag&quot;,&quot;Samstag&quot;,&quot;Sonntag&quot;) typeof(wochentage) ## [1] &quot;character&quot; wochentage_fac &lt;- as.factor(wochentage) wochentage ## [1] &quot;Montag&quot; &quot;Dienstag&quot; &quot;Mittwoch&quot; &quot;Donnerstag&quot; &quot;Freitag&quot; ## [6] &quot;Samstag&quot; &quot;Sonntag&quot; &quot;Montag&quot; &quot;Dienstag&quot; &quot;Mittwoch&quot; ## [11] &quot;Donnerstag&quot; &quot;Freitag&quot; &quot;Samstag&quot; &quot;Sonntag&quot; wochentage_fac ## [1] Montag Dienstag Mittwoch Donnerstag Freitag Samstag ## [7] Sonntag Montag Dienstag Mittwoch Donnerstag Freitag ## [13] Samstag Sonntag ## Levels: Dienstag Donnerstag Freitag Mittwoch Montag Samstag Sonntag Wie man oben sieht, unterscheiden sich character vectors und factors v.a. dadurch, dass letztere über sogenannte levels verfügt. Diese levels entsprechen den Eindeutigen (unique) Werten. levels(wochentage_fac) ## [1] &quot;Dienstag&quot; &quot;Donnerstag&quot; &quot;Freitag&quot; &quot;Mittwoch&quot; &quot;Montag&quot; ## [6] &quot;Samstag&quot; &quot;Sonntag&quot; unique(wochentage) ## [1] &quot;Montag&quot; &quot;Dienstag&quot; &quot;Mittwoch&quot; &quot;Donnerstag&quot; &quot;Freitag&quot; ## [6] &quot;Samstag&quot; &quot;Sonntag&quot; 2.1.1.7 Zeit/Datum Um in R mit Datum/Zeit Datentypen umzugehen, müssen sie als POSIXct eingelesen werden (es gibt alternativ noch POSIXlt, aber diese ignorieren wir mal). Anders als Beispielsweise bei Excel, sollten in R Datum und Uhrzeit immer in einer Spalte gespeichert werden. datum &lt;- &quot;2017-10-01 13:45:10&quot; as.POSIXct(datum) ## [1] &quot;2017-10-01 13:45:10 CEST&quot; Wenn das die Zeichenkette in dem obigen Format (Jahr-Monat-Tag Stunde:Minute:Sekunde) daher kommt, braucht as.POSIXctkeine weiteren Informationen. Sollte das Format von dem aber Abweichen, muss man der Funktion das genaue Schema jedoch mitteilen. Der Syntax dafür kann via ?strptime nachgeschlagen werden. datum &lt;- &quot;01.10.2017 13:45&quot; as.POSIXct(datum,format = &quot;%d.%m.%Y %H:%M&quot;) ## [1] &quot;2017-10-01 13:45:00 CEST&quot; Beachtet, dass in den den obigen Beispiel R automatisch eine Zeitzone angenommen hat (CEST). R geht davon aus, dass die Zeitzone der System Timezone (Sys.timezone()) entspricht. 2.1.2 Data Frames und Conveniance Variabeln Eine data.frame ist die gängigste Art, Tabellarische Daten zu speichern. df &lt;- data.frame( Stadt = c(&quot;Zürich&quot;,&quot;Genf&quot;,&quot;Basel&quot;,&quot;Bern&quot;,&quot;Lausanne&quot;), Einwohner = c(396027,194565,175131,140634,135629), Ankunft = c(&quot;1.1.2017 10:00&quot;,&quot;1.1.2017 14:00&quot;, &quot;1.1.2017 13:00&quot;,&quot;1.1.2017 18:00&quot;,&quot;1.1.2017 21:00&quot;) ) str(df) ## &#39;data.frame&#39;: 5 obs. of 3 variables: ## $ Stadt : Factor w/ 5 levels &quot;Basel&quot;,&quot;Bern&quot;,..: 5 3 1 2 4 ## $ Einwohner: num 396027 194565 175131 140634 135629 ## $ Ankunft : Factor w/ 5 levels &quot;1.1.2017 10:00&quot;,..: 1 3 2 4 5 In der obigen data.frame wurde die Spalte Einwohner als Fliesskommazahl abgespeichert. Dies ist zwar nicht tragisch, aber da wir wissen das es sich hier sicher um Ganzzahlen handelt, können wir das korrigieren. Wichtiger ist aber, dass wir die Ankunftszeit (SpalteAnkunft) von einem Factor in ein Zeitformat (POSIXct) umwandeln. df$Einwohner &lt;- as.integer(df$Einwohner) df$Einwohner ## [1] 396027 194565 175131 140634 135629 df$Ankunft &lt;- as.POSIXct(df$Ankunft, format = &quot;%d.%m.%Y %H:%M&quot;) df$Ankunft ## [1] &quot;2017-01-01 10:00:00 CET&quot; &quot;2017-01-01 14:00:00 CET&quot; ## [3] &quot;2017-01-01 13:00:00 CET&quot; &quot;2017-01-01 18:00:00 CET&quot; ## [5] &quot;2017-01-01 21:00:00 CET&quot; Diese Rohdaten können nun helfen, um Hilfsvariablen (convenience variables) zu erstellen. Z.B. können wir die Städte einteilen in gross, mittel und klein. df$Groesse[df$Einwohner &gt; 300000] &lt;- &quot;gross&quot; df$Groesse[df$Einwohner &lt;= 300000 &amp; df$Einwohner &gt; 150000] &lt;- &quot;mittel&quot; df$Groesse[df$Einwohner &lt;= 150000] &lt;- &quot;klein&quot; Oder aber, die Ankunftszeit kann von der Spalte Ankunftabgeleitet werden. Dazu brauchen wir aber das Package lubridate library(lubridate) df$Ankunft_stunde &lt;- hour(df$Ankunft) 2.1.3 Quellen Dieses Kapitel verwendet folgende Libraries: Spinu, Grolemund, and Wickham (2018) References "],
["2-2-ubung-a.html", "2.2 Übung A", " 2.2 Übung A R ist ohne Zusatzpackete nicht mehr denkbar. Die allermeisten Packages werden auf CRAN gehostet und können leicht mittels install.packages() installiert werden. Eine sehr wichtige Sammlung von Packages wird von RStudio entwickelt. Unter dem Namen Tidyverse werden eine Reihe von Packages angeboten, den R-Alltag enorm erleichtert. Wir werden später näher auf das “Tidy”-Universum eingehen, an dieser Stelle können wir die Sammlung einfach mal installieren. install.packages(&quot;tidyverse&quot;) Um ein package in R verwenden zu können, gibt es zwei Möglichkeiten: entweder man lädt es zu Beginn der R-session mittles library(). oder man ruft eine function mit vorangestelltem Packetname sowie zwei Doppelpunkten auf. dplyr::filter() ruft die Funktion filter() des Packets dplyr auf. Letztere Notation ist vor allem dann sinnvoll, wenn sich zwei unterschiedliche Funktionen mit dem gleichen namen in verschiedenen pacakges existieren. filter() existiert als Funktion einersits im package dplyr sowie in stats. Dieses Phänomen nennt man “masking”. Zu beginn laden wir die nötigen Pakete: 2.2.1 Aufgabe 1 Erstelle eine data.frame mit nachstehenden Daten. Tipps: Eine leere data.frame zu erstellen ist schwieriger als wenn erstellen und befüllen der data.frame in einem Schritt erfolgt R ist dafür gedacht, Spalte für Spalte zu arbeiten (warum?), nicht Reihe für Reihe. Versuche dich an dieses Schema zu halten. Tierart Anzahl Gewicht Geschlecht Beschreibung Fuchs 2 4.4 m Rötlich Bär 5 40.3 f Braun, gross Hase 1 1.1 m klein, mit langen Ohren Elch 3 120.0 m Lange Beine, Schaufelgeweih 2.2.2 Aufgabe 2 Was für Datentypen wurden (in Aufgabe 1) von R automatisch angenommen? Sind diese sinnvoll? Tipp: Nutze dazu str() ## &#39;data.frame&#39;: 4 obs. of 5 variables: ## $ Tierart : Factor w/ 4 levels &quot;Bär&quot;,&quot;Elch&quot;,&quot;Fuchs&quot;,..: 3 1 4 2 ## $ Anzahl : num 2 5 1 3 ## $ Gewicht : num 4.4 40.3 1.1 120 ## $ Geschlecht : Factor w/ 2 levels &quot;f&quot;,&quot;m&quot;: 2 1 2 2 ## $ Beschreibung: Factor w/ 4 levels &quot;Braun, gross&quot;,..: 4 1 2 3 ## [1] &quot;double&quot; 2.2.3 Aufgabe 3 Nutze die Spalte Gewicht um die Tiere in 3 Gewichtskategorien einzuteilen: leicht: &lt; 5kg mittel: 5 - 100 kg schwer: &gt; 100kg Tierart Anzahl Gewicht Geschlecht Beschreibung Gewichtsklasse Fuchs 2 4.4 m Rötlich leicht Bär 5 40.3 f Braun, gross mittel Hase 1 1.1 m klein, mit langen Ohren leicht Elch 3 120.0 m Lange Beine, Schaufelgeweih schwer 2.2.4 Aufgabe 4 Importiere den Datensatz order_52252_data.txt. Es handelt sich dabei um die stündlich gemittelten Temperaturdaten an verschiedenen Standorten in der Schweiz im Zeitraum 2000 - 2005. Wir empfehlen read_table()1 anstelle von read.table(). stn time tre200h0 ABO 2000010100 -2.6 ABO 2000010101 -2.5 ABO 2000010102 -3.1 ABO 2000010103 -2.4 ABO 2000010104 -2.5 ABO 2000010105 -3.0 ABO 2000010106 -3.7 ABO 2000010107 -4.4 ABO 2000010108 -4.1 ABO 2000010109 -4.1 2.2.5 Aufgabe 5 Schau dir die Rückmeldung von read_table()an. Sind die Daten korrekt interpretiert worden? 2.2.6 Aufgabe 6 Die Spalte time ist eine Datum/Zeitangabe im Format JJJJMMTTHH (siehe meta.txt). Damit R dies als Datum-/Zeitangabe erkennt, müssen wir die Spalte in einem R-Format (POSIXct) einlesen und dabei R mitteilen, wie sie aktuell formatiert ist. Lies die Spalte mit as.POSIXct() (oder parse_datetime) ein und spezifiziere sowohl format wie auch tz. Tipps: Wenn keine Zeitzone festgelegt wird, trifft as.POSIXct() eine Annahme (basierend auf Sys.timezone()). In unserem Fall handelt es sich aber um Werte in UTC (siehe meta.txt) as.POSIXcterwartet character: Wenn du eine Fehlermeldung hast die 'origin' must be supplied (o.ä) heisst, hast du der Funktion vermutlich einen Numeric übergeben. ## [1] &quot;2000-01-01 00:00:00 UTC&quot; &quot;2000-01-01 01:00:00 UTC&quot; ## [3] &quot;2000-01-01 02:00:00 UTC&quot; &quot;2000-01-01 03:00:00 UTC&quot; ## [5] &quot;2000-01-01 04:00:00 UTC&quot; &quot;2000-01-01 05:00:00 UTC&quot; ## [7] &quot;2000-01-01 06:00:00 UTC&quot; &quot;2000-01-01 07:00:00 UTC&quot; ## [9] &quot;2000-01-01 08:00:00 UTC&quot; &quot;2000-01-01 09:00:00 UTC&quot; stn time tre200h0 ABO 2000-01-01 00:00:00 -2.6 ABO 2000-01-01 01:00:00 -2.5 ABO 2000-01-01 02:00:00 -3.1 ABO 2000-01-01 03:00:00 -2.4 ABO 2000-01-01 04:00:00 -2.5 ABO 2000-01-01 05:00:00 -3.0 ABO 2000-01-01 06:00:00 -3.7 ABO 2000-01-01 07:00:00 -4.4 ABO 2000-01-01 08:00:00 -4.1 ABO 2000-01-01 09:00:00 -4.1 2.2.7 Aufgabe 7 Erstelle zwei neue Spalten mit Wochentag (Montag, Dienstag, etc) und Kalenderwoche. Verwende dazu die neu erstellte POSIXct-Spalte stn time tre200h0 wochentag kw ABO 2000-01-01 00:00:00 -2.6 Sat 1 ABO 2000-01-01 01:00:00 -2.5 Sat 1 ABO 2000-01-01 02:00:00 -3.1 Sat 1 ABO 2000-01-01 03:00:00 -2.4 Sat 1 ABO 2000-01-01 04:00:00 -2.5 Sat 1 ABO 2000-01-01 05:00:00 -3.0 Sat 1 ABO 2000-01-01 06:00:00 -3.7 Sat 1 ABO 2000-01-01 07:00:00 -4.4 Sat 1 ABO 2000-01-01 08:00:00 -4.1 Sat 1 ABO 2000-01-01 09:00:00 -4.1 Sat 1 2.2.8 Aufgabe 8 Erstelle eine neue Spalte basierend auf die Temperaturwerte mit der Einteilung “kalt” (Unter Null Grad) und “warm” (über Null Grad) stn time tre200h0 wochentag kw temp_kat ABO 2000-01-01 00:00:00 -2.6 Sat 1 kalt ABO 2000-01-01 01:00:00 -2.5 Sat 1 kalt ABO 2000-01-01 02:00:00 -3.1 Sat 1 kalt ABO 2000-01-01 03:00:00 -2.4 Sat 1 kalt ABO 2000-01-01 04:00:00 -2.5 Sat 1 kalt ABO 2000-01-01 05:00:00 -3.0 Sat 1 kalt ABO 2000-01-01 06:00:00 -3.7 Sat 1 kalt ABO 2000-01-01 07:00:00 -4.4 Sat 1 kalt ABO 2000-01-01 08:00:00 -4.1 Sat 1 kalt ABO 2000-01-01 09:00:00 -4.1 Sat 1 kalt Wickham and Grolemund (2017), Kapitel 8 bzw. http://r4ds.had.co.nz/data-import.html)↩ "],
["2-3-ubung-a-losung.html", "2.3 Übung A Lösung", " 2.3 Übung A Lösung R-Script als Download library(tidyverse) # Im Unterschied zu `install.packages()` werden bei `library()` keine Anführungs- # und Schlusszeichen gesetzt. library(lubridate) # Im Unterschied zu install.packages(&quot;tidyverse&quot;) wird bei library(tidyverse) # das package lubridate nicht berücksichtigt # Lösung Aufgabe 1 df &lt;- data_frame( Tierart = c(&quot;Fuchs&quot;,&quot;Bär&quot;,&quot;Hase&quot;,&quot;Elch&quot;), Anzahl = c(2,5,1,3), Gewicht = c(4.4, 40.3,1.1,120), Geschlecht = c(&quot;m&quot;,&quot;f&quot;,&quot;m&quot;,&quot;m&quot;), Beschreibung = c(&quot;Rötlich&quot;,&quot;Braun, gross&quot;, &quot;klein, mit langen Ohren&quot;,&quot;Lange Beine, Schaufelgeweih&quot;) ) # Lösung Aufgabe 2 str(df) # Anzahl wurde als `double` interpretiert, ist aber eigentlich ein `integer`. # Mit data.frame() wurde Beschreibung wurde als `factor` interpretiert, ist # aber eigentlich `character` typeof(df$Anzahl) df$Anzahl &lt;- as.integer(df$Anzahl) df$Beschreibung &lt;- as.character(df$Beschreibung) # Lösung Aufgabe 3 df$Gewichtsklasse[df$Gewicht &gt; 100] &lt;- &quot;schwer&quot; df$Gewichtsklasse[df$Gewicht &lt;= 100 &amp; df$Gewicht &gt; 5] &lt;- &quot;mittel&quot; df$Gewichtsklasse[df$Gewicht &lt;= 5] &lt;- &quot;leicht&quot; # Lösung Aufgabe 4 wetter &lt;- readr::read_table(&quot;09_PrePro1/data/order_52252_data.txt&quot;) # Lösung Aufgabe 5 # Die Spalte &#39;time&#39; wurde als &#39;integer&#39; interpretiert. Dabei handelt es # sich offensichtlich um Zeitangaben. # Lösung Aufgabe 6 # mit readr parse_datetime(as.character(wetter$time[1:10]), format = &quot;%Y%m%d%H&quot;) # mit as.POSIXct() wetter$time &lt;- as.POSIXct(as.character(wetter$time), format = &quot;%Y%m%d%H&quot;,tz = &quot;UTC&quot;) # Lösung Aufgabe 7 wetter$wochentag &lt;- wday(wetter$time,label = T) wetter$kw &lt;- week(wetter$time) # Lösung Aufgabe 8 wetter$temp_kat[wetter$tre200h0&gt;0] &lt;- &quot;warm&quot; wetter$temp_kat[wetter$tre200h0&lt;=0] &lt;- &quot;kalt&quot; "],
["2-4-ubung-b.html", "2.4 Übung B", " 2.4 Übung B Fahre mit dem Datensatz wetter aus Übung A fort. 2.4.1 Aufgabe 1 Nutze plot() um die Temparaturkurve zu visualisieren. Verwende aber vorher filter() um dich auf eine Station (z.B. “ABO”) zu beschränken (es handelt sich sonst um zuviele Datenpunkte). Nun schauen wir uns das plotten mit ggplot2 an. Ein simpler Plot wie der in der vorherigen Aufgabe ist in ggplot2 zugegebenermassen etwas komplizierter. ggplot2 wird aber rasch einfacher, wenn die Grafiken komplexer werden. Wir empfehlen deshalb stark, ggplot2 zu verwenden. Schau dir ein paar online Tutorials zu ggplot2 an (siehe2) und reproduziere den obigen Plot mit ggplot2 2.4.2 Aufgabe 2 Spiele mit Hilfe der erwähnten Tutorials mit dem Plot etwas rum. Versuche die x-/y-Achsen zu beschriften sowie einen Titel hinzu zu fügen. 2.4.3 Aufgabe 3 Reduziere den x-Achsenausschnitt auf einen kleineren Zeitraum, beispielsweise einn beliebigen Monat. Verwende dazu lims() zusammen mit as.POSIXct() oder mache ein Subset von deinem Datensatz mit einer convenience-Variabel und filter(). Wickham and Grolemund (2017), Kapitel 1 bzw. http://r4ds.had.co.nz/data-visualisation.html oder hier ein sehr schönes Video: Learn R: An Introduction to ggplot2↩ "],
["2-5-ubung-b-losung.html", "2.5 Übung B Lösung", " 2.5 Übung B Lösung R-Code als Download library(tidyverse) # Lösung Aufgabe 1 wetter_fil &lt;- dplyr::filter(wetter, stn == &quot;ABO&quot;) plot(wetter_fil$time,wetter_fil$tre200h0, type = &quot;l&quot;) p &lt;- ggplot(wetter_fil, aes(time,tre200h0)) + geom_line() p # Lösung Aufgabe 2 p &lt;- p + labs(x = &quot;Datum&quot;, y = &quot;Temperatur&quot;, title = &quot;Stündlich gemittelte Temperaturwerte&quot;) p # Lösung Aufgabe 3 limits &lt;- as.POSIXct(c(&quot;2002-01-01 00:00:00&quot;,&quot;2002-02-01 00:00:00&quot;),tz = &quot;UTC&quot;) p + lims(x = limits) "],
["3-prepro2-15-10-2019.html", "Kapitel 3 PrePro2 (15.10.2019)", " Kapitel 3 PrePro2 (15.10.2019) Die Lerneinheit vermittelt zentralste Fertigkeiten zur Vorverarbeitung von strukturierten Daten in der umweltwissenschaftlichen Forschung: Datensätze verbinden (Joins) und umformen („reshape“, „split-apply-combine“). Im Anwendungskontext haben Daten selten von Anfang an diejenige Struktur, welche für die statistische Auswertung oder für die Informationsvisualisierung erforderlich wäre. In dieser Lerneinheit lernen die Studierenden die für diese oft zeitraubenden Preprocessing-Schritte notwendigen Konzepte und R-Werkzeuge kennen und kompetent anzuwenden. "],
["3-1-erganzungen-zu-prepro-1.html", "3.1 Ergänzungen zu PrePro 1", " 3.1 Ergänzungen zu PrePro 1 3.1.1 Integer mit “L” In R kann eine Zahl mit dem Suffix “L” explizit als Integer spezifiziert werden. typeof(42) ## [1] &quot;double&quot; typeof(42L) ## [1] &quot;integer&quot; Warum dazu der Buchstabe “L” verwendet wird ist nirgends offiziell Dokumentiert (zumindest haben wir nichts gefunden). Die gängigste Meinung, die auch von renommierten R-Profis vertreten wird ist, dass damit Long integer abgekürzt wird. 3.1.2 Arbeiten mit RStudio “Project” Wir empfehlen die Verwendung von “Projects” innerhalb von RStudio. RStudio legt für jedes Projekt dann einen Ordner an, in welches die Projekt-Datei abgelegt wird (Dateiendung .Rproj). Sollen innerhalb des Projekts dann R-Skripts geladen oder erzeugt werden, werden diese dann auch im angelegten Ordner abgelegt. Mehr zu RStudio Projects findet ihr hier. Das Verwenden von Projects bringt verschiedene Vorteile, wie zum Beispiel: Festlegen der Working Directory ohne die Verwendung des expliziten Pfades (setwd()). Das ist sinnvoll, da sich dieser Pfad ändern kann (Zusammenarbeit mit anderen Usern, Ausführung des Scripts zu einem späteren Zeitpunkt) Automatisches Zwischenspeichern geöffneter Scripts und Wiederherstellung der geöffneten Scripts bei der nächsten Session Festlegen verschiedener projektspezifischer Optionen Verwendung von Versionsverwaltungssystemen (Github oder SVN) 3.1.3 Arbeiten mit factors Wie bereits angedeutet, ist das Arbeiten mit factors etwas gewöhnungsbedürftig. Wir gehen hier auf ein paar Stolpersteine ein. zahlen &lt;- factor(c(&quot;null&quot;,&quot;eins&quot;,&quot;zwei&quot;,&quot;drei&quot;)) zahlen ## [1] null eins zwei drei ## Levels: drei eins null zwei Offensichtlich sollten diese factors geordnet sein, R weiss davon aber nichts. Eine Ordnung kann man mit dem Befehl ordered = T festlegen. Beachtet: ordered = T kann nur bei der Funktion factor() spezifiziert werden, nicht bei as.factor(). Ansonsten sind factor() und as.factor() sehr ähnlich. zahlen &lt;- factor(zahlen,ordered = T) zahlen ## [1] null eins zwei drei ## Levels: drei &lt; eins &lt; null &lt; zwei Beachtet das “&lt;”-Zeichen zwischen den Levels. Die Zahlen werden nicht in der korrekten Reihenfolge, sondern Alphabetisch geordnet. Die richtige Reihenfolge kann man mit levels = festlegen. zahlen &lt;- factor(zahlen,ordered = T,levels = c(&quot;null&quot;,&quot;eins&quot;,&quot;zwei&quot;,&quot;drei&quot;,&quot;vier&quot;)) zahlen ## [1] null eins zwei drei ## Levels: null &lt; eins &lt; zwei &lt; drei &lt; vier Wie auch schon erwähnt werden factors als character Vektor dargestellt, aber als Integers gespeichert. Das führt zu einem scheinbaren Wiederspruch wenn man den Datentyp auf unterschiedliche Weise abfragt. typeof(zahlen) ## [1] &quot;integer&quot; is.integer(zahlen) ## [1] FALSE Mit typeof() wird eben diese Form der Speicherung abgefragt und deshalb mit integer beantwortet. Da es sich aber nicht um einen eigentlichen Integer Vektor handelt, wird die Frage is.integer() mit FALSE beantwortet. Das ist etwas verwirrend, beruht aber darauf, dass die beiden Funktionen die Frage von unterschiedlichen Perspektiven beantworten. In diesem Fall schafft class() Klarheit: class(zahlen) ## [1] &quot;ordered&quot; &quot;factor&quot; Wirklich verwirrend wird es, wenn factors in numeric umgewandelt werden sollen. zahlen ## [1] null eins zwei drei ## Levels: null &lt; eins &lt; zwei &lt; drei &lt; vier as.integer(zahlen) ## [1] 1 2 3 4 Das die Übersetzung der auf Deutsch ausgeschriebenen Nummern in nummerische Zahlen nicht funktionieren würde, war ja klar. Weniger klar ist es jedoch, wenn die factors bereits aus nummerischen Zahlen bestehen. zahlen2 &lt;- factor(c(&quot;3&quot;,&quot;2&quot;,&quot;1&quot;,&quot;0&quot;)) as.integer(zahlen2) ## [1] 4 3 2 1 In diesem Fall müssen die factors erstmals in character umgewandelt werden. zahlen2 &lt;- factor(c(&quot;3&quot;,&quot;2&quot;,&quot;1&quot;,&quot;0&quot;)) as.integer(as.character(zahlen2)) ## [1] 3 2 1 0 3.1.4 Heikle Annahmen - bessere Alternativen Aus oben beschriebenen Grund ist es auch problematisch, dass data.frame() sowie alle read.* Funktionen (read.table, read.csv etc) immer davon ausgehen, dass Strings als factors interpretiert werden sollten. Es gibt in Base R einige Funktionen, welche Annahmen treffen die problematisch sein können. Ein weiteres Beispiel ist die Annahme der Zeitzone und Verwendung von Sommerzeit bei as.POSIXct(). Oft gibt es dafür im Tidyverse alternative Funktionen, in denen diese Probleme besser gelöst sind. Wir empfehlen, wenn immer Möglich die Tidyverse-Alternativen zu verwenden. Beispiele: data_frame() statt data.frame() read_* statt read.* parse_datetime statt as.POSIXct() Beim Import von Daten kann es sinnvoll sein, die Datentypen der Spalten bereits im Importbefehl zu spezifizieren. So vermeidet man die anschliessende Typumwandlung und die damit verbundenen Fehlerquellen. Zudem wird der Importprozess beschleunigt, da R keine Zeit daran verschwenden muss die Datentypen (aufgrund der ersten 1000 Zeilen) zu erraten. library(tidyverse) df1 &lt;- read_table(&quot;09_PrePro1/data/order_52252_data.txt&quot;, col_types = list( col_character(), # Macht aus der 1.Spalte ein character col_datetime(format = &quot;%Y%m%d%H&quot;),# Macht aus der 2.Spalte ein POSIXct col_double() # Macht aus der 3.Spalte ein double ) ) df1 ## # A tibble: 1,262,615 x 3 ## stn time tre200h0 ## &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; ## 1 ABO 2000-01-01 00:00:00 -2.6 ## 2 ABO 2000-01-01 01:00:00 -2.5 ## 3 ABO 2000-01-01 02:00:00 -3.1 ## 4 ABO 2000-01-01 03:00:00 -2.4 ## 5 ABO 2000-01-01 04:00:00 -2.5 ## 6 ABO 2000-01-01 05:00:00 -3 ## 7 ABO 2000-01-01 06:00:00 -3.7 ## 8 ABO 2000-01-01 07:00:00 -4.4 ## 9 ABO 2000-01-01 08:00:00 -4.1 ## 10 ABO 2000-01-01 09:00:00 -4.1 ## # … with 1,262,605 more rows df1 &lt;- read_table(&quot;09_PrePro1/data/order_52252_data.txt&quot;, col_types = list( col_factor(levels = NULL), # Macht aus der 1.Spalte ein factor col_datetime(format = &quot;%Y%m%d%H&quot;),# Macht aus der 2.Spalte ein POSIXct col_double() # Macht aus der 3.Spalte ein double ) ) df1 ## # A tibble: 1,262,615 x 3 ## stn time tre200h0 ## &lt;fct&gt; &lt;dttm&gt; &lt;dbl&gt; ## 1 ABO 2000-01-01 00:00:00 -2.6 ## 2 ABO 2000-01-01 01:00:00 -2.5 ## 3 ABO 2000-01-01 02:00:00 -3.1 ## 4 ABO 2000-01-01 03:00:00 -2.4 ## 5 ABO 2000-01-01 04:00:00 -2.5 ## 6 ABO 2000-01-01 05:00:00 -3 ## 7 ABO 2000-01-01 06:00:00 -3.7 ## 8 ABO 2000-01-01 07:00:00 -4.4 ## 9 ABO 2000-01-01 08:00:00 -4.1 ## 10 ABO 2000-01-01 09:00:00 -4.1 ## # … with 1,262,605 more rows "],
["3-2-demo-tidyverse.html", "3.2 Demo: tidyverse", " 3.2 Demo: tidyverse Demoscript als Download Hier möchten wir euch mit einer Sammlung von Tools vertraut machen, die spezifisch für das Daten prozessieren in Data Science entwickelt wurden. Der Prozess und das Modell ist hier3 schön beschrieben. Die Sammlung von Tools wird unter dem Namen tidyverse vertrieben, welches wir ja schon zu Beginn der ersten Übung installiert und geladen haben. Die Tools erleichtern den Umgang mit Daten ungeheuer und haben sich mittlerweile zu einem “must have” im Umgang mit Daten in R entwickelt. Wir können Euch nicht sämtliche Möglichkeiten von tidyverse zeigen. Wir fokussieren uns deshalb auf einzelne Komponenten4 und zeigen ein paar Funktionalitäten, die wir oft verwenden und Euch ggf. noch nicht bekannt sind. Wer sich vertieft mit dem Thema auseinandersetzen möchte, der sollte sich unbedingt das Buch Wickham and Grolemund (2017) beschaffen. Eine umfangreiche, aber nicht ganz vollständige Version gibt es online5, das vollständige eBook kann über die Bibliothek bezogen werden6. 3.2.1 Split-Apply-Combine 3.2.1.1 Packete laden library(tidyverse) Mit library(tidyverse) werden nicht alle Packete geladen, die mit install.packages(tidyverse) intalliert wurden (warum?). Unter anderem muss lubridate noch separat geladen werden: library(lubridate) 3.2.1.2 Daten Laden Wir laden die Wetterdaten von der letzten Übung. wetter &lt;- read_table(&quot;09_PrePro1/data/order_52252_data.txt&quot;, col_types = list( col_factor(levels = NULL), col_datetime(format = &quot;%Y%m%d%H&quot;), col_double() ) ) 3.2.1.3 Kennwerte berechnen Wir möchten den Mittelwert aller gemessenen Temperaturwerte berechnen. Dazu könnten wir folgenden Befehl verwenden: mean(wetter$tre200h0, na.rm = TRUE) ## [1] 8.962106 Die Option na.rm = T bedeutet, dass NA Werte von der Berechnung ausgeschlossen werden sollen. Mit der selben Herangehensweise können diverse Werte berechnet werden (z.B. das Maximum (max()), Minimum (min()), Median (median()) u.v.m.). Diese Herangehensweise funktioniert nur dann gut, wenn wir die Kennwerte über alle Beobachtungen (Zeilen) für eine Variable (Spalte) berechnen wollen. Sobald wir die Beobachtungen gruppieren wollen, wird es schwierig. Zum Beispiel, wenn wir die durchschnittliche Temperatur pro Jahr berechnen wollen. 3.2.1.4 Convenience Variablen Um diese Aufgabe zu lösen, muss zuerst das “Jahr” berechne werden (das Jahr ist die convenience variabel). Hierfür brauchen wir die Funktion year() (von lubridate). Nun kann kann die convenience Variable “Jahr” erstellt werden. Ohne dpylr wird eine neue Spalte wird folgendermassen hinzugefügt. wetter$year &lt;- year(wetter$time) Mit dplyr (siehe7) sieht der gleiche Befehl folgendermassen aus: wetter &lt;- mutate(wetter,year = year(time)) Der grosse Vorteil von dplyr ist an dieser Stelle noch nicht ersichtlich. Dieser wird aber später klar. 3.2.1.5 Kennwerte nach Gruppen berechnen Jetzt kann man die data.frame mithilfe der Spalte “Jahr” filtern. mean(wetter$tre200h0[wetter$year == 2000], na.rm = TRUE) ## [1] 9.281542 Dies müssen wir pro Jahr wiederholen, was natürlich sehr umständlich ist, v.a. wenn man eine Vielzahl an Gruppen hat (z.B. Kalenderwochen statt Jahre). Deshalb nutzen wir das package dplyr. Damit geht die Aufgabe (Temperaturmittel pro Jahr berechnen) folgendermassen: summarise(group_by(wetter,year),temp_mittel = mean(tre200h0, na.rm = TRUE)) ## # A tibble: 7 x 2 ## year temp_mittel ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2000 9.28 ## 2 2001 8.76 ## 3 2002 9.30 ## 4 2003 9.48 ## 5 2004 8.64 ## 6 2005 8.31 ## 7 NA NaN 3.2.1.6 Verketten vs. verschachteln Auf Deutsch übersetzt heisst die obige Operation folgendermassen: nimm den Datensatz wetter Bilde Gruppen pro Jahr (group_by(wetter,year)) Berechne das Temperaturmittel (mean(tre200h0)) Diese Übersetzung R-&gt; Deutsch unterscheidet sich vor allem darin, dass die Operation auf Deutsch verkettet ausgesprochen wird (Operation 1-&gt;2-&gt;3) während der Computer verschachtelt liest 3(2(1)). Um R näher an die gesprochene Sprache zu bringen, kann man den %&gt;%-Operator verwenden (siehe8). summarise(group_by(wetter,year),temp_mittel = mean(tre200h0)) # wird zu: wetter %&gt;% #1) nimm den Datensatz &quot;wetter&quot; group_by(year) %&gt;% #2) Bilde Gruppen pro Jahr summarise(temp_mittel = mean(tre200h0)) #3) berechne das Temperaturmittel Dieses Verketten mittels %&gt;% macht den Code einiges schreib- und leserfreundlicher, und wir werden ihn in den nachfolgenden Übungen verwenden. Dabei handelt es sich um das package magrittr, welches mit tidyverse mitgeliefert wird. Zu dplyr und magrittrgibt es etliche Tutorials online (siehe9), deshalb werden wir diese Tools nicht in allen Details erläutern. Nur noch folgenden wichtigen Unterschied zu zwei wichtigen Funktionen in dpylr: mutate() und summarise(). summarise() fasst einen Datensatz zusammen. Dabei reduziert sich die Anzahl Beobachtungen (Zeilen) auf die Anzahl Gruppen (z.B. eine zusammengefasste Beobachtung (Zeile) pro Jahr). Zudem reduziert sich die Anzahl Variablen (Spalten) auf diejenigen, die in der “summarise” Funktion spezifiziert wurde (z.B. temp_mittel). mit mutate wird ein data.frame vom Umfang her belassen, es werden lediglich zusätzliche Variablen (Spalten) hinzugefügt (siehe Beispiel unten). # Maximal und minimal Temperatur pro Kalenderwoche wetter %&gt;% #1) nimm den Datensatz &quot;wetter&quot; filter(stn == &quot;ABO&quot;) %&gt;% #2) filter auf Station namnes &quot;ABO&quot; mutate(kw = week(time)) %&gt;% #3) erstelle eine neue Spalte &quot;kw&quot; group_by(kw) %&gt;% #4) Nutze die neue Spalte um Guppen zu bilden summarise( temp_max = max(tre200h0, na.rm = TRUE),#5) Berechne das Maximum temp_min = min(tre200h0, na.rm = TRUE) #6) Berechne das Minimum ) ## # A tibble: 53 x 3 ## kw temp_max temp_min ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 11.4 -15.2 ## 2 2 12.9 -15.9 ## 3 3 8.2 -11.3 ## 4 4 9.6 -15.9 ## 5 5 16.9 -17.5 ## 6 6 13.5 -13.1 ## 7 7 12.9 -15.4 ## 8 8 11 -14.4 ## 9 9 12.9 -17.6 ## 10 10 15.4 -16.3 ## # … with 43 more rows 3.2.1.7 Resultate plotten Mit diesen Tools können wir nun auch eine neue Grafik plotten, ähnlich wie in der Übung 1. Dafür müssen wir die ganzen Operationen aber zuerst in einer Variabel speichern (bis jetzt hat R zwar alles schön berechnet, aber uns nur auf die Konsole ausgegeben). wetter_sry &lt;- wetter %&gt;% mutate( kw = week(time) ) %&gt;% filter(stn == &quot;ABO&quot;) %&gt;% group_by(kw) %&gt;% summarise( temp_max = max(tre200h0), temp_min = min(tre200h0), temp_mean = mean(tre200h0) ) Dieses Mal plotten wir nur mit ggplot2 (siehe10) ggplot() + geom_line(data = wetter_sry, aes(kw,temp_max), colour = &quot;yellow&quot;) + geom_line(data = wetter_sry, aes(kw,temp_mean), colour = &quot;pink&quot;) + geom_line(data = wetter_sry, aes(kw,temp_min), colour = &quot;black&quot;) + labs(y = &quot;temp&quot;) Das sieht schon mal gut aus. Nur, wir mussten pro Linie einen eigene Zeile schreiben (geom_line()) und dieser eine Farbe zuweisen. Bei drei Werten ist das ja ok, aber wie sieht es denn aus wenn es Hunderte sind? Da hat ggplot natürlich eine Lösung, dafür müssen aber alle Werte in einer Spalte daher kommen. Das ist ein häufiges Problem: Wir haben eine breite Tabelle (viele Spalten), bräuchten aber eine lange Tabelle (viele Zeilen). 3.2.2 Reshaping data 3.2.2.1 Breit -&gt; lang Da kommt tidyverse wieder ins Spiel. Die Umformung von Tabellen breit-&gt;lang erfolgt mittels tidyr(siehe11). Auch dieses package funktioniert wunderbar mit piping (%&gt;%). wetter_sry_long &lt;- wetter_sry %&gt;% gather(Key, Value, c(temp_max,temp_min,temp_mean)) Im Befehl gather() braucht es drei Werte: beliebiger Name der neuen Variablen (Spalte) für die Schlüssel: “temp_mean”, “temp_min”… (ich verwenden den Namen: Key) beliebiger Name der neuen Variablen (Spalte) für die effektiven Werte: 5°C, 10°C (ich verwenden den Namen: Value) Name der (bestehenden) Variablen (Spalten), die zusammen gefasst werden sollten: (hier: temp_max,temp_min,temp_mean) Die ersten 6 Zeilen von wetter_sry: kw temp_max temp_min temp_mean 1 11.4 -15.2 -1.2593254 2 12.9 -15.9 -1.5572421 3 8.2 -11.3 -1.8832341 4 9.6 -15.9 -2.8375000 5 16.9 -17.5 -0.9789683 6 13.5 -13.1 0.4392857 Die ersten 6 Zeilen von wetter_sry_long: kw Key Value 1 temp_max 11.400000 1 temp_min -15.200000 1 temp_mean -1.259325 2 temp_max 12.900000 2 temp_min -15.900000 2 temp_mean -1.557242 Beachte: wetter_sry_long umfasst 159 Beobachtungen (Zeilen), das sind 3 mal soviel wie wetter_sry, da wir ja drei Spalten zusammengefasst haben. nrow(wetter_sry) ## [1] 53 nrow(wetter_sry_long) ## [1] 159 Statt die Variablen (Spalten) zu benennen, die zusammengefasst werden sollten, wäre es in unserem Fall einfacher, die Variablen (Spalten) zu benennen die nicht zusammengefasst werden sollen (kw): wetter_sry_long &lt;- wetter_sry %&gt;% gather(Key, Value, -kw) Nun können wir den obigen Plot viel einfacher erstellen: ggplot(wetter_sry_long, aes(kw,Value, colour = Key)) + geom_line() Beachtet, dass wir gegenüber dem letzten Plot colour nun innerhalb von aes() festlegen und nicht mit einem expliziten Farbwert, sondern mit dem Verweis auf die Spalte key. 3.2.2.2 Lang -&gt; breit Um unsere lange Tabelle wieder zurück in eine breite zu überführen, brauchen wir lediglich einen Befehl (spread): wetter_sry_long %&gt;% spread(Key,Value) ## # A tibble: 53 x 4 ## kw temp_max temp_mean temp_min ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 11.4 -1.26 -15.2 ## 2 2 12.9 -1.56 -15.9 ## 3 3 8.2 -1.88 -11.3 ## 4 4 9.6 -2.84 -15.9 ## 5 5 16.9 -0.979 -17.5 ## 6 6 13.5 0.439 -13.1 ## 7 7 12.9 -2.32 -15.4 ## 8 8 11 -2.84 -14.4 ## 9 9 12.9 -2.20 -17.6 ## 10 10 15.4 0.917 -16.3 ## # … with 43 more rows 3.2.3 Quellen Dieses Kapitel verwendet folgende Libraries: Spinu, Grolemund, and Wickham (2018), Wickham (2018a), Wickham (2018c), Wickham, François, et al. (2018), Henry and Wickham (2018), Wickham, Hester, and Francois (2017), Wickham and Henry (2018), Müller and Wickham (2018), Wickham, Chang, et al. (2018), Wickham (2017) References "],
["3-3-ubung-a-1.html", "3.3 Übung A", " 3.3 Übung A 3.3.1 Aufgabe 1 Lade die Wetterdaten aus der letzten Übung. 3.3.2 Aufgabe 2 Bereinige den Datensatz. Entferne z.B. alle Zeilen, bei dem der Stationsnahme oder Temperaturwerte fehlen 3.3.3 Aufgabe 3 Überführe die lange Tabelle über in eine breite. Dabei sollte jede Station eine eigene Spalte enthalten (key), gefüllt mit den Temperaturwerten (value). Speichere diese Tabelle in einer neuen Variabel. 3.3.4 Aufgabe 4 Importiere die Datei order_52252_legend.csv (z.B. mit read_delim). Hinweis: Wenn Umlaute und Sonderzeichen nicht korrekt dargestellt werden (z.B. in Genève), hat das vermutlich mit der Zeichencodierung zu tun. Das File ist aktuell in ‘ANSI’ Codiert, welche für gewisse Betriebssysteme / R-Versionen ein Problem darstellt. Um das Problem zu umgehen muss man das File mit einem Editor öffnen (Windows ‘Editor’ oder ‘Notepad++’, Mac: ‘TextEdit’) und mit einer neuen Codierung (z.B ‘UTF-8’) abspeichern. Danach kann die Codierung spezifitiert werden (bei read_delim(): mitlocale = locale(encoding = “UTF-8”)`) 3.3.5 Aufgabe 5 Die x-/y-Koordinaten sind aktuell in einer Spalte erfasst. Um mit den Koordinaten sinnvoll arbeiten zu können, brauchen wir die Koordinaten getrennt. Trenne die x und y Koordinaten aus der Spalte Koordinaten (Tipp: nutze dafür tidyr::separate()). 3.3.6 Aufgabe 6 Nun wollen wir den Datensatz wettermit den Informationen aus wetter_legendeanreichern. Uns interessiert aber nur das Stationskürzel, der Name, die x/y Koordinaten sowie die Meereshöhe. Lösche die nicht benötigten Spalten (oder selektiere die benötigten Spalten). Tipp: Nutze select() von dplyr 3.3.7 Aufgabe 7 Nun ist der Datensatz wetter_legendegenügend vorbereitet. Jetzt kann er mit dem Datensatz wetter verbunden werden. Überlege dir, welcher Join dafür sinnvoll ist und mit welchem Attribut wir “joinen” können. Nutze die Join-Möglichkeiten von dplyr (Hilfe via ?dplyr::join) um die Datensätze wetter und wetter_legendezu verbinden. 3.3.8 Aufgabe 8 Berechne die Durchschnittstemperatur pro Station. Nutze dabei dplyr::summarise() und wenn möglich %&gt;%. Speichere das Resultat in einer neuen Variabel. 3.3.9 Aufgabe 9 Nun wollen wir das Resultat aus Aufgabe 7 nutzen, um die Durchschnittstemperatur der Meereshöhe gegenüber zu stellen. Dummerweise ging das Attribut Meereshoehe bei der summarise() Operation verloren (da bei summarise() alle Spalten weg fallen, die nicht in group_by() definiert wurden). Um die Spalte Meereshoehe beizubehalten, muss sie also unter group_by() aufgelistet werden. Wiederhole Übung 7 und siehe zu, dass die Meereshöhe beibehalten wird. Stelle danach in einem Scatterplot (wenn möglich mit ggplot()) die Meereshöhe der Durchschnittstemperatur gegenüber. "],
["3-4-ubung-a-losung-1.html", "3.4 Übung A: Lösung", " 3.4 Übung A: Lösung R-Code als Download library(tidyverse) library(lubridate) library(stringr) # Lösung Aufgabe 1 wetter &lt;- read_table(&quot;09_PrePro1/data/order_52252_data.txt&quot;, col_types = list( col_factor(levels = NULL), col_datetime(format = &quot;%Y%m%d%H&quot;), col_double() ) ) # Lösung Aufgabe 2 wetter &lt;- wetter %&gt;% filter(!is.na(stn)) %&gt;% filter(!is.na(tre200h0)) # Lösung Aufgabe 3 wetter_spread &lt;- spread(wetter, stn,tre200h0) # Lösung Aufgabe 4 wetter_legende &lt;- read_delim(&quot;09_PrePro1/data/order_52252_legend.csv&quot;,delim = &quot;;&quot;, locale = locale(encoding = &quot;UTF-8&quot;)) # Lösung Aufgabe 5 koordinaten &lt;- str_split_fixed(wetter_legende$Koordinaten, &quot;/&quot;, 2) colnames(koordinaten) &lt;- c(&quot;x&quot;,&quot;y&quot;) wetter_legende &lt;- cbind(wetter_legende,koordinaten) # Lösung Aufgabe 6 wetter_legende &lt;- dplyr::select(wetter_legende, stn, Name, x,y,Meereshoehe) # Lösung Aufgabe 7 wetter &lt;- left_join(wetter,wetter_legende,by = &quot;stn&quot;) # Jointyp: Left-Join auf &#39;wetter&#39;, da uns nur die Stationen im Datensatz &#39;wetter&#39; interessieren. # Attribut: &quot;stn&quot; # Lösung Aufgabe 8 wetter_sry &lt;- wetter %&gt;% group_by(stn) %&gt;% summarise(temp_mean = mean(tre200h0)) # Lösung Aufgabe 9 wetter_sry &lt;- wetter %&gt;% group_by(stn,Meereshoehe) %&gt;% summarise(temp_mean = mean(tre200h0)) # Achtung: wenn mehrere Argumente in group_by() definiert werden führt das # üblicherweise zu Untergruppen. In unserem Fall hat jede Station nur EINE # Meereshöhe, deshalb wird die Zahl der Gruppen nicht erhöht. ggplot(wetter_sry, aes(temp_mean,Meereshoehe)) + geom_point() "],
["3-5-ubung-b-1.html", "3.5 Übung B", " 3.5 Übung B 3.5.1 Aufgabe 1 Gegeben sind die Daten von drei Sensoren (sensor1.csv, sensor2.csv, sensor3.csv). Lade die Datensätze runter und lese sie ein. 3.5.2 Aufgabe 2 Füge die drei Tabellen zu einer zusammen. Dazu kannst du entweder die Spalten (Variablen) mittels join() oder die Zeilen (Beobachtungen) mittels rbind() zusammen “kleben”. Überführe zudem die Spalte Datetime in ein POSIXct-Format. Das ursprüngliche Format lautet:DDMMYYYY_HHMM 3.5.3 Aufgabe 3 Importiere die Datei sensor_1_fail.csv in R. sensor_fail.csv hat eine Variabel SensorStatus: 1 bedeutet der Sensor misst, 0 bedeutet der Sensor miss nicht. Fälschlicherweise wurde auch dann der Messwert Temp = 0 erfasst, wenn Sensorstatus = 0. Richtig wäre hier NA (not available). Korrigiere den Datensatz entsprechend. 3.5.4 Aufgabe 4 Warum spielt das es eine Rolle, ob 0 oder NA erfasst wird? Vergleiche dazu die Mittlere Temperatur / Feuchtigkeit vor und nach der Korrektur. "],
["3-6-ubung-b-losung-1.html", "3.6 Übung B: Lösung", " 3.6 Übung B: Lösung R-Code als Download library(tidyverse) library(lubridate) library(stringr) # Lösung Aufgabe 1 sensor1 &lt;- read_delim(&quot;10_PrePro2/data/sensor1.csv&quot;,&quot;;&quot;) sensor2 &lt;- read_delim(&quot;10_PrePro2/data/sensor2.csv&quot;,&quot;;&quot;) sensor3 &lt;- read_delim(&quot;10_PrePro2/data/sensor3.csv&quot;,&quot;;&quot;) # Lösung Aufgabe 2 (Var 1: Spalten [Variabeln] zusammen &#39;kleben&#39;) sensor_all &lt;- sensor1 %&gt;% rename(sensor1 = Temp) %&gt;% # Spalte &quot;Temp&quot; in &quot;sensor1&quot; umbenennen full_join(sensor2,by = &quot;Datetime&quot;) %&gt;% rename(sensor2 = Temp) %&gt;% full_join(sensor3, by = &quot;Datetime&quot;) %&gt;% rename(sensor3 = Temp) %&gt;% mutate(Datetime = as.POSIXct(Datetime,format = &quot;%d%m%Y_%H%M&quot;)) # Lösung Aufgabe 2 (Var 2: Zeilen [Beobachtungen] zusammen &#39;kleben) sensor1$sensor &lt;- &quot;sensor1&quot; sensor2$sensor &lt;- &quot;sensor2&quot; sensor3$sensor &lt;- &quot;sensor3&quot; sensor_all &lt;- rbind(sensor1,sensor2,sensor3) sensor_all &lt;- sensor_all %&gt;% mutate( Datetime = as.POSIXct(Datetime,format = &quot;%d%m%Y_%H%M&quot;) ) %&gt;% spread(sensor, Temp) # Lösung Aufgabe 3 sensor_fail &lt;- read_delim(&quot;10_PrePro2/data/sensor_fail.csv&quot;, delim = &quot;;&quot;) # Lösungsweg 1 sensor_fail$Datetime &lt;- as.POSIXct(sensor_fail$Datetime,format = &quot;%d%m%Y_%H%M&quot;) sensor_fail$`Hum_%`[sensor_fail$SensorStatus == 0] &lt;- NA sensor_fail$Temp[sensor_fail$SensorStatus == 0] &lt;- NA # Lösungsweg 2 sensor_fail &lt;- read_delim(&quot;10_PrePro2/data/sensor_fail.csv&quot;, delim = &quot;;&quot;) sensor_fail_corr &lt;- sensor_fail %&gt;% mutate( Datetime = as.POSIXct(Datetime,format = &quot;%d%m%Y_%H%M&quot;) ) %&gt;% rename(Humidity = `Hum_%`) %&gt;% # Weil R &quot;%&quot; in Headers nicht mag gather(key,val, c(Temp, Humidity)) %&gt;% mutate( val = ifelse(SensorStatus == 0,NA,val) ) %&gt;% spread(key,val) # Lösung Aufgabe 4 # Mittelwerte der unkorrigierten Sensordaten (`NA` als `0`) mean(sensor_fail$Temp) mean(sensor_fail$`Hum_%`) # Mittelwerte der korrigierten Sensordaten (`NA` als `NA`). Hier müssen wir die Option # `na.rm = T` (Remove NA = T) wählen, denn `mean()` (und ähnliche Funktionen) retourieren # immer `NA`, sobald ein **einzelner** Wert in der Reihe `NA`ist. mean(sensor_fail_corr$Temp, na.rm = T) mean(sensor_fail_corr$Humidity, na.rm = T) "],
["4-infovis1-21-10-2019.html", "Kapitel 4 InfoVis1 (21.10.2019)", " Kapitel 4 InfoVis1 (21.10.2019) Die konventionelle schliessende Statistik arbeitet in der Regel konfirmatorisch, sprich aus der bestehenden Theorie heraus werden Hypothesen formuliert, welche sodann durch Experimente geprüft und akzeptiert oder verworfen werden. Die Explorative Datenanalyse (EDA) nimmt dazu eine antagonistische Analyseperspektive ein und will in den Daten zunächst Zusammenhänge aufdecken, welche dann wiederum zur Formulierung von prüfbaren Hypothesen führen kann. Die Einheit stellt dazu den klassischen 5-stufigen EDA-Prozess nach Tukey (1980!) vor. Abschliessend wird dann noch die Brücke geschlagen zur modernen Umsetzung der EDA in Form von Visual Analytics. "],
["4-1-eda-beispiel-vorlesung.html", "4.1 EDA Beispiel Vorlesung", " 4.1 EDA Beispiel Vorlesung Demoscript als Download library(tidyverse) library(scales) # create some data about age and height of people people &lt;- data.frame( ID = c(1:30), age = c(5.0, 7.0, 6.5 ,9.0, 8.0, 5.0, 8.6, 7.5, 9.0, 6.0, 63.5 ,65.7, 57.6, 98.6, 76.5, 78.0, 93.4, 77.5, 256.6, 512.3, 15.5, 18.6, 18.5, 22.8, 28.5, 39.5, 55.9, 50.3, 31.9, 41.3), height = c(0.85, 0.93, 1.1, 1.25, 1.33, 1.17, 1.32, 0.82, 0.89, 1.13, 1.62, 1.87, 1.67, 1.76, 1.56, 1.71, 1.65, 1.55, 1.87, 1.69, 1.49, 1.68, 1.41, 1.55, 1.84, 1.69, 0.85, 1.65, 1.94, 1.80), weight = c(45.5, 54.3, 76.5, 60.4, 43.4, 36.4, 50.3, 27.8, 34.7, 47.6, 84.3, 90.4, 76.5, 55.6, 54.3, 83.2, 80.7, 55.6, 87.6, 69.5, 48.0, 55.6, 47.6, 60.5, 54.3, 59.5, 34.5, 55.4, 100.4, 110.3) ) # build a scatterplot for a first inspection ggplot(people, aes(x=age, y=height)) + geom_point() + scale_y_continuous(limits=c(0.75, 2.0)) + theme(axis.text=element_text(size=12), axis.title=element_text(size=20,face=&quot;bold&quot;)) # Go to help page: http://docs.ggplot2.org/current/ -&gt; Search for icon of fit-line # http://docs.ggplot2.org/current/geom_smooth.html # build a scatterplot for a first inspection, with regression line ggplot(people, aes(x=age, y=height)) + geom_point() + scale_y_continuous(limits=c(0, 2.0)) + geom_smooth(method=&quot;loess&quot;, fill=&#39;lightblue&#39;, size=0.5, alpha=0.5) + theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face=&quot;bold&quot;)) ?stem # stem and leaf plot stem(people$height) ## ## The decimal point is 1 digit(s) to the left of the | ## ## 8 | 25593 ## 10 | 037 ## 12 | 523 ## 14 | 19556 ## 16 | 255789916 ## 18 | 04774 stem(people$height, scale=2) ## ## The decimal point is 1 digit(s) to the left of the | ## ## 8 | 2559 ## 9 | 3 ## 10 | ## 11 | 037 ## 12 | 5 ## 13 | 23 ## 14 | 19 ## 15 | 556 ## 16 | 2557899 ## 17 | 16 ## 18 | 0477 ## 19 | 4 # explore the two variables with box-whiskerplots summary(people$age) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 5.00 8.70 30.20 59.14 65.15 512.30 boxplot(people$age) boxplot(people$age) summary(people$height) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.820 1.190 1.555 1.455 1.690 1.940 boxplot(people$height) boxplot(people$height) # explore data with a histgram ggplot(people, aes(x=age)) + geom_histogram(stat=&quot;bin&quot;, fill=&#39;green&#39;, binwidth=20) + theme_bw() + labs(x = &#39;\\nage&#39;, y = &#39;count\\n&#39;) + theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face=&quot;bold&quot;)) density(x = people$height) ## ## Call: ## density.default(x = people$height) ## ## Data: people$height (30 obs.); Bandwidth &#39;bw&#39; = 0.1576 ## ## x y ## Min. :0.3472 Min. :0.001593 ## 1st Qu.:0.8636 1st Qu.:0.102953 ## Median :1.3800 Median :0.510601 ## Mean :1.3800 Mean :0.483553 ## 3rd Qu.:1.8964 3rd Qu.:0.722660 ## Max. :2.4128 Max. :1.216350 # re-expression: use log or sqrt axes # # Find here guideline about scaling axes # http://www.cookbook-r.com/Graphs/Axes_(ggplot2)/ # http://docs.ggplot2.org/0.9.3.1/scale_continuous.html # logarithmic axis: respond to skewness in the data, e.g. log10 ggplot(people, aes(x=age, y=height)) + geom_point() + scale_y_continuous(limits=c(0, 2.0)) + geom_smooth(method=&quot;lm&quot;, fill=&#39;lightblue&#39;, size=0.5, alpha=0.5) + theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face=&quot;bold&quot;)) + scale_x_log10() # logarithmic axis: show multiplicative factors, e.g. log2 ggplot(people, aes(x=age, y=height)) + geom_point() + scale_y_continuous(limits=c(0, 2.0)) + geom_smooth(method=&quot;lm&quot;, fill=&#39;lightblue&#39;, size=0.5, alpha=0.5) + theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face=&quot;bold&quot;)) + scale_x_continuous(trans = log2_trans(), breaks = trans_breaks(&quot;log2&quot;, function(x) 2^x), labels = trans_format(&quot;log2&quot;, math_format(2^.x))) # outliers: Remove very small and very old people peopleTemp &lt;- subset(people, ID != 27) # Diese Person war zu klein. peopleClean &lt;- subset(peopleTemp, age &lt; 100) # Fehler in der Erhebung des Alters # re-explore cleaned data with a histgram ggplot(peopleClean, aes(x=age)) + geom_histogram(stat=&quot;bin&quot;, fill=&#39;#6baed6&#39;, binwidth=10) + theme_bw() + labs(x = &#39;\\nage&#39;, y = &#39;count\\n&#39;) + theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face=&quot;bold&quot;)) ggplot(peopleClean, aes(x=age, y=height)) + geom_point() + scale_y_continuous(limits=c(0, 2.0)) + geom_smooth(method=&quot;lm&quot;, fill=&#39;lightblue&#39;, size=0.5, alpha=0.5) + theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face=&quot;bold&quot;)) # with custom binwidth ggplot(peopleClean, aes(x=age)) + geom_histogram(stat=&quot;bin&quot;, fill=&#39;#6baed6&#39;, binwidth=10) + theme_bw() + labs(x = &#39;\\nAlter&#39;, y = &#39;Anzahl\\n&#39;) # quadratic axis ggplot(peopleClean, aes(x=age, y=height)) + geom_point() + scale_y_continuous(limits=c(0, 2.0)) + geom_smooth(method=&quot;lm&quot;, fill=&#39;lightblue&#39;, size=0.5, alpha=0.5) + theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face=&quot;bold&quot;)) + scale_x_sqrt() # subset &quot;teenies&quot;: No trend kids &lt;- subset(peopleClean, age &lt; 15) ggplot(kids, aes(x=age, y=height)) + geom_point() + scale_y_continuous(limits=c(0, 2.0)) + geom_smooth(method=&quot;lm&quot;, fill=&#39;lightblue&#39;, size=0.5, alpha=0.5) + theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face=&quot;bold&quot;)) # subset &quot;teenies&quot;: No trend oldies &lt;- subset(peopleClean, age &gt; 55) ggplot(oldies, aes(x=age, y=height)) + geom_point() + scale_y_continuous(limits=c(0, 2.0)) + geom_smooth(method=&quot;lm&quot;, fill=&#39;lightblue&#39;, size=0.5, alpha=0.5) + theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face=&quot;bold&quot;)) # Onwards towards multidimensional data # Finally, make a scatterplot matrix pairs(peopleClean[,2:4], panel=panel.smooth) pairs(peopleClean[,2:4], panel=panel.smooth) # Or as a bubble chart peopleClean$radius &lt;- sqrt( peopleClean$weight/ pi ) symbols(peopleClean$age, peopleClean$height, circles=peopleClean$radius) symbols(peopleClean$age, peopleClean$height, circles=peopleClean$radius) 4.1.1 Quellen Dieses Kapitel verwendet folgende Libraries: Wickham (2018b), Wickham (2018a), Wickham (2018c), Wickham, François, et al. (2018), Henry and Wickham (2018), Wickham, Hester, and Francois (2017), Wickham and Henry (2018), Müller and Wickham (2018), Wickham, Chang, et al. (2018), Wickham (2017) library(tidyverse) library(lubridate) References "],
["4-2-demo-ggplot2.html", "4.2 Demo: ggplot2", " 4.2 Demo: ggplot2 Demoscript als Download Als erstes laden wir den Wetterdatensatz von der Übung Prepro1 ein. wetter &lt;- read_table(&quot;09_PrePro1/data/order_52252_data.txt&quot;, col_types = list( col_factor(levels = NULL), col_datetime(format = &quot;%Y%m%d%H&quot;), col_double() ) ) stn time tre200h0 ABO 2000-01-01 00:00:00 -2.6 ABO 2000-01-01 01:00:00 -2.5 ABO 2000-01-01 02:00:00 -3.1 ABO 2000-01-01 03:00:00 -2.4 ABO 2000-01-01 04:00:00 -2.5 ABO 2000-01-01 05:00:00 -3.0 Der Datensatz hat 1262615 Zeilen. Bevor wir mit plotten beginnen, müssen wir den Datensatz etwas filtern da die Plots ansonsten zu schwerfällig werden. Wir filtern deshalb auf Januar 2000. wetter_fil &lt;- wetter %&gt;% mutate( year = year(time), month = month(time) ) %&gt;% filter(year == 2000 &amp; month == 1) Ein ggplot wird durch den Befehl ggplot() initiiert. Hier wird einerseits der Datensatz festgelegt, auf dem der Plot beruht (data =), sowie die Variablen innerhalb des Datensatzes, die Einfluss auf den Plot ausüben (mapping = aes()). Weiter braucht es mindestens ein “Layer” der beschreibt, wie die Daten dargestellt werden sollen (z.B. geom_point()). Anders als bei “Piping” (%&gt;%) wird ein Layer mit + hinzugefügt. # Datensatz: &quot;wetter_fil&quot; | Beeinflussende Variabeln: &quot;time&quot; und &quot;tre200h0&quot; ggplot(data = wetter_fil, mapping = aes(time,tre200h0)) + # Layer: &quot;geom_point&quot; entspricht Punkten in einem Scatterplot geom_point() Da ggplot die Eingaben in der Reihenfolge data = und dann mapping =erwartet, können wir diese Spezifizierungen auch weglassen. ggplot(wetter_fil, aes(time,tre200h0)) + geom_point() Nun wollen wir die unterschiedlichen Stationen unterschiedlich einfärben. Da wir Variablen definieren wollen, welche Einfluss auf die Grafik haben sollen, gehört diese Information in aes(). ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) + geom_point() Wir können noch einen Layer mit Linien hinzufügen: ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) + geom_point() + geom_line() Weiter können wir die Achsen beschriften und einen Titel hinzufügen. Zudem lasse ich die Punkte (geom_point()) nun weg, da mir diese nicht gefallen. ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) + geom_line() + labs(x = &quot;Woche&quot;, y = &quot;Temperatur in Grad C°&quot;, title = &quot;Temperaturdaten Schweiz&quot;, subtitle = &quot;Januar 2000&quot;) Man kann auch Einfluss auf die x-/y-Achsen nehmen. Dabei muss man zuerst festlegen, was für ein Achsentyp der Plot hat (vorher hat ggplot eine Annahme auf der Basis der Daten getroffen). Bei unserer y-Achse handelt es sich um numerische Daten, ggplot nennt diese: scale_y_continuous(). Unter ggplot2.tidyverse.org findet man noch andere x/y-Achsentypen (scale_x_irgenwas bzw. scale_y_irgendwas). ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) + geom_line() + labs(x = &quot;Woche&quot;, y = &quot;Temperatur in Grad C°&quot;, title = &quot;Temperaturdaten Schweiz&quot;, subtitle = &quot;Januar 2000&quot;) + scale_y_continuous(limits = c(-30,30)) # y-Achsenabschnitt bestimmen Das gleiche Spiel kann man für die y-Achse betreiben. Bei unserer y-Achse handelt es sich ja um unsere POSIXct Daten. ggplot nennt diese: scale_x_datetime(). ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) + geom_line() + labs(x = &quot;Woche&quot;, y = &quot;Temperatur in Grad C°&quot;, title = &quot;Temperaturdaten Schweiz&quot;, subtitle = &quot;Januar 2000&quot;) + scale_y_continuous(limits = c(-30,30)) + scale_x_datetime(date_breaks = &quot;1 week&quot;, date_minor_breaks = &quot;1 day&quot;, date_labels = &quot;KW%W&quot;) Mit theme verändert man das allgmeine Layout der Plots. Beispielsweise kann man mit theme_classic() ggplot-Grafiken etwas weniger “Poppig” erscheinen lassen: so sind sie besser für Bachelor- / Masterarbeiten sowie Publikationen geeignet. theme_classic() kann man indiviudell pro Plot anwenden, oder für die aktuelle Session global setzen (s.u.) Individuell pro Plot: ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) + geom_line() + labs(x = &quot;Woche&quot;, y = &quot;Temperatur in Grad C°&quot;, title = &quot;Temperaturdaten Schweiz&quot;, subtitle = &quot;Januar 2000&quot;) + scale_y_continuous(limits = c(-30,30)) + scale_x_datetime(date_breaks = &quot;1 week&quot;, date_minor_breaks = &quot;1 day&quot;, date_labels = &quot;KW%W&quot;) + theme_classic() Global (für alle nachfolgenden Plots der aktuellen Session): theme_set(theme_classic()) Sehr praktisch sind auch die Funktionen für “Small multiples”. Dies erreicht man mit facet_wrap() (oder facet_grid(), mehr dazu später). Man muss mit einem Tilde-Symbol “~” nur festlegen, welche Variable für das Aufteilen des Plots in kleinere Subplots verantwortlich sein soll. ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) + geom_line() + labs(x = &quot;Woche&quot;, y = &quot;Temperatur in Grad C°&quot;, title = &quot;Temperaturdaten Schweiz&quot;, subtitle = &quot;Januar 2000&quot;) + scale_y_continuous(limits = c(-30,30)) + scale_x_datetime(date_breaks = &quot;2 weeks&quot;, date_minor_breaks = &quot;1 day&quot;, date_labels = &quot;KW%W&quot;) + facet_wrap(~stn) Auch facet_wrap kann man auf seine Bedürfnisse anpassen. Da wir 24 Stationen haben möchte ich lieber 3 pro Zeile, damit es schön aufgeht. Dies erreiche ich mit ncol = 3. Zudem brauchen wir die Legende nicht mehr, da der Stationsnamen über jedem Facet steht. Ich setze deshalb theme(legend.position=&quot;none&quot;) ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) + geom_line() + labs(x = &quot;Woche&quot;, y = &quot;Temperatur in Grad C°&quot;, title = &quot;Temperaturdaten Schweiz&quot;, subtitle = &quot;Januar 2000&quot;) + scale_y_continuous(limits = c(-30,30)) + scale_x_datetime(date_breaks = &quot;1 week&quot;, date_minor_breaks = &quot;1 day&quot;, date_labels = &quot;KW%W&quot;) + facet_wrap(~stn,ncol = 3) + theme(legend.position=&quot;none&quot;) Genau wie data.frames und andere Objekte, kann man einen ganzen Plot auch in einer Variabel speichern. Dies kann nützlich sein um einen Plot zu exportieren (als png, jpg usw.) oder sukzessive erweitern wie in diesem Beispiel. p &lt;- ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) + geom_line() + labs(x = &quot;Woche&quot;, y = &quot;Temperatur in Grad C°&quot;, title = &quot;Temperaturdaten Schweiz&quot;, subtitle = &quot;Januar 2000&quot;) + scale_y_continuous(limits = c(-30,30)) + scale_x_datetime(date_breaks = &quot;1 week&quot;, date_minor_breaks = &quot;1 day&quot;, date_labels = &quot;KW%W&quot;) + facet_wrap(~stn,ncol = 3) # ich habe an dieser Stelle theme(legend.position=&quot;none&quot;) entfernt Folgendermassen kann ich den Plot als png-File abspeichern (ohne Angabe von “plot =” wird einfach der letzte Plot gespeichert) ggsave(filename = &quot;11_InfoVis1/plot.png&quot;,plot = p) .. und so kann ich einen bestehenden Plot (in einer Variabel) mit einem Layer / einer Option erweitern p + theme(legend.position=&quot;none&quot;) Wie üblich wurde diese Änderung nicht gespeichert, sondern nur das Resultat davon ausgeben. Wenn die Änderung in meinem Plot (in der Variabel) abspeichern will, muss ich die Variabel überschreiben: p &lt;- p + theme(legend.position=&quot;none&quot;) Mit geom_smooth() kann ggplot eine Trendlinie auf der Baiss von Punktdaten berechnen. Die zugrunde liegende statistische Methode kann selbst gewählt werden. Wenn nichts angegeben wird verwendet ggplot bei weniger als 1’000 Messungen, die Methode loess (local smooths). p &lt;- p + geom_smooth(colour = &quot;black&quot;) p 4.2.1 Quellen Dieses Kapitel verwendet folgende Libraries: Spinu, Grolemund, and Wickham (2018), Wickham (2018a), Wickham (2018c), Wickham, François, et al. (2018), Henry and Wickham (2018), Wickham, Hester, and Francois (2017), Wickham and Henry (2018), Müller and Wickham (2018), Wickham, Chang, et al. (2018), Wickham (2017) References "],
["4-3-ubung.html", "4.3 Übung", " 4.3 Übung In dieser Übung geht es darum, die Grafiken aus dem Blog-post von Marko Kovic (blog.tagesanzeiger.ch) zu rekonstruieren. Freundlicherweise hat Herr Kovic meist die ggplot2 Standardeinstellungen benutzt, was die Rekonstruktion relativ einfach macht. Die Links im Text verweisen auf die Originalgrafik, die eingebetteten Plots sind meine eigenen Rekonstruktionen. Importiere als erstes den Datensatz initiative_masseneinwanderung_kanton.csv (auf der Blog-Seite erhältlich). 4.3.1 Aufgabe 1 Rekonstruiere Grafik 1 von Kovic. Erstelle dazu einen Scatterplot wo der Ausländeranteil der Kantone dem Ja-Anteil gegenüber gestellt wird. Speichere den Plot einer Variabel plot1. nutze coord_fixed() um die beiden Achsen in ein fixes Verhältnis zu setzen (1:1). setze die Achsen Start- und Endwerte mittels lims() oder scale_y_continuousbzw. scale_x_continuous. Optional: Setze analog Kovic die breaks (0.0, 0.1…0.7) manuell Rekonstruktion: 4.3.2 Aufgabe 2 Rekonstruiere Grafik 2. Erweitere dazu plot1 mit einer Trendlinie. 4.3.3 Aufgabe 3 Importiere die Gemeindedaten initiative_masseneinwanderung_gemeinde.csv: gemeinde &lt;- read_delim(&quot;11_InfoVis1/data/initiative_masseneinwanderung_gemeinde.csv&quot;,&quot;,&quot;,locale = locale(encoding = &quot;UTF-8&quot;)) Rekonstruiere Grafik 3. Stelle dazu den Ausländeranteil aller Gemeinden dem Ja-Stimmen-Anteil gegenüber. Speichere den Plot als plot2 4.3.4 Aufgabe 4 Rekonstruiere Grafik 4 indem plot2 mit einer Trendlinie erweitert wird. 4.3.5 Aufgabe 5 Rekonstruiere Grafik 5 indem plot2 mit facetting erweitert wird. Die Facets sollen die einzelnen Kantone sein. Speichere den Plot als plot3. 4.3.6 Aufgabe 6 Rekonstruiere Grafik 6 indem plot3 mit einer Trendlinie erweitert wird. Rekonstruktion: 4.3.7 Aufgabe 7 Rekonstruiere Grafik 7 indem plot2mit facetting erweitert wird. Die Facets sollen nun den Grössen-Quantilen entsprechen. Speichere den Plot unter plot4. Rekonstruktion: 4.3.8 Aufgabe 8 Rekonstruiere Grafik 8 indem plot4 mit einer Trendlinie ausgestattet wird. 4.3.9 Aufgabe 9 (Fortgeschritten) Rekonstruiere die Korrelationstabelle. Tipp: - Nutze group_by() und summarise() - Nutze cor.test() um den Korrelationskoeffizienten sowie den p-Wert zu erhalten. - Mit $estimate und $p.value können die entsprechenden Werte direkt angesprochen werden Hinweis: aus bisher unerklärlichen Gründen weiche gewisse meiner Werte leicht von den Berechnungen des Herrn Kovics ab. Kanton Korr.Koeffizient Signifikanz AG -0.2362552 *** AI -0.7828022 - AR -0.0892817 - BE -0.4422003 *** BL -0.2919712 ** BS -0.9935385 - FR -0.4217634 *** GE 0.3753004 * GL -0.4070120 - GR -0.0426607 - JU -0.2252540 - LU -0.3048455 ** NE -0.5214180 *** NW -0.2018174 - OW -0.4813090 - SG -0.2449093 * SH -0.2995527 - SO -0.0533442 - SZ -0.7259276 *** TG -0.5522862 *** TI 0.1512509 - UR -0.3848167 - VD -0.2685301 *** VS -0.1736954 * ZG 0.0407166 - ZH -0.2744683 *** "],
["4-4-losung.html", "4.4 Lösung", " 4.4 Lösung RCode als Download library(tidyverse) library(ggplot2) library(stringr) ## # Es kann sein, dass man die Codierung des Files spezifizieren muss. Mit `readr::read_delim()` ## # läuft dies mit der Option locale = locale(encoding = &quot;UTF-8&quot;) wobei anstelle von UTF-8 die ## # entsprechende Codierung angegeben wird. ## # Tipp: Excel speichert CSV oft in ANSI, welches für den Import in R nicht sonderlich geeignet ## # ist. Falls Probleme auftreten muss das File mittels einer geeigneter Software (Widows: &quot;Editor&quot; ## # oder &quot;Notepad++&quot;, Mac: &quot;TextEdit&quot;) und mit einer neuen Codierung (z.B. `UTF-8`) abgespeichert ## # werden. kanton &lt;- read_delim(&quot;11_InfoVis1/data/initiative_masseneinwanderung_kanton.csv&quot;,&quot;,&quot;,locale = locale(encoding = &quot;UTF-8&quot;)) # Lösung zu Aufgabe 1 # da die Spalten in Kovic&#39;s Daten Umlaute und Sonderzeichen enthalten, müssen diese in R mit Graviszeichen # angesprochen werden. Dieses Zeichen wirder Schweizer Tastatur [1] mit # Shitft + Gravis (Links von der Backspace taste) + Leerschlag erstellt # [1] https://de.wikipedia.org/wiki/Tastaturbelegung#Schweiz # Alternativ können die Spalten im Originalfile oder mit dplyr::rename() umbenannt werden plot1 &lt;- ggplot(kanton, aes(`Ausländeranteil`, `Ja-Anteil`)) + geom_point() + coord_fixed(1) + scale_y_continuous(breaks = c(0,0.1,0.3,0.5,0.7),limits = c(0,0.7)) + scale_x_continuous(breaks = c(0,0.1,0.3,0.5,0.7),limits = c(0,0.7)) + labs(y = &quot;Anteil Ja-Stimmen&quot;) plot1 # Lösung zu Aufgabe 2 plot1 + geom_smooth() gemeinde &lt;- read_delim(&quot;11_InfoVis1/data/initiative_masseneinwanderung_gemeinde.csv&quot;,&quot;,&quot;,locale = locale(encoding = &quot;UTF-8&quot;)) # Lösung zu Aufgabe 3 plot2 &lt;- ggplot(gemeinde, aes(`Anteil Ausl`, `Anteil Ja`)) + geom_point() + labs(x = &quot;Ausländeranteil&quot;,y = &quot;Anteil Ja-Stimmen&quot;) + coord_fixed(1) + lims(x = c(0,1), y = c(0,1)) plot2 # Lösung zu Aufgabe 4 plot2 + geom_smooth() # Lösung zu Aufgabe 5 plot3 &lt;- plot2 + facet_wrap(~Kanton) plot3 # Lösung zu Aufgabe 6 plot3 + geom_smooth() # Lösung zu Aufgabe 7 plot4 &lt;- plot2 + facet_wrap(~Quantile) plot4 # Lösung zu Aufgabe 8 plot4 + geom_smooth() # Lösung zu Aufgabe 9 korr_tab &lt;- gemeinde %&gt;% group_by(Kanton) %&gt;% summarise( Korr.Koeffizient = cor.test(`Anteil Ja`,`Anteil Ausl`,method = &quot;pearson&quot;)$estimate, Signifikanz_val = cor.test(`Anteil Ja`,`Anteil Ausl`,method = &quot;pearson&quot;)$p.value, Signifikanz = ifelse(Signifikanz_val &lt; 0.001,&quot;***&quot;,ifelse(Signifikanz_val&lt;0.01,&quot;**&quot;,ifelse(Signifikanz_val&lt;0.05,&quot;*&quot;,&quot;-&quot;))) ) %&gt;% select(-Signifikanz_val) "],
["5-infovis2-22-10-2019.html", "Kapitel 5 InfoVis2 (22.10.2019)", " Kapitel 5 InfoVis2 (22.10.2019) Die Informationsvisualisierung ist eine vielseitige, effektive und effiziente Methode für die explorative Datenanalyse. Während Scatterplots und Histogramme weitherum bekannt sind, bieten weniger bekannte Informationsvisualisierungs-Typen wie etwa Parallelkoordinatenplots, TreeMaps oder Chorddiagramme originelle alternative Darstellungsformen zur visuellen Analyse von Datensätze, welche stets grösser und komplexer werden. Die Studierenden lernen in dieser Lerneinheit eine Reihe von Informationsvisualisierungstypen kennen, lernen diese zielführend zu gestalten und selber zu erstellen. "],
["5-1-ubung-a-2.html", "5.1 Übung A", " 5.1 Übung A library(tidyverse) library(lubridate) Laden den wetter-Datensatz, bereinige ihn wenn nötig (NA-Werte entfernen) und importiere auch den Datensatz order_52252_legend.csv und verbinde die Datensätze mit einem join via dem Stationskürzel. wetter &lt;- read_table(&quot;09_PrePro1/data/order_52252_data.txt&quot;, col_types = list( col_character(), col_datetime(format = &quot;%Y%m%d%H&quot;), col_double() ) ) wetter &lt;- wetter %&gt;% filter(!is.na(stn)) %&gt;% filter(!is.na(time)) station_meta &lt;- read_delim(&quot;09_PrePro1/data/order_52252_legend.csv&quot;,&quot;;&quot;) wetter &lt;- left_join(wetter,station_meta,by = &quot;stn&quot;) 5.1.1 Aufgabe 1 Erstelle zwei Hilfsspalten (convenience variables) “Jahr” und “Monat”. Filtere auf ein beliebiges Jahr und zwei beliebige Monate. Speichere den gefilterten Datensatz in einer neuen Variablen ab. Verwende diesen Datensatz für alle folgenden Übungen. 5.1.2 Aufgabe 2 Erstelle ein Scatterplot (time vs. tre200h0) wobei die Punkte aufgrund ihrer Meereshöhe eingefärbt werden sollen. Tiefe Werte sollen dabei blau eingefärbt werden und hohe Werte rot. Verkleinere die Punkte um übermässiges Überplotten der Punkten zu vermeiden. Weiter sollen im Abstand von zwei Wochen die Kalenderwochen auf der Achse erscheinen. Speichere den Plot in einer Variabel p ab. 5.1.3 Aufgabe 3 Füge am obigen Plot (gespeichert als Variabel p) eine schwarze, gestrichelte Trendlinie hinzu und aktualisiere p (p &lt;- p + ...). 5.1.4 Aufgabe 4 Positioniere die Legende oberhalb des Plots und lege sie quer (nutze dazu theme() mit legend.direction und legend.position). Speichere diese Änderungen in p. 5.1.5 Aufgabe 5 (für ambitionierte) Füge den Temperaturwerten auf der y-Ache ein °C hinzu (siehe unten und studiere diesen Tipp zur Hilfe). Aktualisiere p an dieser Stelle noch nicht. 5.1.6 Aufgabe 6 (für noch ambitioniertere) Füge dem Plot eine zweite, korrekt ausgerichtete Achse mit Kelvin oder Farenheit hinzu (siehe sec_axis). Wenn du es vorherigen Übung schon geschafft hast, setze auch hier die Einheit (K rep. °F) hinter die Werte auf der Achse. \\[ K = °C + 273,15\\] \\[°F = °C × \\frac{9}{5} + 32\\] 5.1.7 Aufgabe 7 Jetzt verlassen wir den scatterplot und machen einen Boxplot mit den Temperaturdaten. Färbe die Boxplots wieder in Abhängigkeit der Meereshöhe ein. Beachte den Unterschied zwischen colour = und fill = Beachte den Unterschied zwischen facet_wrap() und facet_grid() facet_grid() braucht übrigens noch einen Punkt (.) zur Tilde (~). Beachte den Unterschied zwischen “.~” und “~.” bei facet_grid() verschiebe nach Bedarf die Legende 5.1.8 Aufgabe 8 Teile die Stationen in verschiedene Höhenlagen ein (Tieflage [&lt; 450 m], Mittellage [450 - 1000 m] und Hochlage [&gt; 1’000 m]). Vergleiche die Verteilung der Temperaturwerte in den verschiedenen Lagen. Nutze dazu facet_grid um die Höhenlage dem Monat gegenüber zu stellen (Monat~Lage) Passe scales = an damit keine leeren Stellen auf der x-Achse entstehen Optional: Verwende den vollen Stationsnamen anstelle des Kürzels und drehe diese ab damit sie sich gegenseitig nicht überschreiben 5.1.9 Aufgabe 9 Als letzter wichtiger Plottyp noch zwei Übungen zum Histogramm. Erstelle ein Histogramm geom_histogram() mit den Temperaturwerten. Färbe Säulen aufgrund ihrer Höhenlage ein und die Begrenzungslinie weiss. Setze die Klassenbreite auf 1 Grad. 5.1.10 Aufgabe 10 Erstelle facets aufgrund der Höhenlage. Setze noch eine Vertikale linie beim Nullpunkt und stelle den x-Achsenabschnit symmetrisch ein (z.B -30 bis + 30°C). "],
["5-2-ubung-a-losung-2.html", "5.2 Übung A: Lösung", " 5.2 Übung A: Lösung RCode als Download library(tidyverse) library(lubridate) wetter &lt;- read_table(&quot;09_PrePro1/data/order_52252_data.txt&quot;, col_types = list( col_character(), col_datetime(format = &quot;%Y%m%d%H&quot;), col_double() ) ) wetter &lt;- wetter %&gt;% filter(!is.na(stn)) %&gt;% filter(!is.na(time)) station_meta &lt;- read_delim(&quot;09_PrePro1/data/order_52252_legend.csv&quot;,&quot;;&quot;) wetter &lt;- left_join(wetter,station_meta,by = &quot;stn&quot;) # Lösung Aufgabe 1 wetter_fil &lt;- wetter %&gt;% mutate( year = year(time), month = month(time) ) %&gt;% filter(year == 2000 &amp; month &lt; 3) # Lösung Aufgabe 2 p &lt;- ggplot(wetter_fil, aes(time,tre200h0, colour = Meereshoehe)) + geom_point(size = 0.5) + labs(x = &quot;Kalenderwoche&quot;, y = &quot;Temperatur in ° Celsius&quot;) + scale_color_continuous(low = &quot;blue&quot;, high = &quot;red&quot;) + scale_x_datetime(date_breaks = &quot;2 week&quot;, date_labels = &quot;KW%W&quot;) p # Lösung Aufgabe 3 p &lt;- p + stat_smooth(colour = &quot;black&quot;,lty = 2) p # Lösung Aufgabe 4 p &lt;- p + theme(legend.direction = &quot;horizontal&quot;,legend.position = &quot;top&quot;) p # Lösung Aufgabe 5 p + scale_y_continuous(labels = function(x)paste0(x,&quot;°C&quot;)) + labs(x = &quot;Kalenderwoche&quot;, y = &quot;Temperatur&quot;) # Lösung Aufgabe 6 p &lt;- p + labs(x = &quot;Kalenderwoche&quot;, y = &quot;Temperatur&quot;) + scale_y_continuous(labels = function(x)paste0(x,&quot;°C&quot;),sec.axis = sec_axis(~.*(9/5)+32,name = &quot;Temperatur&quot;,labels = function(x)paste0(x,&quot;° F&quot;))) p # Lösung Aufgabe 7 wetter_fil &lt;- mutate(wetter_fil,monat = month(time,label = T,abbr = F)) ggplot(wetter_fil, aes(stn,tre200h0, fill = Meereshoehe)) + geom_boxplot() + facet_grid(monat~.) + labs(x = &quot;Station&quot;, y = &quot;Temperatur&quot;) + theme(legend.direction = &quot;horizontal&quot;,legend.position = &quot;top&quot;) # Lösung Aufgabe 8 wetter_fil$Lage[wetter_fil$Meereshoehe &lt; 450] &lt;- &quot;Tieflage&quot; wetter_fil$Lage[wetter_fil$Meereshoehe &gt;= 450 &amp; wetter_fil$Meereshoehe &lt;1000] &lt;- &quot;Mittellage&quot; wetter_fil$Lage[wetter_fil$Meereshoehe &gt;= 1000] &lt;- &quot;Hochlage&quot; ggplot(wetter_fil, aes(Name,tre200h0)) + geom_boxplot() + facet_grid(monat~Lage, scales = &quot;free_x&quot;) + labs(x = &quot;Lage&quot;, y = &quot;Temperatur&quot;) + theme(axis.text.x = element_text(angle = 45,hjust = 1)) # Lösung Aufgabe 9 h &lt;- ggplot(wetter_fil,aes(tre200h0, fill = Lage)) + geom_histogram(binwidth = 1, colour = &quot;white&quot;) + labs(x = &quot;Temperatur in °C&quot;, y = &quot;Anzahl&quot;) h # Lösung Aufgabe 10 h + geom_vline(xintercept = 0, lty = 2, alpha = 0.5) + facet_wrap(~Lage) + lims(x = c(-30,30)) + theme(legend.position = &quot;none&quot;) "],
["5-3-ubung-b-2.html", "5.3 Übung B", " 5.3 Übung B In dieser Übung bauen wir einige etwas unübliche Plots aus der Vorlesung nach. Dafür verwenden wir Datensätze, die in R bereits integriert sind. Eine Liste dieser Datensätze findet man hier oder mit der Hilfe ?datasets. Dazu verwenden wir vor allem das Package plotly welches im Gegensatz zu ggplot2 ein paar zusätzliche Plot-Typen kennt und zudem noch interaktiv ist. Leider scheinen gewisse Browsers (z.B. Firefox) sowie der Viewer Pane mit plotly Mühe zu haben. Deshalb empfehlen wir folgendes: Übungsunterlagen für InfoVis2 in Chrome zu öffnen Falls ihr auf dem RStudio Server arbeitet: hier ebenfalls in Chrome arbeiten Falls ihr lokal mit RStudio arbeitet: Mit der Option options(viewer=NULL) werden Plots mit dem Standart Browser. 5.3.1 Aufgabe 1: Parallel coordinate plots Erstelle einen parallel coordinate plot. Dafür eignet sich der integrierte Datensatz mtcars: mpg cyl disp hp drat wt qsec vs am gear carb Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 # Nur nötig, wenn ihr mit einer lokalen Installation von RStudio arbeitet # (also nicht auf dem Server). options(viewer=NULL) Parallel Coordinates lassen sich mit nativem ggplot2 nicht herstellen. Es braucht dazu entweder Erweiterungen oder “standalone” Tools. Als “standalone” Tool kann ich plotly stark empfehlen. Plotly verfügt zwar über eine etwas eigenwillige Syntax, bietet dafür über sehr vielseitige zusätzliche Möglichkeiten. Vor allem aber sind sämtliche plotly Grafiken webbasiert und interaktiv. Hier findet ihr eine Anleitung zur Herstellung eines Parallel Coordinates Plot mit plotly: https://plot.ly/r/parallel-coordinates-plot/ So sieht der fertige Plot aus: 5.3.2 Aufgabe 2: Polar Plot mit Biber Daten Polar Plots (welche man ebenfalls mit Plotly erstellen kann) eignen sich unter anderem für Daten, die zyklischer Natur sind, wie zum Beispiel zeitlich geprägte Daten (Tages-, Wochen-, oder Jahresrhythmen). Aus den Beispiels-Datensätzen habe ich zwei Datensätze gefunden, die zeitlich geprägt sind: beaver1 und beaver2 AirPassenger Beide Datensätze müssen noch etwas umgeformt werden, bevor wir sie für einen Radialplot verwenden können. In Aufgabe 2 verwenden wir die Biber-Datensätze, in der nächsten Aufgabe (3) die Passagier-Daten. Wenn wir die Daten von beiden Bibern verwenden wollen, müssen wir diese noch zusammenfügen: beaver1_new &lt;- beaver1 %&gt;% mutate(beaver = &quot;nr1&quot;) beaver2_new &lt;- beaver2 %&gt;% mutate(beaver = &quot;nr2&quot;) beaver_new &lt;- rbind(beaver1_new,beaver2_new) Zudem müssen wir die Zeitangabe noch anpassen: Gemäss der Datenbeschreibung handelt es sich bei der Zeitangabe um ein sehr programmier-unfreundliches Format. 3:30 wird als “0330” notiert. Wir müssen diese Zeitangabe, noch in ein Dezimalsystem umwandeln: beaver_new &lt;- beaver_new %&gt;% mutate( hour_dec = (time/100)%/%1, # Ganze Stunden (mittels ganzzaliger Division) min_dec = (time/100)%%1/0.6, # Dezimalminuten (15 min wird zu 0.25, via Modulo) hour_min_dec = hour_dec+min_dec # Dezimal-Zeitangabe (03:30 wird zu 3.5) ) Der Datensatz: day time temp activ beaver hour_dec min_dec hour_min_dec 346 840 36.33 0 nr1 8 0.6666667 8.666667 346 850 36.34 0 nr1 8 0.8333333 8.833333 346 900 36.35 0 nr1 9 0.0000000 9.000000 346 910 36.42 0 nr1 9 0.1666667 9.166667 346 920 36.55 0 nr1 9 0.3333333 9.333333 346 930 36.69 0 nr1 9 0.5000000 9.500000 So sieht der fertige Plot aus. Rekonstruiere dies mit plotly: 5.3.3 Aufgabe 3: Polar Plot mit Passagier-Daten Analog Aufgabe 2, dieses Mal mit dem Datensatz AirPassanger AirPassengers kommt in einem Format daher, das ich selbst noch gar nicht kannte. Es sieht zwar aus wie ein data.frame oder eine matrix, ist aber von der Klasse ts. AirPassengers ## Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec ## 1949 112 118 132 129 121 135 148 148 136 119 104 118 ## 1950 115 126 141 135 125 149 170 170 158 133 114 140 ## 1951 145 150 178 163 172 178 199 199 184 162 146 166 ## 1952 171 180 193 181 183 218 230 242 209 191 172 194 ## 1953 196 196 236 235 229 243 264 272 237 211 180 201 ## 1954 204 188 235 227 234 264 302 293 259 229 203 229 ## 1955 242 233 267 269 270 315 364 347 312 274 237 278 ## 1956 284 277 317 313 318 374 413 405 355 306 271 306 ## 1957 315 301 356 348 355 422 465 467 404 347 305 336 ## 1958 340 318 362 348 363 435 491 505 404 359 310 337 ## 1959 360 342 406 396 420 472 548 559 463 407 362 405 ## 1960 417 391 419 461 472 535 622 606 508 461 390 432 class(AirPassengers) ## [1] &quot;ts&quot; Damit wir den Datensatz verwenden können, müssen wir ihn zuerst in eine matrix umwandeln. Wie das geht habe ich hier erfahren. AirPassengers2 &lt;- tapply(AirPassengers, list(year = floor(time(AirPassengers)), month = month.abb[cycle(AirPassengers)]), c) Aus der matrix muss noch ein Dataframe her, zudem müssen wir aus der breiten Tabelle eine lange Tabelle machen: AirPassengers3 &lt;- AirPassengers2 %&gt;% as.data.frame() %&gt;% rownames_to_column(&quot;year&quot;) %&gt;% gather(month,n,-year) %&gt;% mutate( # ich nutze einen billigen Trick um ausgeschriebene Monate in Nummern umzuwandeln [1] month = factor(month, levels = month.abb,ordered = T), month_numb = as.integer(month), year = factor(year, ordered = T) ) # [1] beachtet an dieser Stelle das Verhalten von as.integer() wenn es sich um factors() handelt. Hier wird das Verhalten genutzt, andersweitig kann es einem zum Verhngnis werden. Das Verhalten wir auch hier verdeutlicht: # as.integer(as.character(&quot;500&quot;)) # as.integer(as.factor(&quot;500&quot;)) Hier der fertige Plot. Rekonstruiere dies mit plotly: 5.3.4 Aufgabe 4: 3D Scatterplot Erstelle einen 3D Scatterplot, ebenfalls mit plotly. Nutze dazu den Datensatz trees. Ein Beispiel für einen 3D Scatterplot findet ihr hier. "],
["5-4-ubung-b-losung-2.html", "5.4 Übung B: Lösung", " 5.4 Übung B: Lösung RCode als Download library(tidyverse) library(plotly) library(pander) library(webshot) ## ## # Nur nötig, wenn ihr mit einer lokalen Installation von RStudio arbeitet ## # (also nicht auf dem Server). ## options(viewer=NULL) ## # Lösung Aufgabe 1 p &lt;- mtcars %&gt;% plot_ly(type = &#39;parcoords&#39;, line = list(color = ~mpg, colorscale = list(c(0,&#39;red&#39;),c(1,&#39;blue&#39;))), dimensions = list( list(label = &#39;mpg&#39;, values = ~mpg), list(label = &#39;disp&#39;, values = ~disp), list(label = &#39;hp&#39;, values = ~hp), list(label = &#39;drat&#39;, values = ~drat), list(label = &#39;wt&#39;, values = ~wt), list(label = &#39;qsec&#39;, values = ~qsec), list(label = &#39;vs&#39;, values = ~vs), list(label = &#39;am&#39;, values = ~am), list(label = &#39;gear&#39;, values = ~gear), list(label = &#39;carb&#39;, values = ~carb) ) ) beaver1_new &lt;- beaver1 %&gt;% mutate(beaver = &quot;nr1&quot;) beaver2_new &lt;- beaver2 %&gt;% mutate(beaver = &quot;nr2&quot;) beaver_new &lt;- rbind(beaver1_new,beaver2_new) beaver_new &lt;- beaver_new %&gt;% mutate( hour_dec = (time/100)%/%1, # Ganze Stunden (mittels ganzzaliger Division) min_dec = (time/100)%%1/0.6, # Dezimalminuten (15 min wird zu 0.25, via Modulo) hour_min_dec = hour_dec+min_dec # Dezimal-Zeitangabe (03:30 wird zu 3.5) ) # Lösung Aufgabe 2 p &lt;- beaver_new %&gt;% plot_ly(r = ~temp, t = ~hour_min_dec, color = ~beaver,mode = &quot;lines&quot;, type = &quot;scatter&quot;) %&gt;% layout( radialaxis = list(range = c(35,39)), angularaxis = list(range = c(0,24)), orientation = 270, showlegend = F ) AirPassengers class(AirPassengers) AirPassengers2 &lt;- tapply(AirPassengers, list(year = floor(time(AirPassengers)), month = month.abb[cycle(AirPassengers)]), c) AirPassengers3 &lt;- AirPassengers2 %&gt;% as.data.frame() %&gt;% rownames_to_column(&quot;year&quot;) %&gt;% gather(month,n,-year) %&gt;% mutate( # ich nutze einen billigen Trick um ausgeschriebene Monate in Nummern umzuwandeln [1] month = factor(month, levels = month.abb,ordered = T), month_numb = as.integer(month), year = factor(year, ordered = T) ) # [1] beachtet an dieser Stelle das Verhalten von as.integer() wenn es sich um factors() handelt. Hier wird das Verhalten genutzt, andersweitig kann es einem zum Verhngnis werden. Das Verhalten wir auch hier verdeutlicht: # as.integer(as.character(&quot;500&quot;)) # as.integer(as.factor(&quot;500&quot;)) # Lösung Aufgabe 3 p &lt;- AirPassengers3 %&gt;% plot_ly(r = ~n, t = ~month_numb, color = ~year, mode = &quot;markers&quot;, type = &quot;scatter&quot;) %&gt;% layout( showlegend = T, angularaxis = list(range = c(0,12)), orientation = 270, legend = list(traceorder = &quot;reversed&quot;) ) # Lösung Aufgabe 4 p &lt;- trees %&gt;% plot_ly(x = ~Girth, y = ~Height, z = ~Volume) "],
["6-statistik-1-28-10-2019.html", "Kapitel 6 Statistik 1 (28.10.2019)", " Kapitel 6 Statistik 1 (28.10.2019) In Statistik 1 lernen die Studierenden, was (Inferenz-) Statistik im Kern leistet und warum sie für wissenschaftliche Erkenntnis (in den meisten Disziplinen) unentbehrlich ist. Nach einer Wiederholung der Rolle von Hypothesen wird erläutert, wie Hypothesentests in der frequentist-Statistik umgesetzt werden, einschliesslich p-Werten und Signifikanz-Levels. Die praktische Statistik beginnt mit den beiden einfachsten Fällen, dem Chi-Quadrat-Test für die Assoziation zwischen zwei kategorialen Variablen und dem t-Test auf Unterschiede in Mittelwerten zwischen zwei Gruppen. Abschliessend beschäftigen wir uns damit, wie man Ergebnisse statistischer Analysen am besten in Abbildungen, Tabellen und Text darstellt. "],
["6-1-demo-stastische-tests.html", "6.1 Demo: Stastische Tests", " 6.1 Demo: Stastische Tests Demoscript als Download 6.1.1 Chi-Quadrat-Test &amp; Fishers Test qchisq(0.95,1) ## [1] 3.841459 count&lt;-matrix(c(38,14,11,51),nrow=2) count ## [,1] [,2] ## [1,] 38 11 ## [2,] 14 51 chisq.test(count) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: count ## X-squared = 33.112, df = 1, p-value = 8.7e-09 fisher.test(count) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: count ## p-value = 2.099e-09 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 4.746351 34.118920 ## sample estimates: ## odds ratio ## 12.22697 6.1.2 t-Test a&lt;-c(20,19,25,10,8,15,13,18,11,14) b&lt;-c(12,15,16,7,8,10,12,11,13,10) blume&lt;-data.frame(a,b) blume ## a b ## 1 20 12 ## 2 19 15 ## 3 25 16 ## 4 10 7 ## 5 8 8 ## 6 15 10 ## 7 13 12 ## 8 18 11 ## 9 11 13 ## 10 14 10 summary(blume) ## a b ## Min. : 8.00 Min. : 7.00 ## 1st Qu.:11.50 1st Qu.:10.00 ## Median :14.50 Median :11.50 ## Mean :15.30 Mean :11.40 ## 3rd Qu.:18.75 3rd Qu.:12.75 ## Max. :25.00 Max. :16.00 boxplot(blume$a,blume$b) boxplot(blume) hist(blume$a) hist(blume$b) t.test(blume$a,blume$b) #zweiseitig ## ## Welch Two Sample t-test ## ## data: blume$a and blume$b ## t = 2.0797, df = 13.907, p-value = 0.05654 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.1245926 7.9245926 ## sample estimates: ## mean of x mean of y ## 15.3 11.4 t.test(blume$a,blume$b, alternative=&quot;greater&quot;) #einseitig ## ## Welch Two Sample t-test ## ## data: blume$a and blume$b ## t = 2.0797, df = 13.907, p-value = 0.02827 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## 0.5954947 Inf ## sample estimates: ## mean of x mean of y ## 15.3 11.4 t.test(blume$a,blume$b, alternative=&quot;less&quot;) #einseitig ## ## Welch Two Sample t-test ## ## data: blume$a and blume$b ## t = 2.0797, df = 13.907, p-value = 0.9717 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf 7.204505 ## sample estimates: ## mean of x mean of y ## 15.3 11.4 t.test(blume$a,blume$b, var.equal=T) #Varianzen gleich, klassischer t-Test ## ## Two Sample t-test ## ## data: blume$a and blume$b ## t = 2.0797, df = 18, p-value = 0.05212 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.03981237 7.83981237 ## sample estimates: ## mean of x mean of y ## 15.3 11.4 t.test(blume$a,blume$b, var.equal=F) #Varianzen ungleich, Welch&#39;s t-Test, ist auch default, d.h. wenn var.equal nicht # definiert wird, wird ein Welch&#39;s t-Test ausgeführt. ## ## Welch Two Sample t-test ## ## data: blume$a and blume$b ## t = 2.0797, df = 13.907, p-value = 0.05654 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.1245926 7.9245926 ## sample estimates: ## mean of x mean of y ## 15.3 11.4 t.test(blume$a,blume$b, paired=T) #gepaarter t-Test ## ## Paired t-test ## ## data: blume$a and blume$b ## t = 3.4821, df = 9, p-value = 0.006916 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 1.366339 6.433661 ## sample estimates: ## mean of the differences ## 3.9 t.test(blume$a,blume$b, paired=T,alternative=&quot;greater&quot;) #gepaarter t-Test ## ## Paired t-test ## ## data: blume$a and blume$b ## t = 3.4821, df = 9, p-value = 0.003458 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## 1.846877 Inf ## sample estimates: ## mean of the differences ## 3.9 shapiro.test(blume$b) ## ## Shapiro-Wilk normality test ## ## data: blume$b ## W = 0.97341, p-value = 0.9206 var.test(blume$a,blume$b) ## ## F test to compare two variances ## ## data: blume$a and blume$b ## F = 3.3715, num df = 9, denom df = 9, p-value = 0.08467 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.8374446 13.5738284 ## sample estimates: ## ratio of variances ## 3.371547 if(!require(car)){install.packages(&quot;car&quot;)} # installiert das Zusatzpacket car (wenn nicht bereits installiert) library(car) leveneTest(blume$a,blume$b,center=mean) ## Levene&#39;s Test for Homogeneity of Variance (center = mean) ## Df F value Pr(&gt;F) ## group 7 2.2598e+30 &lt; 2.2e-16 *** ## 2 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 wilcox.test(blume$a,blume$b) ## ## Wilcoxon rank sum test with continuity correction ## ## data: blume$a and blume$b ## W = 73, p-value = 0.08789 ## alternative hypothesis: true location shift is not equal to 0 Das gleiche mit einem “long table” cultivar&lt;-c(rep(&quot;a&quot;,10),rep(&quot;b&quot;,10)) size&lt;-c(a,b) blume.long&lt;-data.frame(cultivar,size) rm(size) #Befehl rm entfernt die nicht mehr benötitgten Objekte aus dem Workspace rm(cultivar) rm(size) #Befehl rm entfernt die nicht mehr benötitgten Objekte aus dem Workspace rm(cultivar) #Das gleiche in einer Zeile blume.long&lt;-data.frame(cultivar=c(rep(&quot;a&quot;,10),rep(&quot;b&quot;,10)),size=c(a,b)) summary(blume.long) ## cultivar size ## a:10 Min. : 7.00 ## b:10 1st Qu.:10.00 ## Median :12.50 ## Mean :13.35 ## 3rd Qu.:15.25 ## Max. :25.00 head(blume.long) ## cultivar size ## 1 a 20 ## 2 a 19 ## 3 a 25 ## 4 a 10 ## 5 a 8 ## 6 a 15 boxplot(size~cultivar, data=blume.long) boxplot(size~cultivar, data=blume.long) t.test(size~cultivar, blume.long, var.equal=T) ## ## Two Sample t-test ## ## data: size by cultivar ## t = 2.0797, df = 18, p-value = 0.05212 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.03981237 7.83981237 ## sample estimates: ## mean in group a mean in group b ## 15.3 11.4 t.test(size~cultivar, blume.long, var.equal=F) ## ## Welch Two Sample t-test ## ## data: size by cultivar ## t = 2.0797, df = 13.907, p-value = 0.05654 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.1245926 7.9245926 ## sample estimates: ## mean in group a mean in group b ## 15.3 11.4 6.1.3 Base R vs. ggplot2 library(tidyverse) ggplot(blume.long, aes(cultivar,size)) + geom_boxplot() ggplot(blume.long, aes(cultivar,size)) + geom_boxplot()+theme_classic() ggplot(blume.long, aes(cultivar,size)) + geom_boxplot(size=1) + theme_classic()+ theme(axis.line = element_line(size=1))+theme(axis.title = element_text(size=14))+ theme(axis.text = element_text(size=14)) ggplot(blume.long, aes(cultivar,size)) + geom_boxplot(size=1) + theme_classic()+ theme(axis.line = element_line(size=1), axis.ticks = element_line(size=1), axis.text = element_text(size = 20), axis.title = element_text(size = 20)) Definieren von mytheme mit allen gewünschten Settings, das man zu Beginn einer Sitzung einmal laden und dann immer wieder ausführen kann (statt des langen Codes) mytheme &lt;- theme_classic() + theme(axis.line = element_line(color = &quot;black&quot;, size=1), axis.text = element_text(size = 20, color = &quot;black&quot;), axis.title = element_text(size = 20, color = &quot;black&quot;), axis.ticks = element_line(size = 1, color = &quot;black&quot;), axis.ticks.length = unit(.5, &quot;cm&quot;)) ggplot(blume.long, aes(cultivar,size)) + geom_boxplot(size=1) + mytheme t_test &lt;- t.test(size~cultivar, blume.long) ggplot(blume.long, aes(cultivar,size)) + geom_boxplot(size=1) + mytheme + annotate(&quot;text&quot;, x = &quot;b&quot;, y = 24, label = paste0(&quot;italic(p) == &quot;, round(t_test$p.value, 3)), parse = TRUE, size = 8) ggplot (blume.long, aes(cultivar,size)) + geom_boxplot(size=1) + mytheme + labs(x=&quot;Cultivar&quot;,y=&quot;Size (cm)&quot;) "],
["6-2-beschreibung-forschungsprojekt-novanimal-nfp69.html", "6.2 Beschreibung Forschungsprojekt NOVANIMAL (NFP69)", " 6.2 Beschreibung Forschungsprojekt NOVANIMAL (NFP69) Im Forschungsprojekt NOVANIMAL wird u.a. der Frage nachgegangen, was es braucht, damit Menschen freiwillig weniger tierische Produkte konsumieren? Ein interessanter Ansatzpunkt ist die Ausser-Haus-Verpflegung. Gemäss der ersten in den Jahren 2014/2015 durchgeführten nationalen Ernährungserhebung menuCH essen 70 % der Bevölkerung zwischen 18 und 75 Jahren am Mittag auswärts (Bochud et al. 2017). Daher rückt die Gastronomie als zentraler Akteur einer innovativen und nachhaltigen Ernährungswirtschaft ins Blickfeld. Welche Innovationen in der Gastronomie könnten dazu beitragen, den Pro-Kopf-Verbrauch an tierischen Nahrungsmitteln zu senken? Dazu wurde u.a. ein Experiment in zwei Hochschulmensen durchgeführt. Forschungsleitend war die Frage, wie die Gäste dazu bewogen werden können, häufiger vegetarische oder vegane Gerichte zu wählen. Konkret wurde untersucht, wie die Gäste auf ein verändertes Menü-Angebot mit einem höheren Anteil an vegetarischen und veganen Gerichten reagieren. Das Experiment fand während 12 Wochen statt und bestand aus zwei Mensazyklen à 6 Wochen. Über den gesamten Untersuchungszeitraum werden insgesamt 90 verschiedene Gerichte angeboten. In den 6 Referenz- bzw. Basiswochen wurden zwei fleisch- oder fischhaltige Menüs und ein vegetarisches Menü angeboten. In den 6 Interventionswochen wurde das Verhältnis umgekehrt und es wurden ein veganes, ein vegetarisches und ein fleisch- oder fischhaltiges Gericht angeboten. Basis- und Interventionsangebote wechselten wöchentlich ab. Während der gesamten 12 Wochen konnten die Gäste jeweils auf ein Buffet ausweichen und ihre Mahlzeit aus warmen und kalten Komponenten selber zusammenstellen. Die Gerichte wurden über drei vorgegebene Menülinien (F, K, W) randomisiert angeboten. Die Abbildung zeigt das Versuchsdesign der ersten 6 Experimentalwochen (Kalenderwoche 40 bis 45). Mehr Informationen über das Forschungsprojekt NOVANIMAL findet ihr auf dieser Webpage 6.2.1 Weitere Erläuterungen zum Datensatz Der Datensatz beinhaltet knapp 1100 Einträge mit 18 Variablen. Die Daten stammen aus dem Kassensystem des Catering-Unternehmen und stellen eine repräsentative Stichprobe des originalen Datensatzes dar. Folgende Variablen sind im Datensatz: Name der Variable Beschreibung der Variable transaction_id Durch das Kassensystem generierte Identifikationsnummer date Datum week Kalenderwoche year Kalenderjahr meal_name Beschreibung der angebotenen Mahlzeit article_description Beschreibung der Menü-Linie (F, Buffet, K, Locals1, W) label_content Beschreibung des Menü-Inhalts (Fleisch2, Buffet, Vegan3, Vegetarisch4) condit Beschreibung der Experimentwochen (Basis- oder Interventionswoche) card_num anonymisierte Kartennummer gender Geschlecht (F, M) member Hochschulzugehörigkeit (Studierende, Mitarbeitende) age Alter in Jahren price_article Preis des gekauften Artikels total_amount_pay Total bezahlter Betrag pay_description Bezahlungsart (Badge (Debit-Karte)) shop_description Ort der Mensa (GR oder VS) tot_ubp Umweltbelastungspunkte pro Gericht buffet_animal Anzahl fleischhaltiger Produkte auf dem Buffet Locals (Local F, Local K, Local W) sind nebst den drei “normalen Menü-Linien” zusätzlich angebotene Gerichte hier werden Gerichte mit Fisch &amp; Geflügel als Fleisch zusammengefasst. Vegane Gerichte enthalten ausschliesslich pflanzliche Zutaten. Im Exeriment wurde zwischen Gerichte mit pflanzlichen Fleischsubstituten und authentischen, eigenständigen veganen Gerichten unterschieden Vegetarisch bedeutet ovo-lakto-vegetarisch, d.h. die Gerichte enthalten Eier und/oder Milchprodukte "],
["6-3-ubungen-1.html", "6.3 Übungen 1", " 6.3 Übungen 1 6.3.1 Übung 1.1: Assoziationstest Datenerhebung für einen Assoziationstest zweier kategorialer Variablen, Dateneingabe und Durchführung von Chi-Quadrat- sowie Fishers exaktem Test 6.3.2 Übung 1.2: \\(\\chi^2\\)-Test Datensatz novanimal.csv Unterscheidet sich die Stichprobe des NOVANIMAL-Projekts von der gesamten Population bezüglich Geschlecht und Hochschulzugehörigkeit? Die Grundgesamtheit setzt sich aus 719 Studentinnen und 816 Studenten, 345 Mitarbeiterinnen und 339 Mitarbeiter. Die gesamte Population umfasst 2219 Personen mit einer aktiven CampusCard. Definiert die Null- (\\(H_0\\)) und die Alternativhypothese (\\(H_1\\)) Führt einen \\(\\chi^2\\)-Test mit dem Datensatz novanimal.csv durch Stellt eure Ergebnisse angemessen dar, d.h. Text mit Abbildung und/oder Tabelle 6.3.3 Übung 1.3: t-Test Werden in den Basis- und Interventionswochen unterschiedlich viele Gerichte verkauft? Definiert die Null- (\\(H_0\\)) und die Alternativhypothese (\\(H_1\\)). Führt einen t-Test mit dem Datensatz novanimal.csv durch. Welche Form von t-Test musst Du anwenden: einseitig/zweiseitig resp. gepaart/ungepaart? Wie gut sind die Voraussetzungen für einen t-Test erfüllt (z.B. Normalverteilung der Residuen und Varianzhomogenität)? Stell eure Ergebnisse angemessen dar, d.h. Text mit Abbildung und/oder Tabelle Das gilt für alle Übungen und wird von euch auch an der Prüfung verlangt. Abzugeben sind am Ende Ein lauffähiges R-Skript begründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation) ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit) "],
["7-statistik-2-29-10-2019.html", "Kapitel 7 Statistik 2 (29.10.2019)", " Kapitel 7 Statistik 2 (29.10.2019) In Statistik 2 lernen die Studierenden die Idee, die Voraussetzungen und die praktische Anwendung „einfacher“ linearer Modelle in R (sowie teilweise ihrer „nicht-parametrischen“ bzw. „robusten“ Äquivalente). Am Anfang steht die Varianzanalyse (ANOVA) als Verallgemeinerung des t-Tests, einschliesslich post-hoc-Tests und mehrfaktorieller ANOVA. Dann geht es um die Voraussetzungen parametrischer (und nicht-parametrischer) Tests und Optionen, wenn diese verletzt sind. Dann beschäftigen wir uns mit Korrelationen, die auf einen linearen Zusammenhang zwischen zwei metrischen Variablen testen, ohne Annahme einer Kausalität. Es folgen einfache lineare Regressionen, die im Prinzip das Gleiche bei klarer Kausalität leisten. Abschliessend besprechen wir, was die grosse Gruppe linearer Modelle (Befehl lm in R) auszeichnet. "],
["7-1-demoskript.html", "7.1 Demoskript", " 7.1 Demoskript Demoscript als Download t-test als ANOVA a&lt;-c(20,19,25,10,8,15,13,18,11,14) b&lt;-c(12,15,16,7,8,10,12,11,13,10) blume&lt;-data.frame(cultivar=c(rep(&quot;a&quot;,10),rep(&quot;b&quot;,10)),size=c(a,b)) par(mfrow=c(1,1)) boxplot (data=blume, size~cultivar, xlab=&quot;Sorte&quot;, ylab=&quot;Bluetengroesse [cm]&quot;) t.test(size~cultivar, blume, var.equal=T) aov(size~cultivar,data=blume) summary(aov(size~cultivar,data=blume)) summary.lm(aov(size~cultivar,data=blume)) Echte ANOVA c&lt;-c(30,19,31,23,18,25,26,24,17,20) blume2&lt;-data.frame(cultivar=c(rep(&quot;a&quot;,10),rep(&quot;b&quot;,10),rep(&quot;c&quot;,10)),size=c(a,b,c)) summary(blume2) head(blume2) par(mfrow=c(1,1)) boxplot (data=blume2, size~cultivar, xlab=&quot;Sorte&quot;, ylab=&quot;Blütengrösse [cm]&quot;) aov(size~cultivar,data=blume2) summary(aov(size~cultivar,data=blume2)) summary.lm(aov(size~cultivar,data=blume2)) aov.1 &lt;- aov(size~cultivar,data=blume2) summary(aov.1) summary.lm(aov.1) #Berechnung Mittelwerte usw. zur Charakterisierung der Gruppen aggregate(size~cultivar,blume2, function(x) c(Mean = mean(x), SD = sd(x), Min=min(x), Max=max(x))) lm.1 &lt;- lm(size~cultivar,data=blume2) summary(lm.1) Tukeys Posthoc-Test if(!require(multcomp)){install.packages(&quot;multcomp&quot;)} library(multcomp) summary(glht(aov(size~cultivar, data=blume2),linfct=mcp(cultivar =&quot;Tukey&quot;))) Beispiel Posthoc-Labels in Plot anova &lt;- aov(Sepal.Width ~ Species, data=iris) letters &lt;- cld(glht(anova, linfct=mcp(Species=&quot;Tukey&quot;))) boxplot(Sepal.Width ~ Species, data=iris) mtext(letters$mcletters$Letters, at=1:3) library(tidyverse) ggplot(iris, aes(Species, Sepal.Width)) + geom_boxplot(size = 1) + annotate(&quot;text&quot;, y = 5, x = 1:3, label = letters$mcletters$Letters) Klassische Tests der Modellannahmen (NICHT EMPFOHLEN!!!) attach(blume2) shapiro.test(size[cultivar == &quot;a&quot;]) var.test(size[cultivar == &quot;a&quot;],size[cultivar == &quot;b&quot;]) if(!require(car)){install.packages(&quot;car&quot;)} library(car) leveneTest(size[cultivar == &quot;a&quot;],size[cultivar == &quot;b&quot;],center=mean) wilcox.test(size[cultivar == &quot;a&quot;],size[cultivar == &quot;b&quot;]) detach(blume2) Nicht-parametrische Alternativen, wenn Modellannahmen der ANVOA massiv verletzt sind Zum Vergleich normale ANOVA noch mal summary(aov(size~cultivar,data=blume2)) Bei starken Abweichungen von der Normalverteilung, aber ähnlichen Varianzen Kruskal-Wallis-Test kruskal.test(data=blume2, size~cultivar) if(!require(FSA)){install.packages(&quot;FSA&quot;)} library(FSA) dunnTest(data=blume2, size~cultivar, method=&quot;bh&quot;) #korrigierte p-Werte nach Bejamini-Hochberg Bei erheblicher Heteroskedastizität, aber relative normal/symmetrisch verteilten Residuen Welch-Test oneway.test(data=blume2, size~cultivar, var.equal=F) 2-faktorielle ANOVA d&lt;-c(10,12,11,13,10,25,12,30,26,13) e&lt;-c(15,13,18,11,14,25,39,38,28,24) f&lt;-c(10,12,11,13,10,9,2,4,7,13) blume3&lt;-data.frame(cultivar=c(rep(&quot;a&quot;,20),rep(&quot;b&quot;,20),rep(&quot;c&quot;,20)), house=c(rep(c(rep(&quot;yes&quot;,10),rep(&quot;no&quot;,10)),3)),size=c(a,b,c,d,e,f)) blume3 boxplot(size~cultivar+house,data=blume3) summary(aov(size~cultivar+house,data=blume3)) summary(aov(size~cultivar+house+cultivar:house,data=blume3)) summary(aov(size~cultivar*house,data=blume3)) #Kurzschreibweise: &quot;*&quot; bedeutet, dass Interaktion zwischen cultivar und house eingeschlossen wird summary.lm(aov(size~cultivar+house,data=blume3)) interaction.plot(blume3$cultivar,blume3$house,blume3$size) interaction.plot(blume3$house,blume3$cultivar,blume3$size) anova(lm(blume3$size~blume3$cultivar*blume3$house),lm(blume3$size~blume3$cultivar+blume3$house)) anova(lm(blume3$size~blume3$house),lm(blume3$size~blume3$cultivar*blume3$house)) Korrelationen library(car) blume&lt;-data.frame(a,b) scatterplot(a~b,blume) cor.test(a,b,data = blume, method=&quot;pearson&quot;) cor.test(a,b,data = blume, method=&quot;spearman&quot;) cor.test(a,b,data = blume, method=&quot;kendall&quot;) #Jetzt als Regression lm.2 &lt;- lm(b~a) anova(lm.2) summary(lm.2) #Model II-Regression if(!require(lmodel2)){install.packages(&quot;lmodel2&quot;)} library(lmodel2) lmodel2(b~a) Beispiele Modelldiagnostik par(mfrow=c(2,2)) #4 Plots in einem Fenster plot(lm(b~a)) if(!require(ggfortify)){install.packages(&quot;ggfortify&quot;)} library(ggfortify) autoplot(lm(b~a)) #Modellstatistik nicht OK g&lt;-c(20,19,25,10,8,15,13,18,11,14,25,39,38,28,24) h&lt;-c(12,15,10,7,8,10,12,11,13,10,25,12,30,26,13) par(mfrow=c(1,1)) plot(h~g,xlim=c(0,40),ylim=c(0,30)) abline(lm(h~g)) plot(lm(h~g)) #Modelldiagnostik mit ggplot df &lt;- data.frame(g,h) ggplot(df, aes(x = g, y = h)) + # scale_x_continuous(limits = c(0,25)) + # scale_y_continuous(limits = c(0,25)) + geom_point() + geom_smooth( method = &quot;lm&quot;, color = &quot;black&quot;, size = .5, se = F) + theme_classic() par(mfrow=c(2,2)) plot(lm(h~g)) autoplot(lm(h~g)) "],
["7-2-ubungen-2.html", "7.2 Übungen 2", " 7.2 Übungen 2 Repetition: Abzugeben sind am Ende a. lauffähiges R-Skript b. begründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation) c. ausformulierter Methoden- und Ergebnisteil (für eine wiss.Arbeit). Bitte erklärt und begründet die einzelnen Schritte, die ihr unternehmt, um zu eurem Ergebnis zu kommen. Dazu erstellt bitte ein Word-Dokument, in dem ihr Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, eure Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere Vorgehen dokumentiert. Dieser Ablauf sollte insbesondere beinhalten: Überprüfen der Datenstruktur nach dem Einlesen, welches sind die abhängige(n) und welches die unabängige(n) Variablen etc. Explorative Datenanalyse, um zu sehen, ob evtl. Dateneingabefehler vorliegen oder Datentransformationen vorgenommen werden sollten Auswahl und Begründung eines statistischen Verfahrens Bestimmung des vollständigen/maximalen Models Selektion des/der besten Models/Modelle Durchführen der Modelldiagnostik für dieses Generieren aller Zahlen, Statistiken und Tabellen, die für eine wiss. Ergebnisdarstellung benötigt werden Formuliert abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen/Tabellen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (je einen ausformulierten Absatz von ca. 60-100 Worten bzw. 3-8 Sätzen). Alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden. 7.2.1 Übung 2.1: Regression (NatWis) Regressionsanalyse mit decay.csv Der Datensatz beschreibt in einem physikalischen Experiment die Zahl der radioaktiven Zerfälle pro Minute in Abhängigkeit vom Zeitpunkt (min nach Start des Experimentes). Ladet den Datensatz in R und macht eine explorative Datenanalyse. Wählt unter den schon gelernten Methoden der Regressionsanalyse einadäquates Vorgehen zur Analyse dieser Daten und führt diese dann durch. Prüft anhand der Residuen, ob die Modellvoraussetzungen erfüllt waren Stellt die erhaltenen Ergebnisse angemessen dar (Text, Abbildung und/oder Tabelle). Kennt ihr ggf. noch eine andere geeignete Herangehensweise? 7.2.2 Übung 2.2: Einfaktrielle ANOVA (SozOek) ANOVA mit novanimal.csv Führt mit dem Datensatz novanimal.csv eine einfaktorielle ANOVA durch. Gibt es Unterschiede zwischen der Anzahl verkaufter Gerichte (Buffet, Fleisch oder Vegetarisch) pro Woche? Hinweise für die Analysen: Fasst die vier Inhalte der Gerichte zu drei Inhalten zusammen. Das heisst, dass die pflanzlichen Gerichte neu zu den vegetarischen Gerichten gezählt werden. Konkret könnt ihr das in R mit einer Umbenennung der Inhalte durchführen (z. B. mit grep()). Danach muss der Datensatz gruppiert und zusammengefasst werden. Unbekannte Menü-Inhalte können vernachlässigt werden. Wie gut sind die Voraussetzungen für eine ANOVA erfüllt? Wären allenfalls auch nicht-parametrische Analysen zulässig? Führt anschliessend Post-hoc-Vergleiche durch. Fasst die Ergebnisse in einem Satz zusammen. 7.2.3 Übung 2.3N: Mehrfaktorielle ANOVA (NatWis) ANOVA mit kormoran.csv Der Datensatz enthält 40 Beobachtungen zu Tauchzeiten zweier Kormoranunterarten (C = Phalocrocorax carbo carbo und S = Phalacrocorax carbo sinensis) aus vier Jahreszeiten (F = Frühling, S = Sommer, H = Herbst, W = Winter). Lest den Datensatz nach R ein und führt eine adäquate Analyse durch, um beantworten zu können, wie Unterart und Jahreszeit die Tauchzeit beeinflussen. Stellt eure Ergebnisse dann angemessen dar (Text mit Abbildung und/oder Tabelle). Gibt es eine Interaktion? 7.2.4 Übung 2.3S: Mehrfaktorielle ANOVA mit Interaktion (SozOek) ANOVA mit novanimal.csv Können die Unterschiede in den verkauften Gerichten (Buffet, Fleisch oder Vegetarisch) durch die beiden Bedingungen (Basis- oder Interventionswochen) erklärt werden? Hinweise für die Analysen: Fasst die vier Inhalte der Gerichte zu drei Inhalten zusammen. Das heisst, dass die pflanzlichen Gerichte neu zu den vegetarischen Gerichten gezählt werden. Konkret könnt ihr das in R mit einer Umbenennung der Inhalte durchführen (z. B. mit grep()). Danach muss der Datensatz gruppiert und zusammengefasst werden. Unbekannte Menü-Inhalte können vernachlässigt werden. Wie gut sind die Voraussetzungen für eine ANOVA erfüllt? Wären allenfalls auch nicht-parametrische Analysen zulässig? Führt anschliessend Post-hoc-Vergleiche durch. Stellt eure Ergebnisse dann angemessen dar (Text mit Abbildung und/oder Tabelle). "]
]
