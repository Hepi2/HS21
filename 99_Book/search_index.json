[
["index.html", "Research Methods Kapitel 1 Einleitung", " Research Methods Juergen Dengler 2019-09-03 Kapitel 1 Einleitung Das Modul „Research Methods“ vermittelt vertiefte Methodenkompetenzen für praxisorientiertes und angewandtes wissenschaftliches Arbeiten im Fachbereich „Umwelt und Natürliche Ressourcen“ auf MSc-Niveau. Die Studierenden erarbeiten sich vertiefte Methodenkompetenzen für die analytische Betrachtung der Zusammenhänge im Gesamtsystem „Umwelt und Natürliche Ressourcen“. Die Studierenden erlernen die methodischen Kompetenzen, auf denen die nachfolgenden Module im MSc Programm UNR aufbauen. Das Modul vermittelt einerseits allgemeine, fächerübergreifende methodische Kompetenzen (z.B. Wissenschaftstheorie, computer-gestützte Datenverar-beitung und Statistik). Auf dieser Plattform (RStudio Connect) werden die Unterlagen für die R-Übungsteile bereitgestellt. Es werden sukzessive sowohl Demo-Files, Aufgabenstellungen und Lösungen veröffentlicht. "],
["2-prepro1-14-10-2019.html", "Kapitel 2 PrePro1 (14.10.2019)", " Kapitel 2 PrePro1 (14.10.2019) Die Datenkunde 2.0 gibt den Studierenden das Wissen und die Fertigkeiten an die Hand, selbst erhobene und bezogene Daten für Ihre eigenen Analysen vorzubereiten und anzureichern (preprocessing). Die Einheit vermittelt zentrale Datenverarbeitungskompetenzen und thematisiert bekannte Problemzonen der umweltwissenschaftlichen Datenverarbeitung – immer mit einer „hands-on“ Perspektive auf die begleitenden R-Übungen. Die Studierenden lernen die Eigenschaften ihrer Datensätze in der Fachsprache korrekt zu beschreiben. Sie lernen ausserdem Metadaten zu verstehen und die Implikationen derselben für ihre eigenen Analyseprojekte kritisch zu beurteilen. Zentrale Konzepte der Lerneinheit sind Skalenniveaus, Datentypen, Zeitdaten und Typumwandlungen. "],
["2-1-demo-datentypen-tabellen.html", "2.1 Demo: Datentypen, Tabellen", " 2.1 Demo: Datentypen, Tabellen R-Code als Download 2.1.1 Datentypen 2.1.1.1 Numerics Unter die Kategorie numeric fallen in R zwei Datentypen: double: Gleitkommazahl (z.B. 10.3, 7.3) integer: Ganzzahl (z.B. 10, 7) 2.1.1.1.1 Doubles Folgendermassen wird eine Gleitkommazahl einer Variabel zuweisen: x &lt;- 10.3 x ## [1] 10.3 typeof(x) ## [1] &quot;double&quot; Statt &lt;-kann auch = verwendet werden. Dies funktioniert aber nicht in allen Situationen, und ist zudem leicht mit == zu verwechseln. y = 7.3 y ## [1] 7.3 Ohne explizite Zuweisung nimmt R immer den Datentyp doublean: z &lt;- 42 typeof(z) ## [1] &quot;double&quot; is.integer(z) ## [1] FALSE is.numeric(z) ## [1] TRUE is.double(z) ## [1] TRUE 2.1.1.2 Ganzzahl / Integer Erst wenn man eine Zahl explizit als integer definiert (mit as.integer() oder L), wird sie auch als solches abgespeichert. a &lt;- as.integer(z) is.numeric(a) ## [1] TRUE is.integer(a) ## [1] TRUE c &lt;- 8L is.numeric(c) ## [1] TRUE is.integer(c) ## [1] TRUE typeof(a) ## [1] &quot;integer&quot; is.numeric(a) ## [1] TRUE is.integer(a) ## [1] TRUE Mit c() können eine Reihe von Werten in einer Variabel zugewiesen werden (als vector). Es gibt zudem auch character verctors. vector &lt;- c(10,20,33,42,54,66,77) vector ## [1] 10 20 33 42 54 66 77 vector[5] ## [1] 54 vector[2:4] ## [1] 20 33 42 vector2 &lt;- vector[2:4] Eine Ganzzahl kann explizit mit as.integer() definiert werden. a &lt;- as.integer(7) b &lt;- as.integer(3.14) a ## [1] 7 b ## [1] 3 typeof(a) ## [1] &quot;integer&quot; typeof(b) ## [1] &quot;integer&quot; is.integer(a) ## [1] TRUE is.integer(b) ## [1] TRUE Eine Zeichenkette kann als Zahl eingelesen werden. c &lt;- as.integer(&quot;3.14&quot;) c ## [1] 3 typeof(c) ## [1] &quot;integer&quot; 2.1.1.3 Logische Abfragen Wird auch auch als boolesch (Eng. boolean) bezeichnet. e &lt;- 3 f &lt;- 6 g &lt;- e &gt; f e ## [1] 3 f ## [1] 6 g ## [1] FALSE typeof(g) ## [1] &quot;logical&quot; 2.1.1.4 Logische Operationen sonnig &lt;- TRUE trocken &lt;- FALSE sonnig &amp; !trocken ## [1] TRUE Oft braucht man auch das Gegenteil / die Negation eines Wertes. Dies wird mittels ! erreicht u &lt;- TRUE v &lt;- !u v ## [1] FALSE 2.1.1.5 Zeichenketten Zeichenketten (Eng. character) stellen Text dar s &lt;- as.character(3.14) s ## [1] &quot;3.14&quot; typeof(s) ## [1] &quot;character&quot; Zeichenketten verbinden / zusammenfügen (Eng. concatenate) fname &lt;- &quot;Hans&quot; lname &lt;- &quot;Muster&quot; paste(fname,lname) ## [1] &quot;Hans Muster&quot; fname2 &lt;- &quot;hans&quot; fname == fname2 ## [1] FALSE 2.1.1.6 Factors Mit Factors wird in R eine Sammlung von Zeichenketten bezeichnet, die sich wiederholen, z.B. Wochentage (es gibt nur 7 unterschiedliche Werte für “Wochentage”). wochentage &lt;- c(&quot;Montag&quot;,&quot;Dienstag&quot;,&quot;Mittwoch&quot;,&quot;Donnerstag&quot;,&quot;Freitag&quot;,&quot;Samstag&quot;,&quot;Sonntag&quot;, &quot;Montag&quot;,&quot;Dienstag&quot;,&quot;Mittwoch&quot;,&quot;Donnerstag&quot;,&quot;Freitag&quot;,&quot;Samstag&quot;,&quot;Sonntag&quot;) typeof(wochentage) ## [1] &quot;character&quot; wochentage_fac &lt;- as.factor(wochentage) wochentage ## [1] &quot;Montag&quot; &quot;Dienstag&quot; &quot;Mittwoch&quot; &quot;Donnerstag&quot; &quot;Freitag&quot; ## [6] &quot;Samstag&quot; &quot;Sonntag&quot; &quot;Montag&quot; &quot;Dienstag&quot; &quot;Mittwoch&quot; ## [11] &quot;Donnerstag&quot; &quot;Freitag&quot; &quot;Samstag&quot; &quot;Sonntag&quot; wochentage_fac ## [1] Montag Dienstag Mittwoch Donnerstag Freitag Samstag ## [7] Sonntag Montag Dienstag Mittwoch Donnerstag Freitag ## [13] Samstag Sonntag ## Levels: Dienstag Donnerstag Freitag Mittwoch Montag Samstag Sonntag Wie man oben sieht, unterscheiden sich character vectors und factors v.a. dadurch, dass letztere über sogenannte levels verfügt. Diese levels entsprechen den Eindeutigen (unique) Werten. levels(wochentage_fac) ## [1] &quot;Dienstag&quot; &quot;Donnerstag&quot; &quot;Freitag&quot; &quot;Mittwoch&quot; &quot;Montag&quot; ## [6] &quot;Samstag&quot; &quot;Sonntag&quot; unique(wochentage) ## [1] &quot;Montag&quot; &quot;Dienstag&quot; &quot;Mittwoch&quot; &quot;Donnerstag&quot; &quot;Freitag&quot; ## [6] &quot;Samstag&quot; &quot;Sonntag&quot; 2.1.1.7 Zeit/Datum Um in R mit Datum/Zeit Datentypen umzugehen, müssen sie als POSIXct eingelesen werden (es gibt alternativ noch POSIXlt, aber diese ignorieren wir mal). Anders als Beispielsweise bei Excel, sollten in R Datum und Uhrzeit immer in einer Spalte gespeichert werden. datum &lt;- &quot;2017-10-01 13:45:10&quot; as.POSIXct(datum) ## [1] &quot;2017-10-01 13:45:10 CEST&quot; Wenn das die Zeichenkette in dem obigen Format (Jahr-Monat-Tag Stunde:Minute:Sekunde) daher kommt, braucht as.POSIXctkeine weiteren Informationen. Sollte das Format von dem aber Abweichen, muss man der Funktion das genaue Schema jedoch mitteilen. Der Syntax dafür kann via ?strptime nachgeschlagen werden. datum &lt;- &quot;01.10.2017 13:45&quot; as.POSIXct(datum,format = &quot;%d.%m.%Y %H:%M&quot;) ## [1] &quot;2017-10-01 13:45:00 CEST&quot; Beachtet, dass in den den obigen Beispiel R automatisch eine Zeitzone angenommen hat (CEST). R geht davon aus, dass die Zeitzone der System Timezone (Sys.timezone()) entspricht. 2.1.2 Data Frames und Conveniance Variabeln Eine data.frame ist die gängigste Art, Tabellarische Daten zu speichern. df &lt;- data.frame( Stadt = c(&quot;Zürich&quot;,&quot;Genf&quot;,&quot;Basel&quot;,&quot;Bern&quot;,&quot;Lausanne&quot;), Einwohner = c(396027,194565,175131,140634,135629), Ankunft = c(&quot;1.1.2017 10:00&quot;,&quot;1.1.2017 14:00&quot;, &quot;1.1.2017 13:00&quot;,&quot;1.1.2017 18:00&quot;,&quot;1.1.2017 21:00&quot;) ) str(df) ## &#39;data.frame&#39;: 5 obs. of 3 variables: ## $ Stadt : Factor w/ 5 levels &quot;Basel&quot;,&quot;Bern&quot;,..: 5 3 1 2 4 ## $ Einwohner: num 396027 194565 175131 140634 135629 ## $ Ankunft : Factor w/ 5 levels &quot;1.1.2017 10:00&quot;,..: 1 3 2 4 5 In der obigen data.frame wurde die Spalte Einwohner als Fliesskommazahl abgespeichert. Dies ist zwar nicht tragisch, aber da wir wissen das es sich hier sicher um Ganzzahlen handelt, können wir das korrigieren. Wichtiger ist aber, dass wir die Ankunftszeit (SpalteAnkunft) von einem Factor in ein Zeitformat (POSIXct) umwandeln. df$Einwohner &lt;- as.integer(df$Einwohner) df$Einwohner ## [1] 396027 194565 175131 140634 135629 df$Ankunft &lt;- as.POSIXct(df$Ankunft, format = &quot;%d.%m.%Y %H:%M&quot;) df$Ankunft ## [1] &quot;2017-01-01 10:00:00 CET&quot; &quot;2017-01-01 14:00:00 CET&quot; ## [3] &quot;2017-01-01 13:00:00 CET&quot; &quot;2017-01-01 18:00:00 CET&quot; ## [5] &quot;2017-01-01 21:00:00 CET&quot; Diese Rohdaten können nun helfen, um Hilfsvariablen (convenience variables) zu erstellen. Z.B. können wir die Städte einteilen in gross, mittel und klein. df$Groesse[df$Einwohner &gt; 300000] &lt;- &quot;gross&quot; df$Groesse[df$Einwohner &lt;= 300000 &amp; df$Einwohner &gt; 150000] &lt;- &quot;mittel&quot; df$Groesse[df$Einwohner &lt;= 150000] &lt;- &quot;klein&quot; Oder aber, die Ankunftszeit kann von der Spalte Ankunftabgeleitet werden. Dazu brauchen wir aber das Package lubridate library(lubridate) df$Ankunft_stunde &lt;- hour(df$Ankunft) 2.1.3 Quellen Dieses Kapitel verwendet folgende Libraries: Spinu, Grolemund, and Wickham (2018) References "],
["2-2-ubung-a.html", "2.2 Übung A", " 2.2 Übung A R ist ohne Zusatzpackete nicht mehr denkbar. Die allermeisten Packages werden auf CRAN gehostet und können leicht mittels install.packages() installiert werden. Eine sehr wichtige Sammlung von Packages wird von RStudio entwickelt. Unter dem Namen Tidyverse werden eine Reihe von Packages angeboten, den R-Alltag enorm erleichtert. Wir werden später näher auf das “Tidy”-Universum eingehen, an dieser Stelle können wir die Sammlung einfach mal installieren. install.packages(&quot;tidyverse&quot;) Um ein package in R verwenden zu können, gibt es zwei Möglichkeiten: entweder man lädt es zu Beginn der R-session mittles library(). oder man ruft eine function mit vorangestelltem Packetname sowie zwei Doppelpunkten auf. dplyr::filter() ruft die Funktion filter() des Packets dplyr auf. Letztere Notation ist vor allem dann sinnvoll, wenn sich zwei unterschiedliche Funktionen mit dem gleichen namen in verschiedenen pacakges existieren. filter() existiert als Funktion einersits im package dplyr sowie in stats. Dieses Phänomen nennt man “masking”. Zu beginn laden wir die nötigen Pakete: 2.2.1 Aufgabe 1 Erstelle eine data.frame mit nachstehenden Daten. Tipps: Eine leere data.frame zu erstellen ist schwieriger als wenn erstellen und befüllen der data.frame in einem Schritt erfolgt R ist dafür gedacht, Spalte für Spalte zu arbeiten (warum?), nicht Reihe für Reihe. Versuche dich an dieses Schema zu halten. Tierart Anzahl Gewicht Geschlecht Beschreibung Fuchs 2 4.4 m Rötlich Bär 5 40.3 f Braun, gross Hase 1 1.1 m klein, mit langen Ohren Elch 3 120.0 m Lange Beine, Schaufelgeweih 2.2.2 Aufgabe 2 Was für Datentypen wurden (in Aufgabe 1) von R automatisch angenommen? Sind diese sinnvoll? Tipp: Nutze dazu str() ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 4 obs. of 5 variables: ## $ Tierart : chr &quot;Fuchs&quot; &quot;Bär&quot; &quot;Hase&quot; &quot;Elch&quot; ## $ Anzahl : num 2 5 1 3 ## $ Gewicht : num 4.4 40.3 1.1 120 ## $ Geschlecht : chr &quot;m&quot; &quot;f&quot; &quot;m&quot; &quot;m&quot; ## $ Beschreibung: chr &quot;Rötlich&quot; &quot;Braun, gross&quot; &quot;klein, mit langen Ohren&quot; &quot;Lange Beine, Schaufelgeweih&quot; ## [1] &quot;double&quot; 2.2.3 Aufgabe 3 Nutze die Spalte Gewicht um die Tiere in 3 Gewichtskategorien einzuteilen: leicht: &lt; 5kg mittel: 5 - 100 kg schwer: &gt; 100kg Tierart Anzahl Gewicht Geschlecht Beschreibung Gewichtsklasse Fuchs 2 4.4 m Rötlich leicht Bär 5 40.3 f Braun, gross mittel Hase 1 1.1 m klein, mit langen Ohren leicht Elch 3 120.0 m Lange Beine, Schaufelgeweih schwer 2.2.4 Aufgabe 4 Importiere den Datensatz order_52252_data.txt. Es handelt sich dabei um die stündlich gemittelten Temperaturdaten an verschiedenen Standorten in der Schweiz im Zeitraum 2000 - 2005. Wir empfehlen read_table()1 anstelle von read.table(). stn time tre200h0 ABO 2000010100 -2.6 ABO 2000010101 -2.5 ABO 2000010102 -3.1 ABO 2000010103 -2.4 ABO 2000010104 -2.5 ABO 2000010105 -3.0 ABO 2000010106 -3.7 ABO 2000010107 -4.4 ABO 2000010108 -4.1 ABO 2000010109 -4.1 2.2.5 Aufgabe 5 Schau dir die Rückmeldung von read_table()an. Sind die Daten korrekt interpretiert worden? 2.2.6 Aufgabe 6 Die Spalte time ist eine Datum/Zeitangabe im Format JJJJMMTTHH (siehe meta.txt). Damit R dies als Datum-/Zeitangabe erkennt, müssen wir die Spalte in einem R-Format (POSIXct) einlesen und dabei R mitteilen, wie sie aktuell formatiert ist. Lies die Spalte mit as.POSIXct() (oder parse_datetime) ein und spezifiziere sowohl format wie auch tz. Tipps: Wenn keine Zeitzone festgelegt wird, trifft as.POSIXct() eine Annahme (basierend auf Sys.timezone()). In unserem Fall handelt es sich aber um Werte in UTC (siehe meta.txt) as.POSIXcterwartet character ## [1] &quot;2000-01-01 00:00:00 UTC&quot; &quot;2000-01-01 01:00:00 UTC&quot; ## [3] &quot;2000-01-01 02:00:00 UTC&quot; &quot;2000-01-01 03:00:00 UTC&quot; ## [5] &quot;2000-01-01 04:00:00 UTC&quot; &quot;2000-01-01 05:00:00 UTC&quot; ## [7] &quot;2000-01-01 06:00:00 UTC&quot; &quot;2000-01-01 07:00:00 UTC&quot; ## [9] &quot;2000-01-01 08:00:00 UTC&quot; &quot;2000-01-01 09:00:00 UTC&quot; stn time tre200h0 ABO 2000-01-01 00:00:00 -2.6 ABO 2000-01-01 01:00:00 -2.5 ABO 2000-01-01 02:00:00 -3.1 ABO 2000-01-01 03:00:00 -2.4 ABO 2000-01-01 04:00:00 -2.5 ABO 2000-01-01 05:00:00 -3.0 ABO 2000-01-01 06:00:00 -3.7 ABO 2000-01-01 07:00:00 -4.4 ABO 2000-01-01 08:00:00 -4.1 ABO 2000-01-01 09:00:00 -4.1 2.2.7 Aufgabe 7 Erstelle zwei neue Spalten mit Wochentag (Montag, Dienstag, etc) und Kalenderwoche. Verwende dazu die neu erstellte POSIXct-Spalte stn time tre200h0 wochentag kw ABO 2000-01-01 00:00:00 -2.6 Sat 1 ABO 2000-01-01 01:00:00 -2.5 Sat 1 ABO 2000-01-01 02:00:00 -3.1 Sat 1 ABO 2000-01-01 03:00:00 -2.4 Sat 1 ABO 2000-01-01 04:00:00 -2.5 Sat 1 ABO 2000-01-01 05:00:00 -3.0 Sat 1 ABO 2000-01-01 06:00:00 -3.7 Sat 1 ABO 2000-01-01 07:00:00 -4.4 Sat 1 ABO 2000-01-01 08:00:00 -4.1 Sat 1 ABO 2000-01-01 09:00:00 -4.1 Sat 1 2.2.8 Aufgabe 8 Erstelle eine neue Spalte basierend auf die Temperaturwerte mit der Einteilung “kalt” (Unter Null Grad) und “warm” (über Null Grad) stn time tre200h0 wochentag kw temp_kat ABO 2000-01-01 00:00:00 -2.6 Sat 1 kalt ABO 2000-01-01 01:00:00 -2.5 Sat 1 kalt ABO 2000-01-01 02:00:00 -3.1 Sat 1 kalt ABO 2000-01-01 03:00:00 -2.4 Sat 1 kalt ABO 2000-01-01 04:00:00 -2.5 Sat 1 kalt ABO 2000-01-01 05:00:00 -3.0 Sat 1 kalt ABO 2000-01-01 06:00:00 -3.7 Sat 1 kalt ABO 2000-01-01 07:00:00 -4.4 Sat 1 kalt ABO 2000-01-01 08:00:00 -4.1 Sat 1 kalt ABO 2000-01-01 09:00:00 -4.1 Sat 1 kalt Wickham and Grolemund (2017), Kapitel 8 bzw. http://r4ds.had.co.nz/data-import.html)↩ "],
["2-3-ubung-a-losung.html", "2.3 Übung A Lösung", " 2.3 Übung A Lösung R-Script als Download library(tidyverse) # Im Unterschied zu `install.packages()` werden bei `library()` keine Anführungs- # und Schlusszeichen gesetzt. library(lubridate) # Im Unterschied zu install.packages(&quot;tidyverse&quot;) wird bei library(tidyverse) # das package lubridate nicht berücksichtigt # Lösung Aufgabe 1 df &lt;- data_frame( Tierart = c(&quot;Fuchs&quot;,&quot;Bär&quot;,&quot;Hase&quot;,&quot;Elch&quot;), Anzahl = c(2,5,1,3), Gewicht = c(4.4, 40.3,1.1,120), Geschlecht = c(&quot;m&quot;,&quot;f&quot;,&quot;m&quot;,&quot;m&quot;), Beschreibung = c(&quot;Rötlich&quot;,&quot;Braun, gross&quot;, &quot;klein, mit langen Ohren&quot;,&quot;Lange Beine, Schaufelgeweih&quot;) ) # Lösung Aufgabe 2 str(df) # Anzahl wurde als `double` interpretiert, ist aber eigentlich ein `integer`. # Mit data.frame() wurde Beschreibung wurde als `factor` interpretiert, ist # aber eigentlich `character` typeof(df$Anzahl) df$Anzahl &lt;- as.integer(df$Anzahl) df$Beschreibung &lt;- as.character(df$Beschreibung) # Lösung Aufgabe 3 df$Gewichtsklasse[df$Gewicht &gt; 100] &lt;- &quot;schwer&quot; df$Gewichtsklasse[df$Gewicht &lt;= 100 &amp; df$Gewicht &gt; 5] &lt;- &quot;mittel&quot; df$Gewichtsklasse[df$Gewicht &lt;= 5] &lt;- &quot;leicht&quot; # Lösung Aufgabe 4 wetter &lt;- readr::read_table(&quot;09_PrePro1/data/order_52252_data.txt&quot;) # Lösung Aufgabe 5 # Die Spalte &#39;time&#39; wurde als &#39;integer&#39; interpretiert. Dabei handelt es # sich offensichtlich um Zeitangaben. # Lösung Aufgabe 6 # mit readr parse_datetime(as.character(wetter$time[1:10]), format = &quot;%Y%m%d%H&quot;) # mit as.POSIXct() wetter$time &lt;- as.POSIXct(as.character(wetter$time), format = &quot;%Y%m%d%H&quot;,tz = &quot;UTC&quot;) # Lösung Aufgabe 7 wetter$wochentag &lt;- wday(wetter$time,label = T) wetter$kw &lt;- week(wetter$time) # Lösung Aufgabe 8 wetter$temp_kat[wetter$tre200h0&gt;0] &lt;- &quot;warm&quot; wetter$temp_kat[wetter$tre200h0&lt;=0] &lt;- &quot;kalt&quot; "],
["2-4-ubung-b.html", "2.4 Übung B", " 2.4 Übung B Fahre mit dem Datensatz wetter aus Übung A fort. 2.4.1 Aufgabe 1 Nutze plot() um die Temparaturkurve zu visualisieren. Verwende aber vorher filter() um dich auf eine Station (z.B. “ABO”) zu beschränken (es handelt sich sonst um zuviele Datenpunkte). Nun schauen wir uns das plotten mit ggplot2 an. Ein simpler Plot wie der in der vorherigen Aufgabe ist in ggplot2 zugegebenermassen etwas komplizierter. ggplot2 wird aber rasch einfacher, wenn die Grafiken komplexer werden. Wir empfehlen deshalb stark, ggplot2 zu verwenden. Schau dir ein paar online Tutorials zu ggplot2 an (siehe2) und reproduziere den obigen Plot mit ggplot2 2.4.2 Aufgabe 2 Spiele mit Hilfe der erwähnten Tutorials mit dem Plot etwas rum. Versuche die x-/y-Achsen zu beschriften sowie einen Titel hinzu zu fügen. 2.4.3 Aufgabe 3 Reduziere den x-Achsenausschnitt auf einen kleineren Zeitraum, beispielsweise einn beliebigen Monat. Verwende dazu lims() zusammen mit as.POSIXct() oder mache ein Subset von deinem Datensatz mit einer convenience-Variabel und filter(). Wickham and Grolemund (2017), Kapitel 1 bzw. http://r4ds.had.co.nz/data-visualisation.html oder hier ein sehr schönes Video: Learn R: An Introduction to ggplot2↩ "],
["2-5-ubung-b-losung.html", "2.5 Übung B Lösung", " 2.5 Übung B Lösung R-Code als Download library(tidyverse) # Lösung Aufgabe 1 wetter_fil &lt;- dplyr::filter(wetter, stn == &quot;ABO&quot;) plot(wetter_fil$time,wetter_fil$tre200h0, type = &quot;l&quot;) p &lt;- ggplot(wetter_fil, aes(time,tre200h0)) + geom_line() p # Lösung Aufgabe 2 p &lt;- p + labs(x = &quot;Datum&quot;, y = &quot;Temperatur&quot;, title = &quot;Stündlich gemittelte Temperaturwerte&quot;) p # Lösung Aufgabe 3 limits &lt;- as.POSIXct(c(&quot;2002-01-01 00:00:00&quot;,&quot;2002-02-01 00:00:00&quot;),tz = &quot;UTC&quot;) p + lims(x = limits) "],
["3-statistik-2-29-10-2019.html", "Kapitel 3 Statistik 2 (29.10.2019)", " Kapitel 3 Statistik 2 (29.10.2019) In Statistik 2 lernen die Studierenden die Idee, die Voraussetzungen und die praktische Anwendung „einfacher“ linearer Modelle in R (sowie teilweise ihrer “nicht-parametrischen” bzw. “robusten” Äquivalente). Am Beginn stehen Korrelationen, die auf einen linearen Zusammenhang zwischen zwei metrischen Variablen testen, ohne Annahme einer Kausalität. Es folgen einfache lineare Regressionen, die im Prinzip das Gleiche bei klarer Kausalität leisten sowie deren Verallgemeinerung als polynomiale Regressionen, die z. B. auch einen Test auf unimodale Beziehungen erlaubt. Aufbauend auf Statistik 1 werden ANOVAs vertieft, u.a. post-hoc-Test und mehrfaktorielle ANOVAs. Zum Schluss wird noch die ANCOVA als eine Technik vorgestellt, die ANOVA und lineare Regression verbindet. "],
["3-1-ubungen.html", "3.1 Übungen", " 3.1 Übungen 3.1.1 Übung 2.1: Regression (NatWis) Regressionsanalyse mit decay.csv Der Datensatz beschreibt in einem physikalischen Experiment die Zahl der radioaktiven Zerfälle pro Minute in Abhängigkeit vom Zeitpunkt (min nach Start des Experimentes). Ladet den Datensatz in R und macht eine explorative Datenanalyse. Wählt unter den schon gelernten Methoden der Regressionsanalyse ein adäquates Vorgehen zur Analyse dieser Daten und führt diese dann durch. Prüft anhand der Residuen, ob die Modellvoraussetzungen erfüllt waren Stellt die erhaltenen Ergebnisse angemessen dar (Text, Abbildung und/oder Tabelle). Kennt ihr ggf. noch eine andere geeignete Herangehensweise? 3.1.2 Übung 2.2: Einfaktrielle ANOVA (SozOek) ANOVA mit novanimal.csv Führt mit dem Datensatz novanimal.csv eine einfaktorielle ANOVA durch.Gibt es Unterschiede zwischen der Anzahl verkaufter Gerichte (Buffet, Fleisch oder Vegetarisch) pro Woche? Hinweise für die Analysen: Fasst die vier Inhalte der Gerichte zu drei Inhalten zusammen. Das heisst, dass die pflanzlichen Gerichte neu zu den vegetarischen Gerichten gezählt werden. Konkret könnt ihr das in R mit einer Umbenennung der Inhalte durchführen (z. B. mit grep()). Danach muss der Datensatz gruppiert und zusammengefasst werden. Unbekannte Menü-Inhalte können vernachlässigt werden. Wie gut sind die Voraussetzungen für eine ANOVA erfüllt? Wären allenfalls auch nicht-parametrische Analysen zulässig? Führt anschliessend Post-hoc-Vergleiche durch. Fasst die Ergebnisse in einem Satz zusammen. 3.1.3 Übung 2.3N: Mehrfaktorielle ANOVA (NatWis) ANOVA mit kormoran.csv Der Datensatz enthält 40 Beobachtungen zu Tauchzeiten zweier Kormoranunterarten (C = Phalocrocorax carbo carbo und S = Phalacrocorax carbo sinensis) aus vier Jahreszeiten (F = Frühling, S = Sommer, H = Herbst, W = Winter). Lest den Datensatz nach R ein und führt eine adäquate Analyse durch, um beantworten zu können, wie Unterart und Jahreszeit die Tauchzeit beeinflussen. Stellt Ihre Ergebnisse dann angemessen dar (Text, Abbildung und/oder Tabelle). Gibt es eine Interaktion? 3.1.4 Übung 2.3S: Mehrfaktorielle ANOVA mit Interaktion (SozOek) ANOVA mit novanimal.csv Können die Unterschiede in den verkauften Gerichten (Buffet, Fleisch oder Vegetarisch) durch die beiden Bedingungen (Basis- oder Interventionswochen) erklärt werden? Hinweise für die Analysen: Fasst die vier Inhalte der Gerichte zu drei Inhalten zusammen. Das heisst, dass die pflanzlichen Gerichte neu zu den vegetarischen Gerichten gezählt werden. Konkret könnt ihr das in R mit einer Umbenennung der Inhalte durchführen (z. B. mit grep()). Danach muss der Datensatz gruppiert und zusammengefasst werden. Unbekannte Menü-Inhalte können vernachlässigt werden. Wie gut sind die Voraussetzungen für eine ANOVA erfüllt? Wären allenfalls auch nicht-parametrische Analysen zulässig? Führt anschliessend Post-hoc-Vergleiche durch. Fasst die Ergebnisse in einem Satz zusammen. 3.1.5 Quellen "],
["3-2-musterlosung-aufgabe-2-1-regression.html", "3.2 Musterlösung Aufgabe 2.1: Regression", " 3.2 Musterlösung Aufgabe 2.1: Regression Übungsaufgabe (hier so ausführlich formuliert, wie dies auch in der Klausur der Fall sein wird) Laden Sie den Datensatz decay.csv. Dieser enthält die Zahl radioaktiver Zerfälle pro Zeiteinheit (amount) für Zeitpunkte (time) nach dem Start des Experimentes. Ermitteln Sie ein statistisches Modell, dass die Zerfallshäufigkeit in Abhängigkeit von der Zeit beschreibt. Bitte erklären und begründen Sie die einzelnen Schritte, die Sie unternehmen, um zu diesem Ergebnis zu kommen. Dazu erstellen Sie bitte ein Word-Dokument, in das Sie Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, Ihre Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere Vorgehen dokumentieren. Dieser Ablauf sollte insbesondere beinhalten: Überprüfen der Datenstruktur nach dem Einlesen, welches sind die abhängige(n) und welches die unabängige(n) Variablen Explorative Datenanalyse, um zu sehen, ob evtl. Dateneingabefehler vorliegen oder Datentransformationen vorgenommen werden sollten Auswahl und Begründung eines statistischen Verfahrens (es gibt hier mehrere statistisch korrekte Möglichkeiten!) Ermittlung eines Modells Durchführen der Modelldiagnostik für das gewählte Modell Generieren aller Zahlen, Statistiken und Tabellen, die für eine wiss. Ergebnisdarstellung benötigt werden Formulieren Sie abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden. Abzugeben sind am Ende (a) Ein lauffähiges R-Skript; (b) begründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation) und (c) ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit). Loesung – Skript Uebung 2.1 - Regressionsanalyse data &lt;-read.csv(&quot;14_Statistik2/data/decay.csv&quot;) data attach(data) summary(data) str(data) Explorative Datenanalyse boxplot(time) boxplot(amount) plot(amount~time) Einfaches lineares Modell model1&lt;-lm(amount~time) summary(model1) Modelldiagnostik par(mfrow=c(2,2)) plot(model1) Ergebnisplot par(mfrow=c(1,1)) plot(time,amount) abline(lm(amount~time),col=&quot;red&quot;) Loesung 1: log-Transformation der abhaengigen Variablen par(mfrow=c(1,2)) boxplot(amount) boxplot(log(amount)) hist(amount) hist(log(amount)) model2&lt;-lm(log(amount)~time) summary(model2) Modelldiagnostik par(mfrow=c(2,2)) plot(model2) Loesung 2: quadratische Regression (kam erst in Statistik 3; koente fuer die Datenverteilung passen, entspricht aber nicht der physikalischen Gesetzmaessigkeit model.quad&lt;-lm(amount~time+I(time^2)) summary(model.quad) Vergleich mit dem einfachen Modell mittels ANOVA (es ginge auch AICc) anova(model1,model.quad) Modelldiagnostik par(mfrow=c(2,2)) plot(model.quad) Loesung 3 (kam erst in Statistik 4; methodisch beste Loesung; mit Startwerten muss man ggf. ausprobieren) model.nls&lt;-nls(amount~a*exp(-b*time),start=(list(a=100,b=1))) summary(model.nls) Modelldiagnostik library(nlstools) residuals.nls &lt;- nlsResiduals(model.nls) plot(residuals.nls) Ergebnisplots par(mfrow=c(1,1)) xv&lt;-seq(0,30,0.1) lineares Modell mit log-transformierter Abhaengiger plot(time,amount) yv1&lt;-exp(predict(model2,list(time=xv))) lines(xv,yv1,col=&quot;red&quot;) quadratisches Modell plot(time,amount) yv2&lt;-predict(model.quad,list(time=xv)) lines(xv,yv2,col=&quot;blue&quot;) nicht-lineares Modell plot(time,amount) yv3&lt;-predict(model.nls,list(time=xv)) lines(xv,yv3,col=&quot;green&quot;) "],
["3-3-musterlosung-aufgabe-2-2-einfaktorielle-anova.html", "3.3 Musterlösung Aufgabe 2.2: einfaktorielle ANOVA", " 3.3 Musterlösung Aufgabe 2.2: einfaktorielle ANOVA df &lt;- nova # klone den originaler Datensatz # fasst die vier Inhalte der Gerichte zu drei Inhalten zusammen. df$label_content[grep(&quot;Pflanzlich+&quot;,df$label_content)] &lt;- &quot;Vegetarisch&quot; # gruppiert Daten nach Menü-Inhalt und Woche df_ &lt;- df %&gt;% group_by(label_content, week) %&gt;% summarise(tot_sold = n()) %&gt;% drop_na() # lasst die unbekannten Menü-Inhalte weg # überprüft die Voraussetzungen für eine ANOVA # Histogramm, sagt aber nicht viel aus ggplot(df_, aes(x = tot_sold, y = ..count..)) + geom_histogram() + labs(x = &quot;\\nAnzahl verkaufte Gerichte pro Woche&quot;, y = &quot;Häufigkeit\\n&quot;) + mytheme # Boxplot ggplot(df_, aes(x = label_content, y= tot_sold)) + geom_boxplot(fill=&quot;white&quot;, color = &quot;black&quot;, size = 1) + labs(x = &quot;\\nMenü-Inhalt&quot;, y = &quot;Anzahl verkaufte Gerichte pro Woche\\n&quot;) + mytheme # klare Varianzheterogenität # definiert das Modell model &lt;- aov(tot_sold ~ label_content, data = df_) summary.lm(model) autoplot(model) + mytheme Fazit: Inspektion der Modellvoraussetzung zeigt klare Verletzungen der Homoskedastizität. Nächster Schritt Welch-Test. # überprüft die Voraussetzungen des Welch-Test. # Gibt es eine hohe Varianzheterogenität und # ist die relative Verteilung der Residuen gegeben? # In diesem Fall wäre ein Welch-Test passend w_test &lt;- oneway.test(data=df_, tot_sold ~ label_content, var.equal=F) w_test 3.3.1 Methoden Ziel war es, die Unterschide in den Verkaufszahlen pro Menü-Inhalt aufzuzeigen. Da die Kriteriumsvariable (Verkaufszahlen) metrisch und die Prädiktorvariable kategorial sind, wurde eine einfaktorielle ANOVA gerechnet. Die visuelle Inspektion der Voraussetzungen zeigte insbesondere schwere Verletzungen der Homoskedastizität. Der Boxplot bestätigt diesen Befund. Daher wurde in einem weiteren Schritt den Welch-Test für ungleiche Varianzen gerechnet. 3.3.2 Ergebnisse Die Menü-Inhalte (Fleisch, Vegetarisch und Buffet) unterscheiden sich in den Verkaufszahlen signifikant (F(2,7) = 65, p &lt; .001). Die Figure 1 zeigt die Verkaufszahlen pro Menü-Inhalt. Figure 3.1: Die wöchentlichen Verkaufzahlen unterscheiden sich je nach Menü-Inhalt stark. "],
["3-4-musterlosung-aufgabe-2-3n-mehrfaktorielle-anova.html", "3.4 Musterlösung Aufgabe 2.3N: Mehrfaktorielle ANOVA", " 3.4 Musterlösung Aufgabe 2.3N: Mehrfaktorielle ANOVA Übungsaufgabe (hier so ausführlich formuliert, wie dies auch in der Klausur der Fall sein wird) Laden Sie den Datensatz kormoran.txt mit read.table. Dieser enthält Tauchzeiten (hier ohne Einheit) von Kormoranen in Abhängigkeit von Jahreszeit und Unterart. Unterarten: Phalacrocorax carbo carbo (C) und Phalacrocorax carbo sinensis (S); Jahreszeiten: F = Frühling, S = Sommer, H = Herbst, W = Winter. Ihre Gesamtaufgabe ist es, aus diesen Daten ein minimal adäquates Modell zu ermitteln, das diese Abhängigkeit beschreibt. Bitte erklären und begründen Sie die einzelnen Schritte, die Sie unternehmen, um zu diesem Ergebnis zu kommen. Dazu erstellen Sie bitte ein Word-Dokument, in das Sie Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, Ihre Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere Vorgehen dokumentieren. Dieser Ablauf sollte insbesondere beinhalten: Überprüfen der Datenstruktur nach dem Einlesen, welches sind die abhängige(n) und welches die unabängige(n) Variablen, welches statistische Verfahren wenden Sie an? Explorative Datenanalyse, um zu sehen, ob schon vor dem Start der Analysen Transformationen o.ä. vorgenommen werden sollten Definition eines vollen Modelles, das nach statistischen Kritierien zum minimal adäquaten Modell reduziert wird Durchführen der Modelldiagnostik, um zu entscheiden, ob das gewählte Vorgehen korrekt war oder ggf. angepasst werden muss Generieren aller Zahlen, Statistiken und Tabellen, die für eine wiss. Ergebnisdarstellung benötigt werden Formulieren Sie abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden. Abzugeben sind am Ende (a) Ein lauffähiges R-Skript; (b) begründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation) und (c) ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit). Loesung – Skript Uebung 2.3N - Mehrfaktorielle ANOVA kormoran &lt;-read.delim(&quot;14_Statistik2/data/kormoran.csv&quot;,sep = &quot;;&quot;) ## Ueberpruefen, ob Einlesen richtig funktioniert hat und welche Datenstruktur vorliegt str(kormoran) ## &#39;data.frame&#39;: 40 obs. of 4 variables: ## $ Obs : int 1 2 3 4 5 6 7 8 9 10 ... ## $ Tauchzeit : num 9.5 11.9 13.4 13.8 15.3 15.5 15.6 16.7 16.8 18.7 ... ## $ Unterart : Factor w/ 2 levels &quot;C&quot;,&quot;S&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ Jahreszeit: Factor w/ 4 levels &quot;F&quot;,&quot;H&quot;,&quot;S&quot;,&quot;W&quot;: 1 1 1 1 1 3 3 3 3 3 ... summary(kormoran) ## Obs Tauchzeit Unterart Jahreszeit ## Min. : 1.00 Min. : 9.50 C:20 F:10 ## 1st Qu.:10.75 1st Qu.:13.38 S:20 H:10 ## Median :20.50 Median :16.75 S:10 ## Mean :20.50 Mean :17.40 W:10 ## 3rd Qu.:30.25 3rd Qu.:20.77 ## Max. :40.00 Max. :30.40 #Um die Variablen im Dataframe im Folgenden direkt (ohne $ bzw. ohne &quot;data = kormoran&quot;) ansprechen zu koennen attach(kormoran) #Umsortieren der Faktoren, damit sie in den Boxplots eine sinnvolle Reihung haben Jahreszeit&lt;-factor(Jahreszeit,levels=c(&quot;F&quot;,&quot;S&quot;,&quot;H&quot;,&quot;W&quot;)) #Explorative Datenanalyse (zeigt uns die Gesamtverteilung) boxplot(Tauchzeit) boxplot(log10(Tauchzeit)) #Explorative Datenanalyse (Check auf Normalverteilung der Residuen und Varianzhomogenitaet) boxplot(Tauchzeit~Jahreszeit*Unterart) boxplot(log10(Tauchzeit)~Jahreszeit*Unterart) #Vollständiges Modell mit Interaktion model &lt;- aov(Tauchzeit~Unterart*Jahreszeit) model ## Call: ## aov(formula = Tauchzeit ~ Unterart * Jahreszeit) ## ## Terms: ## Unterart Jahreszeit Unterart:Jahreszeit Residuals ## Sum of Squares 106.929 756.170 11.009 84.992 ## Deg. of Freedom 1 3 3 32 ## ## Residual standard error: 1.629724 ## Estimated effects may be unbalanced summary(model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Unterart 1 106.9 106.93 40.259 4.01e-07 *** ## Jahreszeit 3 756.2 252.06 94.901 5.19e-16 *** ## Unterart:Jahreszeit 3 11.0 3.67 1.382 0.266 ## Residuals 32 85.0 2.66 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #p-Wert der Interaktion ist 0.266 #Modellvereinfachung model2 &lt;- aov(Tauchzeit~Unterart+Jahreszeit) model2 ## Call: ## aov(formula = Tauchzeit ~ Unterart + Jahreszeit) ## ## Terms: ## Unterart Jahreszeit Residuals ## Sum of Squares 106.929 756.170 96.001 ## Deg. of Freedom 1 3 35 ## ## Residual standard error: 1.656166 ## Estimated effects may be unbalanced summary(model2) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Unterart 1 106.9 106.93 38.98 3.69e-07 *** ## Jahreszeit 3 756.2 252.06 91.89 &lt; 2e-16 *** ## Residuals 35 96.0 2.74 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #Anderer Weg, um zu pruefen, ob man das komplexere Modell mit Interaktion behalten soll anova(model,model2) ## Analysis of Variance Table ## ## Model 1: Tauchzeit ~ Unterart * Jahreszeit ## Model 2: Tauchzeit ~ Unterart + Jahreszeit ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 32 84.992 ## 2 35 96.001 -3 -11.009 1.3817 0.2661 #in diesem Fall bekommen wir den gleichen p-Wert wie oben (0.266) #Modelldiagnostik par(mfrow=c(2,2)) #alle vier Abbildungen in einem 2 x 2 Raster plot(model2) influence.measures(model2) # kann man sich zusaezlich zum &quot;plot&quot; ansehen, um herauszufinden, ob es evtl. sehr einflussreiche Werte mit Cook&#39;s D von 1 oder grösser gibt ## Influence measures of ## aov(formula = Tauchzeit ~ Unterart + Jahreszeit) : ## ## dfb.1_ dfb.UntS dfb.JhrS dfb.JhrH dfb.JhrW dffit cov.r cook.d ## 1 -1.06739 0.47735 6.75e-01 6.75e-01 6.75e-01 -1.06739 0.461 1.90e-01 ## 2 -0.38948 0.17418 2.46e-01 2.46e-01 2.46e-01 -0.38948 1.133 3.03e-02 ## 3 -0.02285 0.01022 1.44e-02 1.44e-02 1.44e-02 -0.02285 1.320 1.07e-04 ## 4 0.07338 -0.03282 -4.64e-02 -4.64e-02 -4.64e-02 0.07338 1.314 1.11e-03 ## 5 0.44271 -0.19798 -2.80e-01 -2.80e-01 -2.80e-01 0.44271 1.084 3.88e-02 ## 6 -0.05945 0.13293 -1.88e-01 8.33e-18 1.67e-17 -0.29723 1.207 1.79e-02 ## 7 -0.05452 0.12190 -1.72e-01 1.39e-17 6.24e-18 -0.27258 1.225 1.51e-02 ## 8 -0.00120 0.00269 -3.80e-03 1.94e-19 3.87e-19 -0.00601 1.321 7.44e-06 ## 9 0.00361 -0.00807 1.14e-02 -9.29e-19 -5.16e-19 0.01804 1.321 6.70e-05 ## 10 0.09727 -0.21750 3.08e-01 3.16e-17 3.38e-17 0.48634 1.042 4.64e-02 ## 11 -0.10715 0.23960 4.16e-17 -3.39e-01 -4.25e-18 -0.53577 0.991 5.58e-02 ## 12 -0.02722 0.06087 -6.76e-18 -8.61e-02 -8.27e-18 -0.13612 1.296 3.80e-03 ## 13 0.03546 -0.07930 -1.22e-16 1.12e-01 -4.14e-17 0.17731 1.279 6.43e-03 ## 14 0.04520 -0.10108 -5.42e-17 1.43e-01 -1.04e-17 0.22601 1.254 1.04e-02 ## 15 0.05501 -0.12300 -1.02e-16 1.74e-01 -3.54e-17 0.27504 1.223 1.53e-02 ## 16 -0.04618 0.10326 -1.99e-18 1.44e-17 -1.46e-01 -0.23090 1.251 1.09e-02 ## 17 -0.04130 0.09235 1.64e-17 2.87e-17 -1.31e-01 -0.20650 1.265 8.70e-03 ## 18 -0.00265 0.00591 5.88e-18 6.26e-18 -8.36e-03 -0.01323 1.321 3.60e-05 ## 19 0.01660 -0.03713 -1.81e-17 -3.58e-18 5.25e-02 0.08302 1.312 1.42e-03 ## 20 0.31644 -0.70758 -2.70e-16 -2.11e-16 1.00e+00 1.58219 0.165 3.40e-01 ## 21 0.01082 0.00807 -1.14e-02 -1.14e-02 -1.14e-02 0.01804 1.321 6.70e-05 ## 22 0.05415 0.04036 -5.71e-02 -5.71e-02 -5.71e-02 0.09025 1.310 1.67e-03 ## 23 0.06862 0.05115 -7.23e-02 -7.23e-02 -7.23e-02 0.11437 1.303 2.69e-03 ## 24 0.15618 0.11641 -1.65e-01 -1.65e-01 -1.65e-01 0.26029 1.233 1.38e-02 ## 25 0.23067 0.17193 -2.43e-01 -2.43e-01 -2.43e-01 0.38445 1.137 2.95e-02 ## 26 0.03643 -0.08147 -1.15e-01 3.07e-18 -1.04e-18 -0.18217 1.277 6.79e-03 ## 27 0.01709 -0.03820 -5.40e-02 -1.24e-17 -1.14e-17 -0.08543 1.311 1.50e-03 ## 28 0.00746 -0.01667 -2.36e-02 3.88e-18 2.84e-18 -0.03728 1.319 2.86e-04 ## 29 -0.01179 0.02636 3.73e-02 -3.20e-18 -1.03e-18 0.05893 1.316 7.15e-04 ## 30 -0.06539 0.14622 2.07e-01 -2.96e-17 -2.30e-17 0.32696 1.185 2.15e-02 ## 31 0.09315 -0.20829 1.52e-16 -2.95e-01 0.00e+00 -0.46574 1.062 4.27e-02 ## 32 0.04814 -0.10764 1.36e-17 -1.52e-01 -1.87e-17 -0.24068 1.245 1.18e-02 ## 33 0.02384 -0.05331 1.69e-17 -7.54e-02 4.14e-18 -0.11920 1.302 2.92e-03 ## 34 -0.06838 0.15290 2.73e-17 2.16e-01 3.34e-17 0.34189 1.173 2.35e-02 ## 35 -0.09366 0.20943 0.00e+00 2.96e-01 2.53e-17 0.46831 1.059 4.32e-02 ## 36 0.10820 -0.24195 8.09e-17 1.18e-16 -3.42e-01 -0.54101 0.986 5.68e-02 ## 37 0.10297 -0.23025 4.06e-18 -2.94e-17 -3.26e-01 -0.51487 1.013 5.18e-02 ## 38 0.06242 -0.13957 3.29e-17 5.78e-17 -1.97e-01 -0.31208 1.196 1.97e-02 ## 39 -0.02964 0.06629 -2.98e-17 -2.87e-17 9.37e-02 0.14822 1.292 4.50e-03 ## 40 -0.05402 0.12080 -2.32e-17 -2.88e-17 1.71e-01 0.27012 1.226 1.48e-02 ## hat inf ## 1 0.125 * ## 2 0.125 ## 3 0.125 ## 4 0.125 ## 5 0.125 ## 6 0.125 ## 7 0.125 ## 8 0.125 ## 9 0.125 ## 10 0.125 ## 11 0.125 ## 12 0.125 ## 13 0.125 ## 14 0.125 ## 15 0.125 ## 16 0.125 ## 17 0.125 ## 18 0.125 ## 19 0.125 ## 20 0.125 * ## 21 0.125 ## 22 0.125 ## 23 0.125 ## 24 0.125 ## 25 0.125 ## 26 0.125 ## 27 0.125 ## 28 0.125 ## 29 0.125 ## 30 0.125 ## 31 0.125 ## 32 0.125 ## 33 0.125 ## 34 0.125 ## 35 0.125 ## 36 0.125 ## 37 0.125 ## 38 0.125 ## 39 0.125 ## 40 0.125 #Alternative mit log10 model3 &lt;-aov(log10(Tauchzeit)~Unterart+Jahreszeit) model3 ## Call: ## aov(formula = log10(Tauchzeit) ~ Unterart + Jahreszeit) ## ## Terms: ## Unterart Jahreszeit Residuals ## Sum of Squares 0.0627004 0.4958434 0.0562031 ## Deg. of Freedom 1 3 35 ## ## Residual standard error: 0.04007247 ## Estimated effects may be unbalanced summary(model3) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Unterart 1 0.0627 0.06270 39.05 3.64e-07 *** ## Jahreszeit 3 0.4958 0.16528 102.93 &lt; 2e-16 *** ## Residuals 35 0.0562 0.00161 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 plot(model3) #Ergebnisdarstellung par(mfrow=c(1,1)) #Zurückschalten auf Einzelplots library(multcomp) letters&lt;-cld(glht(model2,linfct=mcp(Unterart=&quot;Tukey&quot;))) boxplot(Tauchzeit~Unterart,xlab=&quot;Unterart&quot;,ylab=&quot;Tauchzeit&quot;) mtext(letters$mcletters$Letters,at=1:2) #genaugenommen braucht man bei nur zwei Kategorien keinen post hoc-Test letters&lt;-cld(glht(model2,linfct=mcp(Jahreszeit=&quot;Tukey&quot;))) boxplot(Tauchzeit~Jahreszeit,xlab=&quot;Jahreszeit&quot;,ylab=&quot;Tauchzeit&quot;) #Achsenbeschriftung nicht vergessen! mtext(letters$mcletters$Letters,at=c(1:4)) #Jetzt brauchen wir noch die Mittelwerte bzw. Effektgroessen summary(lm(Tauchzeit~Jahreszeit)) ## ## Call: ## lm(formula = Tauchzeit ~ Jahreszeit) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.820 -1.617 -0.145 1.587 6.980 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 11.8600 0.7508 15.797 &lt; 2e-16 *** ## JahreszeitS 3.2300 1.0618 3.042 0.00437 ** ## JahreszeitH 7.3700 1.0618 6.941 3.92e-08 *** ## JahreszeitW 11.5600 1.0618 10.887 6.11e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.374 on 36 degrees of freedom ## Multiple R-squared: 0.7884, Adjusted R-squared: 0.7708 ## F-statistic: 44.72 on 3 and 36 DF, p-value: 3.156e-12 summary(lm(Tauchzeit~Unterart)) ## ## Call: ## lm(formula = Tauchzeit ~ Unterart) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.535 -3.585 -0.335 3.760 11.365 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 19.035 1.059 17.976 &lt;2e-16 *** ## UnterartS -3.270 1.498 -2.184 0.0352 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.736 on 38 degrees of freedom ## Multiple R-squared: 0.1115, Adjusted R-squared: 0.08811 ## F-statistic: 4.768 on 1 and 38 DF, p-value: 0.03523 aggregate(kormoran[,1],list(Unterart),mean) ## Group.1 x ## 1 C 10.5 ## 2 S 30.5 aggregate(kormoran[,1],list(Jahreszeit),mean) ## Group.1 x ## 1 F 13 ## 2 S 18 ## 3 H 23 ## 4 W 28 knitr::opts_chunk$set(fig.width = 15, fig.height = 12, warning = F, message = F, fig.pos = &#39;H&#39;) #knitr::opts_chunk$get(&quot;root.dir&quot;) "],
["3-5-musterlosung-aufgabe-2-3s-anova-mit-interaktion.html", "3.5 Musterlösung Aufgabe 2.3S: ANOVA mit Interaktion", " 3.5 Musterlösung Aufgabe 2.3S: ANOVA mit Interaktion # klone den originaler Datensatz df &lt;- nova # fasst die vier Inhalte der Gerichte zu drei Inhalten zusammen. df$label_content[grep(&quot;Pflanzlich+&quot;,df$label_content)] &lt;- &quot;Vegetarisch&quot; # gruppiert Daten gemäss Bedingungen, Menü-Inhalt und Wochen df_ &lt;- df %&gt;% group_by(condit, label_content, week) %&gt;% summarise(tot_sold = n()) %&gt;% drop_na() # lasst die unbekannten Menü-Inhalte weg # überprüft Voraussetzungen für eine ANOVA # Boxplots zeigt klare Varianzheterogenität ggplot(df_, aes(x = interaction(label_content, condit), y = tot_sold)) + geom_boxplot(fill=&quot;white&quot;, size = 1) + labs(x = &quot;\\nMenü-Inhalt&quot;, y = &quot;Anzahl verkaufte Gerichte pro Woche\\n&quot;) + mytheme # definiert das Modell mit Interaktion model1 &lt;- aov(tot_sold ~ label_content * condit, data = df_) summary.lm(model1) ## ## Call: ## aov(formula = tot_sold ~ label_content * condit, data = df_) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.0000 -4.0000 -0.1667 1.9167 9.3333 ## ## Coefficients: ## Estimate Std. Error t value ## (Intercept) 27.333 3.127 8.741 ## label_contentFleisch 80.333 4.422 18.166 ## label_contentVegetarisch 24.667 4.422 5.578 ## conditIntervention -3.333 4.422 -0.754 ## label_contentFleisch:conditIntervention -24.000 6.254 -3.838 ## label_contentVegetarisch:conditIntervention 28.333 6.254 4.531 ## Pr(&gt;|t|) ## (Intercept) 1.50e-06 *** ## label_contentFleisch 4.27e-10 *** ## label_contentVegetarisch 0.000120 *** ## conditIntervention 0.465516 ## label_contentFleisch:conditIntervention 0.002363 ** ## label_contentVegetarisch:conditIntervention 0.000689 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.416 on 12 degrees of freedom ## Multiple R-squared: 0.9787, Adjusted R-squared: 0.9698 ## F-statistic: 110.3 on 5 and 12 DF, p-value: 1.334e-09 autoplot(model1) + mytheme # Inspektion der Modellvoraussetzung: ist ok Fazit: Die Inspektion des Modells zeigt keine schwerwiegenden Verletzungen der Modellvoraussetzung. Nächster Schritt post-hoc-Tests nach Tukey. # post-hoc-Tests nach Tukey TukeyHSD(model1) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = tot_sold ~ label_content * condit, data = df_) ## ## $label_content ## diff lwr upr p adj ## Fleisch-Buffet 68.33333 59.99107 76.67559 0.0e+00 ## Vegetarisch-Buffet 38.83333 30.49107 47.17559 1.0e-07 ## Vegetarisch-Fleisch -29.50000 -37.84226 -21.15774 1.9e-06 ## ## $condit ## diff lwr upr p adj ## Intervention-Basis -1.888889 -7.451701 3.673923 0.4736291 ## ## $`label_content:condit` ## diff lwr ## Fleisch:Basis-Buffet:Basis 80.333333 65.47963 ## Vegetarisch:Basis-Buffet:Basis 24.666667 9.81296 ## Buffet:Intervention-Buffet:Basis -3.333333 -18.18704 ## Fleisch:Intervention-Buffet:Basis 53.000000 38.14629 ## Vegetarisch:Intervention-Buffet:Basis 49.666667 34.81296 ## Vegetarisch:Basis-Fleisch:Basis -55.666667 -70.52037 ## Buffet:Intervention-Fleisch:Basis -83.666667 -98.52037 ## Fleisch:Intervention-Fleisch:Basis -27.333333 -42.18704 ## Vegetarisch:Intervention-Fleisch:Basis -30.666667 -45.52037 ## Buffet:Intervention-Vegetarisch:Basis -28.000000 -42.85371 ## Fleisch:Intervention-Vegetarisch:Basis 28.333333 13.47963 ## Vegetarisch:Intervention-Vegetarisch:Basis 25.000000 10.14629 ## Fleisch:Intervention-Buffet:Intervention 56.333333 41.47963 ## Vegetarisch:Intervention-Buffet:Intervention 53.000000 38.14629 ## Vegetarisch:Intervention-Fleisch:Intervention -3.333333 -18.18704 ## upr p adj ## Fleisch:Basis-Buffet:Basis 95.18704 0.0000000 ## Vegetarisch:Basis-Buffet:Basis 39.52037 0.0012966 ## Buffet:Intervention-Buffet:Basis 11.52037 0.9704018 ## Fleisch:Intervention-Buffet:Basis 67.85371 0.0000006 ## Vegetarisch:Intervention-Buffet:Basis 64.52037 0.0000012 ## Vegetarisch:Basis-Fleisch:Basis -40.81296 0.0000003 ## Buffet:Intervention-Fleisch:Basis -68.81296 0.0000000 ## Fleisch:Intervention-Fleisch:Basis -12.47963 0.0005207 ## Vegetarisch:Intervention-Fleisch:Basis -15.81296 0.0001771 ## Buffet:Intervention-Vegetarisch:Basis -13.14629 0.0004174 ## Fleisch:Intervention-Vegetarisch:Basis 43.18704 0.0003741 ## Vegetarisch:Intervention-Vegetarisch:Basis 39.85371 0.0011541 ## Fleisch:Intervention-Buffet:Intervention 71.18704 0.0000003 ## Vegetarisch:Intervention-Buffet:Intervention 67.85371 0.0000006 ## Vegetarisch:Intervention-Fleisch:Intervention 11.52037 0.9704018 3.5.1 Methoden Ziel war es, die Unterschiede in den Verkaufszahlen pro Menü-Inhalt und pro Bedingung aufzuzeigen. Da die Kriteriumsvariable (Verkaufszahlen) metrisch und die beiden Prädiktorvariablen kategorial sind, wurde eine zweifaktorielle ANOVA mit Interaktion gerechnet. Die visuelle Inspektion des Models zeigte keine schwerwiegenden Verletzungen der Voraussetzungen. Um die Einzelvergleiche zu sehen, wurde einen post-hoc-Test nach Tukey durchgeführt. 3.5.2 Ergebnisse Die Menü-Inhalte (Fleisch, Vegetarisch und Buffet) zwischen den Bedingungen Basis oder Interventionswochen unterscheiden sich in den Verkaufszahlen signifikant (F(5, 12) = 110.252, p &lt; .001). Anschliessend durchgeführte post-hoc-Tests (Tukey) zeigen vor allem zwei interessante Ergbenisse: 1) in den Interventionswochen wurden signifikant weniger Fleischgerichte gekauft als in den Basiswochen 2) in den Interventionswochen wurden signifikant mehr vegetarische Gerichte verkauft (siehe Figure 1 oder Figure 2). Figure 3.2: Box-Whisker-Plots der wöchentlichen Verkaufszahlen pro Menü-Inhalte. Kleinbuchstaben bezeichnen homogene Gruppen auf p &lt; .05 nach Tukeys post-hoc-Test. Figure 3.3: Wöchentliche Verkaufszahlen aggregiert für die drei Menü-Inhalte. "],
["4-statistik-3-04-11-2019.html", "Kapitel 4 Statistik 3 (04.11.2019)", " Kapitel 4 Statistik 3 (04.11.2019) Statistik 3 beschäftigt sich schwerpunktmässig mit multiplen Regressionen, bei denen eine abhängige Variable durch zwei oder mehr Prädiktorvariablen erklärt werden soll. Wir thematisieren verschiedene dabei auftretende Probleme und ihre Lösung, insbesondere den Umgang mit korrelierten Prädiktoren und das Aufspüren des besten unter mehreren möglichen statistischen Modellen. Hieran wir auch der informatian theoretician-Ansatz der Statstitik und multimodel inference eingeführt. Dann folgt ein Einstieg in nicht-lineare Regressionen, die es erlauben, etwa Potenzgesetze direkt zu modellieren. Am Ende steht ein kurzer Ausblick auf Glättungsverfahren (LOESS), general additive models (GAMs) und evtl. boosted regression trees (BRTs). "],
["4-1-statistik-3-demoskript.html", "4.1 Statistik 3 - Demoskript", " 4.1 Statistik 3 - Demoskript Polynomische Regression e&lt;-c(20,19,25,10,8,15,13,18,11,14,25,39,38,28,24) f&lt;-c(12,15,10,7,2,10,12,11,13,10,9,2,4,7,13) par(mfrow=c(1,1)) summary(lm(f~e)) plot(f~e,xlim=c(0,40),ylim=c(0,30)) abline(lm(f~e)) par(mfrow=c(2,2)) plot(lm(f~e)) plot(lm(f~e+I(e^2))) summary(lm(f~e+I(e^2))) Multiple lineare Regression basierend auf Logan, Beispiel 9A loyn &lt;- read.table(&quot;15_Statistik3/data/loyn.csv&quot;, header=T,sep=&quot;,&quot;) loyn library(car) summary(loyn) model &lt;- lm (ABUND ~ YR.ISOL + ALT + GRAZE, data=loyn) summary(model) aov(model) par(mfrow=c(2,2)) plot(model) influence.measures(model) cor &lt;- cor(loyn[,2:7]) print(cor, digits=2) cor[abs(cor)&lt;0.6] &lt;- 0 cor print(cor, digits=3) vif(model) Simulation Overfitting test &lt;- data.frame(&quot;x&quot;=c(1,2,3,4,5,6),&quot;y&quot;=c(34,21,70,47,23,45)) attach(test) plot(x,y) lm0=lm(y~1) lm1=lm(y~x) lm2=lm(y~x+I(x^2)) lm3=lm(y~x+I(x^2)+I(x^3)) lm4=lm(y~x+I(x^2)+I(x^3)+I(x^4)) lm5=lm(y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)) lm6=lm(y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)+I(x^6)) summary(lm0) summary(lm1) summary(lm2) summary(lm3) summary(lm4) summary(lm5) xv&lt;-seq(from=0,to=10,by=0.1) plot(x,y,cex=2,col=&quot;black&quot;,lwd=3) yv&lt;-predict(lm1,list(x=xv)) lines(xv,yv,col=&quot;red&quot;,lwd=3) yv&lt;-predict(lm2,list(x=xv)) lines(xv,yv,col=&quot;blue&quot;,lwd=3) yv&lt;-predict(lm3,list(x=xv)) lines(xv,yv,col=&quot;green&quot;,lwd=3) yv&lt;-predict(lm4,list(x=xv)) lines(xv,yv,col=&quot;orange&quot;,lwd=3) yv&lt;-predict(lm5,list(x=xv)) lines(xv,yv,col=&quot;black&quot;,lwd=3) Modellvereinfachung (mit Loyn-Datensatz) model1 &lt;- lm (ABUND ~ YR.ISOL + ALT + GRAZE, data=loyn) summary(model1) model2 &lt;- update(model1,~.-YR.ISOL) summary(model2) anova(model1,model2) Hierarchical partitioning library(hier.part) loyn.preds &lt;-with(loyn, data.frame(YR.ISOL,ALT,GRAZE)) par(mfrow=c(1,1)) hier.part(loyn$ABUND,loyn.preds,gof=&quot;Rsqu&quot;) Partial regressions avPlots(model,ask=F) multimodel inference library(MuMIn) global.model &lt;- lm (ABUND ~ YR.ISOL + ALT + GRAZE, data=loyn) options(na.action=&quot;na.fail&quot;) allmodels &lt;- dredge(global.model) allmodels importance(allmodels) avgmodel&lt;-model.avg(get.models(dredge(model,rank=&quot;AICc&quot;),subset=TRUE)) summary(avgmodel) knitr::opts_chunk$set(results = &quot;hide&quot;, fig.width = 20, fig.height = 12, warning = F, message = F, fig.pos = &#39;H&#39;) "],
["4-2-ubung-3-1-multiple-regression.html", "4.2 Übung 3.1: Multiple Regression", " 4.2 Übung 3.1: Multiple Regression Übungsaufgabe (hier so ausführlich formuliert, wie dies auch in der Klausur der Fall sein wird) Bereiten Sie den Datensatz Ukraine.xlsx für das Einlesen in R vor und lesen Sie ihn dann ein. Dieser enthält gemittelte Pflanzenartenzahlen (Species.richness) von 199 10 m² grossen Plots (Vegetationsaufnahmen) von Steppenrasen in der Ukraine sowie zahlreiche Umweltvariablen, deren Bedeutung und Einheiten im Kopf der ExcelTabelle angegeben sind. Ermitteln Sie ein minimal adäquates Modell, das den Artenreichtum in den Plots durch die Umweltvariablen erklärt. Bitte erklären und begründen Sie die einzelnen Schritte, die Sie unternehmen, um zu diesem Ergebnis zu kommen. Dazu erstellen Sie bitte ein Word-Dokument, in das Sie Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, Ihre Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere Vorgehen dokumentieren. Dieser Ablauf sollte insbesondere beinhalten: Überprüfen der Datenstruktur nach dem Einlesen: welches sind die abhängige(n) und welches die unabängige(n) Variablen, sind alle Variablen für die Analyse geeignet? Explorative Datenanalyse, um zu sehen, ob die abhängige Variable in der vorliegenden Form für die Analyse geeignet ist Definition eines globalen Modelles und dessen Reduktion zu einem minimal adäquaten Modell Durchführen der Modelldiagnostik für dieses Generieren aller Zahlen, Statistiken und Tabellen, die für eine wiss. Ergebnisdarstellung benötigt werden Formulieren Sie abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden. Abzugeben sind am Ende (a) Ein lauffähiges R-Skript; (b) begründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation) und (c) ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit). "],
["4-3-loesung-ubung-3-1-multiple-regression.html", "4.3 Loesung Übung 3.1: Multiple Regression", " 4.3 Loesung Übung 3.1: Multiple Regression # Aus der Excel-Tabelle wurde das relevante Arbeitsblatt als csv gespeichert ukraine &lt;-read.csv(&quot;15_Statistik3/data/Ukraine_bearbeitet.csv&quot;,sep=&quot;;&quot;) attach(ukraine) ukraine str(ukraine) summary(ukraine) Man erkennt, dass alle Spalten bis auf die erste mit der Plot ID numerisch (num oder int) und dass die abhängige Variable in Spalte 2 sowie die Prediktorvariablen in den Spalten 3 bis 23 stehen. #Explorative Datenanalyse der abhängigen Variablen boxplot(Species.richness) Der Boxplot sieht sehr gut symmetrisch aus. Insofern gibt es keinen Anlass über eine Transformation nachzudenken. (Da es sich bei Artenzahlen um Zähldaten handelt, müsste man theoretisch ein glm mit Poisson-Verteilung rechnen; bei einem Mittelwert, der hinreichend von Null verschieden ist (hier: ca. 40), ist eine Poisson-Verteilung aber praktisch nicht von einer Normalverteilung zu unterscheiden und wir können uns den Aufwand auch sparen). cor &lt;- cor(ukraine[,3:23]) cor cor[abs(cor)&lt;0.7] &lt;- 0 cor Die Korrelationsanalyse dient dazu, zu entscheiden, ob die Prädiktorvariablen hinreichend voneinander unabhängig sind, um alle in das globale Modell hinein zu nehmen. Bei Pearson’s Korrelationskoeffizienten r, die betragsmässig grösser als 0.7 sind, würde es problematisch. Alternativ hätten wir auch den VIF (Variance Inflation Factor) als Kriterium für den möglichen Ausschluss von Variablen aus dem globalen Modell nehmen können. Diese initiale Korrelationsanalyse zeigt uns aber, dass unsere Daten noch ein anderes Problem haben: für die drei Korngrössenfraktionen des Bodens (Sand, Silt, Clay) stehen lauter NA’s. Um herauszufinden, was das Problem ist, geben wir ein: summary(ukraine$Sand) Da gibt es offensichtlich je ein NA in jeder dieser Zeilen. Jetzt können wir entscheiden, entweder auf die drei Variablen oder auf die eine Beobachtung zu verzichten. Da wir eh schon eher mehr unabhängige Variablen haben als wir händeln können, entscheide ich pragmatisch für ersteres. Wir rechnen die Korrelation also noch einmal ohne diese drei Spalten (es sind die Nummern 12:14, wie wir aus der anfänglichen Variablenbetrachtung oben wissen). cor &lt;- cor(ukraine[,c(3:11,15:23)]) cor[abs(cor)&lt;0.7] &lt;- 0 cor Wenn man auf cor nun doppel-clickt und es in einem separaten Fenster öffnet, sieht man, wo es problematische Korrelationen zwischen Variablenpaaren gibt. Es sind dies Altitude vs. Temperature und N.total vs. C.org. Wir müssen aus jedem dieser Paare jetzt eine Variable rauswerfen, am besten jene, die weniger gut interpretierbar ist. Ich entscheide mich dafür Temperature statt Altitude (weil das der direktere ökologische Wirkfaktor ist) und C.org statt N.total zu behalten (weil es in der Literatur mehr Daten zum Humusgehalt als zum N-Gehalt gibt, damit eine bessere Vergleichbarkeit erzielt wird). Die Aussagen, die wir für die beibehaltene Variable erzielen, stehen aber +/- auch für die entfernte. Das Problem ist aber, dass wir immer noch 16 Variablen haben, was einen sehr leistungsfähigen Rechner oder sehr lange Rechenzeit erfordern würde. Wir sollten also unter 15 Variablen kommen. Wir könnten uns jetzt überlegen, welche uns ökologisch am wichtigsten sind, oder ein noch strengeres Kriterium bei r verwenden, etwa 0.6 cor &lt;- cor(ukraine[,c(3:11,15:23)]) cor[abs(cor)&lt;0.6] &lt;- 0 cor Entsprechend „werfen“ wir auch noch die folgenden Variablen „raus“: Temperature.range (positiv mit Temperature), Precipitation (negativ mit Temperature) sowie Conductivity (positiv mit pH). Nun können wir das globale Modell definieren, indem wir alle verbleibenden Variablen aufnehmen, das sind 13. (Wenn das nicht eh schon so viele wären, dass es uns an die Grenze der Rechenleistung bringt, hätten wir auch noch darüber nachdenken können, einzelne quadratische Terme oder Interaktionsterme zu berücksichtigen). global.model &lt;- lm (Species.richness ~ Inclination+Heat.index+Microrelief+Grazing.intensity+Litter+ Stones.and.rocks+Gravel+Fine.soil+pH+CaCO3+C.org+CN.ratio+Temperature) Nun gibt es im Prinzip zwei Möglichkeiten, vom globalen (vollen) Modell zu einem minimal adäquaten Modell zu kommen. (1) Der Ansatz der „frequentist statistic“, in dem man aus dem vollen Modell so lange schrittweise Variablen entfernt, bis nur noch signifikante Variablen verbleiben. (2) Den informationstheoretischen Ansatz, bei dem alle denkbaren Modelle berechnet und verglichen werden (also alle möglichen Kombinationen von 13,12,…, 1, 0 Parametern). Diese Lösung stelle ich im Folgenden vor: #Multimodel inference library(MuMIn) options(na.action=&quot;na.fail&quot;) allmodels&lt;-dredge(global.model) allmodels Jetzt bekommen wir die besten der insgesamt 8192 möglichen Modelle gelistet mit ihren Parameterschätzungen und ihrem AICc. Das beste Modell umfasst 5 Parameter (CaCO3, CN.ratio, Grazing.intensity. Heat.index, Litter). Allerdings ist das nächstbeste Modell (mit 6 Parametern) nur wenig schlechter (delta AICc = 0.71), was sich in fast gleichen (und zudem sehr niedrigen) Akaike weights bemerkbar macht. Nach dem Verständnis des Information theoretician approach, sollte man in einer solchen Situation nicht das eine „beste“ Modell benennen, sondern eine Aussage über die Gruppe der insgesamt brauchbaren Modelle treffen. Hierzu kann man (a) Importance der Parameter über alle Modelle hinweg berechnen (= Summe der Akaike weights aller Modelle, die den betreffenden Parameter enthalten) und/oder (b) ein nach Akaike weights gemitteltes Modell berechnen. #Importance values der Variablen importance(allmodels) Demnach ist Heat.index die wichtigste Variable (in 100% aller relevanten Modelle), während ferner Litter, CaCO3, CN.ratio und Grazing.intensity in mehr als 50% der relevanten Modelle enthalten sind. #Modelaveraging (Achtung: dauert mit 13 Variablen einige Minuten) summary(model.avg(get.models(dredge(global.model,rank=&quot;AICc&quot;),subset =TRUE))) Aus dem gemittelten Modell können wir die Richtung der Beziehung (positiv oder negativ) und ggf. die Effektgrössen (wie verändert sich die Artenzahl, wenn die Prädiktorvariable um eine Einheit zunimmt?) ermitteln. #Modelldiagnostik nicht vergessen par(mfrow=c(2,2)) plot(global.model) plot(lm(Species.richness~Heat.index+Litter+CaCO3+CN.ratio+Grazing.intensity)) Wie immer kommt am Ende die Modelldiagnostik. Wir können uns entweder das globale Modell oder das Modell mit den 5 Variablen mit importance &gt; 50% anschauen. Das Bild sieht fast identisch aus und zeigt keinerlei problematische Abweichungen, d. h. links oben weder ein Keil, noch eine Banane, rechts oben eine nahezu perfekte Gerade. Darstellung der Vorgehensweise und Ergebnisse (in einer wiss. Arbeit) Methoden Es wurden Pflanzenartenzahlen von Steppenrasen in der Ukraine auf 199 10 m² grossen Probeflächen erhoben und zu diesen 23 Umweltvariablen erhoben. Da jeweils ein Messwert fehlte, wurden die Bodenvariablen Sand-, Schluff- und Tongehalt von der weiteren statistischen Analyse ausgeschlossen. Mittels Pearson’s Korrelationskoeffizient wurde auf Abhängigkeiten zwischen den Prädiktorvariablen getestet und aus Paaren mit einer Beziehung mit |r| &gt; 0.6 nur jeweils eine Variable beibehalten. Entsprechend wurden die hoch mit Jahresmitteltemperatur korrelierten Werte Meereshöhe (negativ), Temperaturamplitude (positiv) und Niederschlag (negativ) ausgeschlossen, ferner Leitfähigkeit (positiv mit pH) und Stickstoffgehalt (positiv mit organischem Kohlenstoffgehalt). Damit umfasste das globale lineare Modell (Funktion lm in R) die in Tab. 1 aufgeführten Variablen. Quadratische Terme oder Interaktionen zwischen Variablen wurden nicht berücksichtigt. Auf ein glm mit Poisson-Regression wurde verzichtet, da eine visuelle Inspektion eines Boxplots der Artenzahlen keine relevanten Abweichungen von einer Normalverteilung ergab. Die Modellauswahl fand mittels Multimodel Inference (dredge-Funktion im MuMIn package in R) statt. Die Modellgüte wurde mittels AICc beurteilt. Zur Beurteilung der Bedeutung von Variablen wurden Importance Values (Summe der Akaike weights in allen Modellen, die die betreffende Variable beinhalten) ausgerechnet. Die Richtung der Beziehung wurde aus der Parameterschätzung im globalen Modell bestimmt. Schliesslich wurde ein gemitteltes Modell (aller möglichen Modelle, gewichtet nach deren Akaike weights) erstellt, um die Effektgrössen zu bestimmen. Die Adäquanz des gewählten Modells wurde in Residualplots visuell inspiziert und ergab keine nennenswerten Verletzungen der Voraussetzungen eines parametrischen Modells. Ergebnisse Das beste Modell nach AICc beinhaltete die Variablen CaCO3, CN-Verhältnis, Beweidungsintensität, Heat load-Index und Streuauflage. Da sich die nächstbesten Modelle aber um weniger als DeltaAICc = 2 unterscheiden, wurden für die Gesamtbeurteilung die Importance values sowie die Koeffizienten des über alle 8192 betrachteten Modelle nach Akaike weights gemittelten average models herangezogen (Tab. 1). Mit einem Importance value von nahezu 100% war der Heat load-Index die wichtigste Einflussgrösse (negativ). Vier weitere Umweltvariablen waren ebenfalls in mehr als 50% der statistisch relevanten Modelle enthalten (in dieser Reihenfolge): Tab. 1. Ergebnisse der Multimodel Inference. Die Variablen sind nach absteigenden Importance values sortiert. Die Parameterschätzung bezieht sich nur auf die Modelle, welche die entsprechende Variable beinhalten und ist nach Akaike weights gewichtet. Unter „Hochkorrelierte Variablen“ sind jene aufgeführt, die wegen ihres engen Zusammenhangs mit der genannten Variablen nicht ins globale Modell aufgenommen wurden. "],
["4-4-ubungen-1.html", "4.4 Übungen", " 4.4 Übungen 4.4.1 Aufgabe 3.1: Multiple Regression Artenzahlen von Vegetationsaufnahmen in der Ukraine vs. diverse Umweltparameter (farbig gruppiert nach Kategorien, aus Kuzemko et al. 2016) Ukraine.xlsx Bestimmt ein minimal adäquates Modell für die Erklärung der Artenzahlen mit allen notwendigen Arbeitsschritten Wahlweise könnt ihr mit AICc und dredge oder mit p-Werten und schrittweiser Vereinfachung eines globalen Models arbeiten "],
["5-statistik-4-05-11-2019.html", "Kapitel 5 Statistik 4 (05.11.2019)", " Kapitel 5 Statistik 4 (05.11.2019) In Statistik 4 lernen die Studierenden Lösungen kennen, welche die diverse Limitierungen von linearen Modellen überwinden, namentlich linear mixed models (LMMs), generalized linear models (GLMs) und generalized linear mixed models (GLMMs). Dabei bezeichnet generalized die explizite Modellierung anderer Fehler- und Varianzstrukturen und mixed die Berücksichtigung von Abhängigkeiten unter den Beobachtungen. Bei generalized linear regressions muss man sich zwischen verschiedenen Verteilungen und link-Strukturen entscheiden. Spezifisch werden wir uns die logistische Regression für ja/nein-Daten anschauen. Abschliessend gibt es eine kurze Einführung in (G)LMMs, die eine Verallgemeinerung von ANOVAs für komplexere experimentelle Setups darstellen. "],
["5-1-statistik-4-demoskript.html", "5.1 Statistik 4 - Demoskript", " 5.1 Statistik 4 - Demoskript (c) Juergen Dengler, 05.11.2018 compensation&lt;-read.table(&quot;16_Statistik4/data/ipomopsis.csv&quot;, header=T,sep=&quot;,&quot;) summary(compensation) attach(compensation) plot(Fruit~Root) plot(Fruit~Grazing) tapply(Fruit,Grazing,mean) model&lt;-lm(Fruit~Root*Grazing) summary.aov(model) model2&lt;-lm(Fruit~Grazing*Root) summary.aov(model2) model3&lt;-lm(Fruit~Grazing+Root) summary.lm(model3) #Plotten der Ergebnisse plot(Fruit~Root,pch=21,bg=(1+as.numeric(Grazing))) #legend(locator(1),c(&quot;grazed&quot;,&quot;ungrazed&quot;),col=c(2,3),pch=16) abline(-127.829,23.56,col=&quot;red&quot;) abline(-127.892+36.103,23.56,col=&quot;green&quot;) ##Nicht-lineare Regression library(AICcmodavg) library(nlstools) loyn &lt;- read.table(&quot;16_Statistik4/data/loyn.csv&quot;, header=T,sep=&quot;,&quot;) attach(loyn) #Selbstdefinierte Funktion, hier Potenzfunktion power.model&lt;-nls(ABUND~c*AREA^z,start=(list(c=1,z=0))) summary(power.model) AICc(power.model) #Modeldiagnostik (in nlstools) plot(nlsResiduals(power.model)) #Vordefinierte &quot;Selbststartfunktionen&quot;# ?selfStart logistic.model&lt;-nls(ABUND~SSlogis(AREA,Asym,xmid,scal)) summary(logistic.model) AICc(logistic.model) #Modeldiagnostik (in nlstools) plot(nlsResiduals(logistic.model)) #Visualisierung plot(ABUND~AREA) par(mfrow=c(1,1)) xv&lt;-seq(0,2000,0.01) # 1. Potenzfunktion yv1 &lt;-predict(power.model,list(AREA=xv)) lines(xv,yv1,col=&quot;green&quot;) # 2. Logistische Funktion yv2 &lt;-predict(logistic.model,list(AREA=xv)) lines(xv,yv2,col=&quot;blue&quot;) #Visualisierung II plot(ABUND~log10(AREA)) par(mfrow=c(1,1)) # 1. Potenzfunktion yv1 &lt;-predict(power.model,list(AREA=xv)) lines(log10(xv),yv1,col=&quot;green&quot;) # 2. Logistische Funktion yv2 &lt;-predict(logistic.model,list(AREA=xv)) lines(log10(xv),yv2,col=&quot;blue&quot;) #Model selection among several non-linear models cand.models&lt;-list() cand.models[[1]]&lt;-power.model cand.models[[2]]&lt;-logistic.model Modnames &lt;- c(&quot;Power&quot;, &quot;Logistic&quot;) aictab(cand.set = cand.models, modnames = Modnames) ##Smoother attach(loyn) log_AREA&lt;-log10(AREA) plot(ABUND~log_AREA) lines(lowess(log_AREA,ABUND,f=0.25),lwd=2,col=&quot;red&quot;) lines(lowess(log_AREA,ABUND,f=0.5),lwd=2,col=&quot;blue&quot;) lines(lowess(log_AREA,ABUND,f=1),lwd=2,col=&quot;green&quot;) #GAMs library(mgcv) model&lt;-gam(ABUND~s(log_AREA)) model summary(model) plot(log_AREA,ABUND,pch=16) xv&lt;-seq(-1,4,by=0.1) yv&lt;-predict(model,list(log_AREA=xv)) lines(xv,yv,lwd=2,col=&quot;red&quot;) AICc(model) summary(model) ##von LMs zu GLMs temp&lt;-c(10,12,16,20,24,25,30,33,37) besucher&lt;-c(40,12,50,500,400,900,1500,900,2000) strand&lt;-data.frame(&quot;Temperatur&quot;=temp,&quot;Besucher&quot;=besucher) attach(strand) par(mfrow=c(1,1)) plot(besucher~temp) lm.strand&lt;-lm(Besucher~Temperatur, data=strand) summary(lm.strand) par(mfrow=c(2,2)) plot(lm.strand) par(mfrow=c(1,1)) xv&lt;-rep(0:40,by=.1) yv&lt;-predict(lm.strand,list(Temperatur=xv)) plot(Temperatur,Besucher,xlim=c(0,40)) lines(xv,yv,lwd=3,col=&quot;blue&quot;) glm.gaussian&lt;-glm(Besucher~Temperatur,family=gaussian) glm.poisson&lt;-glm(Besucher~Temperatur,family=poisson) summary(glm.gaussian) summary(glm.poisson) glm.quasi&lt;-glm(Besucher~Temperatur,family=quasipoisson) summary(glm.quasi) par(mfrow=c(2,2)) plot(glm.gaussian) plot(glm.poisson) plot(glm.quasi) par(mfrow=c(1,1)) plot(Temperatur,Besucher,xlim=c(0,40)) xv&lt;-rep(0:40,by=.1) yv&lt;-predict(lm.strand,list(Temperatur=xv)) lines(xv,yv,lwd=3,col=&quot;blue&quot;) yv2&lt;-predict(glm.poisson,list(Temperatur=xv)) lines(xv,exp(yv2),lwd=3,col=&quot;red&quot;) yv3&lt;-predict(glm.quasi,list(Temperatur=xv)) lines(xv,exp(yv3),lwd=3,col=&quot;green&quot;) #test for overdispersion library(AER) dispersiontest(glm.poisson) ##logistic regression bathing&lt;-data.frame(&quot;temperature&quot;=c(1,2,5,9,14,14,15,19,22,24,25,26,27,28,29), &quot;bathing&quot;=c(0,0,0,0,0,1,0,0,1,0,1,1,1,1,1)) plot(bathing~temperature, data=bathing) model&lt;-glm(bathing~temperature,data=bathing,family=&quot;binomial&quot;) summary(model) #Modeldiagnostik (wenn nicht signifikant, dann OK) 1 - pchisq (model$deviance,model$df.resid) #Modellge (pseudo-R) 1 - (model$dev / model$null) #Steilheit der Beziehung (relative Aenderung der odds bei x + 1 vs. x) exp(model$coefficients[2]) #LD50 (also hier: Temperatur, bei der 50% der Touristen baden) -model$coefficients[1]/model$coefficients[2] #Vorhersagen predicted &lt;- predict(model, type=&quot;response&quot;) #Konfusionsmatrix km &lt;- table(bathing$bathing, predicted &gt; 0.5) km # Missklassifizierungsrate 1-sum(diag(km)/sum(km)) #Plotting xs&lt;-seq(0,30,l=1000) model.predict&lt;-predict(model,type=&quot;response&quot;,se=T,newdata=data.frame(temperature=xs)) plot(bathing~temperature,data=bathing,xlab=&quot;Temperature (C)&quot;,ylab=&quot;% Bathing&quot;,pch=16, col=&quot;red&quot;) points(model.predict$fit ~ xs,type=&quot;l&quot;) lines(model.predict$fit+model.predict$se.fit ~ xs, type=&quot;l&quot;,lty=2) lines(model.predict$fit-model.predict$se.fit ~ xs, type=&quot;l&quot;,lty=2) "],
["5-2-statistik-4-ubungen.html", "5.2 Statistik 4: Übungen", " 5.2 Statistik 4: Übungen Abzugeben sind am Ende lauffähiges R-Skript begründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation) ausformulierter Methoden- und Ergebnisteil (für eine wiss.Arbeit). Bitte erklärt und begründet die einzelnen Schritte, die ihr unternehmt, um zu eurem Ergebnis zu kommen. Dazu erstellt bitte ein Word-Dokument, in dem ihr Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, eure Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere Vorgehen dokumentiert. Dieser Ablauf sollte insbesondere beinhalten: Überprüfen der Datenstruktur nach dem Einlesen, welches sind die abhängige(n) und welches die unabängige(n) Variablen etc. Explorative Datenanalyse, um zu sehen, ob evtl. Dateneingabefehler vorliegen oder Datentransformationen vorgenommen werden sollten Auswahl und Begründung eines statistischen Verfahrens Bestimmung des vollständigen/maximalen Models Selektion des/der besten Models/Modelle Durchführen der Modelldiagnostik für dieses Generieren aller Zahlen, Statistiken und Tabellen, die für eine wiss. Ergebnisdarstellung benötigt werden Formuliert abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen/Tabellen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (je einen ausformulierten Absatz von ca. 60-100 Worten bzw. 3-8 Sätzen). Alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden. 5.2.1 Übung 4.1: Nicht-lineare Regression (naturwissenschaftlich) Datensatz Curonian_Spit.xlsx Dieser enthält gemittelte Pflanzenartenzahlen (Species.richness) von geschachtelten Plots (Vegetationsaufnahmen) der Pflanzengesellschaft LolioCynosuretum im Nationalpark Kurische Nehrung (Russland) auf Flächengrössen (Area) von 0.0001 bis 900 m². Ermittelt den funktionellen Zusammenhang (das beste Modell), der die Zunahme der Artenzahl mit der Flächengrösse am besten beschreibt.Berücksichtigt dabei mindestens die Potenzfunktion (power function,\\(S=\\)c\\(A^{n}\\)), die logarithmische Funktion (logarithmic function,\\(S\\)=\\(b_{0}\\)\\(+\\)\\(b_{1}\\)\\(\\log_{10}\\)A)und eine Funktion mit Sättigung (saturation, asymptote) eurer Wahl. 5.2.2 Übung 4.2N: Multiple logistische Regression (naturwissenschaftlich) Datensatz isolation.csv Dieser enthält für 50 Inseln die Information, ob eine bestimmte Vogelart dort vorkommt (incidence = 1) oder nicht vorkommt (incidence = 0). Für jede der Inseln sind zudem zwei Umweltvariablen angegeben: area (Fläche in km²) und isolation (Entfernung vom Festland in km). Ermittelt das minimal adäquate statistische Modell, das die Vorkommenswahrscheinlichkeit der Vogelart in Abhängigkeit von Flächengrösse und Entfernung beschreibt. 5.2.3 Übung 4.2S: Multiple logistische Regression (sozioökonomisch) Führt mit dem Datensatz novanimal.csv eine logistische Regression durch.Kann der Fleischkonsum durch das Geschlecht, die Hochschulzugehörigkeit und das Alter erklärt werden? Hinweise: Generiert eine neue Variable “Fleisch” (0 = kein Fleisch, 1 = Fleisch) Entfernt fehlende Werte aus der Variable “Fleisch” Lasst für die Analyse den Menü-Inhalt «Buffet» weg Definiert das Modell und wendet es auf den Datensatz an Berechnet eine Vorhersage des Modells mit predict() Eruiert den Modellfit und die Modellgenauigkeit Berechnet eine Konfusionsmatrix und zieht euer Fazit daraus knitr::opts_chunk$set(fig.width = 20, fig.height = 12, warning = F, message = F, fig.pos = &#39;H&#39;,results = &quot;show&quot;) "],
["5-3-musterlosung-aufgabe-4-1-nicht-lineare-regression.html", "5.3 Musterlösung Aufgabe 4.1: Nicht-lineare Regression", " 5.3 Musterlösung Aufgabe 4.1: Nicht-lineare Regression Übungsaufgabe (hier so ausführlich formuliert, wie dies auch in der Klausur der Fall sein wird) Laden Sie den Datensatz Curonia_spit.xlsx. Dieser enthält gemittelte Pflanzenartenzahlen (Species.richness) von geschachtelten Plots (Vegetationsaufnahmen) der Pflanzengesellschaft Lolio-Cynosuretum im Nationalpark Kurische Nehrung (Russland) auf Flächengrössen (Area) von 0.0001 bis 900 m². Ermitteln Sie den funktionellen Zusammenhang, der die Zunahme der Artenzahl mit der Flächengrösse am besten beschreibt. Berücksichtigen Sie dabei mindestens die Potenzfunktion (power function), die logarithmische Funktion (logarithmic function) und eine Funktion mit Sättigung (saturation, asymptote) Ihrer Wahl Bitte erklären und begründen Sie die einzelnen Schritte, die Sie unternehmen, um zu diesem Ergebnis zu kommen. Dazu erstellen Sie bitte ein Word-Dokument, in das Sie Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, Ihre Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere Vorgehen dokumentieren. Dieser Ablauf sollte insbesondere beinhalten: Überprüfen der Datenstruktur nach dem Einlesen, welches sind die abhängige(n) und welches die unabängige(n) Variablen Explorative Datenanalyse, um zu sehen, ob eine nicht-lineare Regression überhaupt nötig ist und ob evtl. Dateneingabefehler vorliegen vorgenommen werden sollten Definition von mindestens drei nicht-linearen Regressionsmodellen Selektion des/der besten Models/Modelle Durchführen der Modelldiagnostik für die Modelle in der engeren Auswahl, um zu entscheiden, ob das gewählte Vorgehen korrekt war oder ggf. angepasst werden muss Generieren aller Zahlen, Statistiken und Tabellen, die für eine wiss. Ergebnisdarstellung benötigt werden Formulieren Sie abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden. Abzugeben sind am Ende (a) Ein lauffähiges R-Skript; (b) begründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation) und (c) ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit). 5.3.1 Übung 4.1 - Nicht-lineare Regression – Lösung Aus der Excel-Tabelle wurde das relevante Arbeitsblatt als csv gespeichert SAR&lt;-read.csv(&quot;16_Statistik4/data/Curonian_Spit.csv&quot;,sep=&quot;;&quot;) str(SAR) ## &#39;data.frame&#39;: 16 obs. of 2 variables: ## $ Area : num 0.0001 0.0025 0.01 0.0625 0.25 1 4 9 16 25 ... ## $ Species.richness: num 2.1 9.1 14.3 23.1 30.1 37.4 48.5 54.5 58 59.9 ... summary(SAR) ## Area Species.richness ## Min. : 0.0001 Min. : 2.10 ## 1st Qu.: 0.2031 1st Qu.:28.35 ## Median : 12.5000 Median :56.25 ## Mean :147.1453 Mean :50.09 ## 3rd Qu.:131.2500 3rd Qu.:69.95 ## Max. :900.0000 Max. :92.40 attach(SAR) #Explorative Datenanalyse plot(Species.richness~Area) Es liegt in der Tat ein nicht-linearer Zusammenhang vor, der sich gut mit nls analysieren lässt. Die Daten beinhalten keine erkennbaren Fehler, da der Artenreichtum der geschachtelten Plots mit der Fläche ansteigt. #Potenzfunktion selbst definiert #power.model&lt;-nls(Species.richness~c*Area^z) #summary(power.model) Falls die Funktion so keine Ergebnisse liefert, oder das Ergebnis unsinnig aussieht, wenn man es später plottet, müsste man hier geeignete Startwerte angeben, die man aus der Betrachtung der Daten oder aus Erfahrungen mit der Funktion für ähnliche Datensets gewinnt,etwa so: power.model&lt;-nls(Species.richness~c*Area^z, start=(list(c=1,z=0.2))) summary(power.model) ## ## Formula: Species.richness ~ c * Area^z ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## c 36.168960 1.408966 25.67 3.56e-13 *** ## z 0.138941 0.007472 18.60 2.88e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.142 on 14 degrees of freedom ## ## Number of iterations to convergence: 9 ## Achieved convergence tolerance: 8.138e-06 Das Ergebnis ist identisch #logarithmische Funktion selbst definiert logarithmic.model&lt;-nls(Species.richness~b0+b1*log10(Area)) summary(logarithmic.model) ## ## Formula: Species.richness ~ b0 + b1 * log10(Area) ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## b0 43.333 1.358 31.91 1.78e-14 *** ## b1 13.281 0.654 20.31 8.75e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.265 on 14 degrees of freedom ## ## Number of iterations to convergence: 1 ## Achieved convergence tolerance: 3.56e-09 # Zu den verschiedenen Funktionen mit Sättigungswert (Asymptote) gehören Michaelis-Menten, das aymptotische Modell durch den Ursprung und die logistische # Funktion. Die meisten gibt es in R # als selbststartende Funktionen, was meist besser funktioniert als # wenn man sich selbst Gedanken # über Startwerte usw. machen muss. Man kann sie aber auch selbst definieren Im Folgenden habe ich ein paar unterschiedliche Sättigungsfunktionen mit verschiedenen Einstellungen durchprobiert, um zu zeigen, was alles passieren kann… micmen.model&lt;-nls(Species.richness~SSmicmen(Area, Vm, K)) summary(micmen.model) ## ## Formula: Species.richness ~ SSmicmen(Area, Vm, K) ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## Vm 72.0108 4.2708 16.861 1.07e-10 *** ## K 0.8477 0.4371 1.939 0.0729 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 11.96 on 14 degrees of freedom ## ## Number of iterations to convergence: 0 ## Achieved convergence tolerance: 3.377e-06 #Dasselbe selbst definiert (mit default-Startwerten) micmen.model2&lt;-nls(Species.richness~Vm*Area/(K+Area)) summary(micmen.model2) ## ## Formula: Species.richness ~ Vm * Area/(K + Area) ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## Vm 46.7020 9.6748 4.827 0.000268 *** ## K -2.1532 0.5852 -3.679 0.002477 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 35.14 on 14 degrees of freedom ## ## Number of iterations to convergence: 23 ## Achieved convergence tolerance: 9.113e-06 Hier ist das Ergebnis deutlich verschieden, ein Phänomen, das einem bei nicht-linearen Regressionen anders als bei linearen Regressionen immer wieder begegnen kann, da der Iterationsalgorithmus in lokalen Optima hängen bleiben kann. Oftmals dürfte die eingebaute Selbststartfunktion bessere Ergebnisse liefern, aber das werden wir unten sehen. #Dasselbe selbst definiert (mit sinnvollen Startwerten, basierend auf dem Plot) micmen.model3&lt;- nls(Species.richness~Vm*Area/(K+Area),start=list(Vm=100,K=1)) summary(micmen.model3) ## ## Formula: Species.richness ~ Vm * Area/(K + Area) ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## Vm 72.0111 4.2708 16.861 1.07e-10 *** ## K 0.8477 0.4371 1.939 0.0729 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 11.96 on 14 degrees of freedom ## ## Number of iterations to convergence: 22 ## Achieved convergence tolerance: 7.026e-06 Wenn man sinnvollere Startwerte als die default-Werte (1 für alle Parameter) eingibt, hier etwas einen mutmasslichen Asymptoten-Wert (aus der Grafik) von Vm = ca. 100, dann bekommt man das gleiche Ergebnis wie bei der Selbsstartfunktion #Eine asymptotische Funktion durch den Ursprung (mit implementierter Selbststartfunktion) asym.model&lt;-nls(Species.richness~SSasympOrig(Area, Asym, lrc)) summary(asym.model) ## ## Formula: Species.richness ~ SSasympOrig(Area, Asym, lrc) ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## Asym 68.5066 4.4278 15.472 3.38e-10 *** ## lrc 0.1184 0.4864 0.244 0.811 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 13.88 on 14 degrees of freedom ## ## Number of iterations to convergence: 0 ## Achieved convergence tolerance: 2.808e-06 #Logistische Regression als Selbststart-Funktion #logistic.model&lt;-nls(Species.richness~SSlogis(Area,asym,xmid,scal)) Error in nls(y ~ 1/(1 + exp((xmid - x)/scal)), data = xy, start = list(xmid= aux[1L], : Iterationenzahl überschritt Maximum 50 Das ist etwas, was einem bei nls immer wieder passieren kann. Die Iteration ist nach der eingestellten max. Iterationszahl noch nicht zu einem Ergebnis konvergiert. Um ein Ergebnis für diese Funktion zu bekommen, müsste man mit den Einstellungen von nls „herumspielen“, etwas bei den Startwerten oder den max. Um das effizient zu machen, braucht man aber etwas Erfahrung Interationszahlen (man kann z. B. manuell die Maximalzahl der Iterationen erhöhen, indem man in den Funktionsaufruf etwa maxiter =100 als zusätzliches Argument reinschreibtn). Da wir aber schon mehrere funktionierende Funktionen mit oberem Grenzwert haben –und damit die Aufgabe erfüllt – lassen wir es hier. #Vergleich der Modellgüte mittels AICc library(AICcmodavg) cand.models&lt;-list() cand.models[[1]]&lt;-power.model cand.models[[2]]&lt;-logarithmic.model cand.models[[3]]&lt;-micmen.model cand.models[[4]]&lt;-micmen.model2 cand.models[[5]]&lt;-asym.model Modnames&lt;-c(&quot;Power&quot;,&quot;Logarithmic&quot;,&quot;Michaelis-Menten (SS)&quot;,&quot;Michaelis-Menten&quot;,&quot;Asymptotic through origin&quot;) aictab(cand.set=cand.models,modnames=Modnames) ## ## Model selection based on AICc: ## ## K AICc Delta_AICc AICcWt Cum.Wt LL ## Power 3 96.75 0.00 0.98 0.98 -44.38 ## Logarithmic 3 104.43 7.68 0.02 1.00 -48.21 ## Michaelis-Menten (SS) 3 130.67 33.92 0.00 1.00 -61.34 ## Asymptotic through origin 3 135.44 38.69 0.00 1.00 -63.72 ## Michaelis-Menten 3 165.17 68.42 0.00 1.00 -78.58 Diese Ergebnistabelle vergleicht die Modellgüte zwischen den fünf Modellen, die wir in unsere Auswahl reingesteckt haben. Alle haben drei geschätzte Parameter (K), also zwei Funktionsparameter und die Varianz. Das beste Modell (niedrigster AICc bzw. Delta = 0) hat das Potenzmodell (power). Das zweitbeste Modell (logarithmic) hat bereits einen Delta-AICc von mehr als 4, ist daher statistisch nicht relevant. Das zeigt sich auch am Akaike weight, das für das zweite Modell nur noch 2 % ist. Die verschiedenen Modelle mit oberem Grenzwert (3-5) sind komplett ungeeignet. #Modelldiagnostik für das beste Modell library(nlstools) plot(nlsResiduals(power.model)) Links oben sieht man zwar ein Muster (liegt daran, dass in diesem Fall die Plots geschachtelt, und nicht unabhängig waren), aber jedenfalls keinen problematischen Fall wie einen Bogen oder einen Keil. Der QQ-Plot rechts unten ist völlig OK. Somit haben wir auch keine problematische Abweichung von der Normalverteilung der Residuen. Da es sich bei den einzelnen Punkten allerdings bereits um arithmetische Mittelwerte aus je 8 Beobachtungen handelt, hätte man sich auch einfach auf das Central Limit Theorem beziehen können, das sagt, dass Mittelwerte automatisch einer Normalverteilung folgen. #Ergebnisplot plot(Area,Species.richness,pch=16,xlab=&quot;Flaeche [m2]&quot;,ylab=&quot;Artenreichtum&quot;) xv&lt;-seq(0,1000,by=0.1) yv&lt;-predict(power.model,list(Area=xv)) lines(xv,yv,lwd=2,col=&quot;red&quot;) #Das ist der Ergebnisplot für das beste Modell. Wichtig ist, dass man die Achsen korrekt beschriftet und nicht einfach die mehr oder weniger kryptischen Spaltennamen aus R nimmt. #Im Weiteren habe ich noch eine Sättigungsfunktion (Michaelis-Menten mit Selbststarter) zum Vergleich hinzugeplottet yv2&lt;-predict(micmen.model,list(Area=xv)) lines(xv,yv2,lwd=2,col=&quot;blue&quot;) Man erkennt, dass die Sättigungsfunktion offensichtlich den tatsächlichen Kurvenverlauf sehr schlecht widergibt. Im mittleren Kurvenbereich sind die Schätzwerte zu hoch, für grosse Flächen dann aber systematisch viel zu niedrig. Man kann die Darstellung im doppeltlogarithmischen Raum wiederholen, um die Kurvenanpassung im linken Bereich besser differenzieren zu können: #Ergebnisplot Double-log plot(log10(Area),log10(Species.richness),pch=16,xlab=&quot;logA&quot;,ylab=&quot;log (S)&quot;) xv&lt;-seq(0,1000,by=0.0001) yv&lt;-predict(power.model,list(Area=xv)) lines(log10(xv),log10(yv),lwd=2,col=&quot;red&quot;) yv2&lt;-predict(micmen.model,list(Area=xv)) lines(log10(xv),log10(yv2),lwd=2,col=&quot;blue&quot;) Auch hier sieht man, dass die rote Kurve zwar nicht perfekt, aber doch viel besser als die blaue Kurve ist. Darstellung der Vorgehensweise und Ergebnisse (in einer wiss. Arbeit) Methoden Es wurden Pflanzenartenzahlen der Pflanzengesellschaft Lolio-Cynosuretum im Nationalpark Kurische Nehrung auf verschieden grossen, geschachtelten Flächen erhoben. Eine solche nested-plot-Serie bestand aus 16 Erhebungen auf Flächen von 0.0001 m² bis 900 m². Die Werte von acht untersuchten nested-plot Serien wurden für die weiteren Analysen gemittelt. Mittels nicht-linearer Regression (Funktion nls in R) wurde ermittelt, welche Funktion die Artenzahlzunahme mit der Flächengrösse am besten beschreibt. Verglichen wurden vier Modelle mit je zwei Funktionsparametern (Potenzfunktion, logarithmische Funktion, Michaelis-Menten-Funktion, asymptotische Funktion durch den Ursprung; Tab. 1). Die Modelle wurden anschliessend mittels AICc verglichen und ihre Validität visuell in den Residualplots begutachtet. Ergebnisse Unter den verglichenen fünf Modellen (vier Funktionen, darunter zwei Variaten von Michaelis-Menten mit unterschiedlichen Startwerten) war die Potenzfunktion mit einem Akaike weight von 0.98 klar die beste (Tab. 2, Abb. 1). Die Logarithmusfunktion als zweitbeste Funktion war mit einem Delta-AICc von 7.68 (Akaike weight = 0.02) schon weit abgeschlagen und die drei Modelle mit einem modellierten oberen Grenzwert statistisch bedeutungslos (Delta-AICc &gt; 33) (Tab. 2). Abb. 1: Modellierte Artenzahl-Areal-Beziehungen für die nested-plot-Serie aus dem Nationalpark Kurische Nehrung. Die Potenzfunktion (rot) gab den Zusammenhang am besten wider (Akaike weight = 0.98), selbst die relativ beste der drei Sättigungsmodelle, die Michaelis-Menten-Funktion (mit Selbststart; blau), zeigte starke Abweichungen (Akaike weight &lt; 0.01); der vorhergesagte Sättigungswert lag weit unter dem empirischen höchsten Wert. Tab. 2. Modellgüte und Funktionsparameter der fünf verglichenen Modelle. A = Fläche in m²,S = Artenreichtum. "]
]
