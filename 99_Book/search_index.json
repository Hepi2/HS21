[
["index.html", "Research Methods Kapitel 1 Einleitung", " Research Methods Juergen Dengler 2019-11-28 Kapitel 1 Einleitung Das Modul „Research Methods“ vermittelt vertiefte Methodenkompetenzen für praxisorientiertes und angewandtes wissenschaftliches Arbeiten im Fachbereich „Umwelt und Natürliche Ressourcen“ auf MSc-Niveau. Die Studierenden erarbeiten sich vertiefte Methodenkompetenzen für die analytische Betrachtung der Zusammenhänge im Gesamtsystem „Umwelt und Natürliche Ressourcen“. Die Studierenden erlernen die methodischen Kompetenzen, auf denen die nachfolgenden Module im MSc Programm UNR aufbauen. Das Modul vermittelt einerseits allgemeine, fächerübergreifende methodische Kompetenzen (z.B. Wissenschaftstheorie, computer-gestützte Datenverar-beitung und Statistik). Auf dieser Plattform (RStudio Connect) werden die Unterlagen für die R-Übungsteile bereitgestellt. Es werden sukzessive sowohl Demo-Files, Aufgabenstellungen und Lösungen veröffentlicht. "],
["2-prepro1-14-10-2019.html", "Kapitel 2 PrePro1 (14.10.2019)", " Kapitel 2 PrePro1 (14.10.2019) Die Datenkunde 2.0 gibt den Studierenden das Wissen und die Fertigkeiten an die Hand, selbst erhobene und bezogene Daten für Ihre eigenen Analysen vorzubereiten und anzureichern (preprocessing). Die Einheit vermittelt zentrale Datenverarbeitungskompetenzen und thematisiert bekannte Problemzonen der umweltwissenschaftlichen Datenverarbeitung – immer mit einer „hands-on“ Perspektive auf die begleitenden R-Übungen. Die Studierenden lernen die Eigenschaften ihrer Datensätze in der Fachsprache korrekt zu beschreiben. Sie lernen ausserdem Metadaten zu verstehen und die Implikationen derselben für ihre eigenen Analyseprojekte kritisch zu beurteilen. Zentrale Konzepte der Lerneinheit sind Skalenniveaus, Datentypen, Zeitdaten und Typumwandlungen. "],
["2-1-demo-datentypen-tabellen.html", "2.1 Demo: Datentypen, Tabellen", " 2.1 Demo: Datentypen, Tabellen R-Code als Download 2.1.1 Datentypen 2.1.1.1 Numerics Unter die Kategorie numeric fallen in R zwei Datentypen: double: Gleitkommazahl (z.B. 10.3, 7.3) integer: Ganzzahl (z.B. 10, 7) 2.1.1.1.1 Doubles Folgendermassen wird eine Gleitkommazahl einer Variabel zuweisen: x &lt;- 10.3 x ## [1] 10.3 typeof(x) ## [1] &quot;double&quot; Statt &lt;-kann auch = verwendet werden. Dies funktioniert aber nicht in allen Situationen, und ist zudem leicht mit == zu verwechseln. y = 7.3 y ## [1] 7.3 Ohne explizite Zuweisung nimmt R immer den Datentyp doublean: z &lt;- 42 typeof(z) ## [1] &quot;double&quot; is.integer(z) ## [1] FALSE is.numeric(z) ## [1] TRUE is.double(z) ## [1] TRUE 2.1.1.2 Ganzzahl / Integer Erst wenn man eine Zahl explizit als integer definiert (mit as.integer() oder L), wird sie auch als solches abgespeichert. a &lt;- as.integer(z) is.numeric(a) ## [1] TRUE is.integer(a) ## [1] TRUE c &lt;- 8L is.numeric(c) ## [1] TRUE is.integer(c) ## [1] TRUE typeof(a) ## [1] &quot;integer&quot; is.numeric(a) ## [1] TRUE is.integer(a) ## [1] TRUE Mit c() können eine Reihe von Werten in einer Variabel zugewiesen werden (als vector). Es gibt zudem auch character vectors. vector &lt;- c(10,20,33,42,54,66,77) vector ## [1] 10 20 33 42 54 66 77 vector[5] ## [1] 54 vector[2:4] ## [1] 20 33 42 vector2 &lt;- vector[2:4] Eine Ganzzahl kann explizit mit as.integer() definiert werden. a &lt;- as.integer(7) b &lt;- as.integer(3.14) a ## [1] 7 b ## [1] 3 typeof(a) ## [1] &quot;integer&quot; typeof(b) ## [1] &quot;integer&quot; is.integer(a) ## [1] TRUE is.integer(b) ## [1] TRUE Eine Zeichenkette kann als Zahl eingelesen werden. c &lt;- as.integer(&quot;3.14&quot;) c ## [1] 3 typeof(c) ## [1] &quot;integer&quot; 2.1.1.3 Logische Abfragen Wird auch auch als boolesch (Eng. boolean) bezeichnet. e &lt;- 3 f &lt;- 6 g &lt;- e &gt; f e ## [1] 3 f ## [1] 6 g ## [1] FALSE typeof(g) ## [1] &quot;logical&quot; 2.1.1.4 Logische Operationen sonnig &lt;- TRUE trocken &lt;- FALSE sonnig &amp; !trocken ## [1] TRUE Oft braucht man auch das Gegenteil / die Negation eines Wertes. Dies wird mittels ! erreicht u &lt;- TRUE v &lt;- !u v ## [1] FALSE 2.1.1.5 Zeichenketten Zeichenketten (Eng. character) stellen Text dar s &lt;- as.character(3.14) s ## [1] &quot;3.14&quot; typeof(s) ## [1] &quot;character&quot; Zeichenketten verbinden / zusammenfügen (Eng. concatenate) fname &lt;- &quot;Hans&quot; lname &lt;- &quot;Muster&quot; paste(fname,lname) ## [1] &quot;Hans Muster&quot; fname2 &lt;- &quot;hans&quot; fname == fname2 ## [1] FALSE 2.1.1.6 Factors Mit Factors wird in R eine Sammlung von Zeichenketten bezeichnet, die sich wiederholen, z.B. Wochentage (es gibt nur 7 unterschiedliche Werte für “Wochentage”). wochentage &lt;- c(&quot;Montag&quot;,&quot;Dienstag&quot;,&quot;Mittwoch&quot;,&quot;Donnerstag&quot;,&quot;Freitag&quot;,&quot;Samstag&quot;,&quot;Sonntag&quot;, &quot;Montag&quot;,&quot;Dienstag&quot;,&quot;Mittwoch&quot;,&quot;Donnerstag&quot;,&quot;Freitag&quot;,&quot;Samstag&quot;,&quot;Sonntag&quot;) typeof(wochentage) ## [1] &quot;character&quot; wochentage_fac &lt;- as.factor(wochentage) wochentage ## [1] &quot;Montag&quot; &quot;Dienstag&quot; &quot;Mittwoch&quot; &quot;Donnerstag&quot; &quot;Freitag&quot; ## [6] &quot;Samstag&quot; &quot;Sonntag&quot; &quot;Montag&quot; &quot;Dienstag&quot; &quot;Mittwoch&quot; ## [11] &quot;Donnerstag&quot; &quot;Freitag&quot; &quot;Samstag&quot; &quot;Sonntag&quot; wochentage_fac ## [1] Montag Dienstag Mittwoch Donnerstag Freitag Samstag ## [7] Sonntag Montag Dienstag Mittwoch Donnerstag Freitag ## [13] Samstag Sonntag ## Levels: Dienstag Donnerstag Freitag Mittwoch Montag Samstag Sonntag Wie man oben sieht, unterscheiden sich character vectors und factors v.a. dadurch, dass letztere über sogenannte levels verfügt. Diese levels entsprechen den Eindeutigen (unique) Werten. levels(wochentage_fac) ## [1] &quot;Dienstag&quot; &quot;Donnerstag&quot; &quot;Freitag&quot; &quot;Mittwoch&quot; &quot;Montag&quot; ## [6] &quot;Samstag&quot; &quot;Sonntag&quot; unique(wochentage) ## [1] &quot;Montag&quot; &quot;Dienstag&quot; &quot;Mittwoch&quot; &quot;Donnerstag&quot; &quot;Freitag&quot; ## [6] &quot;Samstag&quot; &quot;Sonntag&quot; 2.1.1.7 Zeit/Datum Um in R mit Datum/Zeit Datentypen umzugehen, müssen sie als POSIXct eingelesen werden (es gibt alternativ noch POSIXlt, aber diese ignorieren wir mal). Anders als Beispielsweise bei Excel, sollten in R Datum und Uhrzeit immer in einer Spalte gespeichert werden. datum &lt;- &quot;2017-10-01 13:45:10&quot; as.POSIXct(datum) ## [1] &quot;2017-10-01 13:45:10 CEST&quot; Wenn das die Zeichenkette in dem obigen Format (Jahr-Monat-Tag Stunde:Minute:Sekunde) daher kommt, braucht as.POSIXctkeine weiteren Informationen. Sollte das Format von dem aber Abweichen, muss man der Funktion das genaue Schema jedoch mitteilen. Der Syntax dafür kann via ?strptime nachgeschlagen werden. datum &lt;- &quot;01.10.2017 13:45&quot; as.POSIXct(datum,format = &quot;%d.%m.%Y %H:%M&quot;) ## [1] &quot;2017-10-01 13:45:00 CEST&quot; Beachtet, dass in den den obigen Beispiel R automatisch eine Zeitzone angenommen hat (CEST). R geht davon aus, dass die Zeitzone der System Timezone (Sys.timezone()) entspricht. 2.1.2 Data Frames und Conveniance Variabeln Eine data.frame ist die gängigste Art, Tabellarische Daten zu speichern. df &lt;- data.frame( Stadt = c(&quot;Zürich&quot;,&quot;Genf&quot;,&quot;Basel&quot;,&quot;Bern&quot;,&quot;Lausanne&quot;), Einwohner = c(396027,194565,175131,140634,135629), Ankunft = c(&quot;1.1.2017 10:00&quot;,&quot;1.1.2017 14:00&quot;, &quot;1.1.2017 13:00&quot;,&quot;1.1.2017 18:00&quot;,&quot;1.1.2017 21:00&quot;) ) str(df) ## &#39;data.frame&#39;: 5 obs. of 3 variables: ## $ Stadt : Factor w/ 5 levels &quot;Basel&quot;,&quot;Bern&quot;,..: 5 3 1 2 4 ## $ Einwohner: num 396027 194565 175131 140634 135629 ## $ Ankunft : Factor w/ 5 levels &quot;1.1.2017 10:00&quot;,..: 1 3 2 4 5 In der obigen data.frame wurde die Spalte Einwohner als Fliesskommazahl abgespeichert. Dies ist zwar nicht tragisch, aber da wir wissen das es sich hier sicher um Ganzzahlen handelt, können wir das korrigieren. Wichtiger ist aber, dass wir die Ankunftszeit (SpalteAnkunft) von einem Factor in ein Zeitformat (POSIXct) umwandeln. df$Einwohner &lt;- as.integer(df$Einwohner) df$Einwohner ## [1] 396027 194565 175131 140634 135629 df$Ankunft &lt;- as.POSIXct(df$Ankunft, format = &quot;%d.%m.%Y %H:%M&quot;) df$Ankunft ## [1] &quot;2017-01-01 10:00:00 CET&quot; &quot;2017-01-01 14:00:00 CET&quot; ## [3] &quot;2017-01-01 13:00:00 CET&quot; &quot;2017-01-01 18:00:00 CET&quot; ## [5] &quot;2017-01-01 21:00:00 CET&quot; Diese Rohdaten können nun helfen, um Hilfsvariablen (convenience variables) zu erstellen. Z.B. können wir die Städte einteilen in gross, mittel und klein. df$Groesse[df$Einwohner &gt; 300000] &lt;- &quot;gross&quot; df$Groesse[df$Einwohner &lt;= 300000 &amp; df$Einwohner &gt; 150000] &lt;- &quot;mittel&quot; df$Groesse[df$Einwohner &lt;= 150000] &lt;- &quot;klein&quot; Oder aber, die Ankunftszeit kann von der Spalte Ankunftabgeleitet werden. Dazu brauchen wir aber das Package lubridate library(lubridate) df$Ankunft_stunde &lt;- hour(df$Ankunft) 2.1.3 Quellen Dieses Kapitel verwendet folgende Libraries: Spinu, Grolemund, and Wickham (2018) References "],
["2-2-ubung-a.html", "2.2 Übung A", " 2.2 Übung A R ist ohne Zusatzpackete nicht mehr denkbar. Die allermeisten Packages werden auf CRAN gehostet und können leicht mittels install.packages() installiert werden. Eine sehr wichtige Sammlung von Packages wird von RStudio entwickelt. Unter dem Namen Tidyverse werden eine Reihe von Packages angeboten, den R-Alltag enorm erleichtert. Wir werden später näher auf das “Tidy”-Universum eingehen, an dieser Stelle können wir die Sammlung einfach mal installieren. install.packages(&quot;tidyverse&quot;) Um ein package in R verwenden zu können, gibt es zwei Möglichkeiten: entweder man lädt es zu Beginn der R-session mittles library(). oder man ruft eine function mit vorangestelltem Packetname sowie zwei Doppelpunkten auf. dplyr::filter() ruft die Funktion filter() des Packets dplyr auf. Letztere Notation ist vor allem dann sinnvoll, wenn sich zwei unterschiedliche Funktionen mit dem gleichen namen in verschiedenen pacakges existieren. filter() existiert als Funktion einersits im package dplyr sowie in stats. Dieses Phänomen nennt man “masking”. Zu beginn laden wir die nötigen Pakete: 2.2.1 Aufgabe 1 Erstelle eine data.frame mit nachstehenden Daten. Tipps: Eine leere data.frame zu erstellen ist schwieriger als wenn erstellen und befüllen der data.frame in einem Schritt erfolgt R ist dafür gedacht, Spalte für Spalte zu arbeiten (warum?), nicht Reihe für Reihe. Versuche dich an dieses Schema zu halten. Tierart Anzahl Gewicht Geschlecht Beschreibung Fuchs 2 4.4 m Rötlich Bär 5 40.3 f Braun, gross Hase 1 1.1 m klein, mit langen Ohren Elch 3 120.0 m Lange Beine, Schaufelgeweih 2.2.2 Aufgabe 2 Was für Datentypen wurden (in Aufgabe 1) von R automatisch angenommen? Sind diese sinnvoll? Tipp: Nutze dazu str() ## &#39;data.frame&#39;: 4 obs. of 5 variables: ## $ Tierart : Factor w/ 4 levels &quot;Bär&quot;,&quot;Elch&quot;,&quot;Fuchs&quot;,..: 3 1 4 2 ## $ Anzahl : num 2 5 1 3 ## $ Gewicht : num 4.4 40.3 1.1 120 ## $ Geschlecht : Factor w/ 2 levels &quot;f&quot;,&quot;m&quot;: 2 1 2 2 ## $ Beschreibung: Factor w/ 4 levels &quot;Braun, gross&quot;,..: 4 1 2 3 ## [1] &quot;double&quot; 2.2.3 Aufgabe 3 Nutze die Spalte Gewicht um die Tiere in 3 Gewichtskategorien einzuteilen: leicht: &lt; 5kg mittel: 5 - 100 kg schwer: &gt; 100kg Tierart Anzahl Gewicht Geschlecht Beschreibung Gewichtsklasse Fuchs 2 4.4 m Rötlich leicht Bär 5 40.3 f Braun, gross mittel Hase 1 1.1 m klein, mit langen Ohren leicht Elch 3 120.0 m Lange Beine, Schaufelgeweih schwer 2.2.4 Aufgabe 4 Importiere den Datensatz order_52252_data.txt. Es handelt sich dabei um die stündlich gemittelten Temperaturdaten an verschiedenen Standorten in der Schweiz im Zeitraum 2000 - 2005. Wir empfehlen read_table()1 anstelle von read.table(). stn time tre200h0 ABO 2000010100 -2.6 ABO 2000010101 -2.5 ABO 2000010102 -3.1 ABO 2000010103 -2.4 ABO 2000010104 -2.5 ABO 2000010105 -3.0 ABO 2000010106 -3.7 ABO 2000010107 -4.4 ABO 2000010108 -4.1 ABO 2000010109 -4.1 2.2.5 Aufgabe 5 Schau dir die Rückmeldung von read_table()an. Sind die Daten korrekt interpretiert worden? 2.2.6 Aufgabe 6 Die Spalte time ist eine Datum/Zeitangabe im Format JJJJMMTTHH (siehe meta.txt). Damit R dies als Datum-/Zeitangabe erkennt, müssen wir die Spalte in einem R-Format (POSIXct) einlesen und dabei R mitteilen, wie sie aktuell formatiert ist. Lies die Spalte mit as.POSIXct() (oder parse_datetime) ein und spezifiziere sowohl format wie auch tz. Tipps: Wenn keine Zeitzone festgelegt wird, trifft as.POSIXct() eine Annahme (basierend auf Sys.timezone()). In unserem Fall handelt es sich aber um Werte in UTC (siehe meta.txt) as.POSIXcterwartet character: Wenn du eine Fehlermeldung hast die 'origin' must be supplied (o.ä) heisst, hast du der Funktion vermutlich einen Numeric übergeben. ## [1] &quot;2000-01-01 00:00:00 UTC&quot; &quot;2000-01-01 01:00:00 UTC&quot; ## [3] &quot;2000-01-01 02:00:00 UTC&quot; &quot;2000-01-01 03:00:00 UTC&quot; ## [5] &quot;2000-01-01 04:00:00 UTC&quot; &quot;2000-01-01 05:00:00 UTC&quot; ## [7] &quot;2000-01-01 06:00:00 UTC&quot; &quot;2000-01-01 07:00:00 UTC&quot; ## [9] &quot;2000-01-01 08:00:00 UTC&quot; &quot;2000-01-01 09:00:00 UTC&quot; stn time tre200h0 ABO 2000-01-01 00:00:00 -2.6 ABO 2000-01-01 01:00:00 -2.5 ABO 2000-01-01 02:00:00 -3.1 ABO 2000-01-01 03:00:00 -2.4 ABO 2000-01-01 04:00:00 -2.5 ABO 2000-01-01 05:00:00 -3.0 ABO 2000-01-01 06:00:00 -3.7 ABO 2000-01-01 07:00:00 -4.4 ABO 2000-01-01 08:00:00 -4.1 ABO 2000-01-01 09:00:00 -4.1 2.2.7 Aufgabe 7 Erstelle zwei neue Spalten mit Wochentag (Montag, Dienstag, etc) und Kalenderwoche. Verwende dazu die neu erstellte POSIXct-Spalte stn time tre200h0 wochentag kw ABO 2000-01-01 00:00:00 -2.6 Sat 1 ABO 2000-01-01 01:00:00 -2.5 Sat 1 ABO 2000-01-01 02:00:00 -3.1 Sat 1 ABO 2000-01-01 03:00:00 -2.4 Sat 1 ABO 2000-01-01 04:00:00 -2.5 Sat 1 ABO 2000-01-01 05:00:00 -3.0 Sat 1 ABO 2000-01-01 06:00:00 -3.7 Sat 1 ABO 2000-01-01 07:00:00 -4.4 Sat 1 ABO 2000-01-01 08:00:00 -4.1 Sat 1 ABO 2000-01-01 09:00:00 -4.1 Sat 1 2.2.8 Aufgabe 8 Erstelle eine neue Spalte basierend auf die Temperaturwerte mit der Einteilung “kalt” (Unter Null Grad) und “warm” (über Null Grad) stn time tre200h0 wochentag kw temp_kat ABO 2000-01-01 00:00:00 -2.6 Sat 1 kalt ABO 2000-01-01 01:00:00 -2.5 Sat 1 kalt ABO 2000-01-01 02:00:00 -3.1 Sat 1 kalt ABO 2000-01-01 03:00:00 -2.4 Sat 1 kalt ABO 2000-01-01 04:00:00 -2.5 Sat 1 kalt ABO 2000-01-01 05:00:00 -3.0 Sat 1 kalt ABO 2000-01-01 06:00:00 -3.7 Sat 1 kalt ABO 2000-01-01 07:00:00 -4.4 Sat 1 kalt ABO 2000-01-01 08:00:00 -4.1 Sat 1 kalt ABO 2000-01-01 09:00:00 -4.1 Sat 1 kalt Wickham and Grolemund (2017), Kapitel 8 bzw. http://r4ds.had.co.nz/data-import.html)↩ "],
["2-3-ubung-a-losung.html", "2.3 Übung A Lösung", " 2.3 Übung A Lösung R-Script als Download library(tidyverse) # Im Unterschied zu `install.packages()` werden bei `library()` keine Anführungs- # und Schlusszeichen gesetzt. library(lubridate) # Im Unterschied zu install.packages(&quot;tidyverse&quot;) wird bei library(tidyverse) # das package lubridate nicht berücksichtigt # Lösung Aufgabe 1 df &lt;- data_frame( Tierart = c(&quot;Fuchs&quot;,&quot;Bär&quot;,&quot;Hase&quot;,&quot;Elch&quot;), Anzahl = c(2,5,1,3), Gewicht = c(4.4, 40.3,1.1,120), Geschlecht = c(&quot;m&quot;,&quot;f&quot;,&quot;m&quot;,&quot;m&quot;), Beschreibung = c(&quot;Rötlich&quot;,&quot;Braun, gross&quot;, &quot;klein, mit langen Ohren&quot;,&quot;Lange Beine, Schaufelgeweih&quot;) ) # Lösung Aufgabe 2 str(df) # Anzahl wurde als `double` interpretiert, ist aber eigentlich ein `integer`. # Mit data.frame() wurde Beschreibung wurde als `factor` interpretiert, ist # aber eigentlich `character` typeof(df$Anzahl) df$Anzahl &lt;- as.integer(df$Anzahl) df$Beschreibung &lt;- as.character(df$Beschreibung) # Lösung Aufgabe 3 df$Gewichtsklasse[df$Gewicht &gt; 100] &lt;- &quot;schwer&quot; df$Gewichtsklasse[df$Gewicht &lt;= 100 &amp; df$Gewicht &gt; 5] &lt;- &quot;mittel&quot; df$Gewichtsklasse[df$Gewicht &lt;= 5] &lt;- &quot;leicht&quot; # Lösung Aufgabe 4 wetter &lt;- readr::read_table(&quot;09_PrePro1/data/order_52252_data.txt&quot;) # Lösung Aufgabe 5 # Die Spalte &#39;time&#39; wurde als &#39;integer&#39; interpretiert. Dabei handelt es # sich offensichtlich um Zeitangaben. # Lösung Aufgabe 6 # mit readr parse_datetime(as.character(wetter$time[1:10]), format = &quot;%Y%m%d%H&quot;) # mit as.POSIXct() wetter$time &lt;- as.POSIXct(as.character(wetter$time), format = &quot;%Y%m%d%H&quot;,tz = &quot;UTC&quot;) # Lösung Aufgabe 7 wetter$wochentag &lt;- wday(wetter$time,label = T) wetter$kw &lt;- week(wetter$time) # Lösung Aufgabe 8 wetter$temp_kat[wetter$tre200h0&gt;0] &lt;- &quot;warm&quot; wetter$temp_kat[wetter$tre200h0&lt;=0] &lt;- &quot;kalt&quot; "],
["2-4-ubung-b.html", "2.4 Übung B", " 2.4 Übung B Fahre mit dem Datensatz wetter aus Übung A fort. 2.4.1 Aufgabe 1 Nutze plot() um die Temparaturkurve zu visualisieren. Verwende aber vorher filter() um dich auf eine Station (z.B. “ABO”) zu beschränken (es handelt sich sonst um zuviele Datenpunkte). Nun schauen wir uns das plotten mit ggplot2 an. Ein simpler Plot wie der in der vorherigen Aufgabe ist in ggplot2 zugegebenermassen etwas komplizierter. ggplot2 wird aber rasch einfacher, wenn die Grafiken komplexer werden. Wir empfehlen deshalb stark, ggplot2 zu verwenden. Schau dir ein paar online Tutorials zu ggplot2 an (siehe2) und reproduziere den obigen Plot mit ggplot2 2.4.2 Aufgabe 2 Spiele mit Hilfe der erwähnten Tutorials mit dem Plot etwas rum. Versuche die x-/y-Achsen zu beschriften sowie einen Titel hinzu zu fügen. 2.4.3 Aufgabe 3 Reduziere den x-Achsenausschnitt auf einen kleineren Zeitraum, beispielsweise einn beliebigen Monat. Verwende dazu lims() zusammen mit as.POSIXct() oder mache ein Subset von deinem Datensatz mit einer convenience-Variabel und filter(). Wickham and Grolemund (2017), Kapitel 1 bzw. http://r4ds.had.co.nz/data-visualisation.html oder hier ein sehr schönes Video: Learn R: An Introduction to ggplot2↩ "],
["2-5-ubung-b-losung.html", "2.5 Übung B Lösung", " 2.5 Übung B Lösung R-Code als Download library(tidyverse) # Lösung Aufgabe 1 wetter_fil &lt;- dplyr::filter(wetter, stn == &quot;ABO&quot;) plot(wetter_fil$time,wetter_fil$tre200h0, type = &quot;l&quot;) p &lt;- ggplot(wetter_fil, aes(time,tre200h0)) + geom_line() p # Lösung Aufgabe 2 p &lt;- p + labs(x = &quot;Datum&quot;, y = &quot;Temperatur&quot;, title = &quot;Stündlich gemittelte Temperaturwerte&quot;) p # Lösung Aufgabe 3 limits &lt;- as.POSIXct(c(&quot;2002-01-01 00:00:00&quot;,&quot;2002-02-01 00:00:00&quot;),tz = &quot;UTC&quot;) p + lims(x = limits) "],
["3-prepro2-15-10-2019.html", "Kapitel 3 PrePro2 (15.10.2019)", " Kapitel 3 PrePro2 (15.10.2019) Die Lerneinheit vermittelt zentralste Fertigkeiten zur Vorverarbeitung von strukturierten Daten in der umweltwissenschaftlichen Forschung: Datensätze verbinden (Joins) und umformen („reshape“, „split-apply-combine“). Im Anwendungskontext haben Daten selten von Anfang an diejenige Struktur, welche für die statistische Auswertung oder für die Informationsvisualisierung erforderlich wäre. In dieser Lerneinheit lernen die Studierenden die für diese oft zeitraubenden Preprocessing-Schritte notwendigen Konzepte und R-Werkzeuge kennen und kompetent anzuwenden. "],
["3-1-erganzungen-zu-prepro-1.html", "3.1 Ergänzungen zu PrePro 1", " 3.1 Ergänzungen zu PrePro 1 3.1.1 Integer mit “L” In R kann eine Zahl mit dem Suffix “L” explizit als Integer spezifiziert werden. typeof(42) ## [1] &quot;double&quot; typeof(42L) ## [1] &quot;integer&quot; Warum dazu der Buchstabe “L” verwendet wird ist nirgends offiziell Dokumentiert (zumindest haben wir nichts gefunden). Die gängigste Meinung, die auch von renommierten R-Profis vertreten wird ist, dass damit Long integer abgekürzt wird. 3.1.2 Arbeiten mit RStudio “Project” Wir empfehlen die Verwendung von “Projects” innerhalb von RStudio. RStudio legt für jedes Projekt dann einen Ordner an, in welches die Projekt-Datei abgelegt wird (Dateiendung .Rproj). Sollen innerhalb des Projekts dann R-Skripts geladen oder erzeugt werden, werden diese dann auch im angelegten Ordner abgelegt. Mehr zu RStudio Projects findet ihr hier. Das Verwenden von Projects bringt verschiedene Vorteile, wie zum Beispiel: Festlegen der Working Directory ohne die Verwendung des expliziten Pfades (setwd()). Das ist sinnvoll, da sich dieser Pfad ändern kann (Zusammenarbeit mit anderen Usern, Ausführung des Scripts zu einem späteren Zeitpunkt) Automatisches Zwischenspeichern geöffneter Scripts und Wiederherstellung der geöffneten Scripts bei der nächsten Session Festlegen verschiedener projektspezifischer Optionen Verwendung von Versionsverwaltungssystemen (Github oder SVN) 3.1.3 Arbeiten mit factors Wie bereits angedeutet, ist das Arbeiten mit factors etwas gewöhnungsbedürftig. Wir gehen hier auf ein paar Stolpersteine ein. zahlen &lt;- factor(c(&quot;null&quot;,&quot;eins&quot;,&quot;zwei&quot;,&quot;drei&quot;)) zahlen ## [1] null eins zwei drei ## Levels: drei eins null zwei Offensichtlich sollten diese factors geordnet sein, R weiss davon aber nichts. Eine Ordnung kann man mit dem Befehl ordered = T festlegen. Beachtet: ordered = T kann nur bei der Funktion factor() spezifiziert werden, nicht bei as.factor(). Ansonsten sind factor() und as.factor() sehr ähnlich. zahlen &lt;- factor(zahlen,ordered = T) zahlen ## [1] null eins zwei drei ## Levels: drei &lt; eins &lt; null &lt; zwei Beachtet das “&lt;”-Zeichen zwischen den Levels. Die Zahlen werden nicht in der korrekten Reihenfolge, sondern Alphabetisch geordnet. Die richtige Reihenfolge kann man mit levels = festlegen. zahlen &lt;- factor(zahlen,ordered = T,levels = c(&quot;null&quot;,&quot;eins&quot;,&quot;zwei&quot;,&quot;drei&quot;,&quot;vier&quot;)) zahlen ## [1] null eins zwei drei ## Levels: null &lt; eins &lt; zwei &lt; drei &lt; vier Wie auch schon erwähnt werden factors als character Vektor dargestellt, aber als Integers gespeichert. Das führt zu einem scheinbaren Wiederspruch wenn man den Datentyp auf unterschiedliche Weise abfragt. typeof(zahlen) ## [1] &quot;integer&quot; is.integer(zahlen) ## [1] FALSE Mit typeof() wird eben diese Form der Speicherung abgefragt und deshalb mit integer beantwortet. Da es sich aber nicht um einen eigentlichen Integer Vektor handelt, wird die Frage is.integer() mit FALSE beantwortet. Das ist etwas verwirrend, beruht aber darauf, dass die beiden Funktionen die Frage von unterschiedlichen Perspektiven beantworten. In diesem Fall schafft class() Klarheit: class(zahlen) ## [1] &quot;ordered&quot; &quot;factor&quot; Wirklich verwirrend wird es, wenn factors in numeric umgewandelt werden sollen. zahlen ## [1] null eins zwei drei ## Levels: null &lt; eins &lt; zwei &lt; drei &lt; vier as.integer(zahlen) ## [1] 1 2 3 4 Das die Übersetzung der auf Deutsch ausgeschriebenen Nummern in nummerische Zahlen nicht funktionieren würde, war ja klar. Weniger klar ist es jedoch, wenn die factors bereits aus nummerischen Zahlen bestehen. zahlen2 &lt;- factor(c(&quot;3&quot;,&quot;2&quot;,&quot;1&quot;,&quot;0&quot;)) as.integer(zahlen2) ## [1] 4 3 2 1 In diesem Fall müssen die factors erstmals in character umgewandelt werden. zahlen2 &lt;- factor(c(&quot;3&quot;,&quot;2&quot;,&quot;1&quot;,&quot;0&quot;)) as.integer(as.character(zahlen2)) ## [1] 3 2 1 0 3.1.4 Heikle Annahmen - bessere Alternativen Aus oben beschriebenen Grund ist es auch problematisch, dass data.frame() sowie alle read.* Funktionen (read.table, read.csv etc) immer davon ausgehen, dass strings als factors interpretiert werden sollten. Es gibt in Base R einige Funktionen, welche Annahmen treffen die problematisch sein können. Ein weiteres Beispiel ist die Annahme der Zeitzone und Verwendung von Sommerzeit bei as.POSIXct(). Oft gibt es dafür im Tidyverse alternative Funktionen, in denen diese Probleme besser gelöst sind. Wir empfehlen, wenn immer Möglich die Tidyverse-Alternativen zu verwenden. Beispiele: data_frame() statt data.frame() read_* statt read.* parse_datetime statt as.POSIXct() Beim Import von Daten kann es sinnvoll sein, die Datentypen der Spalten bereits im Importbefehl zu spezifizieren. So vermeidet man die anschliessende Typumwandlung und die damit verbundenen Fehlerquellen. Zudem wird der Importprozess beschleunigt, da R keine Zeit daran verschwenden muss die Datentypen (aufgrund der ersten 1000 Zeilen) zu erraten. library(tidyverse) df1 &lt;- read_table(&quot;09_PrePro1/data/order_52252_data.txt&quot;, col_types = list( col_character(), # Macht aus der 1.Spalte ein character col_datetime(format = &quot;%Y%m%d%H&quot;),# Macht aus der 2.Spalte ein POSIXct col_double() # Macht aus der 3.Spalte ein double ) ) df1 &lt;- read_table(&quot;09_PrePro1/data/order_52252_data.txt&quot;, col_types = list( col_factor(levels = NULL), # Macht aus der 1.Spalte ein factor col_datetime(format = &quot;%Y%m%d%H&quot;),# Macht aus der 2.Spalte ein POSIXct col_double() # Macht aus der 3.Spalte ein double ) ) "],
["3-2-demo-tidyverse.html", "3.2 Demo: tidyverse", " 3.2 Demo: tidyverse Demoscript als Download Hier möchten wir euch mit einer Sammlung von Tools vertraut machen, die spezifisch für das Daten prozessieren in Data Science entwickelt wurden. Der Prozess und das Modell ist hier3 schön beschrieben. Die Sammlung von Tools wird unter dem Namen tidyverse vertrieben, welches wir ja schon zu Beginn der ersten Übung installiert und geladen haben. Die Tools erleichtern den Umgang mit Daten ungeheuer und haben sich mittlerweile zu einem “must have” im Umgang mit Daten in R entwickelt. Wir können Euch nicht sämtliche Möglichkeiten von tidyverse zeigen. Wir fokussieren uns deshalb auf einzelne Komponenten4 und zeigen ein paar Funktionalitäten, die wir oft verwenden und Euch ggf. noch nicht bekannt sind. Wer sich vertieft mit dem Thema auseinandersetzen möchte, der sollte sich unbedingt das Buch Wickham and Grolemund (2017) beschaffen. Eine umfangreiche, aber nicht ganz vollständige Version gibt es online5, das vollständige eBook kann über die Bibliothek bezogen werden6. 3.2.1 Split-Apply-Combine 3.2.1.1 Packete laden library(tidyverse) Mit library(tidyverse) werden nicht alle Packete geladen, die mit install.packages(tidyverse) intalliert wurden (warum?). Unter anderem muss lubridate noch separat geladen werden: library(lubridate) 3.2.1.2 Daten Laden Wir laden die Wetterdaten von der letzten Übung. wetter &lt;- read_table(&quot;09_PrePro1/data/order_52252_data.txt&quot;, col_types = list( col_factor(levels = NULL), col_datetime(format = &quot;%Y%m%d%H&quot;), col_double() ) ) 3.2.1.3 Kennwerte berechnen Wir möchten den Mittelwert aller gemessenen Temperaturwerte berechnen. Dazu könnten wir folgenden Befehl verwenden: mean(wetter$tre200h0, na.rm = TRUE) ## [1] 8.962106 Die Option na.rm = T bedeutet, dass NA Werte von der Berechnung ausgeschlossen werden sollen. Mit der selben Herangehensweise können diverse Werte berechnet werden (z.B. das Maximum (max()), Minimum (min()), Median (median()) u.v.m.). Diese Herangehensweise funktioniert nur dann gut, wenn wir die Kennwerte über alle Beobachtungen (Zeilen) für eine Variable (Spalte) berechnen wollen. Sobald wir die Beobachtungen gruppieren wollen, wird es schwierig. Zum Beispiel, wenn wir die durchschnittliche Temperatur pro Jahr berechnen wollen. 3.2.1.4 Convenience Variablen Um diese Aufgabe zu lösen, muss zuerst das “Jahr” berechne werden (das Jahr ist die convenience variabel). Hierfür brauchen wir die Funktion year() (von lubridate). Nun kann kann die convenience Variable “Jahr” erstellt werden. Ohne dpylr wird eine neue Spalte wird folgendermassen hinzugefügt. wetter$year &lt;- year(wetter$time) Mit dplyr (siehe7) sieht der gleiche Befehl folgendermassen aus: wetter &lt;- mutate(wetter,year = year(time)) Der grosse Vorteil von dplyr ist an dieser Stelle noch nicht ersichtlich. Dieser wird aber später klar. 3.2.1.5 Kennwerte nach Gruppen berechnen Jetzt kann man die data.frame mithilfe der Spalte “Jahr” filtern. mean(wetter$tre200h0[wetter$year == 2000], na.rm = TRUE) ## [1] 9.281542 Dies müssen wir pro Jahr wiederholen, was natürlich sehr umständlich ist, v.a. wenn man eine Vielzahl an Gruppen hat (z.B. Kalenderwochen statt Jahre). Deshalb nutzen wir das package dplyr. Damit geht die Aufgabe (Temperaturmittel pro Jahr berechnen) folgendermassen: summarise(group_by(wetter,year),temp_mittel = mean(tre200h0, na.rm = TRUE)) ## # A tibble: 7 x 2 ## year temp_mittel ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2000 9.28 ## 2 2001 8.76 ## 3 2002 9.30 ## 4 2003 9.48 ## 5 2004 8.64 ## 6 2005 8.31 ## 7 NA NaN 3.2.1.6 Verketten vs. verschachteln Auf Deutsch übersetzt heisst die obige Operation folgendermassen: nimm den Datensatz wetter Bilde Gruppen pro Jahr (group_by(wetter,year)) Berechne das Temperaturmittel (mean(tre200h0)) Diese Übersetzung R-&gt; Deutsch unterscheidet sich vor allem darin, dass die Operation auf Deutsch verkettet ausgesprochen wird (Operation 1-&gt;2-&gt;3) während der Computer verschachtelt liest 3(2(1)). Um R näher an die gesprochene Sprache zu bringen, kann man den %&gt;%-Operator verwenden (siehe8). summarise(group_by(wetter,year),temp_mittel = mean(tre200h0)) # wird zu: wetter %&gt;% #1) nimm den Datensatz &quot;wetter&quot; group_by(year) %&gt;% #2) Bilde Gruppen pro Jahr summarise(temp_mittel = mean(tre200h0)) #3) berechne das Temperaturmittel Dieses Verketten mittels %&gt;% macht den Code einiges schreib- und leserfreundlicher, und wir werden ihn in den nachfolgenden Übungen verwenden. Dabei handelt es sich um das package magrittr, welches mit tidyverse mitgeliefert wird. Zu dplyr und magrittrgibt es etliche Tutorials online (siehe9), deshalb werden wir diese Tools nicht in allen Details erläutern. Nur noch folgenden wichtigen Unterschied zu zwei wichtigen Funktionen in dpylr: mutate() und summarise(). summarise() fasst einen Datensatz zusammen. Dabei reduziert sich die Anzahl Beobachtungen (Zeilen) auf die Anzahl Gruppen (z.B. eine zusammengefasste Beobachtung (Zeile) pro Jahr). Zudem reduziert sich die Anzahl Variablen (Spalten) auf diejenigen, die in der “summarise” Funktion spezifiziert wurde (z.B. temp_mittel). mit mutate wird ein data.frame vom Umfang her belassen, es werden lediglich zusätzliche Variablen (Spalten) hinzugefügt (siehe Beispiel unten). # Maximal und minimal Temperatur pro Kalenderwoche wetter %&gt;% #1) nimm den Datensatz &quot;wetter&quot; filter(stn == &quot;ABO&quot;) %&gt;% #2) filter auf Station namnes &quot;ABO&quot; mutate(kw = week(time)) %&gt;% #3) erstelle eine neue Spalte &quot;kw&quot; group_by(kw) %&gt;% #4) Nutze die neue Spalte um Guppen zu bilden summarise( temp_max = max(tre200h0, na.rm = TRUE),#5) Berechne das Maximum temp_min = min(tre200h0, na.rm = TRUE) #6) Berechne das Minimum ) ## # A tibble: 53 x 3 ## kw temp_max temp_min ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 11.4 -15.2 ## 2 2 12.9 -15.9 ## 3 3 8.2 -11.3 ## 4 4 9.6 -15.9 ## 5 5 16.9 -17.5 ## 6 6 13.5 -13.1 ## 7 7 12.9 -15.4 ## 8 8 11 -14.4 ## 9 9 12.9 -17.6 ## 10 10 15.4 -16.3 ## # … with 43 more rows 3.2.1.7 Resultate plotten Mit diesen Tools können wir nun auch eine neue Grafik plotten, ähnlich wie in der Übung 1. Dafür müssen wir die ganzen Operationen aber zuerst in einer Variabel speichern (bis jetzt hat R zwar alles schön berechnet, aber uns nur auf die Konsole ausgegeben). wetter_sry &lt;- wetter %&gt;% mutate( kw = week(time) ) %&gt;% filter(stn == &quot;ABO&quot;) %&gt;% group_by(kw) %&gt;% summarise( temp_max = max(tre200h0), temp_min = min(tre200h0), temp_mean = mean(tre200h0) ) Dieses Mal plotten wir nur mit ggplot2 (siehe10) ggplot() + geom_line(data = wetter_sry, aes(kw,temp_max), colour = &quot;yellow&quot;) + geom_line(data = wetter_sry, aes(kw,temp_mean), colour = &quot;pink&quot;) + geom_line(data = wetter_sry, aes(kw,temp_min), colour = &quot;black&quot;) + labs(y = &quot;temp&quot;) Das sieht schon mal gut aus. Nur, wir mussten pro Linie einen eigene Zeile schreiben (geom_line()) und dieser eine Farbe zuweisen. Bei drei Werten ist das ja ok, aber wie sieht es denn aus wenn es Hunderte sind? Da hat ggplot natürlich eine Lösung, dafür müssen aber alle Werte in einer Spalte daher kommen. Das ist ein häufiges Problem: Wir haben eine breite Tabelle (viele Spalten), bräuchten aber eine lange Tabelle (viele Zeilen). 3.2.2 Reshaping data 3.2.2.1 Breit -&gt; lang Da kommt tidyverse wieder ins Spiel. Die Umformung von Tabellen breit-&gt;lang erfolgt mittels tidyr(siehe11). Auch dieses package funktioniert wunderbar mit piping (%&gt;%). wetter_sry_long &lt;- wetter_sry %&gt;% gather(Key, Value, c(temp_max,temp_min,temp_mean)) Im Befehl gather() braucht es drei Werte: beliebiger Name der neuen Variablen (Spalte) für die Schlüssel: “temp_mean”, “temp_min”… (ich verwenden den Namen: Key) beliebiger Name der neuen Variablen (Spalte) für die effektiven Werte: 5°C, 10°C (ich verwenden den Namen: Value) Name der (bestehenden) Variablen (Spalten), die zusammen gefasst werden sollten: (hier: temp_max,temp_min,temp_mean) Die ersten 6 Zeilen von wetter_sry: kw temp_max temp_min temp_mean 1 11.4 -15.2 -1.2593254 2 12.9 -15.9 -1.5572421 3 8.2 -11.3 -1.8832341 4 9.6 -15.9 -2.8375000 5 16.9 -17.5 -0.9789683 6 13.5 -13.1 0.4392857 Die ersten 6 Zeilen von wetter_sry_long: kw Key Value 1 temp_max 11.400000 1 temp_min -15.200000 1 temp_mean -1.259325 2 temp_max 12.900000 2 temp_min -15.900000 2 temp_mean -1.557242 Beachte: wetter_sry_long umfasst 159 Beobachtungen (Zeilen), das sind 3 mal soviel wie wetter_sry, da wir ja drei Spalten zusammengefasst haben. nrow(wetter_sry) ## [1] 53 nrow(wetter_sry_long) ## [1] 159 Statt die Variablen (Spalten) zu benennen, die zusammengefasst werden sollten, wäre es in unserem Fall einfacher, die Variablen (Spalten) zu benennen die nicht zusammengefasst werden sollen (kw): wetter_sry_long &lt;- wetter_sry %&gt;% gather(Key, Value, -kw) Nun können wir den obigen Plot viel einfacher erstellen: ggplot(wetter_sry_long, aes(kw,Value, colour = Key)) + geom_line() Beachtet, dass wir gegenüber dem letzten Plot colour nun innerhalb von aes() festlegen und nicht mit einem expliziten Farbwert, sondern mit dem Verweis auf die Spalte key. 3.2.2.2 Lang -&gt; breit Um unsere lange Tabelle wieder zurück in eine breite zu überführen, brauchen wir lediglich einen Befehl (spread): wetter_sry_long %&gt;% spread(Key,Value) ## # A tibble: 53 x 4 ## kw temp_max temp_mean temp_min ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 11.4 -1.26 -15.2 ## 2 2 12.9 -1.56 -15.9 ## 3 3 8.2 -1.88 -11.3 ## 4 4 9.6 -2.84 -15.9 ## 5 5 16.9 -0.979 -17.5 ## 6 6 13.5 0.439 -13.1 ## 7 7 12.9 -2.32 -15.4 ## 8 8 11 -2.84 -14.4 ## 9 9 12.9 -2.20 -17.6 ## 10 10 15.4 0.917 -16.3 ## # … with 43 more rows 3.2.3 Quellen Dieses Kapitel verwendet folgende Libraries: Spinu, Grolemund, and Wickham (2018), Wickham (2018a), Wickham (2018c), Wickham, François, et al. (2018), Henry and Wickham (2018), Wickham, Hester, and Francois (2017), Wickham and Henry (2018), Müller and Wickham (2018), Wickham, Chang, et al. (2018), Wickham (2017) References "],
["3-3-ubung-a-1.html", "3.3 Übung A", " 3.3 Übung A 3.3.1 Aufgabe 1 Lade die Wetterdaten aus der letzten Übung. 3.3.2 Aufgabe 2 Bereinige den Datensatz. Entferne z.B. alle Zeilen, bei dem der Stationsnahme oder Temperaturwerte fehlen 3.3.3 Aufgabe 3 Überführe die lange Tabelle über in eine breite. Dabei sollte jede Station eine eigene Spalte enthalten (key), gefüllt mit den Temperaturwerten (value). Speichere diese Tabelle in einer neuen Variabel. 3.3.4 Aufgabe 4 Importiere die Datei order_52252_legend.csv (z.B. mit read_delim). Hinweis: Wenn Umlaute und Sonderzeichen nicht korrekt dargestellt werden (z.B. in Genève), hat das vermutlich mit der Zeichencodierung zu tun. Das File ist aktuell in ‘ANSI’ Codiert, welche für gewisse Betriebssysteme / R-Versionen ein Problem darstellt. Um das Problem zu umgehen muss man das File mit einem Editor öffnen (Windows ‘Editor’ oder ‘Notepad++’, Mac: ‘TextEdit’) und mit einer neuen Codierung (z.B ‘UTF-8’) abspeichern. Danach kann die Codierung spezifitiert werden (bei read_delim(): mitlocale = locale(encoding = “UTF-8”)`) 3.3.5 Aufgabe 5 Die x-/y-Koordinaten sind aktuell in einer Spalte erfasst. Um mit den Koordinaten sinnvoll arbeiten zu können, brauchen wir die Koordinaten getrennt. Trenne die x und y Koordinaten aus der Spalte Koordinaten (Tipp: nutze dafür tidyr::separate()). 3.3.6 Aufgabe 6 Nun wollen wir den Datensatz wettermit den Informationen aus wetter_legendeanreichern. Uns interessiert aber nur das Stationskürzel, der Name, die x/y Koordinaten sowie die Meereshöhe. Lösche die nicht benötigten Spalten (oder selektiere die benötigten Spalten). Tipp: Nutze select() von dplyr 3.3.7 Aufgabe 7 Nun ist der Datensatz wetter_legendegenügend vorbereitet. Jetzt kann er mit dem Datensatz wetter verbunden werden. Überlege dir, welcher Join dafür sinnvoll ist und mit welchem Attribut wir “joinen” können. Nutze die Join-Möglichkeiten von dplyr (Hilfe via ?dplyr::join) um die Datensätze wetter und wetter_legendezu verbinden. 3.3.8 Aufgabe 8 Berechne die Durchschnittstemperatur pro Station. Nutze dabei dplyr::summarise() und wenn möglich %&gt;%. Speichere das Resultat in einer neuen Variabel. 3.3.9 Aufgabe 9 Nun wollen wir das Resultat aus Aufgabe 7 nutzen, um die Durchschnittstemperatur der Meereshöhe gegenüber zu stellen. Dummerweise ging das Attribut Meereshoehe bei der summarise() Operation verloren (da bei summarise() alle Spalten weg fallen, die nicht in group_by() definiert wurden). Um die Spalte Meereshoehe beizubehalten, muss sie also unter group_by() aufgelistet werden. Wiederhole Übung 7 und siehe zu, dass die Meereshöhe beibehalten wird. Stelle danach in einem Scatterplot (wenn möglich mit ggplot()) die Meereshöhe der Durchschnittstemperatur gegenüber. "],
["3-4-ubung-a-losung-1.html", "3.4 Übung A: Lösung", " 3.4 Übung A: Lösung R-Code als Download library(tidyverse) library(lubridate) library(stringr) # Lösung Aufgabe 1 wetter &lt;- read_table(&quot;09_PrePro1/data/order_52252_data.txt&quot;, col_types = list( col_factor(levels = NULL), col_datetime(format = &quot;%Y%m%d%H&quot;), col_double() ) ) # Lösung Aufgabe 2 wetter &lt;- wetter %&gt;% filter(!is.na(stn)) %&gt;% filter(!is.na(tre200h0)) # Lösung Aufgabe 3 wetter_spread &lt;- spread(wetter, stn,tre200h0) # Lösung Aufgabe 4 wetter_legende &lt;- read_delim(&quot;09_PrePro1/data/order_52252_legend.csv&quot;,delim = &quot;;&quot;, locale = locale(encoding = &quot;UTF-8&quot;)) # Lösung Aufgabe 5 koordinaten &lt;- str_split_fixed(wetter_legende$Koordinaten, &quot;/&quot;, 2) colnames(koordinaten) &lt;- c(&quot;x&quot;,&quot;y&quot;) wetter_legende &lt;- cbind(wetter_legende,koordinaten) # Lösung Aufgabe 6 wetter_legende &lt;- dplyr::select(wetter_legende, stn, Name, x,y,Meereshoehe) # Lösung Aufgabe 7 wetter &lt;- left_join(wetter,wetter_legende,by = &quot;stn&quot;) # Jointyp: Left-Join auf &#39;wetter&#39;, da uns nur die Stationen im Datensatz &#39;wetter&#39; interessieren. # Attribut: &quot;stn&quot; # Lösung Aufgabe 8 wetter_sry &lt;- wetter %&gt;% group_by(stn) %&gt;% summarise(temp_mean = mean(tre200h0)) # Lösung Aufgabe 9 wetter_sry &lt;- wetter %&gt;% group_by(stn,Meereshoehe) %&gt;% summarise(temp_mean = mean(tre200h0)) # Achtung: wenn mehrere Argumente in group_by() definiert werden führt das # üblicherweise zu Untergruppen. In unserem Fall hat jede Station nur EINE # Meereshöhe, deshalb wird die Zahl der Gruppen nicht erhöht. ggplot(wetter_sry, aes(temp_mean,Meereshoehe)) + geom_point() "],
["3-5-ubung-b-1.html", "3.5 Übung B", " 3.5 Übung B 3.5.1 Aufgabe 1 Gegeben sind die Daten von drei Sensoren (sensor1.csv, sensor2.csv, sensor3.csv). Lade die Datensätze runter und lese sie ein. 3.5.2 Aufgabe 2 Füge die drei Tabellen zu einer zusammen. Dazu kannst du entweder die Spalten (Variablen) mittels join() oder die Zeilen (Beobachtungen) mittels rbind() zusammen “kleben”. Überführe zudem die Spalte Datetime in ein POSIXct-Format. Das ursprüngliche Format lautet:DDMMYYYY_HHMM 3.5.3 Aufgabe 3 Importiere die Datei sensor_1_fail.csv in R. sensor_fail.csv hat eine Variabel SensorStatus: 1 bedeutet der Sensor misst, 0 bedeutet der Sensor miss nicht. Fälschlicherweise wurde auch dann der Messwert Temp = 0 erfasst, wenn Sensorstatus = 0. Richtig wäre hier NA (not available). Korrigiere den Datensatz entsprechend. 3.5.4 Aufgabe 4 Warum spielt das es eine Rolle, ob 0 oder NA erfasst wird? Vergleiche dazu die Mittlere Temperatur / Feuchtigkeit vor und nach der Korrektur. "],
["3-6-ubung-b-losung-1.html", "3.6 Übung B: Lösung", " 3.6 Übung B: Lösung R-Code als Download library(tidyverse) library(lubridate) library(stringr) # Lösung Aufgabe 1 sensor1 &lt;- read_delim(&quot;10_PrePro2/data/sensor1.csv&quot;,&quot;;&quot;) sensor2 &lt;- read_delim(&quot;10_PrePro2/data/sensor2.csv&quot;,&quot;;&quot;) sensor3 &lt;- read_delim(&quot;10_PrePro2/data/sensor3.csv&quot;,&quot;;&quot;) # Lösung Aufgabe 2 (Var 1: Spalten [Variabeln] zusammen &#39;kleben&#39;) sensor_all &lt;- sensor1 %&gt;% rename(sensor1 = Temp) %&gt;% # Spalte &quot;Temp&quot; in &quot;sensor1&quot; umbenennen full_join(sensor2,by = &quot;Datetime&quot;) %&gt;% rename(sensor2 = Temp) %&gt;% full_join(sensor3, by = &quot;Datetime&quot;) %&gt;% rename(sensor3 = Temp) %&gt;% mutate(Datetime = as.POSIXct(Datetime,format = &quot;%d%m%Y_%H%M&quot;)) # Lösung Aufgabe 2 (Var 2: Zeilen [Beobachtungen] zusammen &#39;kleben) sensor1$sensor &lt;- &quot;sensor1&quot; sensor2$sensor &lt;- &quot;sensor2&quot; sensor3$sensor &lt;- &quot;sensor3&quot; sensor_all &lt;- rbind(sensor1,sensor2,sensor3) sensor_all &lt;- sensor_all %&gt;% mutate( Datetime = as.POSIXct(Datetime,format = &quot;%d%m%Y_%H%M&quot;) ) %&gt;% spread(sensor, Temp) # Lösung Aufgabe 3 sensor_fail &lt;- read_delim(&quot;10_PrePro2/data/sensor_fail.csv&quot;, delim = &quot;;&quot;) # Lösungsweg 1 sensor_fail$Datetime &lt;- as.POSIXct(sensor_fail$Datetime,format = &quot;%d%m%Y_%H%M&quot;) sensor_fail$`Hum_%`[sensor_fail$SensorStatus == 0] &lt;- NA sensor_fail$Temp[sensor_fail$SensorStatus == 0] &lt;- NA # Lösungsweg 2 sensor_fail &lt;- read_delim(&quot;10_PrePro2/data/sensor_fail.csv&quot;, delim = &quot;;&quot;) sensor_fail_corr &lt;- sensor_fail %&gt;% mutate( Datetime = as.POSIXct(Datetime,format = &quot;%d%m%Y_%H%M&quot;) ) %&gt;% rename(Humidity = `Hum_%`) %&gt;% # Weil R &quot;%&quot; in Headers nicht mag gather(key,val, c(Temp, Humidity)) %&gt;% mutate( val = ifelse(SensorStatus == 0,NA,val) ) %&gt;% spread(key,val) # Lösung Aufgabe 4 # Mittelwerte der unkorrigierten Sensordaten (`NA` als `0`) mean(sensor_fail$Temp) mean(sensor_fail$`Hum_%`) # Mittelwerte der korrigierten Sensordaten (`NA` als `NA`). Hier müssen wir die Option # `na.rm = T` (Remove NA = T) wählen, denn `mean()` (und ähnliche Funktionen) retourieren # immer `NA`, sobald ein **einzelner** Wert in der Reihe `NA`ist. mean(sensor_fail_corr$Temp, na.rm = T) mean(sensor_fail_corr$Humidity, na.rm = T) "],
["4-infovis1-21-10-2019.html", "Kapitel 4 InfoVis1 (21.10.2019)", " Kapitel 4 InfoVis1 (21.10.2019) Die konventionelle schliessende Statistik arbeitet in der Regel konfirmatorisch, sprich aus der bestehenden Theorie heraus werden Hypothesen formuliert, welche sodann durch Experimente geprüft und akzeptiert oder verworfen werden. Die Explorative Datenanalyse (EDA) nimmt dazu eine antagonistische Analyseperspektive ein und will in den Daten zunächst Zusammenhänge aufdecken, welche dann wiederum zur Formulierung von prüfbaren Hypothesen führen kann. Die Einheit stellt dazu den klassischen 5-stufigen EDA-Prozess nach Tukey (1980!) vor. Abschliessend wird dann noch die Brücke geschlagen zur modernen Umsetzung der EDA in Form von Visual Analytics. "],
["4-1-eda-beispiel-vorlesung.html", "4.1 EDA Beispiel Vorlesung", " 4.1 EDA Beispiel Vorlesung Demoscript als Download library(tidyverse) library(scales) # create some data about age and height of people people &lt;- data.frame( ID = c(1:30), age = c(5.0, 7.0, 6.5 ,9.0, 8.0, 5.0, 8.6, 7.5, 9.0, 6.0, 63.5 ,65.7, 57.6, 98.6, 76.5, 78.0, 93.4, 77.5, 256.6, 512.3, 15.5, 18.6, 18.5, 22.8, 28.5, 39.5, 55.9, 50.3, 31.9, 41.3), height = c(0.85, 0.93, 1.1, 1.25, 1.33, 1.17, 1.32, 0.82, 0.89, 1.13, 1.62, 1.87, 1.67, 1.76, 1.56, 1.71, 1.65, 1.55, 1.87, 1.69, 1.49, 1.68, 1.41, 1.55, 1.84, 1.69, 0.85, 1.65, 1.94, 1.80), weight = c(45.5, 54.3, 76.5, 60.4, 43.4, 36.4, 50.3, 27.8, 34.7, 47.6, 84.3, 90.4, 76.5, 55.6, 54.3, 83.2, 80.7, 55.6, 87.6, 69.5, 48.0, 55.6, 47.6, 60.5, 54.3, 59.5, 34.5, 55.4, 100.4, 110.3) ) # build a scatterplot for a first inspection ggplot(people, aes(x=age, y=height)) + geom_point() + scale_y_continuous(limits=c(0.75, 2.0)) + theme(axis.text=element_text(size=12), axis.title=element_text(size=20,face=&quot;bold&quot;)) # Go to help page: http://docs.ggplot2.org/current/ -&gt; Search for icon of fit-line # http://docs.ggplot2.org/current/geom_smooth.html # build a scatterplot for a first inspection, with regression line ggplot(people, aes(x=age, y=height)) + geom_point() + scale_y_continuous(limits=c(0, 2.0)) + geom_smooth(method=&quot;loess&quot;, fill=&#39;lightblue&#39;, size=0.5, alpha=0.5) + theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face=&quot;bold&quot;)) ?stem # stem and leaf plot stem(people$height) ## ## The decimal point is 1 digit(s) to the left of the | ## ## 8 | 25593 ## 10 | 037 ## 12 | 523 ## 14 | 19556 ## 16 | 255789916 ## 18 | 04774 stem(people$height, scale=2) ## ## The decimal point is 1 digit(s) to the left of the | ## ## 8 | 2559 ## 9 | 3 ## 10 | ## 11 | 037 ## 12 | 5 ## 13 | 23 ## 14 | 19 ## 15 | 556 ## 16 | 2557899 ## 17 | 16 ## 18 | 0477 ## 19 | 4 # explore the two variables with box-whiskerplots summary(people$age) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 5.00 8.70 30.20 59.14 65.15 512.30 boxplot(people$age) boxplot(people$age) summary(people$height) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.820 1.190 1.555 1.455 1.690 1.940 boxplot(people$height) boxplot(people$height) # explore data with a histgram ggplot(people, aes(x=age)) + geom_histogram(stat=&quot;bin&quot;, fill=&#39;green&#39;, binwidth=20) + theme_bw() + labs(x = &#39;\\nage&#39;, y = &#39;count\\n&#39;) + theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face=&quot;bold&quot;)) density(x = people$height) ## ## Call: ## density.default(x = people$height) ## ## Data: people$height (30 obs.); Bandwidth &#39;bw&#39; = 0.1576 ## ## x y ## Min. :0.3472 Min. :0.001593 ## 1st Qu.:0.8636 1st Qu.:0.102953 ## Median :1.3800 Median :0.510601 ## Mean :1.3800 Mean :0.483553 ## 3rd Qu.:1.8964 3rd Qu.:0.722660 ## Max. :2.4128 Max. :1.216350 # re-expression: use log or sqrt axes # # Find here guideline about scaling axes # http://www.cookbook-r.com/Graphs/Axes_(ggplot2)/ # http://docs.ggplot2.org/0.9.3.1/scale_continuous.html # logarithmic axis: respond to skewness in the data, e.g. log10 ggplot(people, aes(x=age, y=height)) + geom_point() + scale_y_continuous(limits=c(0, 2.0)) + geom_smooth(method=&quot;lm&quot;, fill=&#39;lightblue&#39;, size=0.5, alpha=0.5) + theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face=&quot;bold&quot;)) + scale_x_log10() # logarithmic axis: show multiplicative factors, e.g. log2 ggplot(people, aes(x=age, y=height)) + geom_point() + scale_y_continuous(limits=c(0, 2.0)) + geom_smooth(method=&quot;lm&quot;, fill=&#39;lightblue&#39;, size=0.5, alpha=0.5) + theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face=&quot;bold&quot;)) + scale_x_continuous(trans = log2_trans(), breaks = trans_breaks(&quot;log2&quot;, function(x) 2^x), labels = trans_format(&quot;log2&quot;, math_format(2^.x))) # outliers: Remove very small and very old people peopleTemp &lt;- subset(people, ID != 27) # Diese Person war zu klein. peopleClean &lt;- subset(peopleTemp, age &lt; 100) # Fehler in der Erhebung des Alters # re-explore cleaned data with a histgram ggplot(peopleClean, aes(x=age)) + geom_histogram(stat=&quot;bin&quot;, fill=&#39;#6baed6&#39;, binwidth=10) + theme_bw() + labs(x = &#39;\\nage&#39;, y = &#39;count\\n&#39;) + theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face=&quot;bold&quot;)) ggplot(peopleClean, aes(x=age, y=height)) + geom_point() + scale_y_continuous(limits=c(0, 2.0)) + geom_smooth(method=&quot;lm&quot;, fill=&#39;lightblue&#39;, size=0.5, alpha=0.5) + theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face=&quot;bold&quot;)) # with custom binwidth ggplot(peopleClean, aes(x=age)) + geom_histogram(stat=&quot;bin&quot;, fill=&#39;#6baed6&#39;, binwidth=10) + theme_bw() + labs(x = &#39;\\nAlter&#39;, y = &#39;Anzahl\\n&#39;) # quadratic axis ggplot(peopleClean, aes(x=age, y=height)) + geom_point() + scale_y_continuous(limits=c(0, 2.0)) + geom_smooth(method=&quot;lm&quot;, fill=&#39;lightblue&#39;, size=0.5, alpha=0.5) + theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face=&quot;bold&quot;)) + scale_x_sqrt() # subset &quot;teenies&quot;: No trend kids &lt;- subset(peopleClean, age &lt; 15) ggplot(kids, aes(x=age, y=height)) + geom_point() + scale_y_continuous(limits=c(0, 2.0)) + geom_smooth(method=&quot;lm&quot;, fill=&#39;lightblue&#39;, size=0.5, alpha=0.5) + theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face=&quot;bold&quot;)) # subset &quot;teenies&quot;: No trend oldies &lt;- subset(peopleClean, age &gt; 55) ggplot(oldies, aes(x=age, y=height)) + geom_point() + scale_y_continuous(limits=c(0, 2.0)) + geom_smooth(method=&quot;lm&quot;, fill=&#39;lightblue&#39;, size=0.5, alpha=0.5) + theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face=&quot;bold&quot;)) # Onwards towards multidimensional data # Finally, make a scatterplot matrix pairs(peopleClean[,2:4], panel=panel.smooth) pairs(peopleClean[,2:4], panel=panel.smooth) # Or as a bubble chart peopleClean$radius &lt;- sqrt( peopleClean$weight/ pi ) symbols(peopleClean$age, peopleClean$height, circles=peopleClean$radius) symbols(peopleClean$age, peopleClean$height, circles=peopleClean$radius) 4.1.1 Quellen Dieses Kapitel verwendet folgende Libraries: Wickham (2018b), Wickham (2018a), Wickham (2018c), Wickham, François, et al. (2018), Henry and Wickham (2018), Wickham, Hester, and Francois (2017), Wickham and Henry (2018), Müller and Wickham (2018), Wickham, Chang, et al. (2018), Wickham (2017) library(tidyverse) library(lubridate) References "],
["4-2-demo-ggplot2.html", "4.2 Demo: ggplot2", " 4.2 Demo: ggplot2 Demoscript als Download Als erstes laden wir den Wetterdatensatz von der Übung Prepro1 ein. wetter &lt;- read_table(&quot;09_PrePro1/data/order_52252_data.txt&quot;, col_types = list( col_factor(levels = NULL), col_datetime(format = &quot;%Y%m%d%H&quot;), col_double() ) ) stn time tre200h0 ABO 2000-01-01 00:00:00 -2.6 ABO 2000-01-01 01:00:00 -2.5 ABO 2000-01-01 02:00:00 -3.1 ABO 2000-01-01 03:00:00 -2.4 ABO 2000-01-01 04:00:00 -2.5 ABO 2000-01-01 05:00:00 -3.0 Der Datensatz hat 1262615 Zeilen. Bevor wir mit plotten beginnen, müssen wir den Datensatz etwas filtern da die Plots ansonsten zu schwerfällig werden. Wir filtern deshalb auf Januar 2000. wetter_fil &lt;- wetter %&gt;% mutate( year = year(time), month = month(time) ) %&gt;% filter(year == 2000 &amp; month == 1) Ein ggplot wird durch den Befehl ggplot() initiiert. Hier wird einerseits der Datensatz festgelegt, auf dem der Plot beruht (data =), sowie die Variablen innerhalb des Datensatzes, die Einfluss auf den Plot ausüben (mapping = aes()). Weiter braucht es mindestens ein “Layer” der beschreibt, wie die Daten dargestellt werden sollen (z.B. geom_point()). Anders als bei “Piping” (%&gt;%) wird ein Layer mit + hinzugefügt. # Datensatz: &quot;wetter_fil&quot; | Beeinflussende Variabeln: &quot;time&quot; und &quot;tre200h0&quot; ggplot(data = wetter_fil, mapping = aes(time,tre200h0)) + # Layer: &quot;geom_point&quot; entspricht Punkten in einem Scatterplot geom_point() Da ggplot die Eingaben in der Reihenfolge data = und dann mapping =erwartet, können wir diese Spezifizierungen auch weglassen. ggplot(wetter_fil, aes(time,tre200h0)) + geom_point() Nun wollen wir die unterschiedlichen Stationen unterschiedlich einfärben. Da wir Variablen definieren wollen, welche Einfluss auf die Grafik haben sollen, gehört diese Information in aes(). ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) + geom_point() Wir können noch einen Layer mit Linien hinzufügen: ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) + geom_point() + geom_line() Weiter können wir die Achsen beschriften und einen Titel hinzufügen. Zudem lasse ich die Punkte (geom_point()) nun weg, da mir diese nicht gefallen. ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) + geom_line() + labs(x = &quot;Woche&quot;, y = &quot;Temperatur in Grad C°&quot;, title = &quot;Temperaturdaten Schweiz&quot;, subtitle = &quot;Januar 2000&quot;) Man kann auch Einfluss auf die x-/y-Achsen nehmen. Dabei muss man zuerst festlegen, was für ein Achsentyp der Plot hat (vorher hat ggplot eine Annahme auf der Basis der Daten getroffen). Bei unserer y-Achse handelt es sich um numerische Daten, ggplot nennt diese: scale_y_continuous(). Unter ggplot2.tidyverse.org findet man noch andere x/y-Achsentypen (scale_x_irgenwas bzw. scale_y_irgendwas). ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) + geom_line() + labs(x = &quot;Woche&quot;, y = &quot;Temperatur in Grad C°&quot;, title = &quot;Temperaturdaten Schweiz&quot;, subtitle = &quot;Januar 2000&quot;) + scale_y_continuous(limits = c(-30,30)) # y-Achsenabschnitt bestimmen Das gleiche Spiel kann man für die y-Achse betreiben. Bei unserer y-Achse handelt es sich ja um unsere POSIXct Daten. ggplot nennt diese: scale_x_datetime(). ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) + geom_line() + labs(x = &quot;Woche&quot;, y = &quot;Temperatur in Grad C°&quot;, title = &quot;Temperaturdaten Schweiz&quot;, subtitle = &quot;Januar 2000&quot;) + scale_y_continuous(limits = c(-30,30)) + scale_x_datetime(date_breaks = &quot;1 week&quot;, date_minor_breaks = &quot;1 day&quot;, date_labels = &quot;KW%W&quot;) Mit theme verändert man das allgmeine Layout der Plots. Beispielsweise kann man mit theme_classic() ggplot-Grafiken etwas weniger “Poppig” erscheinen lassen: so sind sie besser für Bachelor- / Masterarbeiten sowie Publikationen geeignet. theme_classic() kann man indiviudell pro Plot anwenden, oder für die aktuelle Session global setzen (s.u.) Individuell pro Plot: ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) + geom_line() + labs(x = &quot;Woche&quot;, y = &quot;Temperatur in Grad C°&quot;, title = &quot;Temperaturdaten Schweiz&quot;, subtitle = &quot;Januar 2000&quot;) + scale_y_continuous(limits = c(-30,30)) + scale_x_datetime(date_breaks = &quot;1 week&quot;, date_minor_breaks = &quot;1 day&quot;, date_labels = &quot;KW%W&quot;) + theme_classic() Global (für alle nachfolgenden Plots der aktuellen Session): theme_set(theme_classic()) Sehr praktisch sind auch die Funktionen für “Small multiples”. Dies erreicht man mit facet_wrap() (oder facet_grid(), mehr dazu später). Man muss mit einem Tilde-Symbol “~” nur festlegen, welche Variable für das Aufteilen des Plots in kleinere Subplots verantwortlich sein soll. ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) + geom_line() + labs(x = &quot;Woche&quot;, y = &quot;Temperatur in Grad C°&quot;, title = &quot;Temperaturdaten Schweiz&quot;, subtitle = &quot;Januar 2000&quot;) + scale_y_continuous(limits = c(-30,30)) + scale_x_datetime(date_breaks = &quot;2 weeks&quot;, date_minor_breaks = &quot;1 day&quot;, date_labels = &quot;KW%W&quot;) + facet_wrap(~stn) Auch facet_wrap kann man auf seine Bedürfnisse anpassen. Da wir 24 Stationen haben möchte ich lieber 3 pro Zeile, damit es schön aufgeht. Dies erreiche ich mit ncol = 3. Zudem brauchen wir die Legende nicht mehr, da der Stationsnamen über jedem Facet steht. Ich setze deshalb theme(legend.position=&quot;none&quot;) ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) + geom_line() + labs(x = &quot;Woche&quot;, y = &quot;Temperatur in Grad C°&quot;, title = &quot;Temperaturdaten Schweiz&quot;, subtitle = &quot;Januar 2000&quot;) + scale_y_continuous(limits = c(-30,30)) + scale_x_datetime(date_breaks = &quot;1 week&quot;, date_minor_breaks = &quot;1 day&quot;, date_labels = &quot;KW%W&quot;) + facet_wrap(~stn,ncol = 3) + theme(legend.position=&quot;none&quot;) Genau wie data.frames und andere Objekte, kann man einen ganzen Plot auch in einer Variabel speichern. Dies kann nützlich sein um einen Plot zu exportieren (als png, jpg usw.) oder sukzessive erweitern wie in diesem Beispiel. p &lt;- ggplot(wetter_fil, aes(time,tre200h0, colour = stn)) + geom_line() + labs(x = &quot;Woche&quot;, y = &quot;Temperatur in Grad C°&quot;, title = &quot;Temperaturdaten Schweiz&quot;, subtitle = &quot;Januar 2000&quot;) + scale_y_continuous(limits = c(-30,30)) + scale_x_datetime(date_breaks = &quot;1 week&quot;, date_minor_breaks = &quot;1 day&quot;, date_labels = &quot;KW%W&quot;) + facet_wrap(~stn,ncol = 3) # ich habe an dieser Stelle theme(legend.position=&quot;none&quot;) entfernt Folgendermassen kann ich den Plot als png-File abspeichern (ohne Angabe von “plot =” wird einfach der letzte Plot gespeichert) ggsave(filename = &quot;11_InfoVis1/plot.png&quot;,plot = p) .. und so kann ich einen bestehenden Plot (in einer Variabel) mit einem Layer / einer Option erweitern p + theme(legend.position=&quot;none&quot;) Wie üblich wurde diese Änderung nicht gespeichert, sondern nur das Resultat davon ausgeben. Wenn die Änderung in meinem Plot (in der Variabel) abspeichern will, muss ich die Variabel überschreiben: p &lt;- p + theme(legend.position=&quot;none&quot;) Mit geom_smooth() kann ggplot eine Trendlinie auf der Baiss von Punktdaten berechnen. Die zugrunde liegende statistische Methode kann selbst gewählt werden. Wenn nichts angegeben wird verwendet ggplot bei weniger als 1’000 Messungen, die Methode loess (local smooths). p &lt;- p + geom_smooth(colour = &quot;black&quot;) p 4.2.1 Quellen Dieses Kapitel verwendet folgende Libraries: Spinu, Grolemund, and Wickham (2018), Wickham (2018a), Wickham (2018c), Wickham, François, et al. (2018), Henry and Wickham (2018), Wickham, Hester, and Francois (2017), Wickham and Henry (2018), Müller and Wickham (2018), Wickham, Chang, et al. (2018), Wickham (2017) References "],
["4-3-ubung.html", "4.3 Übung", " 4.3 Übung In dieser Übung geht es darum, die Grafiken aus dem Blog-post von Marko Kovic (blog.tagesanzeiger.ch) zu rekonstruieren. Freundlicherweise hat Herr Kovic meist die ggplot2 Standardeinstellungen benutzt, was die Rekonstruktion relativ einfach macht. Die Links im Text verweisen auf die Originalgrafik, die eingebetteten Plots sind meine eigenen Rekonstruktionen. Importiere als erstes den Datensatz initiative_masseneinwanderung_kanton.csv (auf der Blog-Seite erhältlich). 4.3.1 Aufgabe 1 Rekonstruiere Grafik 1 von Kovic. Erstelle dazu einen Scatterplot wo der Ausländeranteil der Kantone dem Ja-Anteil gegenüber gestellt wird. Speichere den Plot einer Variabel plot1. nutze coord_fixed() um die beiden Achsen in ein fixes Verhältnis zu setzen (1:1). setze die Achsen Start- und Endwerte mittels lims() oder scale_y_continuousbzw. scale_x_continuous. Optional: Setze analog Kovic die breaks (0.0, 0.1…0.7) manuell Rekonstruktion: 4.3.2 Aufgabe 2 Rekonstruiere Grafik 2. Erweitere dazu plot1 mit einer Trendlinie. 4.3.3 Aufgabe 3 Importiere die Gemeindedaten initiative_masseneinwanderung_gemeinde.csv: gemeinde &lt;- read_delim(&quot;11_InfoVis1/data/initiative_masseneinwanderung_gemeinde.csv&quot;,&quot;,&quot;,locale = locale(encoding = &quot;UTF-8&quot;)) Rekonstruiere Grafik 3. Stelle dazu den Ausländeranteil aller Gemeinden dem Ja-Stimmen-Anteil gegenüber. Speichere den Plot als plot2 4.3.4 Aufgabe 4 Rekonstruiere Grafik 4 indem plot2 mit einer Trendlinie erweitert wird. 4.3.5 Aufgabe 5 Rekonstruiere Grafik 5 indem plot2 mit facetting erweitert wird. Die Facets sollen die einzelnen Kantone sein. Speichere den Plot als plot3. 4.3.6 Aufgabe 6 Rekonstruiere Grafik 6 indem plot3 mit einer Trendlinie erweitert wird. Rekonstruktion: 4.3.7 Aufgabe 7 Rekonstruiere Grafik 7 indem plot2mit facetting erweitert wird. Die Facets sollen nun den Grössen-Quantilen entsprechen. Speichere den Plot unter plot4. Rekonstruktion: 4.3.8 Aufgabe 8 Rekonstruiere Grafik 8 indem plot4 mit einer Trendlinie ausgestattet wird. 4.3.9 Aufgabe 9 (Fortgeschritten) Rekonstruiere die Korrelationstabelle. Tipp: - Nutze group_by() und summarise() - Nutze cor.test() um den Korrelationskoeffizienten sowie den p-Wert zu erhalten. - Mit $estimate und $p.value können die entsprechenden Werte direkt angesprochen werden Hinweis: aus bisher unerklärlichen Gründen weiche gewisse meiner Werte leicht von den Berechnungen des Herrn Kovics ab. Kanton Korr.Koeffizient Signifikanz AG -0.2362552 *** AI -0.7828022 - AR -0.0892817 - BE -0.4422003 *** BL -0.2919712 ** BS -0.9935385 - FR -0.4217634 *** GE 0.3753004 * GL -0.4070120 - GR -0.0426607 - JU -0.2252540 - LU -0.3048455 ** NE -0.5214180 *** NW -0.2018174 - OW -0.4813090 - SG -0.2449093 * SH -0.2995527 - SO -0.0533442 - SZ -0.7259276 *** TG -0.5522862 *** TI 0.1512509 - UR -0.3848167 - VD -0.2685301 *** VS -0.1736954 * ZG 0.0407166 - ZH -0.2744683 *** "],
["4-4-losung.html", "4.4 Lösung", " 4.4 Lösung RCode als Download library(tidyverse) library(ggplot2) library(stringr) ## # Es kann sein, dass man die Codierung des Files spezifizieren muss. Mit `readr::read_delim()` ## # läuft dies mit der Option locale = locale(encoding = &quot;UTF-8&quot;) wobei anstelle von UTF-8 die ## # entsprechende Codierung angegeben wird. ## # Tipp: Excel speichert CSV oft in ANSI, welches für den Import in R nicht sonderlich geeignet ## # ist. Falls Probleme auftreten muss das File mittels einer geeigneter Software (Widows: &quot;Editor&quot; ## # oder &quot;Notepad++&quot;, Mac: &quot;TextEdit&quot;) und mit einer neuen Codierung (z.B. `UTF-8`) abgespeichert ## # werden. kanton &lt;- read_delim(&quot;11_InfoVis1/data/initiative_masseneinwanderung_kanton.csv&quot;,&quot;,&quot;,locale = locale(encoding = &quot;UTF-8&quot;)) # Lösung zu Aufgabe 1 # da die Spalten in Kovic&#39;s Daten Umlaute und Sonderzeichen enthalten, müssen diese in R mit Graviszeichen # angesprochen werden. Dieses Zeichen wirder Schweizer Tastatur [1] mit # Shitft + Gravis (Links von der Backspace taste) + Leerschlag erstellt # [1] https://de.wikipedia.org/wiki/Tastaturbelegung#Schweiz # Alternativ können die Spalten im Originalfile oder mit dplyr::rename() umbenannt werden plot1 &lt;- ggplot(kanton, aes(`Ausländeranteil`, `Ja-Anteil`)) + geom_point() + coord_fixed(1) + scale_y_continuous(breaks = c(0,0.1,0.3,0.5,0.7),limits = c(0,0.7)) + scale_x_continuous(breaks = c(0,0.1,0.3,0.5,0.7),limits = c(0,0.7)) + labs(y = &quot;Anteil Ja-Stimmen&quot;) plot1 # Lösung zu Aufgabe 2 plot1 + geom_smooth() gemeinde &lt;- read_delim(&quot;11_InfoVis1/data/initiative_masseneinwanderung_gemeinde.csv&quot;,&quot;,&quot;,locale = locale(encoding = &quot;UTF-8&quot;)) # Lösung zu Aufgabe 3 plot2 &lt;- ggplot(gemeinde, aes(`Anteil Ausl`, `Anteil Ja`)) + geom_point() + labs(x = &quot;Ausländeranteil&quot;,y = &quot;Anteil Ja-Stimmen&quot;) + coord_fixed(1) + lims(x = c(0,1), y = c(0,1)) plot2 # Lösung zu Aufgabe 4 plot2 + geom_smooth() # Lösung zu Aufgabe 5 plot3 &lt;- plot2 + facet_wrap(~Kanton) plot3 # Lösung zu Aufgabe 6 plot3 + geom_smooth() # Lösung zu Aufgabe 7 plot4 &lt;- plot2 + facet_wrap(~Quantile) plot4 # Lösung zu Aufgabe 8 plot4 + geom_smooth() # Lösung zu Aufgabe 9 korr_tab &lt;- gemeinde %&gt;% group_by(Kanton) %&gt;% summarise( Korr.Koeffizient = cor.test(`Anteil Ja`,`Anteil Ausl`,method = &quot;pearson&quot;)$estimate, Signifikanz_val = cor.test(`Anteil Ja`,`Anteil Ausl`,method = &quot;pearson&quot;)$p.value, Signifikanz = ifelse(Signifikanz_val &lt; 0.001,&quot;***&quot;,ifelse(Signifikanz_val&lt;0.01,&quot;**&quot;,ifelse(Signifikanz_val&lt;0.05,&quot;*&quot;,&quot;-&quot;))) ) %&gt;% select(-Signifikanz_val) "],
["5-infovis2-22-10-2019.html", "Kapitel 5 InfoVis2 (22.10.2019)", " Kapitel 5 InfoVis2 (22.10.2019) Die Informationsvisualisierung ist eine vielseitige, effektive und effiziente Methode für die explorative Datenanalyse. Während Scatterplots und Histogramme weitherum bekannt sind, bieten weniger bekannte Informationsvisualisierungs-Typen wie etwa Parallelkoordinatenplots, TreeMaps oder Chorddiagramme originelle alternative Darstellungsformen zur visuellen Analyse von Datensätze, welche stets grösser und komplexer werden. Die Studierenden lernen in dieser Lerneinheit eine Reihe von Informationsvisualisierungstypen kennen, lernen diese zielführend zu gestalten und selber zu erstellen. "],
["5-1-ubung-a-2.html", "5.1 Übung A", " 5.1 Übung A library(tidyverse) library(lubridate) Laden den wetter-Datensatz, bereinige ihn wenn nötig (NA-Werte entfernen) und importiere auch den Datensatz order_52252_legend.csv und verbinde die Datensätze mit einem join via dem Stationskürzel. wetter &lt;- read_table(&quot;09_PrePro1/data/order_52252_data.txt&quot;, col_types = list( col_character(), col_datetime(format = &quot;%Y%m%d%H&quot;), col_double() ) ) wetter &lt;- wetter %&gt;% filter(!is.na(stn)) %&gt;% filter(!is.na(time)) station_meta &lt;- read_delim(&quot;09_PrePro1/data/order_52252_legend.csv&quot;,&quot;;&quot;) wetter &lt;- left_join(wetter,station_meta,by = &quot;stn&quot;) 5.1.1 Aufgabe 1 Erstelle zwei Hilfsspalten (convenience variables) “Jahr” und “Monat”. Filtere auf ein beliebiges Jahr und zwei beliebige Monate. Speichere den gefilterten Datensatz in einer neuen Variablen ab. Verwende diesen Datensatz für alle folgenden Übungen. 5.1.2 Aufgabe 2 Erstelle ein Scatterplot (time vs. tre200h0) wobei die Punkte aufgrund ihrer Meereshöhe eingefärbt werden sollen. Tiefe Werte sollen dabei blau eingefärbt werden und hohe Werte rot. Verkleinere die Punkte um übermässiges Überplotten der Punkten zu vermeiden. Weiter sollen im Abstand von zwei Wochen die Kalenderwochen auf der Achse erscheinen. Speichere den Plot in einer Variabel p ab. 5.1.3 Aufgabe 3 Füge am obigen Plot (gespeichert als Variabel p) eine schwarze, gestrichelte Trendlinie hinzu und aktualisiere p (p &lt;- p + ...). 5.1.4 Aufgabe 4 Positioniere die Legende oberhalb des Plots und lege sie quer (nutze dazu theme() mit legend.direction und legend.position). Speichere diese Änderungen in p. 5.1.5 Aufgabe 5 (für ambitionierte) Füge den Temperaturwerten auf der y-Ache ein °C hinzu (siehe unten und studiere diesen Tipp zur Hilfe). Aktualisiere p an dieser Stelle noch nicht. 5.1.6 Aufgabe 6 (für noch ambitioniertere) Füge dem Plot eine zweite, korrekt ausgerichtete Achse mit Kelvin oder Farenheit hinzu (siehe sec_axis). Wenn du es vorherigen Übung schon geschafft hast, setze auch hier die Einheit (K rep. °F) hinter die Werte auf der Achse. \\[ K = °C + 273,15\\] \\[°F = °C × \\frac{9}{5} + 32\\] 5.1.7 Aufgabe 7 Jetzt verlassen wir den scatterplot und machen einen Boxplot mit den Temperaturdaten. Färbe die Boxplots wieder in Abhängigkeit der Meereshöhe ein. Beachte den Unterschied zwischen colour = und fill = Beachte den Unterschied zwischen facet_wrap() und facet_grid() facet_grid() braucht übrigens noch einen Punkt (.) zur Tilde (~). Beachte den Unterschied zwischen “.~” und “~.” bei facet_grid() verschiebe nach Bedarf die Legende 5.1.8 Aufgabe 8 Teile die Stationen in verschiedene Höhenlagen ein (Tieflage [&lt; 450 m], Mittellage [450 - 1000 m] und Hochlage [&gt; 1’000 m]). Vergleiche die Verteilung der Temperaturwerte in den verschiedenen Lagen. Nutze dazu facet_grid um die Höhenlage dem Monat gegenüber zu stellen (Monat~Lage) Passe scales = an damit keine leeren Stellen auf der x-Achse entstehen Optional: Verwende den vollen Stationsnamen anstelle des Kürzels und drehe diese ab damit sie sich gegenseitig nicht überschreiben 5.1.9 Aufgabe 9 Als letzter wichtiger Plottyp noch zwei Übungen zum Histogramm. Erstelle ein Histogramm geom_histogram() mit den Temperaturwerten. Färbe Säulen aufgrund ihrer Höhenlage ein und die Begrenzungslinie weiss. Setze die Klassenbreite auf 1 Grad. 5.1.10 Aufgabe 10 Erstelle facets aufgrund der Höhenlage. Setze noch eine Vertikale linie beim Nullpunkt und stelle den x-Achsenabschnit symmetrisch ein (z.B -30 bis + 30°C). "],
["5-2-ubung-a-losung-2.html", "5.2 Übung A: Lösung", " 5.2 Übung A: Lösung RCode als Download library(tidyverse) library(lubridate) wetter &lt;- read_table(&quot;09_PrePro1/data/order_52252_data.txt&quot;, col_types = list( col_character(), col_datetime(format = &quot;%Y%m%d%H&quot;), col_double() ) ) wetter &lt;- wetter %&gt;% filter(!is.na(stn)) %&gt;% filter(!is.na(time)) station_meta &lt;- read_delim(&quot;09_PrePro1/data/order_52252_legend.csv&quot;,&quot;;&quot;) wetter &lt;- left_join(wetter,station_meta,by = &quot;stn&quot;) # Lösung Aufgabe 1 wetter_fil &lt;- wetter %&gt;% mutate( year = year(time), month = month(time) ) %&gt;% filter(year == 2000 &amp; month &lt; 3) # Lösung Aufgabe 2 p &lt;- ggplot(wetter_fil, aes(time,tre200h0, colour = Meereshoehe)) + geom_point(size = 0.5) + labs(x = &quot;Kalenderwoche&quot;, y = &quot;Temperatur in ° Celsius&quot;) + scale_color_continuous(low = &quot;blue&quot;, high = &quot;red&quot;) + scale_x_datetime(date_breaks = &quot;2 week&quot;, date_labels = &quot;KW%W&quot;) p # Lösung Aufgabe 3 p &lt;- p + stat_smooth(colour = &quot;black&quot;,lty = 2) p # Lösung Aufgabe 4 p &lt;- p + theme(legend.direction = &quot;horizontal&quot;,legend.position = &quot;top&quot;) p # Lösung Aufgabe 5 p + scale_y_continuous(labels = function(x)paste0(x,&quot;°C&quot;)) + labs(x = &quot;Kalenderwoche&quot;, y = &quot;Temperatur&quot;) # Lösung Aufgabe 6 p &lt;- p + labs(x = &quot;Kalenderwoche&quot;, y = &quot;Temperatur&quot;) + scale_y_continuous(labels = function(x)paste0(x,&quot;°C&quot;),sec.axis = sec_axis(~.*(9/5)+32,name = &quot;Temperatur&quot;,labels = function(x)paste0(x,&quot;° F&quot;))) p # Lösung Aufgabe 7 wetter_fil &lt;- mutate(wetter_fil,monat = month(time,label = T,abbr = F)) ggplot(wetter_fil, aes(stn,tre200h0, fill = Meereshoehe)) + geom_boxplot() + facet_grid(monat~.) + labs(x = &quot;Station&quot;, y = &quot;Temperatur&quot;) + theme(legend.direction = &quot;horizontal&quot;,legend.position = &quot;top&quot;) # Lösung Aufgabe 8 wetter_fil$Lage[wetter_fil$Meereshoehe &lt; 450] &lt;- &quot;Tieflage&quot; wetter_fil$Lage[wetter_fil$Meereshoehe &gt;= 450 &amp; wetter_fil$Meereshoehe &lt;1000] &lt;- &quot;Mittellage&quot; wetter_fil$Lage[wetter_fil$Meereshoehe &gt;= 1000] &lt;- &quot;Hochlage&quot; ggplot(wetter_fil, aes(Name,tre200h0)) + geom_boxplot() + facet_grid(monat~Lage, scales = &quot;free_x&quot;) + labs(x = &quot;Lage&quot;, y = &quot;Temperatur&quot;) + theme(axis.text.x = element_text(angle = 45,hjust = 1)) # Lösung Aufgabe 9 h &lt;- ggplot(wetter_fil,aes(tre200h0, fill = Lage)) + geom_histogram(binwidth = 1, colour = &quot;white&quot;) + labs(x = &quot;Temperatur in °C&quot;, y = &quot;Anzahl&quot;) h # Lösung Aufgabe 10 h + geom_vline(xintercept = 0, lty = 2, alpha = 0.5) + facet_wrap(~Lage) + lims(x = c(-30,30)) + theme(legend.position = &quot;none&quot;) "],
["5-3-ubung-b-2.html", "5.3 Übung B", " 5.3 Übung B In dieser Übung bauen wir einige etwas unübliche Plots aus der Vorlesung nach. Dafür verwenden wir Datensätze, die in R bereits integriert sind. Eine Liste dieser Datensätze findet man hier oder mit der Hilfe ?datasets. Dazu verwenden wir vor allem das Package plotly welches im Gegensatz zu ggplot2 ein paar zusätzliche Plot-Typen kennt und zudem noch interaktiv ist. Leider scheinen gewisse Browsers (z.B. Firefox) sowie der Viewer Pane mit plotly Mühe zu haben. Deshalb empfehlen wir folgendes: Übungsunterlagen für InfoVis2 in Chrome zu öffnen Falls ihr auf dem RStudio Server arbeitet: hier ebenfalls in Chrome arbeiten Falls ihr lokal mit RStudio arbeitet: Mit der Option options(viewer=NULL) werden Plots mit dem Standart Browser. 5.3.1 Aufgabe 1: Parallel coordinate plots Erstelle einen parallel coordinate plot. Dafür eignet sich der integrierte Datensatz mtcars: mpg cyl disp hp drat wt qsec vs am gear carb Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 # Nur nötig, wenn ihr mit einer lokalen Installation von RStudio arbeitet # (also nicht auf dem Server). options(viewer=NULL) Parallel Coordinates lassen sich mit nativem ggplot2 nicht herstellen. Es braucht dazu entweder Erweiterungen oder “standalone” Tools. Als “standalone” Tool kann ich plotly stark empfehlen. Plotly verfügt zwar über eine etwas eigenwillige Syntax, bietet dafür über sehr vielseitige zusätzliche Möglichkeiten. Vor allem aber sind sämtliche plotly Grafiken webbasiert und interaktiv. Hier findet ihr eine Anleitung zur Herstellung eines Parallel Coordinates Plot mit plotly: https://plot.ly/r/parallel-coordinates-plot/ So sieht der fertige Plot aus: 5.3.2 Aufgabe 2: Polar Plot mit Biber Daten Polar Plots (welche man ebenfalls mit Plotly erstellen kann) eignen sich unter anderem für Daten, die zyklischer Natur sind, wie zum Beispiel zeitlich geprägte Daten (Tages-, Wochen-, oder Jahresrhythmen). Aus den Beispiels-Datensätzen habe ich zwei Datensätze gefunden, die zeitlich geprägt sind: beaver1 und beaver2 AirPassenger Beide Datensätze müssen noch etwas umgeformt werden, bevor wir sie für einen Radialplot verwenden können. In Aufgabe 2 verwenden wir die Biber-Datensätze, in der nächsten Aufgabe (3) die Passagier-Daten. Wenn wir die Daten von beiden Bibern verwenden wollen, müssen wir diese noch zusammenfügen: beaver1_new &lt;- beaver1 %&gt;% mutate(beaver = &quot;nr1&quot;) beaver2_new &lt;- beaver2 %&gt;% mutate(beaver = &quot;nr2&quot;) beaver_new &lt;- rbind(beaver1_new,beaver2_new) Zudem müssen wir die Zeitangabe noch anpassen: Gemäss der Datenbeschreibung handelt es sich bei der Zeitangabe um ein sehr programmier-unfreundliches Format. 3:30 wird als “0330” notiert. Wir müssen diese Zeitangabe, noch in ein Dezimalsystem umwandeln: beaver_new &lt;- beaver_new %&gt;% mutate( hour_dec = (time/100)%/%1, # Ganze Stunden (mittels ganzzaliger Division) min_dec = (time/100)%%1/0.6, # Dezimalminuten (15 min wird zu 0.25, via Modulo) hour_min_dec = hour_dec+min_dec # Dezimal-Zeitangabe (03:30 wird zu 3.5) ) Der Datensatz: day time temp activ beaver hour_dec min_dec hour_min_dec 346 840 36.33 0 nr1 8 0.6666667 8.666667 346 850 36.34 0 nr1 8 0.8333333 8.833333 346 900 36.35 0 nr1 9 0.0000000 9.000000 346 910 36.42 0 nr1 9 0.1666667 9.166667 346 920 36.55 0 nr1 9 0.3333333 9.333333 346 930 36.69 0 nr1 9 0.5000000 9.500000 So sieht der fertige Plot aus. Rekonstruiere dies mit plotly: 5.3.3 Aufgabe 3: Polar Plot mit Passagier-Daten Analog Aufgabe 2, dieses Mal mit dem Datensatz AirPassanger AirPassengers kommt in einem Format daher, das ich selbst noch gar nicht kannte. Es sieht zwar aus wie ein data.frame oder eine matrix, ist aber von der Klasse ts. AirPassengers ## Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec ## 1949 112 118 132 129 121 135 148 148 136 119 104 118 ## 1950 115 126 141 135 125 149 170 170 158 133 114 140 ## 1951 145 150 178 163 172 178 199 199 184 162 146 166 ## 1952 171 180 193 181 183 218 230 242 209 191 172 194 ## 1953 196 196 236 235 229 243 264 272 237 211 180 201 ## 1954 204 188 235 227 234 264 302 293 259 229 203 229 ## 1955 242 233 267 269 270 315 364 347 312 274 237 278 ## 1956 284 277 317 313 318 374 413 405 355 306 271 306 ## 1957 315 301 356 348 355 422 465 467 404 347 305 336 ## 1958 340 318 362 348 363 435 491 505 404 359 310 337 ## 1959 360 342 406 396 420 472 548 559 463 407 362 405 ## 1960 417 391 419 461 472 535 622 606 508 461 390 432 class(AirPassengers) ## [1] &quot;ts&quot; Damit wir den Datensatz verwenden können, müssen wir ihn zuerst in eine matrix umwandeln. Wie das geht habe ich hier erfahren. AirPassengers2 &lt;- tapply(AirPassengers, list(year = floor(time(AirPassengers)), month = month.abb[cycle(AirPassengers)]), c) Aus der matrix muss noch ein Dataframe her, zudem müssen wir aus der breiten Tabelle eine lange Tabelle machen: AirPassengers3 &lt;- AirPassengers2 %&gt;% as.data.frame() %&gt;% rownames_to_column(&quot;year&quot;) %&gt;% gather(month,n,-year) %&gt;% mutate( # ich nutze einen billigen Trick um ausgeschriebene Monate in Nummern umzuwandeln [1] month = factor(month, levels = month.abb,ordered = T), month_numb = as.integer(month), year = factor(year, ordered = T) ) # [1] beachtet an dieser Stelle das Verhalten von as.integer() wenn es sich um factors() handelt. Hier wird das Verhalten genutzt, andersweitig kann es einem zum Verhngnis werden. Das Verhalten wir auch hier verdeutlicht: # as.integer(as.character(&quot;500&quot;)) # as.integer(as.factor(&quot;500&quot;)) Hier der fertige Plot. Rekonstruiere dies mit plotly: 5.3.4 Aufgabe 4: 3D Scatterplot Erstelle einen 3D Scatterplot, ebenfalls mit plotly. Nutze dazu den Datensatz trees. Ein Beispiel für einen 3D Scatterplot findet ihr hier. "],
["5-4-ubung-b-losung-2.html", "5.4 Übung B: Lösung", " 5.4 Übung B: Lösung RCode als Download library(tidyverse) library(plotly) library(pander) library(webshot) ## ## # Nur nötig, wenn ihr mit einer lokalen Installation von RStudio arbeitet ## # (also nicht auf dem Server). ## options(viewer=NULL) ## # Lösung Aufgabe 1 p &lt;- mtcars %&gt;% plot_ly(type = &#39;parcoords&#39;, line = list(color = ~mpg, colorscale = list(c(0,&#39;red&#39;),c(1,&#39;blue&#39;))), dimensions = list( list(label = &#39;mpg&#39;, values = ~mpg), list(label = &#39;disp&#39;, values = ~disp), list(label = &#39;hp&#39;, values = ~hp), list(label = &#39;drat&#39;, values = ~drat), list(label = &#39;wt&#39;, values = ~wt), list(label = &#39;qsec&#39;, values = ~qsec), list(label = &#39;vs&#39;, values = ~vs), list(label = &#39;am&#39;, values = ~am), list(label = &#39;gear&#39;, values = ~gear), list(label = &#39;carb&#39;, values = ~carb) ) ) beaver1_new &lt;- beaver1 %&gt;% mutate(beaver = &quot;nr1&quot;) beaver2_new &lt;- beaver2 %&gt;% mutate(beaver = &quot;nr2&quot;) beaver_new &lt;- rbind(beaver1_new,beaver2_new) beaver_new &lt;- beaver_new %&gt;% mutate( hour_dec = (time/100)%/%1, # Ganze Stunden (mittels ganzzaliger Division) min_dec = (time/100)%%1/0.6, # Dezimalminuten (15 min wird zu 0.25, via Modulo) hour_min_dec = hour_dec+min_dec # Dezimal-Zeitangabe (03:30 wird zu 3.5) ) # Lösung Aufgabe 2 p &lt;- beaver_new %&gt;% plot_ly(r = ~temp, t = ~hour_min_dec, color = ~beaver,mode = &quot;lines&quot;, type = &quot;scatter&quot;) %&gt;% layout( radialaxis = list(range = c(35,39)), angularaxis = list(range = c(0,24)), orientation = 270, showlegend = F ) AirPassengers class(AirPassengers) AirPassengers2 &lt;- tapply(AirPassengers, list(year = floor(time(AirPassengers)), month = month.abb[cycle(AirPassengers)]), c) AirPassengers3 &lt;- AirPassengers2 %&gt;% as.data.frame() %&gt;% rownames_to_column(&quot;year&quot;) %&gt;% gather(month,n,-year) %&gt;% mutate( # ich nutze einen billigen Trick um ausgeschriebene Monate in Nummern umzuwandeln [1] month = factor(month, levels = month.abb,ordered = T), month_numb = as.integer(month), year = factor(year, ordered = T) ) # [1] beachtet an dieser Stelle das Verhalten von as.integer() wenn es sich um factors() handelt. Hier wird das Verhalten genutzt, andersweitig kann es einem zum Verhngnis werden. Das Verhalten wir auch hier verdeutlicht: # as.integer(as.character(&quot;500&quot;)) # as.integer(as.factor(&quot;500&quot;)) # Lösung Aufgabe 3 p &lt;- AirPassengers3 %&gt;% plot_ly(r = ~n, t = ~month_numb, color = ~year, mode = &quot;markers&quot;, type = &quot;scatter&quot;) %&gt;% layout( showlegend = T, angularaxis = list(range = c(0,12)), orientation = 270, legend = list(traceorder = &quot;reversed&quot;) ) # Lösung Aufgabe 4 p &lt;- trees %&gt;% plot_ly(x = ~Girth, y = ~Height, z = ~Volume) "],
["6-statistik-1-28-10-2019.html", "Kapitel 6 Statistik 1 (28.10.2019)", " Kapitel 6 Statistik 1 (28.10.2019) In Statistik 1 lernen die Studierenden, was (Inferenz-) Statistik im Kern leistet und warum sie für wissenschaftliche Erkenntnis (in den meisten Disziplinen) unentbehrlich ist. Nach einer Wiederholung der Rolle von Hypothesen wird erläutert, wie Hypothesentests in der frequentist-Statistik umgesetzt werden, einschliesslich p-Werten und Signifikanz-Levels. Die praktische Statistik beginnt mit den beiden einfachsten Fällen, dem Chi-Quadrat-Test für die Assoziation zwischen zwei kategorialen Variablen und dem t-Test auf Unterschiede in Mittelwerten zwischen zwei Gruppen. Abschliessend beschäftigen wir uns damit, wie man Ergebnisse statistischer Analysen am besten in Abbildungen, Tabellen und Text darstellt. "],
["6-1-demo-stastische-tests.html", "6.1 Demo: Stastische Tests", " 6.1 Demo: Stastische Tests Demoscript als Download 6.1.1 Chi-Quadrat-Test &amp; Fishers Test qchisq(0.95,1) ## [1] 3.841459 count&lt;-matrix(c(38,14,11,51),nrow=2) count ## [,1] [,2] ## [1,] 38 11 ## [2,] 14 51 chisq.test(count) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: count ## X-squared = 33.112, df = 1, p-value = 8.7e-09 fisher.test(count) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: count ## p-value = 2.099e-09 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 4.746351 34.118920 ## sample estimates: ## odds ratio ## 12.22697 6.1.2 t-Test a&lt;-c(20,19,25,10,8,15,13,18,11,14) b&lt;-c(12,15,16,7,8,10,12,11,13,10) blume&lt;-data.frame(a,b) blume ## a b ## 1 20 12 ## 2 19 15 ## 3 25 16 ## 4 10 7 ## 5 8 8 ## 6 15 10 ## 7 13 12 ## 8 18 11 ## 9 11 13 ## 10 14 10 summary(blume) ## a b ## Min. : 8.00 Min. : 7.00 ## 1st Qu.:11.50 1st Qu.:10.00 ## Median :14.50 Median :11.50 ## Mean :15.30 Mean :11.40 ## 3rd Qu.:18.75 3rd Qu.:12.75 ## Max. :25.00 Max. :16.00 boxplot(blume$a,blume$b) boxplot(blume) hist(blume$a) hist(blume$b) t.test(blume$a,blume$b) #zweiseitig ## ## Welch Two Sample t-test ## ## data: blume$a and blume$b ## t = 2.0797, df = 13.907, p-value = 0.05654 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.1245926 7.9245926 ## sample estimates: ## mean of x mean of y ## 15.3 11.4 t.test(blume$a,blume$b, alternative=&quot;greater&quot;) #einseitig ## ## Welch Two Sample t-test ## ## data: blume$a and blume$b ## t = 2.0797, df = 13.907, p-value = 0.02827 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## 0.5954947 Inf ## sample estimates: ## mean of x mean of y ## 15.3 11.4 t.test(blume$a,blume$b, alternative=&quot;less&quot;) #einseitig ## ## Welch Two Sample t-test ## ## data: blume$a and blume$b ## t = 2.0797, df = 13.907, p-value = 0.9717 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf 7.204505 ## sample estimates: ## mean of x mean of y ## 15.3 11.4 t.test(blume$a,blume$b, var.equal=T) #Varianzen gleich, klassischer t-Test ## ## Two Sample t-test ## ## data: blume$a and blume$b ## t = 2.0797, df = 18, p-value = 0.05212 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.03981237 7.83981237 ## sample estimates: ## mean of x mean of y ## 15.3 11.4 t.test(blume$a,blume$b, var.equal=F) #Varianzen ungleich, Welch&#39;s t-Test, ist auch default, d.h. wenn var.equal nicht # definiert wird, wird ein Welch&#39;s t-Test ausgeführt. ## ## Welch Two Sample t-test ## ## data: blume$a and blume$b ## t = 2.0797, df = 13.907, p-value = 0.05654 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.1245926 7.9245926 ## sample estimates: ## mean of x mean of y ## 15.3 11.4 t.test(blume$a,blume$b, paired=T) #gepaarter t-Test ## ## Paired t-test ## ## data: blume$a and blume$b ## t = 3.4821, df = 9, p-value = 0.006916 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 1.366339 6.433661 ## sample estimates: ## mean of the differences ## 3.9 t.test(blume$a,blume$b, paired=T,alternative=&quot;greater&quot;) #gepaarter t-Test ## ## Paired t-test ## ## data: blume$a and blume$b ## t = 3.4821, df = 9, p-value = 0.003458 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## 1.846877 Inf ## sample estimates: ## mean of the differences ## 3.9 shapiro.test(blume$b) ## ## Shapiro-Wilk normality test ## ## data: blume$b ## W = 0.97341, p-value = 0.9206 var.test(blume$a,blume$b) ## ## F test to compare two variances ## ## data: blume$a and blume$b ## F = 3.3715, num df = 9, denom df = 9, p-value = 0.08467 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.8374446 13.5738284 ## sample estimates: ## ratio of variances ## 3.371547 if(!require(car)){install.packages(&quot;car&quot;)} # installiert das Zusatzpacket car (wenn nicht bereits installiert) library(car) leveneTest(blume$a,blume$b,center=mean) ## Levene&#39;s Test for Homogeneity of Variance (center = mean) ## Df F value Pr(&gt;F) ## group 7 2.2598e+30 &lt; 2.2e-16 *** ## 2 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 wilcox.test(blume$a,blume$b) ## ## Wilcoxon rank sum test with continuity correction ## ## data: blume$a and blume$b ## W = 73, p-value = 0.08789 ## alternative hypothesis: true location shift is not equal to 0 Das gleiche mit einem “long table” cultivar&lt;-c(rep(&quot;a&quot;,10),rep(&quot;b&quot;,10)) size&lt;-c(a,b) blume.long&lt;-data.frame(cultivar,size) rm(size) #Befehl rm entfernt die nicht mehr benötitgten Objekte aus dem Workspace rm(cultivar) rm(size) #Befehl rm entfernt die nicht mehr benötitgten Objekte aus dem Workspace rm(cultivar) #Das gleiche in einer Zeile blume.long&lt;-data.frame(cultivar=c(rep(&quot;a&quot;,10),rep(&quot;b&quot;,10)),size=c(a,b)) summary(blume.long) ## cultivar size ## a:10 Min. : 7.00 ## b:10 1st Qu.:10.00 ## Median :12.50 ## Mean :13.35 ## 3rd Qu.:15.25 ## Max. :25.00 head(blume.long) ## cultivar size ## 1 a 20 ## 2 a 19 ## 3 a 25 ## 4 a 10 ## 5 a 8 ## 6 a 15 boxplot(size~cultivar, data=blume.long) boxplot(size~cultivar, data=blume.long) t.test(size~cultivar, blume.long, var.equal=T) ## ## Two Sample t-test ## ## data: size by cultivar ## t = 2.0797, df = 18, p-value = 0.05212 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.03981237 7.83981237 ## sample estimates: ## mean in group a mean in group b ## 15.3 11.4 t.test(size~cultivar, blume.long, var.equal=F) ## ## Welch Two Sample t-test ## ## data: size by cultivar ## t = 2.0797, df = 13.907, p-value = 0.05654 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.1245926 7.9245926 ## sample estimates: ## mean in group a mean in group b ## 15.3 11.4 6.1.3 Base R vs. ggplot2 library(tidyverse) ggplot(blume.long, aes(cultivar,size)) + geom_boxplot() ggplot(blume.long, aes(cultivar,size)) + geom_boxplot()+theme_classic() ggplot(blume.long, aes(cultivar,size)) + geom_boxplot(size=1) + theme_classic()+ theme(axis.line = element_line(size=1))+theme(axis.title = element_text(size=14))+ theme(axis.text = element_text(size=14)) ggplot(blume.long, aes(cultivar,size)) + geom_boxplot(size=1) + theme_classic()+ theme(axis.line = element_line(size=1), axis.ticks = element_line(size=1), axis.text = element_text(size = 20), axis.title = element_text(size = 20)) Definieren von mytheme mit allen gewünschten Settings, das man zu Beginn einer Sitzung einmal laden und dann immer wieder ausführen kann (statt des langen Codes) mytheme &lt;- theme_classic() + theme(axis.line = element_line(color = &quot;black&quot;, size=1), axis.text = element_text(size = 20, color = &quot;black&quot;), axis.title = element_text(size = 20, color = &quot;black&quot;), axis.ticks = element_line(size = 1, color = &quot;black&quot;), axis.ticks.length = unit(.5, &quot;cm&quot;)) ggplot(blume.long, aes(cultivar,size)) + geom_boxplot(size=1) + mytheme t_test &lt;- t.test(size~cultivar, blume.long) ggplot(blume.long, aes(cultivar,size)) + geom_boxplot(size=1) + mytheme + annotate(&quot;text&quot;, x = &quot;b&quot;, y = 24, label = paste0(&quot;italic(p) == &quot;, round(t_test$p.value, 3)), parse = TRUE, size = 8) ggplot (blume.long, aes(cultivar,size)) + geom_boxplot(size=1) + mytheme + labs(x=&quot;Cultivar&quot;,y=&quot;Size (cm)&quot;) "],
["6-2-beschreibung-forschungsprojekt-novanimal-nfp69.html", "6.2 Beschreibung Forschungsprojekt NOVANIMAL (NFP69)", " 6.2 Beschreibung Forschungsprojekt NOVANIMAL (NFP69) Im Forschungsprojekt NOVANIMAL wird u.a. der Frage nachgegangen, was es braucht, damit Menschen freiwillig weniger tierische Produkte konsumieren? Ein interessanter Ansatzpunkt ist die Ausser-Haus-Verpflegung. Gemäss der ersten in den Jahren 2014/2015 durchgeführten nationalen Ernährungserhebung menuCH essen 70 % der Bevölkerung zwischen 18 und 75 Jahren am Mittag auswärts (Bochud et al. 2017). Daher rückt die Gastronomie als zentraler Akteur einer innovativen und nachhaltigen Ernährungswirtschaft ins Blickfeld. Welche Innovationen in der Gastronomie könnten dazu beitragen, den Pro-Kopf-Verbrauch an tierischen Nahrungsmitteln zu senken? Dazu wurde u.a. ein Experiment in zwei Hochschulmensen durchgeführt. Forschungsleitend war die Frage, wie die Gäste dazu bewogen werden können, häufiger vegetarische oder vegane Gerichte zu wählen. Konkret wurde untersucht, wie die Gäste auf ein verändertes Menü-Angebot mit einem höheren Anteil an vegetarischen und veganen Gerichten reagieren. Das Experiment fand während 12 Wochen statt und bestand aus zwei Mensazyklen à 6 Wochen. Über den gesamten Untersuchungszeitraum werden insgesamt 90 verschiedene Gerichte angeboten. In den 6 Referenz- bzw. Basiswochen wurden zwei fleisch- oder fischhaltige Menüs und ein vegetarisches Menü angeboten. In den 6 Interventionswochen wurde das Verhältnis umgekehrt und es wurden ein veganes, ein vegetarisches und ein fleisch- oder fischhaltiges Gericht angeboten. Basis- und Interventionsangebote wechselten wöchentlich ab. Während der gesamten 12 Wochen konnten die Gäste jeweils auf ein Buffet ausweichen und ihre Mahlzeit aus warmen und kalten Komponenten selber zusammenstellen. Die Gerichte wurden über drei vorgegebene Menülinien (F, K, W) randomisiert angeboten. Die Abbildung zeigt das Versuchsdesign der ersten 6 Experimentalwochen (Kalenderwoche 40 bis 45). Mehr Informationen über das Forschungsprojekt NOVANIMAL findet ihr auf dieser Webpage 6.2.1 Weitere Erläuterungen zum Datensatz Der Datensatz beinhaltet knapp 1100 Einträge mit 18 Variablen. Die Daten stammen aus dem Kassensystem des Catering-Unternehmen und stellen eine repräsentative Stichprobe des originalen Datensatzes dar. Folgende Variablen sind im Datensatz: Name der Variable Beschreibung der Variable transaction_id Durch das Kassensystem generierte Identifikationsnummer date Datum week Kalenderwoche year Kalenderjahr meal_name Beschreibung der angebotenen Mahlzeit article_description Beschreibung der Menü-Linie (F, Buffet, K, Locals1, W) label_content Beschreibung des Menü-Inhalts (Fleisch2, Buffet, Vegan3, Vegetarisch4) condit Beschreibung der Experimentwochen (Basis- oder Interventionswoche) card_num anonymisierte Kartennummer gender Geschlecht (F, M) member Hochschulzugehörigkeit (Studierende, Mitarbeitende) age Alter in Jahren price_article Preis des gekauften Artikels total_amount_pay Total bezahlter Betrag pay_description Bezahlungsart (Badge (Debit-Karte)) shop_description Ort der Mensa (GR oder VS) tot_ubp Umweltbelastungspunkte pro Gericht buffet_animal Anzahl fleischhaltiger Produkte auf dem Buffet Locals (Local F, Local K, Local W) sind nebst den drei “normalen Menü-Linien” zusätzlich angebotene Gerichte hier werden Gerichte mit Fisch &amp; Geflügel als Fleisch zusammengefasst. Vegane Gerichte enthalten ausschliesslich pflanzliche Zutaten. Im Exeriment wurde zwischen Gerichte mit pflanzlichen Fleischsubstituten und authentischen, eigenständigen veganen Gerichten unterschieden Vegetarisch bedeutet ovo-lakto-vegetarisch, d.h. die Gerichte enthalten Eier und/oder Milchprodukte "],
["6-3-ubungen-1.html", "6.3 Übungen 1", " 6.3 Übungen 1 6.3.1 Übung 1.1: Assoziationstest Assoziationstest zweier kategorialer Variablen, Dateneingabe und Durchführung von Chi-Quadrat- sowie Fishers exaktem Test mit Daten die selber erhoben wurden oder dem Novanimal Datensatz. 6.3.2 Übung 1.2: \\(\\chi^2\\)-Test Datensatz novanimal.csv Unterscheidet sich die Stichprobe des NOVANIMAL-Projekts von der gesamten Population bezüglich Geschlecht und Hochschulzugehörigkeit? Die Grundgesamtheit setzt sich aus 719 Studentinnen und 816 Studenten, 345 Mitarbeiterinnen und 339 Mitarbeiter. Die gesamte Population umfasst 2219 Personen mit einer aktiven CampusCard. Definiert die Null- (\\(H_0\\)) und die Alternativhypothese (\\(H_1\\)) Führt einen \\(\\chi^2\\)-Test mit dem Datensatz novanimal.csv durch Stellt eure Ergebnisse angemessen dar, d.h. Text mit Abbildung und/oder Tabelle 6.3.3 Übung 1.3: t-Test Werden in den Basis- und Interventionswochen unterschiedlich viele Gerichte verkauft? Definiert die Null- (\\(H_0\\)) und die Alternativhypothese (\\(H_1\\)). Führt einen t-Test mit dem Datensatz novanimal.csv durch. Welche Form von t-Test musst Du anwenden: einseitig/zweiseitig resp. gepaart/ungepaart? Wie gut sind die Voraussetzungen für einen t-Test erfüllt (z.B. Normalverteilung der Residuen und Varianzhomogenität)? Stell eure Ergebnisse angemessen dar, d.h. Text mit Abbildung und/oder Tabelle Das gilt für alle Übungen und wird von euch auch an der Prüfung verlangt. Abzugeben sind am Ende Ein lauffähiges R-Skript begründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation) ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit) "],
["6-4-musterlosung-aufgabe-1-1-assoziationstest.html", "6.4 Musterlösung Aufgabe 1.1: Assoziationstest", " 6.4 Musterlösung Aufgabe 1.1: Assoziationstest Download R-Skript kommentierte Musterlösungen # Als eine Möglichkeit, die Aufgabe 1.1 zu bearbeiten, nehmen wir hier den novanimal-Datensatz und gehen der folgenden Frage nach: Gibt es einen Zusammenhang zwischen Geschlecht und der Wahl des Menüinhalts (vegtarisch vs. fleischhaltig) in der Mensa # berücksichtigt nur vegetarische und fleischhaltige Menüs # 1) alle Buffet-Menüs weglassen # 2) alle veganen Gerichte zu vegetarische Gerichte umbenennen nova2&lt;-subset(nova, !(nova$label_content==&quot;Buffet&quot;)) #Subset des Datensatzes ohne Buffet (Da Buffet nicht Fleisch/Vegetarisch zugordnet werden kann) nova2$label_content[nova2$label_content %in% c(&#39;Pflanzlich&#39;,&#39;Pflanzlich+&#39;)] &lt;- &quot;Vegetarisch&quot; # Überschreibt das Label &quot;Pflanzlich&quot;,&quot;Pflanzlich+&quot; mit &quot;Vegetarisch&quot; nova2$label_content[grep(&quot;Pflanzlich+&quot;, nova2$label_content)] &lt;- &quot;Vegetarisch&quot; # alternativer Lösungsweg nova3&lt;-droplevels(nova2) #entfernt die Kategorien die nicht mehr mehr benutzt werden #Anzahl Vegetarische/FleischMenüs pro Geschlecht~Vegetarisch Base R observed&lt;-table(nova2$gender, nova2$label_content) #Anzahl Vegetarische/FleischMenüs pro Geschlecht~Vegetarisch Tidyverse #braucht in diesem speziellen Fall viel mehr Code, da der Chi-Quadrat-Test am liebsten Matrizen will # unser Vorschlag, nehmt den table Befehl # untenstehend der vollständige Code in Tidyverse observed_t &lt;- nova2 %&gt;% group_by(gender, label_content) %&gt;% summarise(tot = n()) %&gt;% ungroup() %&gt;% spread(., key = label_content, value = tot) %&gt;% # # set_rownames(.$gender) %&gt;% funktioniert leider nicht mehr mit tibble select(-gender) %&gt;% as.matrix() #Chi-squared Test chi_sq &lt;- chisq.test(observed) chi_sq ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: observed ## X-squared = 26.749, df = 1, p-value = 2.317e-07 #Fisher&#39;s Test fisher.test(observed) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: observed ## p-value = 2.007e-07 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 0.3631105 0.6409997 ## sample estimates: ## odds ratio ## 0.4827451 ** Ergebnisse ** Der \\(\\chi^2\\)-Test sagt uns, dass das Geschlecht und die Kaufentscheidung eines Menüinhalts zusammenhängen. Es gibt signifikante Unterscheide zwischen dem Geschlecht und dem Menüinhalt (\\(\\chi^2\\)(1) = 26.749, p &gt; .001). Es sieht so aus, dass Männer rund doppel so viel Fleischgerichte wählen als Frauen (siehe Tabelle 1). Die Ergebnisse müssen jedoch mit Vorsicht interpretiert werden, denn der \\(\\chi^2\\)-Test gibt uns nur an, dass ein signifikanter Unterschied zwischen Geschlecht und Menüinhalt vorliegt. Um die Unterschiede innerhalb der Gruppen festzustellen bedarf es weiterer Analysen z. B. einer mehrfaktorieller ANOVA mit anschliessenden Post-hoc Tests (siehe Statistik 2, Folien 3 bis 11). Table 6.1: Tabelle 1 Verkaufszahlen des Menüinhalts nach Geschlecht Geschlecht Menüinhalt Verkaufszahlen Verkaufszahlen (%) Frauen Fleisch 150 15.8 Männer Fleisch 414 43.5 Frauen Vegetarisch 166 17.5 Männer Vegetarisch 221 23.2 6.4.1 Musterlösung Aufgabe 1.2: \\(\\chi^2\\)-Test Zur eurer Info: dies ist eine spezielle, aber wichtige Anwendung eines \\(\\chi^2\\)-Tests Meine Empfehlung Kapitel “Single factor classification” von Manny Gimond ** Null- und Alternativhypothese** &gt; Beachtet: ungerichtet vs. gerichtete Hypothesen (z. B. Statistik 1, Folie 24) &gt; Überblick zu Hypothesentestung dazu: https://www.youtube.com/watch?v=F4c0EjsDvzo \\(H_0\\): Es gibt keine Unterschiede zwischen der Population und der Stichprobe bezüglich Geschlecht und Hochschulzugehörigkeit. \\(H_1\\): Es gibt Unterschiede zwischen der Population und der Stichprobe bezüglich Geschlecht und Hochschulzugehörigkeit. # bereitet eure Daten auf # gruppiert die Variablen und fasst sie # nach Geschlecht und Hochschulzugehörigkeit zusammen # fügt Information aus der Aufgabenstellung hinzu: absolute Häufigkeiten der Gesamtheit # für den Chi-Quardrat-Test ist eine Berechnung der relativen Häufigkeiten nötig df_t &lt;- group_by(nova, gender, member) %&gt;% summarise(stichprobe = n()) %&gt;% ungroup() %&gt;% mutate(canteen_member = c(&quot;Mitarbeiterinnen&quot;, &quot;Studentinnen&quot;, &quot;Mitarbeiter&quot;, &quot;Studenten&quot;), # Achtung: Reihenfolge muss stimmten vgl. canteen_member gesamtheit = c(345, 719, 339, 816), # Achtung: Reihenfolge muss stimmten vgl. oben gesamtheit_pct = gesamtheit / sum(gesamtheit), stichprobe_pct = stichprobe / sum(stichprobe)) # Berechnung der relativen Häufigkeiten # berechnet den Chi-Quadrat-Test chi_sq &lt;- chisq.test(df_t$stichprobe, p = df_t$gesamtheit_pct) # es werden zwei Informationen übergeben, eure beobachteten Werte (stichprobe) und die in der Grundgesamtheit/Population erwarteten Werte (wichtig als relative Häufigkeiten) chi_sq ## ## Chi-squared test for given probabilities ## ## data: df_t$stichprobe ## X-squared = 113.18, df = 3, p-value &lt; 2.2e-16 ** Methoden ** Ziel war es die NOVANIMAL Stichprobe gemäss Geschlecht und Hochschulzugehörigkeit in der Grundgesamtheit besser einzuordnen. Die Grundgesamtheit definiert sich durch alle aktiven CampusCards. Dafür ist ein einfaktorieller \\(\\chi^2\\)-Test notwendig (siehe Manny Gimond). Dieser sagt uns nämlich, ob die beobachteten Häufigkeiten/Frequenzen aus unser Stichprobe mit einer definierten erwarteten Häufigkeit/Frequenz (hier der Grundgesamtheit) übereinstimmen. ** Ergebnisse ** Der \\(\\chi^2\\)-Test sagt uns, dass die NOVANIMAL-Stichprobe von der Population signifikant unterscheidet (\\(\\chi^2\\)(3) = 113.178, p &gt; .001). Demnach ist unsere Stichprobe bezüglich den Variablen Geschlecht und Hochschulzugehörigkeit nicht repräsentativ für die Grundgesamtheit. Es scheint, dass die Studentinnen unter- und die Studenten übervertreten sind (siehe Tabelle 2). Table 6.2: Tabelle 2 Anzahl und Anteil Beobachtungen in der Population und in der Stichprobe Hochschulzugehörigkeit Anzahl Population Anteil Population (%) Anzahl Stichprobe Anteil Stichprobe (%) Mitarbeitende 345 15.54754 209 19 Studierende 719 32.40198 212 19 Mitarbeitende 339 15.27715 255 23 Studierende 816 36.77332 440 39 6.4.2 Musterlösung Aufgabe 1.3: t-Test Meine Empfehlung Kapitel 2 von Manny Gimond ** Null- und Alternativhypothese ** &lt;&gt;\\(H_0\\): Es gibt keine Unterschiede in den Verkaufszahlen zwischen Basis- und Interventionswochen. &lt;&gt; \\(H_1\\): Es gibt Unterschiede in den Verkaufszahlen zwischen Basis- und Interventionswochen. # Gemäss Aufgabenstellung müsset die Daten zuerst nach Kalenderwochen &quot;week&quot; und Bedingungen &quot;condition&quot; zusammengefasst werden df &lt;- nova %&gt;% group_by(week, condit) %&gt;% summarise(tot_sold = n()) # überprüft die Voraussetzungen für einen t-Test ggplot(df, aes(x = condit, y= tot_sold)) + # achtung 0 Punkt fehlt geom_boxplot(fill = &quot;white&quot;, color = &quot;black&quot;, size = 1) + labs(x=&quot;\\nBedingungen&quot;, y=&quot;Durchschnittlich verkaufte Gerichte pro Woche\\n&quot;) + mytheme # Auf den ersten Blick scheint es keine starken Abweichungen zu einer Normalverteilung zu geben resp. es sind keine extremen schiefen Verteilungen ersichtlich (vgl. Statistik 2, Folien 12-21) # führt einen t-Tests durch; # es wird angenommen, dass die Verkaufszahlen zwischen den Bedingungen unabhängig sind t_test &lt;- t.test(tot_sold~condit, data=df) t.test(df[df$condit == &quot;Basis&quot;, ]$tot_sold, df[df$condit == &quot;Intervention&quot;, ]$tot_sold) #alternative Formulierung ## ## Welch Two Sample t-test ## ## data: df[df$condit == &quot;Basis&quot;, ]$tot_sold and df[df$condit == &quot;Intervention&quot;, ]$tot_sold ## t = 1.5367, df = 3.5781, p-value = 0.2074 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -8.343572 27.010238 ## sample estimates: ## mean of x mean of y ## 190.6667 181.3333 ** Methoden ** Ziel war es die wöchentlichen Verkaufszahlen zwischen den Interventions- und Basiswochen zu vergleichen. Die Annahme war, dass die wöchentlichen Verkaufszahlen unabhängig sind. Daher können die mittleren Verkaufszahlen pro Woche zwischen den beiden Bedingungen mittels t-Test geprüft werden. Obwohl die visuelle Inspektion keine schwerwiegenden Verletzungen der Modelvoraussetzung zeigte, wurde einen Welch t-Test gerechnet. ** Ergebnisse ** In den Basiswochen werden mehr Gerichte pro Woche verkauft als in den Interventionsowochen (siehe Abbildung 1). Die wöchentlichen Verkaufszahlen zwischen den Bedigungen (Basis oder Intervention) unterscheiden sich gemäss Welch t-Test jedoch nicht signifikant (t(4) = 1.537 , p = 0.207). Figure 6.1: Abbildung1. Die wöchentlichen Verkaufszahlen für die Interventions- und Basiswochen unterscheiden sich nicht signifikant. "],
["7-statistik-2-29-10-2019.html", "Kapitel 7 Statistik 2 (29.10.2019)", " Kapitel 7 Statistik 2 (29.10.2019) In Statistik 2 lernen die Studierenden die Idee, die Voraussetzungen und die praktische Anwendung „einfacher“ linearer Modelle in R (sowie teilweise ihrer „nicht-parametrischen“ bzw. „robusten“ Äquivalente). Am Anfang steht die Varianzanalyse (ANOVA) als Verallgemeinerung des t-Tests, einschliesslich post-hoc-Tests und mehrfaktorieller ANOVA. Dann geht es um die Voraussetzungen parametrischer (und nicht-parametrischer) Tests und Optionen, wenn diese verletzt sind. Dann beschäftigen wir uns mit Korrelationen, die auf einen linearen Zusammenhang zwischen zwei metrischen Variablen testen, ohne Annahme einer Kausalität. Es folgen einfache lineare Regressionen, die im Prinzip das Gleiche bei klarer Kausalität leisten. Abschliessend besprechen wir, was die grosse Gruppe linearer Modelle (Befehl lm in R) auszeichnet. "],
["7-1-demoskript.html", "7.1 Demoskript", " 7.1 Demoskript Demoscript als Download t-test als ANOVA a&lt;-c(20,19,25,10,8,15,13,18,11,14) b&lt;-c(12,15,16,7,8,10,12,11,13,10) blume&lt;-data.frame(cultivar=c(rep(&quot;a&quot;,10),rep(&quot;b&quot;,10)),size=c(a,b)) par(mfrow=c(1,1)) boxplot (data=blume, size~cultivar, xlab=&quot;Sorte&quot;, ylab=&quot;Bluetengroesse [cm]&quot;) t.test(size~cultivar, blume, var.equal=T) aov(size~cultivar,data=blume) summary(aov(size~cultivar,data=blume)) summary.lm(aov(size~cultivar,data=blume)) Echte ANOVA c&lt;-c(30,19,31,23,18,25,26,24,17,20) blume2&lt;-data.frame(cultivar=c(rep(&quot;a&quot;,10),rep(&quot;b&quot;,10),rep(&quot;c&quot;,10)),size=c(a,b,c)) summary(blume2) head(blume2) par(mfrow=c(1,1)) boxplot (data=blume2, size~cultivar, xlab=&quot;Sorte&quot;, ylab=&quot;Blütengrösse [cm]&quot;) aov(size~cultivar,data=blume2) summary(aov(size~cultivar,data=blume2)) summary.lm(aov(size~cultivar,data=blume2)) aov.1 &lt;- aov(size~cultivar,data=blume2) summary(aov.1) summary.lm(aov.1) #Berechnung Mittelwerte usw. zur Charakterisierung der Gruppen aggregate(size~cultivar,blume2, function(x) c(Mean = mean(x), SD = sd(x), Min=min(x), Max=max(x))) lm.1 &lt;- lm(size~cultivar,data=blume2) summary(lm.1) Tukeys Posthoc-Test if(!require(multcomp)){install.packages(&quot;multcomp&quot;)} library(multcomp) summary(glht(aov(size~cultivar, data=blume2),linfct=mcp(cultivar =&quot;Tukey&quot;))) Beispiel Posthoc-Labels in Plot anova &lt;- aov(Sepal.Width ~ Species, data=iris) letters &lt;- cld(glht(anova, linfct=mcp(Species=&quot;Tukey&quot;))) boxplot(Sepal.Width ~ Species, data=iris) mtext(letters$mcletters$Letters, at=1:3) library(tidyverse) ggplot(iris, aes(Species, Sepal.Width)) + geom_boxplot(size = 1) + annotate(&quot;text&quot;, y = 5, x = 1:3, label = letters$mcletters$Letters) Klassische Tests der Modellannahmen (NICHT EMPFOHLEN!!!) attach(blume2) shapiro.test(size[cultivar == &quot;a&quot;]) var.test(size[cultivar == &quot;a&quot;],size[cultivar == &quot;b&quot;]) if(!require(car)){install.packages(&quot;car&quot;)} library(car) leveneTest(size[cultivar == &quot;a&quot;],size[cultivar == &quot;b&quot;],center=mean) wilcox.test(size[cultivar == &quot;a&quot;],size[cultivar == &quot;b&quot;]) detach(blume2) Nicht-parametrische Alternativen, wenn Modellannahmen der ANVOA massiv verletzt sind Zum Vergleich normale ANOVA noch mal summary(aov(size~cultivar,data=blume2)) Bei starken Abweichungen von der Normalverteilung, aber ähnlichen Varianzen Kruskal-Wallis-Test kruskal.test(data=blume2, size~cultivar) if(!require(FSA)){install.packages(&quot;FSA&quot;)} library(FSA) dunnTest(data=blume2, size~cultivar, method=&quot;bh&quot;) #korrigierte p-Werte nach Bejamini-Hochberg Bei erheblicher Heteroskedastizität, aber relative normal/symmetrisch verteilten Residuen Welch-Test oneway.test(data=blume2, size~cultivar, var.equal=F) 2-faktorielle ANOVA d&lt;-c(10,12,11,13,10,25,12,30,26,13) e&lt;-c(15,13,18,11,14,25,39,38,28,24) f&lt;-c(10,12,11,13,10,9,2,4,7,13) blume3&lt;-data.frame(cultivar=c(rep(&quot;a&quot;,20),rep(&quot;b&quot;,20),rep(&quot;c&quot;,20)), house=c(rep(c(rep(&quot;yes&quot;,10),rep(&quot;no&quot;,10)),3)),size=c(a,b,c,d,e,f)) blume3 boxplot(size~cultivar+house,data=blume3) summary(aov(size~cultivar+house,data=blume3)) summary(aov(size~cultivar+house+cultivar:house,data=blume3)) summary(aov(size~cultivar*house,data=blume3)) #Kurzschreibweise: &quot;*&quot; bedeutet, dass Interaktion zwischen cultivar und house eingeschlossen wird summary.lm(aov(size~cultivar+house,data=blume3)) interaction.plot(blume3$cultivar,blume3$house,blume3$size) interaction.plot(blume3$house,blume3$cultivar,blume3$size) anova(lm(blume3$size~blume3$cultivar*blume3$house),lm(blume3$size~blume3$cultivar+blume3$house)) anova(lm(blume3$size~blume3$house),lm(blume3$size~blume3$cultivar*blume3$house)) Korrelationen library(car) blume&lt;-data.frame(a,b) scatterplot(a~b,blume) cor.test(a,b,data = blume, method=&quot;pearson&quot;) cor.test(a,b,data = blume, method=&quot;spearman&quot;) cor.test(a,b,data = blume, method=&quot;kendall&quot;) #Jetzt als Regression lm.2 &lt;- lm(b~a) anova(lm.2) summary(lm.2) #Model II-Regression if(!require(lmodel2)){install.packages(&quot;lmodel2&quot;)} library(lmodel2) lmodel2(b~a) Beispiele Modelldiagnostik par(mfrow=c(2,2)) #4 Plots in einem Fenster plot(lm(b~a)) if(!require(ggfortify)){install.packages(&quot;ggfortify&quot;)} library(ggfortify) autoplot(lm(b~a)) #Modellstatistik nicht OK g&lt;-c(20,19,25,10,8,15,13,18,11,14,25,39,38,28,24) h&lt;-c(12,15,10,7,8,10,12,11,13,10,25,12,30,26,13) par(mfrow=c(1,1)) plot(h~g,xlim=c(0,40),ylim=c(0,30)) abline(lm(h~g)) par(mfrow=c(2,2)) plot(lm(h~g)) #Modelldiagnostik mit ggplot df &lt;- data.frame(g,h) ggplot(df, aes(x = g, y = h)) + # scale_x_continuous(limits = c(0,25)) + # scale_y_continuous(limits = c(0,25)) + geom_point() + geom_smooth( method = &quot;lm&quot;, color = &quot;black&quot;, size = .5, se = F) + theme_classic() par(mfrow=c(2,2)) plot(lm(h~g)) autoplot(lm(h~g)) "],
["7-2-ubungen-2.html", "7.2 Übungen 2", " 7.2 Übungen 2 Repetition: Abzugeben sind am Ende a. lauffähiges R-Skript b. begründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation) c. ausformulierter Methoden- und Ergebnisteil (für eine wiss.Arbeit). Bitte erklärt und begründet die einzelnen Schritte, die ihr unternehmt, um zu eurem Ergebnis zu kommen. Dazu erstellt bitte ein Word-Dokument, in dem ihr Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, eure Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere Vorgehen dokumentiert. Dieser Ablauf sollte insbesondere beinhalten: Überprüfen der Datenstruktur nach dem Einlesen, welches sind die abhängige(n) und welches die unabängige(n) Variablen etc. Explorative Datenanalyse, um zu sehen, ob evtl. Dateneingabefehler vorliegen oder Datentransformationen vorgenommen werden sollten Auswahl und Begründung eines statistischen Verfahrens Bestimmung des vollständigen/maximalen Models Selektion des/der besten Models/Modelle Durchführen der Modelldiagnostik für dieses Generieren aller Zahlen, Statistiken und Tabellen, die für eine wiss. Ergebnisdarstellung benötigt werden Formuliert abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen/Tabellen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (je einen ausformulierten Absatz von ca. 60-100 Worten bzw. 3-8 Sätzen). Alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden. 7.2.1 Übung 2.1: Regression (NatWis) Regressionsanalyse mit decay.csv Der Datensatz beschreibt in einem physikalischen Experiment die Zahl der radioaktiven Zerfälle pro Minute in Abhängigkeit vom Zeitpunkt (min nach Start des Experimentes). Ladet den Datensatz in R und macht eine explorative Datenanalyse. Wählt unter den schon gelernten Methoden der Regressionsanalyse einadäquates Vorgehen zur Analyse dieser Daten und führt diese dann durch. Prüft anhand der Residuen, ob die Modellvoraussetzungen erfüllt waren Stellt die erhaltenen Ergebnisse angemessen dar (Text, Abbildung und/oder Tabelle). Kennt ihr ggf. noch eine andere geeignete Herangehensweise? 7.2.2 Übung 2.2: Einfaktorielle ANOVA (SozOek) ANOVA mit novanimal.csv Führt mit dem Datensatz novanimal.csv eine einfaktorielle ANOVA durch. Gibt es Unterschiede zwischen der Anzahl verkaufter Gerichte (Buffet, Fleisch oder Vegetarisch) pro Woche? Hinweise für die Analysen: Fasst die vier Inhalte der Gerichte zu drei Inhalten zusammen. Das heisst, dass die pflanzlichen Gerichte neu zu den vegetarischen Gerichten gezählt werden. Konkret könnt ihr das in R mit einer Umbenennung der Inhalte durchführen (z. B. mit grep()). Danach muss der Datensatz gruppiert und zusammengefasst werden. Unbekannte Menü-Inhalte können vernachlässigt werden. Wie gut sind die Voraussetzungen für eine ANOVA erfüllt? Wären allenfalls auch nicht-parametrische Analysen zulässig? Führt anschliessend Post-hoc-Vergleiche durch. Fasst die Ergebnisse in einem Satz zusammen. 7.2.3 Übung 2.3N: Mehrfaktorielle ANOVA (NatWis) ANOVA mit kormoran.csv Der Datensatz enthält 40 Beobachtungen zu Tauchzeiten zweier Kormoranunterarten (C = Phalocrocorax carbo carbo und S = Phalacrocorax carbo sinensis) aus vier Jahreszeiten (F = Frühling, S = Sommer, H = Herbst, W = Winter). Lest den Datensatz nach R ein und führt eine adäquate Analyse durch, um beantworten zu können, wie Unterart und Jahreszeit die Tauchzeit beeinflussen. Stellt eure Ergebnisse dann angemessen dar (Text mit Abbildung und/oder Tabelle). Gibt es eine Interaktion? 7.2.4 Übung 2.3S: Mehrfaktorielle ANOVA mit Interaktion (SozOek) ANOVA mit novanimal.csv Können die Unterschiede in den verkauften Gerichten (Buffet, Fleisch oder Vegetarisch) durch die beiden Bedingungen (Basis- oder Interventionswochen) erklärt werden? Hinweise für die Analysen: Fasst die vier Inhalte der Gerichte zu drei Inhalten zusammen. Das heisst, dass die pflanzlichen Gerichte neu zu den vegetarischen Gerichten gezählt werden. Konkret könnt ihr das in R mit einer Umbenennung der Inhalte durchführen (z. B. mit grep()). Danach muss der Datensatz gruppiert und zusammengefasst werden. Unbekannte Menü-Inhalte können vernachlässigt werden. Wie gut sind die Voraussetzungen für eine ANOVA erfüllt? Wären allenfalls auch nicht-parametrische Analysen zulässig? Führt anschliessend Post-hoc-Vergleiche durch. Stellt eure Ergebnisse dann angemessen dar (Text mit Abbildung und/oder Tabelle). "],
["7-3-musterlosung-aufgabe-2-1-regression.html", "7.3 Musterlösung Aufgabe 2.1: Regression", " 7.3 Musterlösung Aufgabe 2.1: Regression RCode als Download Übungsaufgabe (hier so ausführlich formuliert, wie dies auch in der Klausur der Fall sein wird) Laden Sie den Datensatz decay.csv. Dieser enthält die Zahl radioaktiver Zerfälle pro Zeiteinheit (amount) für Zeitpunkte (time) nach dem Start des Experimentes. Ermitteln Sie ein statistisches Modell, dass die Zerfallshäufigkeit in Abhängigkeit von der Zeit beschreibt. Bitte erklären und begründen Sie die einzelnen Schritte, die Sie unternehmen, um zu diesem Ergebnis zu kommen. Dazu erstellen Sie bitte ein Word-Dokument, in das Sie Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, Ihre Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere Vorgehen dokumentieren. Dieser Ablauf sollte insbesondere beinhalten: Überprüfen der Datenstruktur nach dem Einlesen, welches sind die abhängige(n) und welches die unabängige(n) Variablen Explorative Datenanalyse, um zu sehen, ob evtl. Dateneingabefehler vorliegen oder Datentransformationen vorgenommen werden sollten Auswahl und Begründung eines statistischen Verfahrens (es gibt hier mehrere statistisch korrekte Möglichkeiten!) Ermittlung eines Modells Durchführen der Modelldiagnostik für das gewählte Modell Generieren aller Zahlen, Statistiken und Tabellen, die für eine wiss. Ergebnisdarstellung benötigt werden Formulieren Sie abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden. Abzugeben sind am Ende (a) Ein lauffähiges R-Skript; (b) begründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation) und (c) ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit). kommentierter Lösungsweg decay &lt;- read.delim(&quot;14_Statistik2/data/decay.csv&quot;,sep = &quot;,&quot;) decay # Um die Variablen im Dataframe im Folgenden direkt (ohne $ bzw. ohne &quot;data = data&quot;) ansprechen zu können attach(decay) summary(decay) str(decay) Man erkennt, dass es 31 Beobachtungen für die Zeit als Integer von Zerfällen gibt, die als rationale Zahlen angegeben werden (dass die Zahl der Zerfälle nicht ganzzahlig ist, deutet darauf hin, dass sie möglicherweise nur in einem Teil des Zeitintervalls oder für einen Teil des betrachteten Raumes gemessen und dann hochgerechnet wurde. Explorative Datenanalyse boxplot(time) boxplot(amount) plot(amount~time) Während der Boxplot für time wunderbar symmetrisch ohne Ausreisser ist, zeigt amount eine stark rechtsschiefe (linkssteile) Verteilung mit einem Ausreiser. Das deutet schon an, dass ein einfaches lineares Modell vermutlich die Modellannahmen verletzen wird. Auch der einfache Scatterplot zeigt, dass ein lineares Modell wohl nicht adäquat ist. Wir rechnen aber erst einmal weiter. Einfaches lineares Modell lm.1&lt;-lm(amount~time) summary(lm.1) Das sieht erst einmal nach einem Supermodell aus, höchstsignifikant und mit einem hohen R² von fast 77%. ABER: wir müssen uns noch die Modelldiagnostik ansehen… Modelldiagnostik par(mfrow=c(2,2)) plot(lm.1) Hier zeigen die wichtigen oberen Plots beide massive Abweichungen vom „Soll“. Der Plot oben links zeigt eine „Banane“ und beim Q-Q-Plot oben rechts weichen die Punkte rechts der Mitte alle stark nach oben von der Solllinie ab. Wir haben unser Modell also offensichtlich falsch spezifiziert. Um eine Idee zu bekommen, was falsch ist, plotten wir noch, wie das Ergebnis dieses Modells aussähe: Ergebnisplot par(mfrow=c(1,1)) plot(time,amount) abline(lm(amount~time),col=&quot;red&quot;) abline(lm.1,col=&quot;red&quot;) Die Punkte links liegen alle über der Regressionslinie, die in der Mitte darunter und die ganz rechts wieder systematisch darüber (darum im Diagnostikplot oben die „Banane“). Es liegt also offensichtlich keine lineare Beziehung vor, sondern eine curvilineare. Um diese korrekt zu analysieren, gibt es im Prinzip drei Möglichkeiten, wovon am zweiten Kurstag nur eine hatten, während die zweite und dritte in Statistik 3 und 4 folgten. Im Folgenden sind alle drei nacheinander dargestellt (in der Klausur würde es aber genügen, eine davon darzustellen, wenn die Aufgabenstellung wie oben lautet). Variante (1): Lineares Modell nach Transformation der abhängigen Variablen Dass die Verteilung der abhängigen Variable nicht normal ist, haben wir ja schon bei der explorativen Datenanalyse am Anfang gesehen. Da sie stark linkssteil ist, zugleich aber keine Nullwerte enthält, bietet sich eine Logarithmustransformation an, hier z. B. mit dem natürlichen Logarithmus. Loesung 1: log-Transformation der abhaengigen Variablen par(mfrow=c(1,2)) boxplot(amount) boxplot(log(amount)) hist(amount) hist(log(amount)) #Die log-transformierte Variante rechts sieht sowohl im Boxplot als auch im #Histogramm viel symmetrischer/besser normalverteilt aus. Damit ergibt sich #dann folgendes lineares Modell lm.2&lt;-lm(log(amount)~time) summary(lm.2) Jetzt ist der R²-Wert noch höher und der p-Wert noch niedriger als im ursprünglichen linearen Modell ohne Transformation. Das erlaubt aber keine Aussage, da wir Äpfel mit Birnen vergleichen, da die abhängige Variable einmal untransformiert und einmal log-transformiert ist. Entscheidend ist die Modelldiagnostik. Modelldiagnostik par(mfrow=c(2,2)) plot(lm.2) Der Q-Q-Plot sieht jetzt exzellent aus, der Plot rechts oben hat kaum noch eine Banane, nur noch einen leichten Keil. Insgesamt deutlich besser und auf jeden Fall ein statistisch korrektes Modell. Lösungen 2 und 3 greifen auf Methoden von Statistik 3 und 4 zurück, sie sind hier nur zum Vergleich angeführt Loesung 2: quadratische Regression (kam erst in Statistik 3; koente fuer die Datenverteilung passen, entspricht aber nicht der physikalischen Gesetzmaessigkeit model.quad&lt;-lm(amount~time+I(time^2)) summary(model.quad) Hier können wir R² mit dem ursprünglichen Modell vergleichen (beide haben amount als abhängige Grösse) und es sieht viel besser aus. Sowohl der lineare als auch der quadratische Term sind hochsignifikant. Sicherheitshalber vergleichen wir die beiden Modelle aber noch mittels ANOVA. Vergleich mit dem einfachen Modell mittels ANOVA (es ginge auch AICc) anova(lm.1,model.quad) In der Tat ist das komplexere Modell (jenes mit dem quadratischen Term) höchstsignifikant besser. Jetzt brauchen wir noch die Modelldiagnostik. Modelldiagnostik par(mfrow=c(2,2)) plot(model.quad) Loesung 3 (die beste, hatten wir aber am 2. Tag noch nicht; mit Startwerten muss man ggf. ausprobieren) mit Startwerten muss man ggf. ausprobieren) model.nls&lt;-nls(amount~a*exp(-b*time),start=(list(a=100,b=1))) summary(model.nls) Modelldiagnostik if(!require(nlstools)){install.packages(&quot;nlstools&quot;)} library(nlstools) residuals.nls &lt;- nlsResiduals(model.nls) plot(residuals.nls) Für nls kann man nicht den normalen Plotbefehl für die Residualdiagnostik nehmen, sondern verwendet das Äquivalent aus nlstools. Die beiden entscheidenden Plots sind jetzt links oben und rechts unten. Der QQ-Plot hat im unteren Bereich einen kleinen Schönheitsfehler, aber ansonsten ist alles OK. Da alle drei Lösungen zumindest statistisch OK waren, sollen jetzt noch die zugehörigen Ergebnisplots erstellt werden. Ergebnisplots par(mfrow=c(1,1)) xv&lt;-seq(0,30,0.1) lineares Modell mit log-transformierter Abhaengiger plot(time,amount) yv1&lt;-exp(predict(lm.2,list(time=xv))) lines(xv,yv1,col=&quot;red&quot;) quadratisches Modell plot(time,amount) yv2&lt;-predict(model.quad,list(time=xv)) lines(xv,yv2,col=&quot;blue&quot;) nicht-lineares Modell plot(time,amount) yv3&lt;-predict(model.nls,list(time=xv)) lines(xv,yv3,col=&quot;green&quot;) Optisch betrachtet, geben (2) und (3) den empirischen Zusammenhang etwas besser wieder als (1), da sie im linken Bereich die hohen Werte besser treffen. Man könnte sogar meinen, bei Betrachtung der Daten, dass die Werte ab time = 28 wieder leicht ansteigen, was die quadratische Funktion wiedergibt. Wer sich aber mit Physik etwas auskennt, weiss, dass Version (2) physikalisch nicht zutrifft, da die Zerfallsrate mit der Zeit immer weiter abfällt. Aufgrund der kurzen Messreihe wäre eine quadratische Funktion trotzdem eine statistisch korrekte Interpretation. Mit längeren Messreihen würde sich jedoch schnell zeigen, dass sie nicht zutrifft. "],
["7-4-musterlosung-aufgabe-2-2-einfaktorielle-anova.html", "7.4 Musterlösung Aufgabe 2.2: einfaktorielle ANOVA", " 7.4 Musterlösung Aufgabe 2.2: einfaktorielle ANOVA Download R-Skript kommentierter Lösungsweg df &lt;- nova # klone den originaler Datensatz # fasst die vier Inhalte der Gerichte zu drei Inhalten zusammen. df$label_content[grep(&quot;Pflanzlich+&quot;,df$label_content)] &lt;- &quot;Vegetarisch&quot; # gruppiert Daten nach Menü-Inhalt und Woche df_ &lt;- df %&gt;% group_by(label_content, week) %&gt;% summarise(tot_sold = n()) %&gt;% drop_na() # lasst die unbekannten Menü-Inhalte weg # überprüft die Voraussetzungen für eine ANOVA # Schaut euch die Verteilungen der Mittelwerte an (plus Standardabweichungen) # Sind Mittelwerte nahe bei Null? Gäbe uns einen weiteren Hinweis auf eine spezielle Binomail-Verteilung (vgl. Statistik 4, FOlie 17) df_ %&gt;% split(.$label_content) %&gt;% # teilt den Datensatz in 3 verschiedene Datensätze auf purrr::map(~ psych::describe(.$tot_sold)) # mit map können andere Funktionen auf den Datensatz angewendet werden (ähnliche Funktion ist aggregate) # Boxplot ggplot(df_, aes(x = label_content, y= tot_sold)) + stat_boxplot(geom = &quot;errorbar&quot;, width = 0.25) + # Achtung: Reihenfolge spielt hier eine Rolle! geom_boxplot(fill=&quot;white&quot;, color = &quot;black&quot;, size = 1, width = .5) + labs(x = &quot;\\nMenü-Inhalt&quot;, y = &quot;Anzahl verkaufte Gerichte pro Woche\\n&quot;) + mytheme # achtung erster Hinweis einer Varianzheterogenität # definiert das Modell (Statistik 2: Folien 4-8) model &lt;- aov(tot_sold ~ label_content, data = df_) summary.lm(model) # überprüft die Modelvoraussetzungen autoplot(model) + mytheme Fazit: Inspektion der Modellvoraussetzung zeigt klare Verletzungen des Residuelplots (zeigt einen “Trichter”, siehe Statistik 2: Folie 13-14; 42), somit Voraussetzung der Homoskedastizität verletzt. Mögliche nächste Schritte: Datentransformation oder nicht-parametrischer Test. # überprüft die Voraussetzungen des Welch-Tests: # Gibt es eine hohe Varianzheterogenität und ist die relative Verteilung der Residuen gegeben? (siehe Folien Statistik 2: Folie 18) # Ja Varianzheterogenität ist gegeben, aber die Verteilung der Residuen folgt einem &quot;Trichter&quot;, also keiner &quot;normalen/symmetrischen&quot; Verteilung um 0 (siehe Folien Statistik 2: Folie 42) # Daher ziehe ich eine Transformation der AV dem nicht-parametrischen Test vor # für weitere Infos: https://data.library.virginia.edu/interpreting-log-transformations-in-a-linear-model/ model1 &lt;- aov(log10(tot_sold) ~ label_content, data = df_)# achtung hier log10, bei Rücktransformation achten autoplot(model1) + mytheme # scheint ok zu sein summary.lm(model1) # Referenzkategorie ist der Buffet-Inhalt TukeyHSD(model1) # (Statistik 2: Folien 9-11) # achtung Beta-Werte resp. Koeffinzienten sind nicht direkt interpretierbar # sie müssten zuerst wieder transformiert werden, hier ein Beispiel dafür: # für Buffet 10^model1$coefficients[1] # für Fleisch 10^(model1$coefficients[1] + model1$coefficients[2]) # für Vegi 10^(model1$coefficients[1] + model1$coefficients[3]) Figure 7.1: Die wöchentlichen Verkaufzahlen unterscheiden sich je nach Menü-Inhalt stark. Das Modell wurde mit den log-tranformierten Daten gerechnet. "],
["7-5-musterlosung-aufgabe-2-3n-mehrfaktorielle-anova.html", "7.5 Musterlösung Aufgabe 2.3N: Mehrfaktorielle ANOVA", " 7.5 Musterlösung Aufgabe 2.3N: Mehrfaktorielle ANOVA RCode als Download Übungsaufgabe (hier so ausführlich formuliert, wie dies auch in der Klausur der Fall sein wird) Laden Sie den Datensatz kormoran.txt mit read.table. Dieser enthält Tauchzeiten (hier ohne Einheit) von Kormoranen in Abhängigkeit von Jahreszeit und Unterart. Unterarten: Phalacrocorax carbo carbo (C) und Phalacrocorax carbo sinensis (S); Jahreszeiten: F = Frühling, S = Sommer, H = Herbst, W = Winter. Ihre Gesamtaufgabe ist es, aus diesen Daten ein minimal adäquates Modell zu ermitteln, das diese Abhängigkeit beschreibt. Bitte erklären und begründen Sie die einzelnen Schritte, die Sie unternehmen, um zu diesem Ergebnis zu kommen. Dazu erstellen Sie bitte ein Word-Dokument, in das Sie Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, Ihre Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere Vorgehen dokumentieren. Dieser Ablauf sollte insbesondere beinhalten: Überprüfen der Datenstruktur nach dem Einlesen, welches sind die abhängige(n) und welches die unabängige(n) Variablen, welches statistische Verfahren wenden Sie an? Explorative Datenanalyse, um zu sehen, ob schon vor dem Start der Analysen Transformationen o.ä. vorgenommen werden sollten Definition eines vollen Modelles, das nach statistischen Kritierien zum minimal adäquaten Modell reduziert wird Durchführen der Modelldiagnostik, um zu entscheiden, ob das gewählte Vorgehen korrekt war oder ggf. angepasst werden muss Generieren aller Zahlen, Statistiken und Tabellen, die für eine wiss. Ergebnisdarstellung benötigt werden Formulieren Sie abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden. Abzugeben sind am Ende (a) Ein lauffähiges R-Skript; (b) begründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation) und (c) ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit). kommentierter Lösungsweg kormoran &lt;-read.delim(&quot;14_Statistik2/data/kormoran.csv&quot;,sep = &quot;;&quot;) ## Ueberpruefen, ob Einlesen richtig funktioniert hat und welche Datenstruktur vorliegt str(kormoran) summary(kormoran) #Man erkennt, dass es sich um einen Dataframe mit einer metrischen (Tauchzeit) und zwei kategorialen (Unterart, Jahreszeit) Variablen handelt. #Die adäquate Analyse (1 metrische Abhängige vs. 2 kategoriale Unabhängige) ist damit eine zweifaktorielle ANOVA #Die Sortierung der Jahreszeiten (default: alphabetisch) ist inhaltlich aber nicht sinnvoll und sollte angepasst werden. # Um die Variablen im Dataframe im Folgenden direkt (ohne $ bzw. ohne &quot;data = kormoran&quot;) ansprechen zu koennen attach(kormoran) # Umsortieren der Faktoren, damit sie in den Boxplots eine sinnvolle Reihung haben Jahreszeit&lt;-factor(Jahreszeit,levels=c(&quot;F&quot;,&quot;S&quot;,&quot;H&quot;,&quot;W&quot;)) # Explorative Datenanalyse (zeigt uns die Gesamtverteilung) boxplot(Tauchzeit) #Das ist noch OK für parametrische Verfahren (Box ziemlich symmetrisch um Median, Whisker etwas asymmetrisch aber nicht kritisch). Wegen der leichten Asymmetrie (Linksschiefe) könnte man eine log-Transformation ausprobieren. boxplot(log10(Tauchzeit)) #Der Gesamtboxplot für log10 sieht perfekt symmetrisch aus, das spräche also für eine log10-Transformation. De facto kommt es aber nicht auf den Gesamtboxplot an, sondern auf die einzelnen. # Explorative Datenanalyse (Check auf Normalverteilung der Residuen und Varianzhomogenitaet) boxplot(Tauchzeit~Jahreszeit*Unterart) boxplot(log10(Tauchzeit)~Jahreszeit*Unterart) #Hier sieht mal die Verteilung für die untransformierten Daten, mal für die transformierten besser aus. Da die Transformation keine klare Verbesserung bringt, bleiben wir im Folgenden bei den untransformierten Daten, da diese leichter (direkter) interpretiert werden können # Vollständiges Modell mit Interaktion aov.1 &lt;- aov(Tauchzeit~Unterart*Jahreszeit) aov.1 summary(aov.1) #p-Wert der Interaktion ist 0.266 #Das volle (maximale) Modell zeigt, dass es keine signifikante Interaktion zwischen Jahreszeit und Unterart gibt. Wir können das Modell also vereinfachen, indem wir die Interaktion herausnehmen (+ statt * in der Modellspezifikation) #Modellvereinfachung aov.2 &lt;- aov(Tauchzeit~Unterart+Jahreszeit) aov.2 summary(aov.2) #Im so vereinfachten Modell sind alle verbleibenden Terme signifikant, wir sind also beim „minimal adäquaten Modell“ angelangt #Anderer Weg, um zu pruefen, ob man das komplexere Modell mit Interaktion behalten soll anova(aov.1,aov.2) #in diesem Fall bekommen wir den gleichen p-Wert wie oben (0.266) #Modelldiagnostik par(mfrow=c(2,2)) #alle vier Abbildungen in einem 2 x 2 Raster plot(aov.2) influence.measures(aov.2) # kann man sich zusätzlich zum &quot;plot&quot; ansehen, um herauszufinden, ob es evtl. sehr einflussreiche Werte mit Cook&#39;s D von 1 oder grösser gibt #Links oben ist alles bestens, d. h. keine Hinweise auf Varianzheterogenität („Keil“) oder Nichtlinearität („Banane“) #Rechts oben ganz gut, allerdings weichen Punkte 1 und 20 deutlich von der optimalen Gerade ab -&gt; aus diesem Grund können wir es doch noch mal mit der log10-Transformation versuchen (s.u.) #Rechts unten: kein Punkt hat einen problematischen Einfluss (die roten Linien für Cook’s D &gt; 0.5 und &gt; 1 sind noch nicht einmal im Bildausschnitt. #Alternative mit log10 aov.3 &lt;-aov(log10(Tauchzeit)~Unterart+Jahreszeit) aov.3 summary(aov.3) plot(aov.3) #Rechts oben: Punkt 20 jetzt auf der Linie, aber Punkt 1 weicht umso deutlicher ab -&gt; keine Verbesserung -&gt; wir bleiben bei den untransformierten Daten. #Ergebnisdarstellung #Da wir keine Interaktion zwischen Unterart und Jahreszeit festgestellt haben, brauchen wir auch keinen Interaktionsplot (unnötig kompliziert), statt dessen können wir die Ergebnisse am besten mit zwei getrennten Plots für die beiden Faktoren darstellen. Bitte die Achsenbeschriftungen und den Tukey post-hoc-Test nicht vergessen. par(mfrow=c(1,1)) #Zurückschalten auf Einzelplots if(!require(multcomp)){install.packages(&quot;multcomp&quot;)} library(multcomp) #letters&lt;-cld(glht(aov.2,linfct=mcp(Unterart=&quot;Tukey&quot;))) boxplot(Tauchzeit~Unterart,xlab=&quot;Unterart&quot;,ylab=&quot;Tauchzeit&quot;) #mtext(letters$mcletters$Letters,at=1:2) #genaugenommen braucht man bei nur zwei Kategorien keinen post hoc-Test letters&lt;-cld(glht(aov.2,linfct=mcp(Jahreszeit=&quot;Tukey&quot;))) boxplot(Tauchzeit~Jahreszeit,xlab=&quot;Jahreszeit&quot;,ylab=&quot;Tauchzeit&quot;) #Achsenbeschriftung nicht vergessen! mtext(letters$mcletters$Letters,at=c(1:4)) #Jetzt brauchen wir noch die Mittelwerte bzw. Effektgroessen #Für den Ergebnistext brauchen wir auch noch Angaben zu den Effektgrössen. Hier sind zwei Möglichkeiten, um an sie zu gelangen. aggregate(Tauchzeit~Jahreszeit, FUN=mean) aggregate(Tauchzeit~Unterart, FUN=mean) summary(lm(Tauchzeit~Jahreszeit)) summary(lm(Tauchzeit~Unterart)) "],
["7-6-musterlosung-aufgabe-2-3s-anova-mit-interaktion.html", "7.6 Musterlösung Aufgabe 2.3S: ANOVA mit Interaktion", " 7.6 Musterlösung Aufgabe 2.3S: ANOVA mit Interaktion Lese-Empfehlung Kapitel 7 von Manny Gimond Download R-Skript kommentierter Lösungsweg # klone den originaler Datensatz df &lt;- nova # fasst die vier Inhalte der Gerichte zu drei Inhalten zusammen df$label_content[grep(&quot;Pflanzlich+&quot;,df$label_content)] &lt;- &quot;Vegetarisch&quot; # ersetzt beide Pflanzlich und Pflanzlich+ # gruppiert Daten gemäss Bedingungen, Menü-Inhalt und Wochen df_ &lt;- df %&gt;% group_by(condit, label_content, week) %&gt;% summarise(tot_sold = n()) %&gt;% drop_na() # lasst die unbekannten Menü-Inhalte weg # überprüft Voraussetzungen für eine ANOVA # Boxplots zeigt klare Varianzheterogenität ggplot(df_, aes(x = interaction(label_content, condit), y = tot_sold)) + stat_boxplot(geom = &quot;errorbar&quot;, width = .25) + geom_boxplot(fill=&quot;white&quot;, size = 1, width = .5) + labs(x = &quot;\\nMenü-Inhalt&quot;, y = &quot;Anzahl verkaufte Gerichte pro Woche\\n&quot;) + mytheme # definiert das Modell mit Interaktion model2 &lt;- aov(tot_sold ~ label_content * condit, data = df_) autoplot(model2) + mytheme # Inspektion der Modellvoraussetzungen sehen nicht schlecht aus =&gt; einzig Normalverteilung Q-Q Plot nicht optimal (vgl. Statistik 2: Folie 42) summary(model2) # Alternativ gibt es zwei Möglichkeiten: #1) Transformation der Daten, model3 &lt;- aov(log10(tot_sold) ~ label_content * condit, data = df_) autoplot(model3) + mytheme #2) nicht-parametrischer Test z.B. Kruskal-Wallis-Test (vgl. Statistik 2: Folie 17-18) inter_action &lt;- interaction(df_$condit, df_$label_content) # zuerst Interaktionsterm definineren, da kruskal.test nicht mit Interaktionen umgehen kann model4 &lt;- kruskal.test(df_$tot_sold ~ inter_action) # in einem nächsten Schritt könnt ihr mit Post-hoc Tests diese Unterschiede genauer betrachten # es gibt die Möglichkeit mit dunnTest (mit Package FSA) # mehr Infos hier: https://rcompanion.org/rcompanion/d_06.html library(FSA) dunn &lt;- dunnTest(tot_sold ~ inter_action, data = df_, method=&quot;bh&quot;) # zur Info: dunnTest kann nur mit Faktoren umgehen dunn # Infos zu Korrektur für Mehrfachvergleiche (vgl. https://mgimond.github.io/Stats-in-R/ANOVA.html#4_identifying_which_levels_are_different) Fazit: Die Inspektion des Modells zeigt leichte Verletzungen beim Q-Q Plot, d.h. die Residuen sind nicht normalverteilt. Aufgrund keiner Verbesserung durch eine Transformation der Responsevariable, entscheide ich mich für eine ANOVA mit nicht tranformierten Daten.. # Post-hoc Vergleiche TukeyHSD(model2) # nimmt aber an, dass Residuen normalverteilt sind # Alternativ library(DescTools) PostHocTest(model2, method = &quot;scheffe&quot;) # sehr konservativ und auch für ungleiche Gruppengrössen geeignet Figure 7.2: Box-Whisker-Plots der wöchentlichen Verkaufszahlen pro Menü-Inhalte. Kleinbuchstaben bezeichnen homogene Gruppen auf p &lt; .05 nach Tukeys post-hoc-Test. Figure 7.3: Wöchentliche Verkaufszahlen aggregiert für die drei Menü-Inhalte. "],
["8-statistik-3-04-11-2019.html", "Kapitel 8 Statistik 3 (04.11.2019)", " Kapitel 8 Statistik 3 (04.11.2019) Statistik 3 fassen wir zu Beginn den generellen Ablauf inferenzstatistischer Analysen in einem Flussdiagramm zusammen. Dann wird die ANCOVA als eine Technik vorgestellt, die eine ANOVA mit einer linearen Regression verbindet. Danach geht es um komplexere Versionen linearer Regressionen. Hier betrachten wir polynomiale Regressionen, die z. B. einen Test auf unimodale Beziehungen erlaubt, indem man dieselbe Prädiktorvariable linear und quadriert einspeist. Multiple Regressionen versuchen dagegen, eine abhängige Variable durch zwei oder mehr verschieden Prädiktorvariablen zu erklären. Wir thematisieren verschiedene dabei auftretende Probleme und ihre Lösung, insbesondere den Umgang mit korrelierten Prädiktoren und das Aufspüren des besten unter mehreren möglichen statistischen Modellen. Hieran wird auch der informatian theoretician-Ansatz der Statistik und die multimodel inference eingeführt. "],
["8-1-demoskript-1.html", "8.1 Demoskript", " 8.1 Demoskript Demoscript als Download Datensatz ipomopsis.csv Datensatz loyn.csv ANCOVA Experiment zur Fruchtproduktion (“Fruit”) von Ipomopsis sp. (“Fruit”) in Abhängigkeit Ungrazedvon der Beweidung (Grazing mit 2 Levels: Grazed, Ungrazed) und korrigiert für die Pflanzengrösse vor der Beweidung (hier ausgedrückt als Durchmesser an der Spitze des Wurzelstock: “Root”) compensation&lt;-read.table(&quot;data/ipomopsis.csv&quot;, header=T,sep=&quot;,&quot;) summary(compensation) attach(compensation) plot(Fruit~Root) plot(Fruit~Grazing) tapply(Fruit,Grazing,mean) aoc.1&lt;-lm(Fruit~Root*Grazing) summary.aov(aoc.1) aoc.2&lt;-lm(Fruit~Grazing*Root) summary.aov(aoc.2) aoc.3&lt;-lm(Fruit~Grazing+Root) summary.lm(aoc.3) # Plotten der Ergebnisse plot(Fruit~Root,pch=21,bg=(1+as.numeric(Grazing))) #legend(locator(1),c(&quot;grazed&quot;,&quot;ungrazed&quot;),col=c(2,3),pch=16) # Position von Legende von Hand setzen #legend(4.5,110,c(&quot;grazed&quot;,&quot;ungrazed&quot;),col=c(2,3),pch=16) # Position von Legende in Code definieren #legend(&quot;topleft&quot;,c(&quot;grazed&quot;,&quot;ungrazed&quot;),col=c(2,3),pch=16) # Alternative position von Legende in Code definieren abline(-127.829,23.56,col=&quot;red&quot;) abline(-127.892+36.103,23.56,col=&quot;green&quot;) Polynomische Regression e&lt;-c(20,19,25,10,8,15,13,18,11,14,25,39,38,28,24) f&lt;-c(12,15,10,7,2,10,12,11,13,10,9,2,4,7,13) summary(lm(f~e)) par(mfrow=c(1,1)) plot(f~e,xlim=c(0,40),ylim=c(0,30)) abline(lm(f~e)) par(mfrow=c(2,2)) plot(lm(f~e)) plot(lm(f~e+I(e^2))) summary(lm(f~e+I(e^2))) Multiple lineare Regression basierend auf Logan, Beispiel 9A loyn &lt;- read.table(&quot;data/loyn.csv&quot;, header=T,sep=&quot;,&quot;) loyn library(car) summary(loyn) lm.1 &lt;- lm (ABUND ~ YR.ISOL + ALT + GRAZE, data=loyn) summary(lm.1) aov(lm.1) par(mfrow=c(2,2)) plot(lm.1) influence.measures(lm.1) cor &lt;- cor(loyn[,2:7]) print(cor, digits=2) cor[abs(cor)&lt;0.6] &lt;- 0 cor print(cor, digits=3) vif(lm.1) Simulation Overfitting test &lt;- data.frame(&quot;x&quot;=c(1,2,3,4,5,6),&quot;y&quot;=c(34,21,70,47,23,45)) attach(test) plot(x,y) lm0 &lt;- lm(y~1) lm1 &lt;- lm(y~x) lm2 &lt;- lm(y~x+I(x^2)) lm3 &lt;- lm(y~x+I(x^2)+I(x^3)) lm4 &lt;- lm(y~x+I(x^2)+I(x^3)+I(x^4)) lm5 &lt;- lm(y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)) lm6 &lt;- lm(y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)+I(x^6)) summary(lm0) summary(lm1) summary(lm2) summary(lm3) summary(lm4) summary(lm5) xv&lt;-seq(from=0,to=10,by=0.1) plot(x,y,cex=2,col=&quot;black&quot;,lwd=3) yv&lt;-predict(lm1,list(x=xv)) lines(xv,yv,col=&quot;red&quot;,lwd=3) yv&lt;-predict(lm2,list(x=xv)) lines(xv,yv,col=&quot;blue&quot;,lwd=3) yv&lt;-predict(lm3,list(x=xv)) lines(xv,yv,col=&quot;green&quot;,lwd=3) yv&lt;-predict(lm4,list(x=xv)) lines(xv,yv,col=&quot;orange&quot;,lwd=3) yv&lt;-predict(lm5,list(x=xv)) lines(xv,yv,col=&quot;black&quot;,lwd=3) Modellvereinfachung (mit Loyn-Datensatz) lm.1 &lt;- lm(ABUND ~ YR.ISOL + ALT + GRAZE, data=loyn) summary(lm.1) lm.2 &lt;- update(lm.1,~.-YR.ISOL) summary(lm.2) anova(lm.1,lm.2) Hierarchical partitioning if(!require(hier.part)){install.packages(&quot;hier.part&quot;)} library(hier.part) loyn.preds &lt;-with(loyn, data.frame(YR.ISOL,ALT,GRAZE)) par(mfrow=c(1,1)) hier.part(loyn$ABUND,loyn.preds,gof=&quot;Rsqu&quot;) Partial regressions avPlots(lm.1,ask=F) Multimodel inference if(!require(MuMIn)){install.packages(&quot;MuMIn&quot;)} library(MuMIn) global.model &lt;- lm (ABUND ~ YR.ISOL + ALT + GRAZE, data=loyn) options(na.action=&quot;na.fail&quot;) allmodels &lt;- dredge(global.model) allmodels importance(allmodels) avgmodel&lt;-model.avg(get.models(dredge(global.model,rank=&quot;AICc&quot;),subset=TRUE)) summary(avgmodel) "],
["8-2-ubung-3.html", "8.2 Übung 3", " 8.2 Übung 3 8.2.1 Aufgabe 3.1: Multiple Regression Datensatz Ukraine_bearbeitet.xlsx Datensatz Ukraine_bearbeitet.csv Artenzahlen von Vegetationsaufnahmen in der Ukraine vs. diverse Umweltparameter (farbig gruppiert nach Kategorien, aus Kuzemko et al. 2016) Bestimmt ein minimal adäquates Modell für die Erklärung der Artenzahlen mit allen notwendigen Arbeitsschritten Wahlweise könnt ihr mit AICc und dredge oder mit p-Werten und schrittweiser Vereinfachung eines globalen Models arbeiten knitr::opts_chunk$set(results = &quot;hide&quot;, fig.width = 20, fig.height = 12, warning = F, message = F, fig.pos = &#39;H&#39;) "],
["8-3-musterlosung-aufgabe-3-1-multiple-regression.html", "8.3 Musterlösung Aufgabe 3.1: Multiple Regression", " 8.3 Musterlösung Aufgabe 3.1: Multiple Regression R-Skript als Download Übungsaufgabe (hier so ausführlich formuliert, wie dies auch in der Klausur der Fall sein wird) Bereiten Sie den Datensatz Ukraine.xlsx für das Einlesen in R vor und lesen Sie ihn dann ein. Dieser enthält gemittelte Pflanzenartenzahlen (Species.richness) von 199 10 m² grossen Plots (Vegetationsaufnahmen) von Steppenrasen in der Ukraine sowie zahlreiche Umweltvariablen, deren Bedeutung und Einheiten im Kopf der ExcelTabelle angegeben sind. Ermitteln Sie ein minimal adäquates Modell, das den Artenreichtum in den Plots durch die Umweltvariablen erklärt. Bitte erklären und begründen Sie die einzelnen Schritte, die Sie unternehmen, um zu diesem Ergebnis zu kommen. Dazu erstellen Sie bitte ein Word-Dokument, in das Sie Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, Ihre Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere Vorgehen dokumentieren. Dieser Ablauf sollte insbesondere beinhalten: Überprüfen der Datenstruktur nach dem Einlesen: welches sind die abhängige(n) und welches die unabängige(n) Variablen, sind alle Variablen für die Analyse geeignet? Explorative Datenanalyse, um zu sehen, ob die abhängige Variable in der vorliegenden Form für die Analyse geeignet ist Definition eines globalen Modelles und dessen Reduktion zu einem minimal adäquaten Modell Durchführen der Modelldiagnostik für dieses Generieren aller Zahlen, Statistiken und Tabellen, die für eine wiss. Ergebnisdarstellung benötigt werden Formulieren Sie abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden. Abzugeben sind am Ende (a) Ein lauffähiges R-Skript; (b) begründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation) und (c) ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit). Loesung Übung 3.1: Multiple Regression Schon vor dem Einlesen kürzt man am besten bereits in Excel die Variablennamen so ab, dass sie noch eindeutig, aber nicht unnötig lang sind, etwa indem man die Einheiten wegstreicht # Aus der Excel-Tabelle wurde das relevante Arbeitsblatt als csv gespeichert ukraine &lt;-read.delim(&quot;15_Statistik3/data/Ukraine_bearbeitet.csv&quot;,sep=&quot;;&quot;) attach(ukraine) ukraine str(ukraine) summary(ukraine) Man erkennt, dass alle Spalten bis auf die erste mit der Plot ID numerisch (num oder int) und dass die abhängige Variable in Spalte 2 sowie die Prediktorvariablen in den Spalten 3 bis 23 stehen. #Explorative Datenanalyse der abhängigen Variablen boxplot(Species.richness) Der Boxplot sieht sehr gut symmetrisch aus. Insofern gibt es keinen Anlass über eine Transformation nachzudenken. (Da es sich bei Artenzahlen um Zähldaten handelt, müsste man theoretisch ein glm mit Poisson-Verteilung rechnen; bei einem Mittelwert, der hinreichend von Null verschieden ist (hier: ca. 40), ist eine Poisson-Verteilung aber praktisch nicht von einer Normalverteilung zu unterscheiden und wir können uns den Aufwand auch sparen). cor &lt;- cor(ukraine[,3:23]) cor cor[abs(cor)&lt;0.7] &lt;- 0 cor Die Korrelationsanalyse dient dazu, zu entscheiden, ob die Prädiktorvariablen hinreichend voneinander unabhängig sind, um alle in das globale Modell hinein zu nehmen. Bei Pearson’s Korrelationskoeffizienten r, die betragsmässig grösser als 0.7 sind, würde es problematisch. Alternativ hätten wir auch den VIF (Variance Inflation Factor) als Kriterium für den möglichen Ausschluss von Variablen aus dem globalen Modell nehmen können. Diese initiale Korrelationsanalyse zeigt uns aber, dass unsere Daten noch ein anderes Problem haben: für die drei Korngrössenfraktionen des Bodens (Sand, Silt, Clay) stehen lauter NA’s. Um herauszufinden, was das Problem ist, geben wir ein: summary(ukraine$Sand) ukraine[!complete.cases(ukraine), ] # Zeigt zeilen mit NAs ein Da gibt es offensichtlich je ein NA in jeder dieser Zeilen. Jetzt können wir entscheiden, entweder auf die drei Variablen oder auf die eine Beobachtung zu verzichten. Da wir eh schon eher mehr unabhängige Variablen haben als wir händeln können, entscheide ich pragmatisch für ersteres. Wir rechnen die Korrelation also noch einmal ohne diese drei Spalten (es sind die Nummern 12:14, wie wir aus der anfänglichen Variablenbetrachtung oben wissen). cor &lt;- cor(ukraine[,c(3:11,15:23)]) cor[abs(cor)&lt;0.7] &lt;- 0 cor Wenn man auf cor nun doppel-clickt und es in einem separaten Fenster öffnet, sieht man, wo es problematische Korrelationen zwischen Variablenpaaren gibt. Es sind dies Altitude vs. Temperature und N.total vs. C.org. Wir müssen aus jedem dieser Paare jetzt eine Variable rauswerfen, am besten jene, die weniger gut interpretierbar ist. Ich entscheide mich dafür Temperature statt Altitude (weil das der direktere ökologische Wirkfaktor ist) und C.org statt N.total zu behalten (weil es in der Literatur mehr Daten zum Humusgehalt als zum N-Gehalt gibt, damit eine bessere Vergleichbarkeit erzielt wird). Die Aussagen, die wir für die beibehaltene Variable erzielen, stehen aber +/- auch für die entfernte. Das Problem ist aber, dass wir immer noch 16 Variablen haben, was einen sehr leistungsfähigen Rechner oder sehr lange Rechenzeit erfordern würde. Wir sollten also unter 15 Variablen kommen. Wir könnten uns jetzt überlegen, welche uns ökologisch am wichtigsten sind, oder ein noch strengeres Kriterium bei r verwenden, etwa 0.6 cor &lt;- cor(ukraine[,c(3:11,15:23)]) cor[abs(cor)&lt;0.6] &lt;- 0 cor Entsprechend „werfen“ wir auch noch die folgenden Variablen „raus“: Temperature.range (positiv mit Temperature), Precipitation (negativ mit Temperature) sowie Conductivity (positiv mit pH). Nun können wir das globale Modell definieren, indem wir alle verbleibenden Variablen aufnehmen, das sind 13. (Wenn das nicht eh schon so viele wären, dass es uns an die Grenze der Rechenleistung bringt, hätten wir auch noch darüber nachdenken können, einzelne quadratische Terme oder Interaktionsterme zu berücksichtigen). global.model &lt;- lm (Species.richness ~ Inclination+Heat.index+Microrelief+Grazing.intensity+Litter+ Stones.and.rocks+Gravel+Fine.soil+pH+CaCO3+C.org+CN.ratio+Temperature) Nun gibt es im Prinzip zwei Möglichkeiten, vom globalen (vollen) Modell zu einem minimal adäquaten Modell zu kommen. (1) Der Ansatz der „frequentist statistic“, in dem man aus dem vollen Modell so lange schrittweise Variablen entfernt, bis nur noch signifikante Variablen verbleiben. (2) Den informationstheoretischen Ansatz, bei dem alle denkbaren Modelle berechnet und verglichen werden (also alle möglichen Kombinationen von 13,12,…, 1, 0 Parametern). Diese Lösung stelle ich im Folgenden vor: #Multimodel inference if(!require(MuMIn)){install.packages(&quot;MuMIn&quot;)} library(MuMIn) options(na.action=&quot;na.fail&quot;) allmodels&lt;-dredge(global.model) allmodels Jetzt bekommen wir die besten der insgesamt 8192 möglichen Modelle gelistet mit ihren Parameterschätzungen und ihrem AICc. Das beste Modell umfasst 5 Parameter (CaCO3, CN.ratio, Grazing.intensity. Heat.index, Litter). Allerdings ist das nächstbeste Modell (mit 6 Parametern) nur wenig schlechter (delta AICc = 0.71), was sich in fast gleichen (und zudem sehr niedrigen) Akaike weights bemerkbar macht. Nach dem Verständnis des Information theoretician approach, sollte man in einer solchen Situation nicht das eine „beste“ Modell benennen, sondern eine Aussage über die Gruppe der insgesamt brauchbaren Modelle treffen. Hierzu kann man (a) Importance der Parameter über alle Modelle hinweg berechnen (= Summe der Akaike weights aller Modelle, die den betreffenden Parameter enthalten) und/oder (b) ein nach Akaike weights gemitteltes Modell berechnen. #Importance values der Variablen importance(allmodels) Demnach ist Heat.index die wichtigste Variable (in 100% aller relevanten Modelle), während ferner Litter, CaCO3, CN.ratio und Grazing.intensity in mehr als 50% der relevanten Modelle enthalten sind. #Modelaveraging (Achtung: dauert mit 13 Variablen einige Minuten) summary(model.avg(get.models(dredge(global.model,rank=&quot;AICc&quot;),subset =TRUE))) Aus dem gemittelten Modell können wir die Richtung der Beziehung (positiv oder negativ) und ggf. die Effektgrössen (wie verändert sich die Artenzahl, wenn die Prädiktorvariable um eine Einheit zunimmt?) ermitteln. #Modelldiagnostik nicht vergessen par(mfrow=c(2,2)) plot(global.model) plot(lm(Species.richness~Heat.index+Litter+CaCO3+CN.ratio+Grazing.intensity)) Wie immer kommt am Ende die Modelldiagnostik. Wir können uns entweder das globale Modell oder das Modell mit den 5 Variablen mit importance &gt; 50% anschauen. Das Bild sieht fast identisch aus und zeigt keinerlei problematische Abweichungen, d. h. links oben weder ein Keil, noch eine Banane, rechts oben eine nahezu perfekte Gerade. Darstellung der Vorgehensweise und Ergebnisse (in einer wiss. Arbeit) Methoden Es wurden Pflanzenartenzahlen von Steppenrasen in der Ukraine auf 199 10 m² grossen Probeflächen erhoben und zu diesen 23 Umweltvariablen erhoben. Da jeweils ein Messwert fehlte, wurden die Bodenvariablen Sand-, Schluff- und Tongehalt von der weiteren statistischen Analyse ausgeschlossen. Mittels Pearson’s Korrelationskoeffizient wurde auf Abhängigkeiten zwischen den Prädiktorvariablen getestet und aus Paaren mit einer Beziehung mit |r| &gt; 0.6 nur jeweils eine Variable beibehalten. Entsprechend wurden die hoch mit Jahresmitteltemperatur korrelierten Werte Meereshöhe (negativ), Temperaturamplitude (positiv) und Niederschlag (negativ) ausgeschlossen, ferner Leitfähigkeit (positiv mit pH) und Stickstoffgehalt (positiv mit organischem Kohlenstoffgehalt). Damit umfasste das globale lineare Modell (Funktion lm in R) die in Tab. 1 aufgeführten Variablen. Quadratische Terme oder Interaktionen zwischen Variablen wurden nicht berücksichtigt. Auf ein glm mit Poisson-Regression wurde verzichtet, da eine visuelle Inspektion eines Boxplots der Artenzahlen keine relevanten Abweichungen von einer Normalverteilung ergab. Die Modellauswahl fand mittels Multimodel Inference (dredge-Funktion im MuMIn package in R) statt. Die Modellgüte wurde mittels AICc beurteilt. Zur Beurteilung der Bedeutung von Variablen wurden Importance Values (Summe der Akaike weights in allen Modellen, die die betreffende Variable beinhalten) ausgerechnet. Die Richtung der Beziehung wurde aus der Parameterschätzung im globalen Modell bestimmt. Schliesslich wurde ein gemitteltes Modell (aller möglichen Modelle, gewichtet nach deren Akaike weights) erstellt, um die Effektgrössen zu bestimmen. Die Adäquanz des gewählten Modells wurde in Residualplots visuell inspiziert und ergab keine nennenswerten Verletzungen der Voraussetzungen eines parametrischen Modells. Ergebnisse Das beste Modell nach AICc beinhaltete die Variablen CaCO3, CN-Verhältnis, Beweidungsintensität, Heat load-Index und Streuauflage. Da sich die nächstbesten Modelle aber um weniger als DeltaAICc = 2 unterscheiden, wurden für die Gesamtbeurteilung die Importance values sowie die Koeffizienten des über alle 8192 betrachteten Modelle nach Akaike weights gemittelten average models herangezogen (Tab. 1). Mit einem Importance value von nahezu 100% war der Heat load-Index die wichtigste Einflussgrösse (negativ). Vier weitere Umweltvariablen waren ebenfalls in mehr als 50% der statistisch relevanten Modelle enthalten (in dieser Reihenfolge): Tab. 1. Ergebnisse der Multimodel Inference. Die Variablen sind nach absteigenden Importance values sortiert. Die Parameterschätzung bezieht sich nur auf die Modelle, welche die entsprechende Variable beinhalten und ist nach Akaike weights gewichtet. Unter „Hochkorrelierte Variablen“ sind jene aufgeführt, die wegen ihres engen Zusammenhangs mit der genannten Variablen nicht ins globale Modell aufgenommen wurden. "],
["9-statistik-4-05-11-2019.html", "Kapitel 9 Statistik 4 (05.11.2019)", " Kapitel 9 Statistik 4 (05.11.2019) Heute geht es hauptsächlich um generalized linear models (GLMs), die einige wesentliche Limitierungen von linearen Modellen überwinden. Indem sie Fehler- und Varianzstrukturen explizit modellieren, ist man nicht mehr an Normalverteilung der Residuen und Varianzhomogenität gebunden. Bei generalized linear regressions muss man sich zwischen verschiedenen Verteilungen und link-Strukturen entscheiden. Spezifisch werden wir uns die Poisson-Regressionen für Zähldaten und die logistische Regression für ja/nein-Daten anschauen. Danach folgt ein Einstieg in nicht-lineare Regressionen, die es erlauben, etwa Potenzgesetze oder Sättigungsfunktionen direkt zu modellieren. Zum Abschluss gibt es einen Ausblick auf Glättungsverfahren (LOWESS) und general additive models (GAMs). "],
["9-1-statistik-4-demoskript.html", "9.1 Statistik 4 - Demoskript", " 9.1 Statistik 4 - Demoskript (c) Juergen Dengler, 05.11.2019 Demoscript als Download von LMs zu GLMs temp&lt;-c(10,12,16,20,24,25,30,33,37) besucher&lt;-c(40,12,50,500,400,900,1500,900,2000) strand&lt;-data.frame(&quot;Temperatur&quot;=temp,&quot;Besucher&quot;=besucher) attach(strand) par(mfrow=c(1,1)) plot(besucher~temp) lm.strand&lt;-lm(Besucher~Temperatur, data=strand) summary(lm.strand) par(mfrow=c(2,2)) plot(lm.strand) par(mfrow=c(1,1)) xv&lt;-rep(0:40,by=.1) yv&lt;-predict(lm.strand,list(Temperatur=xv)) plot(Temperatur,Besucher,xlim=c(0,40)) lines(xv,yv,lwd=3,col=&quot;blue&quot;) glm.gaussian&lt;-glm(Besucher~Temperatur,family=gaussian) glm.poisson&lt;-glm(Besucher~Temperatur,family=poisson) summary(glm.gaussian) summary(glm.poisson) #Rücktranformation der Werte auf die orginale Skale (Hier Exponentialfunktion da family=possion als Link-Funktion den natürlichen Logarithmus (log) verwendet) #Besucher = exp(3.50 + 0.11 Temperatur/°C) exp(3.500301) #Anzahl besucher bei 0°C exp(3.500301 + 30*0.112817) #Anzahl besucher bei 30°C # Test für Overdispersion library(AER) dispersiontest(glm.poisson) glm.quasi&lt;-glm(Besucher~Temperatur,family=quasipoisson) summary(glm.quasi) par(mfrow=c(2,2)) plot(glm.gaussian) plot(glm.poisson) plot(glm.quasi) par(mfrow=c(1,1)) plot(Temperatur,Besucher,xlim=c(0,40)) xv&lt;-rep(0:40,by=.1) yv&lt;-predict(lm.strand,list(Temperatur=xv)) lines(xv,yv,lwd=3,col=&quot;blue&quot;) yv2&lt;-predict(glm.poisson,list(Temperatur=xv)) lines(xv,exp(yv2),lwd=3,col=&quot;red&quot;) yv3&lt;-predict(glm.quasi,list(Temperatur=xv)) lines(xv,exp(yv3),lwd=3,col=&quot;green&quot;) Logistische Regression bathing&lt;-data.frame(&quot;temperature&quot;=c(1,2,5,9,14,14,15,19,22,24,25,26,27,28,29), &quot;bathing&quot;=c(0,0,0,0,0,1,0,0,1,0,1,1,1,1,1)) plot(bathing~temperature, data=bathing) glm.1&lt;-glm(bathing~temperature, data=bathing, family=&quot;binomial&quot;) summary(glm.1) #Modeldiagnostik (wenn nicht signifikant, dann OK) 1 - pchisq (glm.1$deviance,glm.1$df.resid) #Modellgüte (pseudo-R²) 1 - (glm.1$dev / glm.1$null) #Steilheit der Beziehung (relative Änderung der odds bei x + 1 vs. x) exp(glm.1$coefficients[2]) #LD50 (also hier: Temperatur, bei der 50% der Touristen baden) -glm.1$coefficients[1]/glm.1$coefficients[2] #Vorhersagen predicted &lt;- predict(glm.1, type=&quot;response&quot;) #Konfusionsmatrix km &lt;- table(bathing$bathing, predicted &gt; 0.5) km #Missklassifizierungsrate 1-sum(diag(km)/sum(km)) #Plotting xs&lt;-seq(0,30,l=1000) model.predict&lt;-predict(glm.1,type=&quot;response&quot;,se=T,newdata=data.frame(temperature=xs)) plot(bathing~temperature,data=bathing,xlab=&quot;Temperature (°C)&quot;,ylab=&quot;% Bathing&quot;,pch=16, col=&quot;red&quot;) points(model.predict$fit ~ xs,type=&quot;l&quot;) lines(model.predict$fit+model.predict$se.fit ~ xs, type=&quot;l&quot;,lty=2) lines(model.predict$fit-model.predict$se.fit ~ xs, type=&quot;l&quot;,lty=2) Nicht-lineare Regression if(!require(AICcmodavg)){install.packages(&quot;AICcmodavg&quot;)} if(!require(nlstools)){install.packages(&quot;nlstools&quot;)} library(AICcmodavg) library(nlstools) loyn &lt;- read.delim(&quot;16_Statistik4/data/loyn.csv&quot;, sep=&quot;,&quot;) attach(loyn) #Selbstdefinierte Funktion, hier Potenzfunktion power.model&lt;-nls(ABUND~c*AREA^z,start=(list(c=1,z=0))) summary(power.model) AICc(power.model) #Modeldiagnostik (in nlstools) plot(nlsResiduals(power.model)) #Vordefinierte &quot;Selbststartfunktionen&quot;# ?selfStart logistic.model&lt;-nls(ABUND~SSlogis(AREA,Asym,xmid,scal)) summary(logistic.model) AICc(logistic.model) #Modeldiagnostik (in nlstools) plot(nlsResiduals(logistic.model)) #Visualisierung plot(ABUND~AREA) par(mfrow=c(1,1)) xv&lt;-seq(0,2000,0.01) # 1. Potenzfunktion yv1 &lt;-predict(power.model,list(AREA=xv)) lines(xv,yv1,col=&quot;green&quot;) # 2. Logistische Funktion yv2 &lt;-predict(logistic.model,list(AREA=xv)) lines(xv,yv2,col=&quot;blue&quot;) #Visualisierung II plot(ABUND~log10(AREA)) par(mfrow=c(1,1)) # 1. Potenzfunktion yv1 &lt;-predict(power.model,list(AREA=xv)) lines(log10(xv),yv1,col=&quot;green&quot;) # 2. Logistische Funktion yv2 &lt;-predict(logistic.model,list(AREA=xv)) lines(log10(xv),yv2,col=&quot;blue&quot;) #Model selection among several non-linear models cand.models&lt;-list() cand.models[[1]]&lt;-power.model cand.models[[2]]&lt;-logistic.model Modnames &lt;- c(&quot;Power&quot;, &quot;Logistic&quot;) aictab(cand.set = cand.models, modnames = Modnames) Smoother attach(loyn) log_AREA&lt;-log10(AREA) plot(ABUND~log_AREA) lines(lowess(log_AREA,ABUND,f=0.25),lwd=2,col=&quot;red&quot;) lines(lowess(log_AREA,ABUND,f=0.5),lwd=2,col=&quot;blue&quot;) lines(lowess(log_AREA,ABUND,f=1),lwd=2,col=&quot;green&quot;) GAMs if(!require(mgcv)){install.packages(&quot;mgcv&quot;)} library(mgcv) gam.1&lt;-gam(ABUND~s(log_AREA)) gam.1 summary(gam.1) plot(log_AREA,ABUND,pch=16) xv&lt;-seq(-1,4,by=0.1) yv&lt;-predict(gam.1,list(log_AREA=xv)) lines(xv,yv,lwd=2,col=&quot;red&quot;) AICc(gam.1) summary(gam.1) "],
["9-2-statistik-4-ubungen.html", "9.2 Statistik 4: Übungen", " 9.2 Statistik 4: Übungen 9.2.1 Übung 4.1: Nicht-lineare Regression (naturwissenschaftlich) Datensatz Curonian_Spit.xlsx Dieser enthält gemittelte Pflanzenartenzahlen (Species.richness) von geschachtelten Plots (Vegetationsaufnahmen) der Pflanzengesellschaft LolioCynosuretum im Nationalpark Kurische Nehrung (Russland) auf Flächengrössen (Area) von 0.0001 bis 900 m². Ermittelt den funktionellen Zusammenhang (das beste Modell), der die Zunahme der Artenzahl mit der Flächengrösse am besten beschreibt.Berücksichtigt dabei mindestens die Potenzfunktion (power function, die logarithmische Funktion (logarithmic function,und eine Funktion mit Sättigung (saturation, asymptote) eurer Wahl. 9.2.2 Übung 4.2N: Multiple logistische Regression (naturwissenschaftlich) Datensatz isolation.csv Dieser enthält für 50 Inseln die Information, ob eine bestimmte Vogelart dort vorkommt (incidence = 1) oder nicht vorkommt (incidence = 0). Für jede der Inseln sind zudem zwei Umweltvariablen angegeben: area (Fläche in km²) und isolation (Entfernung vom Festland in km). Ermittelt das minimal adäquate statistische Modell, das die Vorkommenswahrscheinlichkeit der Vogelart in Abhängigkeit von Flächengrösse und Entfernung beschreibt. 9.2.3 Übung 4.2S: Multiple logistische Regression (sozioökonomisch) Führt mit dem Datensatz novanimal.csv eine logistische Regression durch.Kann der Fleischkonsum durch das Geschlecht, die Hochschulzugehörigkeit und das Alter erklärt werden? Hinweise: Generiert eine neue Variable “Fleisch” (0 = kein Fleisch, 1 = Fleisch) Entfernt fehlende Werte aus der Variable “Fleisch” Lasst für die Analyse den Menü-Inhalt «Buffet» weg Definiert das Modell und wendet es auf den Datensatz an Berechnet eine Vorhersage des Modells mit predict() Eruiert den Modellfit und die Modellgenauigkeit Für Motivierte: Berechnet eine Konfusionsmatrix und zieht euer Fazit daraus (vgl. ) Stellt eure Ergebnisse dann angemessen dar (Text und/oder Tabelle). knitr::opts_chunk$set(fig.width = 20, fig.height = 12, warning = F, message = F, fig.pos = &#39;H&#39;,results = &quot;show&quot;) "],
["9-3-musterlosung-aufgabe-4-1-nicht-lineare-regression.html", "9.3 Musterlösung Aufgabe 4.1: Nicht-lineare Regression", " 9.3 Musterlösung Aufgabe 4.1: Nicht-lineare Regression R-Skript als Download Übungsaufgabe (hier so ausführlich formuliert, wie dies auch in der Klausur der Fall sein wird) Laden Sie den Datensatz Curonia_spit.xlsx. Dieser enthält gemittelte Pflanzenartenzahlen (Species.richness) von geschachtelten Plots (Vegetationsaufnahmen) der Pflanzengesellschaft Lolio-Cynosuretum im Nationalpark Kurische Nehrung (Russland) auf Flächengrössen (Area) von 0.0001 bis 900 m². Ermitteln Sie den funktionellen Zusammenhang, der die Zunahme der Artenzahl mit der Flächengrösse am besten beschreibt. Berücksichtigen Sie dabei mindestens die Potenzfunktion (power function), die logarithmische Funktion (logarithmic function) und eine Funktion mit Sättigung (saturation, asymptote) Ihrer Wahl Bitte erklären und begründen Sie die einzelnen Schritte, die Sie unternehmen, um zu diesem Ergebnis zu kommen. Dazu erstellen Sie bitte ein Word-Dokument, in das Sie Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, Ihre Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere Vorgehen dokumentieren. Dieser Ablauf sollte insbesondere beinhalten: Überprüfen der Datenstruktur nach dem Einlesen, welches sind die abhängige(n) und welches die unabängige(n) Variablen Explorative Datenanalyse, um zu sehen, ob eine nicht-lineare Regression überhaupt nötig ist und ob evtl. Dateneingabefehler vorliegen vorgenommen werden sollten Definition von mindestens drei nicht-linearen Regressionsmodellen Selektion des/der besten Models/Modelle Durchführen der Modelldiagnostik für die Modelle in der engeren Auswahl, um zu entscheiden, ob das gewählte Vorgehen korrekt war oder ggf. angepasst werden muss Generieren aller Zahlen, Statistiken und Tabellen, die für eine wiss. Ergebnisdarstellung benötigt werden Formulieren Sie abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden. Abzugeben sind am Ende (a) Ein lauffähiges R-Skript; (b) begründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation) und (c) ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit). 9.3.1 Übung 4.1 - Nicht-lineare Regression – Lösung Aus der Excel-Tabelle wurde das relevante Arbeitsblatt als csv gespeichert SAR&lt;-read.delim(&quot;16_Statistik4/data/Curonian_Spit.csv&quot;,sep=&quot;;&quot;) str(SAR) ## &#39;data.frame&#39;: 16 obs. of 2 variables: ## $ Area : num 0.0001 0.0025 0.01 0.0625 0.25 1 4 9 16 25 ... ## $ Species.richness: num 2.1 9.1 14.3 23.1 30.1 37.4 48.5 54.5 58 59.9 ... summary(SAR) ## Area Species.richness ## Min. : 0.0001 Min. : 2.10 ## 1st Qu.: 0.2031 1st Qu.:28.35 ## Median : 12.5000 Median :56.25 ## Mean :147.1453 Mean :50.09 ## 3rd Qu.:131.2500 3rd Qu.:69.95 ## Max. :900.0000 Max. :92.40 attach(SAR) #Explorative Datenanalyse plot(Species.richness~Area) Es liegt in der Tat ein nicht-linearer Zusammenhang vor, der sich gut mit nls analysieren lässt. Die Daten beinhalten keine erkennbaren Fehler, da der Artenreichtum der geschachtelten Plots mit der Fläche ansteigt. #Potenzfunktion selbst definiert if(!require(nlstools)){install.packages(&quot;nlstools&quot;)} library(nlstools) #power.model&lt;-nls(Species.richness~c*Area^z) #summary(power.model) Falls die Funktion so keine Ergebnisse liefert, oder das Ergebnis unsinnig aussieht, wenn man es später plottet, müsste man hier geeignete Startwerte angeben, die man aus der Betrachtung der Daten oder aus Erfahrungen mit der Funktion für ähnliche Datensets gewinnt,etwa so: power.model&lt;-nls(Species.richness~c*Area^z, start=(list(c=1,z=0.2))) summary(power.model) ## ## Formula: Species.richness ~ c * Area^z ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## c 36.168960 1.408966 25.67 3.56e-13 *** ## z 0.138941 0.007472 18.60 2.88e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.142 on 14 degrees of freedom ## ## Number of iterations to convergence: 9 ## Achieved convergence tolerance: 8.138e-06 Das Ergebnis ist identisch #logarithmische Funktion selbst definiert logarithmic.model&lt;-nls(Species.richness~b0+b1*log10(Area)) summary(logarithmic.model) ## ## Formula: Species.richness ~ b0 + b1 * log10(Area) ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## b0 43.333 1.358 31.91 1.78e-14 *** ## b1 13.281 0.654 20.31 8.75e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.265 on 14 degrees of freedom ## ## Number of iterations to convergence: 1 ## Achieved convergence tolerance: 3.56e-09 # Zu den verschiedenen Funktionen mit Sättigungswert (Asymptote) gehören Michaelis-Menten, das aymptotische Modell durch den Ursprung und die logistische # Funktion. Die meisten gibt es in R # als selbststartende Funktionen, was meist besser funktioniert als # wenn man sich selbst Gedanken # über Startwerte usw. machen muss. Man kann sie aber auch selbst definieren Im Folgenden habe ich ein paar unterschiedliche Sättigungsfunktionen mit verschiedenen Einstellungen durchprobiert, um zu zeigen, was alles passieren kann… micmen.1&lt;-nls(Species.richness~SSmicmen(Area, Vm, K)) summary(micmen.1) ## ## Formula: Species.richness ~ SSmicmen(Area, Vm, K) ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## Vm 72.0108 4.2708 16.861 1.07e-10 *** ## K 0.8477 0.4371 1.939 0.0729 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 11.96 on 14 degrees of freedom ## ## Number of iterations to convergence: 0 ## Achieved convergence tolerance: 3.377e-06 #Dasselbe selbst definiert (mit default-Startwerten) micmen.2&lt;-nls(Species.richness~Vm*Area/(K+Area)) summary(micmen.2) ## ## Formula: Species.richness ~ Vm * Area/(K + Area) ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## Vm 46.7020 9.6748 4.827 0.000268 *** ## K -2.1532 0.5852 -3.679 0.002477 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 35.14 on 14 degrees of freedom ## ## Number of iterations to convergence: 23 ## Achieved convergence tolerance: 9.113e-06 Hier ist das Ergebnis deutlich verschieden, ein Phänomen, das einem bei nicht-linearen Regressionen anders als bei linearen Regressionen immer wieder begegnen kann, da der Iterationsalgorithmus in lokalen Optima hängen bleiben kann. Oftmals dürfte die eingebaute Selbststartfunktion bessere Ergebnisse liefern, aber das werden wir unten sehen. #Dasselbe selbst definiert (mit sinnvollen Startwerten, basierend auf dem Plot) micmen.model3&lt;- nls(Species.richness~Vm*Area/(K+Area),start=list(Vm=100,K=1)) summary(micmen.model3) ## ## Formula: Species.richness ~ Vm * Area/(K + Area) ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## Vm 72.0111 4.2708 16.861 1.07e-10 *** ## K 0.8477 0.4371 1.939 0.0729 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 11.96 on 14 degrees of freedom ## ## Number of iterations to convergence: 22 ## Achieved convergence tolerance: 7.026e-06 Wenn man sinnvollere Startwerte als die default-Werte (1 für alle Parameter) eingibt, hier etwas einen mutmasslichen Asymptoten-Wert (aus der Grafik) von Vm = ca. 100, dann bekommt man das gleiche Ergebnis wie bei der Selbsstartfunktion #Eine asymptotische Funktion durch den Ursprung (mit implementierter Selbststartfunktion) asym.model&lt;-nls(Species.richness~SSasympOrig(Area, Asym, lrc)) summary(asym.model) ## ## Formula: Species.richness ~ SSasympOrig(Area, Asym, lrc) ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## Asym 68.5066 4.4278 15.472 3.38e-10 *** ## lrc 0.1184 0.4864 0.244 0.811 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 13.88 on 14 degrees of freedom ## ## Number of iterations to convergence: 0 ## Achieved convergence tolerance: 2.808e-06 #Logistische Regression als Selbststart-Funktion #logistic.model&lt;-nls(Species.richness~SSlogis(Area,asym,xmid,scal)) Error in nls(y ~ 1/(1 + exp((xmid - x)/scal)), data = xy, start = list(xmid= aux[1L], : Iterationenzahl überschritt Maximum 50 Das ist etwas, was einem bei nls immer wieder passieren kann. Die Iteration ist nach der eingestellten max. Iterationszahl noch nicht zu einem Ergebnis konvergiert. Um ein Ergebnis für diese Funktion zu bekommen, müsste man mit den Einstellungen von nls „herumspielen“, etwas bei den Startwerten oder den max. Um das effizient zu machen, braucht man aber etwas Erfahrung Interationszahlen (man kann z. B. manuell die Maximalzahl der Iterationen erhöhen, indem man in den Funktionsaufruf etwa maxiter =100 als zusätzliches Argument reinschreibtn). Da wir aber schon mehrere funktionierende Funktionen mit oberem Grenzwert haben –und damit die Aufgabe erfüllt – lassen wir es hier. #Vergleich der Modellgüte mittels AICc library(AICcmodavg) cand.models&lt;-list() cand.models[[1]]&lt;-power.model cand.models[[2]]&lt;-logarithmic.model cand.models[[3]]&lt;-micmen.1 cand.models[[4]]&lt;-micmen.2 cand.models[[5]]&lt;-asym.model Modnames&lt;-c(&quot;Power&quot;,&quot;Logarithmic&quot;,&quot;Michaelis-Menten (SS)&quot;,&quot;Michaelis-Menten&quot;,&quot;Asymptotic through origin&quot;) aictab(cand.set=cand.models,modnames=Modnames) ## ## Model selection based on AICc: ## ## K AICc Delta_AICc AICcWt Cum.Wt LL ## Power 3 96.75 0.00 0.98 0.98 -44.38 ## Logarithmic 3 104.43 7.68 0.02 1.00 -48.21 ## Michaelis-Menten (SS) 3 130.67 33.92 0.00 1.00 -61.34 ## Asymptotic through origin 3 135.44 38.69 0.00 1.00 -63.72 ## Michaelis-Menten 3 165.17 68.42 0.00 1.00 -78.58 Diese Ergebnistabelle vergleicht die Modellgüte zwischen den fünf Modellen, die wir in unsere Auswahl reingesteckt haben. Alle haben drei geschätzte Parameter (K), also zwei Funktionsparameter und die Varianz. Das beste Modell (niedrigster AICc bzw. Delta = 0) hat das Potenzmodell (power). Das zweitbeste Modell (logarithmic) hat bereits einen Delta-AICc von mehr als 4, ist daher statistisch nicht relevant. Das zeigt sich auch am Akaike weight, das für das zweite Modell nur noch 2 % ist. Die verschiedenen Modelle mit oberem Grenzwert (3-5) sind komplett ungeeignet. #Modelldiagnostik für das beste Modell library(nlstools) plot(nlsResiduals(power.model)) Links oben sieht man zwar ein Muster (liegt daran, dass in diesem Fall die Plots geschachtelt, und nicht unabhängig waren), aber jedenfalls keinen problematischen Fall wie einen Bogen oder einen Keil. Der QQ-Plot rechts unten ist völlig OK. Somit haben wir auch keine problematische Abweichung von der Normalverteilung der Residuen. Da es sich bei den einzelnen Punkten allerdings bereits um arithmetische Mittelwerte aus je 8 Beobachtungen handelt, hätte man sich auch einfach auf das Central Limit Theorem beziehen können, das sagt, dass Mittelwerte automatisch einer Normalverteilung folgen. #Ergebnisplot plot(Area,Species.richness,pch=16,xlab=&quot;Flaeche [m2]&quot;,ylab=&quot;Artenreichtum&quot;) xv&lt;-seq(0,1000,by=0.1) yv&lt;-predict(power.model,list(Area=xv)) lines(xv,yv,lwd=2,col=&quot;red&quot;) #Das ist der Ergebnisplot für das beste Modell. Wichtig ist, dass man die Achsen korrekt beschriftet und nicht einfach die mehr oder weniger kryptischen Spaltennamen aus R nimmt. #Im Weiteren habe ich noch eine Sättigungsfunktion (Michaelis-Menten mit Selbststarter) zum Vergleich hinzugeplottet yv2&lt;-predict(micmen.1,list(Area=xv)) lines(xv,yv2,lwd=2,col=&quot;blue&quot;) Man erkennt, dass die Sättigungsfunktion offensichtlich den tatsächlichen Kurvenverlauf sehr schlecht widergibt. Im mittleren Kurvenbereich sind die Schätzwerte zu hoch, für grosse Flächen dann aber systematisch viel zu niedrig. Man kann die Darstellung im doppeltlogarithmischen Raum wiederholen, um die Kurvenanpassung im linken Bereich besser differenzieren zu können: #Ergebnisplot Double-log plot(log10(Area),log10(Species.richness),pch=16,xlab=&quot;logA&quot;,ylab=&quot;log (S)&quot;) xv&lt;-seq(0,1000,by=0.0001) yv&lt;-predict(power.model,list(Area=xv)) lines(log10(xv),log10(yv),lwd=2,col=&quot;red&quot;) yv2&lt;-predict(micmen.1,list(Area=xv)) lines(log10(xv),log10(yv2),lwd=2,col=&quot;blue&quot;) Auch hier sieht man, dass die rote Kurve zwar nicht perfekt, aber doch viel besser als die blaue Kurve ist. "],
["9-4-musterlosung-aufgabe-4-2n-multiple-logistische-regression.html", "9.4 Musterlösung Aufgabe 4.2N: Multiple logistische Regression", " 9.4 Musterlösung Aufgabe 4.2N: Multiple logistische Regression R-Skript als Download Übungsaufgabe (hier so ausführlich formuliert, wie dies auch in der Klausur der Fall sein wird) Laden Sie den Datensatz isolation.csv. Dieser enthält für 50 Inseln die Information, ob eine bestimmte Vogelart dort vorkommt (incidence = 1) oder nicht vorkommt (incidence = 0). Für jede der Inseln sind zudem zwei Umweltvariablen angegeben: area (Fläche in km²) und isolation (Entfernung vom Festland in km). Ermitteln Sie das minimal adäquate statistische Modell, das die Vorkommenswahrscheinlichkeit der Vogelart in Abhängigkeit von Flächengrösse und Entfernung beschreibt. Bitte erklären und begründen Sie die einzelnen Schritte, die Sie unternehmen, um zu diesem Ergebnis zu kommen. Dazu erstellen Sie bitte ein Word-Dokument, in das Sie Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, Ihre Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere Vorgehen dokumentieren. Dieser Ablauf sollte insbesondere beinhalten: Überprüfen der Datenstruktur nach dem Einlesen, welches sind die abhängige(n) und welches die unabängige(n) Variablen Explorative Datenanalyse, um zu sehen, ob evtl. Dateneingabefehler vorliegen oder Datentransformationen vorgenommen werden sollten Auswahl und Begründung eines statistischen Verfahrens Bestimmung des vollständigen/maximalen Models Selektion des/der besten Models/Modelle Durchführen der Modelldiagnostik für d Generieren aller Zahlen, Statistiken und Tabellen, die für eine wiss. Ergebnisdarstellung benötigt werden Formulieren Sie abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden. Abzugeben sind am Ende (a) Ein lauffähiges R-Skript; (b) begründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation) und (c) ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit). 9.4.1 Übung 4.2 - Multiple logistische Regression** islands &lt;-read.csv(&quot;16_Statistik4/data/isolation.csv&quot;) islands ## incidence area isolation ## 1 1 7.928 3.317 ## 2 0 1.925 7.554 ## 3 1 2.045 5.883 ## 4 0 4.781 5.932 ## 5 0 1.536 5.308 ## 6 1 7.369 4.934 ## 7 1 8.599 2.882 ## 8 0 2.422 8.771 ## 9 1 6.403 6.092 ## 10 1 7.199 6.977 ## 11 0 2.654 7.748 ## 12 1 4.128 4.297 ## 13 0 4.175 8.516 ## 14 1 7.098 3.318 ## 15 0 2.392 9.292 ## 16 1 8.690 5.923 ## 17 1 4.667 4.997 ## 18 1 2.307 2.434 ## 19 1 1.053 6.842 ## 20 0 0.657 6.452 ## 21 1 5.632 2.506 ## 22 1 5.895 6.613 ## 23 0 4.855 9.577 ## 24 1 8.241 6.330 ## 25 1 2.898 6.649 ## 26 1 4.445 5.032 ## 27 1 6.027 2.023 ## 28 0 1.574 5.737 ## 29 0 2.700 5.762 ## 30 0 3.106 6.924 ## 31 0 4.330 7.262 ## 32 1 7.799 3.219 ## 33 0 4.630 9.229 ## 34 1 6.951 5.841 ## 35 0 0.859 9.294 ## 36 1 3.657 2.759 ## 37 0 2.696 8.342 ## 38 0 4.171 8.805 ## 39 1 8.063 2.274 ## 40 0 0.153 5.650 ## 41 1 1.948 5.447 ## 42 1 2.006 5.480 ## 43 1 6.508 4.007 ## 44 1 1.501 5.400 ## 45 1 9.269 5.272 ## 46 1 4.170 4.786 ## 47 0 3.376 5.219 ## 48 0 2.228 7.990 ## 49 0 1.801 8.466 ## 50 1 6.441 3.451 str(islands) ## &#39;data.frame&#39;: 50 obs. of 3 variables: ## $ incidence: int 1 0 1 0 0 1 1 0 1 1 ... ## $ area : num 7.93 1.93 2.04 4.78 1.54 ... ## $ isolation: num 3.32 7.55 5.88 5.93 5.31 ... summary(islands) ## incidence area isolation ## Min. :0.00 Min. :0.153 Min. :2.023 ## 1st Qu.:0.00 1st Qu.:2.248 1st Qu.:4.823 ## Median :1.00 Median :4.170 Median :5.801 ## Mean :0.58 Mean :4.319 Mean :5.856 ## 3rd Qu.:1.00 3rd Qu.:6.431 3rd Qu.:7.191 ## Max. :1.00 Max. :9.269 Max. :9.577 Man erkennt, dass islands 50 Beobachtungen von drei Parametern enthält, wobei incidence nur 0 oder 1 enthält, während area und isolation metrisch sind. attach(islands) #Explorative Datenanalyse boxplot(area) boxplot(isolation) Die Boxplots der beiden unabhängigen Variablen sind sehr symmetrisch, deshalb besteht kein Anlass über eine evtl. log-Transformation nachzudenken (Tatsächlich sehen die Boxplots so aus, als seien Fläche und Entfernung bereits log-transformiert, obgleich das Buch von Crawley (2015), aus dem dieses Beispiel stammt), behauptet, es seien die untransformierten Originaldaten). Achtung: die Voraussetzung der ungefähren Normalverteilung gilt nur für die abhängige Variable, oder noch genauer für deren Residuen nach Berechnung des Models. Trotzdem kann u.U. eine Transformation von sehr schief verteilten unabhängigen Variablen sinnvoll sein, um die Linearität der Beziehung zu verbessern. #Definition der unterschiedlichen Modelle model.mult &lt;- glm(incidence~area*isolation,binomial) model.add &lt;- glm(incidence~area+isolation,binomial) model.area &lt;- glm(incidence~area,binomial) model.isolation &lt;- glm(incidence~isolation,binomial) Es werden alle vier möglichen Modelle definiert: Mit area, isolation und area:isolationInteraktion (mult), mit area und isolation (add), nur mit area und nur mit isolation. #Modellergebnisse summary(model.mult) ## ## Call: ## glm(formula = incidence ~ area * isolation, family = binomial) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.84481 -0.33295 0.02027 0.34581 2.01591 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 4.0313 7.1747 0.562 0.574 ## area 1.3807 2.1373 0.646 0.518 ## isolation -0.9422 1.1689 -0.806 0.420 ## area:isolation -0.1291 0.3389 -0.381 0.703 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 68.029 on 49 degrees of freedom ## Residual deviance: 28.252 on 46 degrees of freedom ## AIC: 36.252 ## ## Number of Fisher Scoring iterations: 7 summary(model.add) ## ## Call: ## glm(formula = incidence ~ area + isolation, family = binomial) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.8189 -0.3089 0.0490 0.3635 2.1192 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 6.6417 2.9218 2.273 0.02302 * ## area 0.5807 0.2478 2.344 0.01909 * ## isolation -1.3719 0.4769 -2.877 0.00401 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 68.029 on 49 degrees of freedom ## Residual deviance: 28.402 on 47 degrees of freedom ## AIC: 34.402 ## ## Number of Fisher Scoring iterations: 6 summary(model.area) ## ## Call: ## glm(formula = incidence ~ area, family = binomial) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.5709 -0.9052 0.3183 0.6588 1.8424 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.1554 0.7545 -2.857 0.004278 ** ## area 0.6272 0.1861 3.370 0.000753 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 68.029 on 49 degrees of freedom ## Residual deviance: 50.172 on 48 degrees of freedom ## AIC: 54.172 ## ## Number of Fisher Scoring iterations: 5 summary(model.isolation) ## ## Call: ## glm(formula = incidence ~ isolation, family = binomial) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.8537 -0.3590 0.1168 0.6272 1.5476 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 8.5223 2.4511 3.477 0.000507 *** ## isolation -1.3416 0.3926 -3.417 0.000633 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 68.029 on 49 degrees of freedom ## Residual deviance: 36.640 on 48 degrees of freedom ## AIC: 40.64 ## ## Number of Fisher Scoring iterations: 6 Um aus diesen vier Modellen, das minimal adäquate Modell herauszufinden, gibt es zwei Möglichkeiten: (1) Man nimmt das volle Modell (mult), vereinfacht es schrittweise und vergleicht jeweils das komplexere mit dem weniger komplexen Modell mittels ANOVA. (2) Man vergleicht alle vier Modelle auf einmal mittel AICc. Man sollte sich für eine der beiden Möglichkeiten entscheiden und nicht beide verfolgen. Im Folgenden sind aber beide darstestellt. Version (1): Schrittweise Modellvereinfachung mittels ANOVA anova(model.mult,model.add,test=&quot;Chi&quot;) ## Analysis of Deviance Table ## ## Model 1: incidence ~ area * isolation ## Model 2: incidence ~ area + isolation ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 46 28.252 ## 2 47 28.402 -1 -0.15043 0.6981 Das komplexere Modell mit Interaktionsterm (mult) ist nicht signifikant besser als das Modell, in dem area und isolation additiv enthalten sind (add). Deshalb kann man den Interaktionsterm streichen. anova(model.add,model.area,test=&quot;Chi&quot;) ## Analysis of Deviance Table ## ## Model 1: incidence ~ area + isolation ## Model 2: incidence ~ area ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 47 28.402 ## 2 48 50.172 -1 -21.77 3.073e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(model.add,model.isolation,test=&quot;Chi&quot;) ## Analysis of Deviance Table ## ## Model 1: incidence ~ area + isolation ## Model 2: incidence ~ isolation ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 47 28.402 ## 2 48 36.640 -1 -8.2375 0.004103 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Das additive Modell ist aber signifikant besser als jedes der beiden Modelle mit nur einem Prediktor. Deshalb ist das minimal adäquate Modell incidence ~ area + isolation. Version (2): Modellselektion mittels AICc library(AICcmodavg) cand.models&lt;-list() cand.models[[1]]&lt;-model.mult cand.models[[2]]&lt;-model.add cand.models[[3]]&lt;-model.area cand.models[[4]]&lt;-model.isolation Modnames&lt;-c(&quot;Area * Isolation&quot;,&quot;Area + Isolation&quot;,&quot;Area&quot;,&quot;Isolation&quot;) aictab(cand.set=cand.models,modnames=Modnames) ## ## Model selection based on AICc: ## ## K AICc Delta_AICc AICcWt Cum.Wt LL ## Area +\\nIsolation 3 34.92 0.00 0.72 0.72 -14.20 ## Area * Isolation 4 37.14 2.22 0.24 0.96 -14.13 ## Isolation 2 40.90 5.97 0.04 1.00 -18.32 ## Area 2 54.43 19.50 0.00 1.00 -25.09 Auch mit Modellselektion mittels AICc kommt man zum gleichen Ergebnis. Das minimal adäquate Modell ist incidence ~ area + isolation. #Modelldiagnostik für das gewählte Modell (wenn nicht signifikant,dann OK) 1 - pchisq(model.add$deviance,model.add$df.resid) ## [1] 0.9854232 #Nicht signifikant, d. h. kein „lack of fit“. #Visuelle Inspektion der Linearität library(car) crPlots(model.add,ask=F) log odds ratio vs. Prediktorvariablen (area bzw. isolation) ist fast perfekt linear (d. h. grüne Linie liegt fast auf der roten Optimallinie). D. h. auch dieser Sicht ist das Modell korrekt spezifiziert. #Modellgüte (pseudo-R²) 1 - (model.add$dev / model.add$null) ## [1] 0.5825004 # 58% der Vorkommenswahrscheinlichkeit der Vogelart wird durch die beiden gewählten Prediktorvariablen erklärt. #Steilheit der Beziehung (relative Änderung der odds von x + 1 vs.x) #area (2. Koeffizient nach dem Achsenabschnitt) exp(model.add$coef[2]) ## area ## 1.787332 #&gt; 1, d. h. Vorkommenswahrscheinlichkeit steigt mit Flächengrösse. #area (3. Koeffizient nach dem Achsenabschnitt) exp(model.add$coef[3]) ## isolation ## 0.2536142 #&lt; 1, d. h. Vorkommenswahrscheinlichkeit sinkt mit zunehmender Isolation. #Ergebnisplots #Da es keine signifikante Interaktion gibt, kann man die separaten Darstellungen der beiden Einzelbeziehungen nehmen. xs&lt;-seq(0,10,l=1000) model.predict&lt;- predict(model.area,type=&quot;response&quot;,se=T,newdata=data.frame(area=xs)) plot(incidence~area,data=islands,xlab=&quot;Fläche (km²)&quot;,ylab=&quot;Vorkommenswahrscheinlichkeit&quot;,pch=16, col=&quot;red&quot;) points(model.predict$fit ~ xs,type=&quot;l&quot;) lines(model.predict$fit+model.predict$se.fit ~ xs, type=&quot;l&quot;,lty=2) lines(model.predict$fit-model.predict$se.fit ~ xs, type=&quot;l&quot;,lty=2) model.predict2&lt;- predict(model.isolation,type=&quot;response&quot;,se=T,newdata=data.frame(isolation=xs)) plot(incidence~isolation,data=islands,xlab=&quot;Entfernung vom Festland (km)&quot;,ylab=&quot;Vorkommenswahrscheinlichkeit&quot;,pch=16, col=&quot;blue&quot;) points(model.predict2$fit ~ xs,type=&quot;l&quot;) lines(model.predict2$fit+model.predict2$se.fit ~ xs, type=&quot;l&quot;,lty=2) lines(model.predict2$fit-model.predict2$se.fit ~ xs, type=&quot;l&quot;,lty=2) Darstellung der Vorgehensweise und Ergebnisse (in einer wiss. Arbeit) Methoden Es wurde das Vorkommen (ja/nein) ein Brutvogelart auf 50 Inseln in Abhängigkeit von deren Flächengrösse (in km²) und Entfernung vom Festland (in km) erhoben. Mittels multipler logistischer Regression wurden unterschiedliche Modelle für diese Abhängigkeit berechnet und daraus dann das minimal adäquate Modell mittels AICc ermittelt. Die Adäquanz des gewählten Modells wurde anschliessend mittels G²-Statistik und „component + residual plots“ überprüft und war nicht verletzt. Ergebnisse Das nach AICc beste Modell enthielt Fläche und Entfernung vom Festland als Parameter, nicht jedoch deren Interaktion. Es wies einen delta-AICc zum Modell mit dem Interaktionsterm von 2.22 sowie 5.97 zum Modell nur mit der Fläche und 19.50 zum Modell nur mit der Entfernung auf. Dabei hatte die Fläche (in km²) einen stark positiven, die Isolation (in km) dagegen einen sehr stark negativen Effekt (Abb. 1). Die Gesamtbeziehung Vorkommenswahrscheinlichkeit = 1 / (1 + exp (–(6.64 + 0.58 * Fläche – 1.37 * Entfernung))) erklärt dabei 58% der Variabilität de Daten. Abb. 1: Modellierte Abhängigkeit der Vorkommenswahrscheinlichkeit der Vogelart auf Inseln von den beiden additiv wirkenden Prädiktoren Fläche und Entfernung. Dargestellt ist die Regressionskurve +/- Standardfehler. "],
["9-5-musterlosung-aufgabe-4-2s-multiple-logistische-regression.html", "9.5 Musterlösung Aufgabe 4.2S: multiple logistische Regression", " 9.5 Musterlösung Aufgabe 4.2S: multiple logistische Regression Meine Empfehlung Kapitel 6 von Manny Gimond https://mgimond.github.io/Stats-in-R/Logistic.html kommentierter Lösungsweg # Genereiert eine Dummyvariable: Fleisch 1, kein Fleisch 0 df &lt;- nova # kopiert originaler Datensatz df$meat &lt;- ifelse(nova$label_content == &quot;Fleisch&quot;, 1, 0) df_ &lt;- df[df$label_content != &quot;Buffet&quot;, ] # entfernt Personen die sich ein Buffet Teller gekauft haben und speichert es in eine neuen Datensatz # Löscht alle Missings bei der Variable &quot;Fleisch&quot; df_ &lt;- df_[!is.na(df_$meat), ] # sieht euch die Verteilung zwischen Fleisch und kein Fleisch table(df_$meat) ## ## 0 1 ## 387 564 # definiert das logistische Modell und wende es auf den Datensatz an # modell nicht signifikant, rechnen mal trotzdem weiter mod0 &lt;- glm(meat ~ gender + member + age, data = df_, binomial(&quot;logit&quot;)) summary.lm(mod0) # Member und Alter scheinen keinen Einfluss zu nehmen, lassen wir also weg ## ## Call: ## glm(formula = meat ~ gender + member + age, family = binomial(&quot;logit&quot;), ## data = df_) ## ## Weighted Residuals: ## Min 1Q Median 3Q Max ## -1.6134 -1.0258 0.7174 0.7443 1.1998 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.407608 0.395553 1.030 0.303 ## genderM 0.733556 0.141743 5.175 2.78e-07 *** ## memberStudierende -0.218506 0.197620 -1.106 0.269 ## age -0.012299 0.009312 -1.321 0.187 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.002 on 947 degrees of freedom ## Multiple R-squared: 0.001772, Adjusted R-squared: -0.00139 ## F-statistic: 0.5603 on 3 and 947 DF, p-value: 0.6413 # neues Modell ohne Alter und Hochschulzugehörigkeit mod1 &lt;- update(mod0, ~. -member - age) summary.lm(mod1) ## ## Call: ## glm(formula = meat ~ gender, family = binomial(&quot;logit&quot;), data = df_) ## ## Weighted Residuals: ## Min 1Q Median 3Q Max ## -1.3687 -0.9506 0.7306 0.7306 1.0520 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.1014 0.1128 -0.899 0.369 ## genderM 0.7291 0.1403 5.198 2.47e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.001 on 949 degrees of freedom ## Multiple R-squared: 0.001681, Adjusted R-squared: 0.0006287 ## F-statistic: 1.598 on 1 and 949 DF, p-value: 0.2065 # Modeldiagnostik (wenn nicht signifikant, dann OK) 1 - pchisq(mod1$deviance, mod1$df.resid) # hochsignifikant, d.h. kein gutes Modell ## [1] 5.353906e-11 #Modellgüte (pseudo-R²) 1 - (mod1$dev / mod1$null) # sehr kleines pseudo-R² ## [1] 0.02121993 # Konfusionsmatrix vom Datensatz # Model Vorhersage # hier ein anderes Beispiel: predicted &lt;- predict(mod1, df_, type = &quot;response&quot;) # erzeugt eine Tabelle mit den beobachteten # Fleischesser/Nichtleischesser und den Vorhersagen des Modells km &lt;- table(df_$meat, predicted &gt; 0.5) dimnames(km) &lt;- list( c(&quot;Beobachtung kein Fleisch&quot;, &quot;Beobachtung Fleisch&quot;), c(&quot;Modell kein Fleisch&quot;, &quot;Modell Fleisch&quot;)) km ## Modell kein Fleisch Modell Fleisch ## Beobachtung kein Fleisch 166 221 ## Beobachtung Fleisch 150 414 # kalkuliert die Missklassifizierungsrate mf &lt;- 1-sum(diag(km)/sum(km)) # ist mit knapp 40% eher hoch mf ## [1] 0.3901157 "],
["10-statistik-5-11-11-2019.html", "Kapitel 10 Statistik 5 (11.11.2019)", " Kapitel 10 Statistik 5 (11.11.2019) In Statistik 5 lernen die Studierenden Lösungen kennen, welche die diversen Limitierungen von linearen Modellen überwinden. Während generalized linear models (GLMs) aus Statistik 4 bekannt sind, geht es jetzt um linear mixed effect models (LMMs und generalized linear mixed effect models (GLMMs). Dabei bezeichnet generalized die explizite Modellierung anderer Fehler- und Varianzstrukturen und mixed die Berücksichtigung von Abhängigkeiten bzw. Schachtelungen unter den Beobachtungen. Einfachere Fälle von LMMs, wie split-plot und repeated-measures ANOVAs, lassen sich noch mit dem aov-Befehl in Base R bewältigen, für komplexere Versuchsdesigns/Analysen gibt es spezielle R packages. Abschliessend gibt es eine kurze Einführung in GLMMs, die eine Analyse komplexerer Beobachtungsdaten z. B. mit räumlichen Abhängigkeiten, erlauben. "],
["10-1-statistik-5-demoskript.html", "10.1 Statistik 5 - Demoskript", " 10.1 Statistik 5 - Demoskript Von linearen Modellen zu GLMMs (c) Juergen Dengler Demoscript als Download Datensatz spf.csv Datensatz DeerEcervi.txt Split-plot ANOVA Based on Logan (2010), Chapter 14 spf&lt;-read.delim(&quot;17_Statistik5/data/spf.csv&quot;, sep = &quot;,&quot;) spf.aov&lt;-aov(Y~A*C+Error(B),spf) summary(spf.aov) attach(spf) interaction.plot(C,A,Y) #nun als LMM if(!require(nlme)){install.packages(&quot;nlme&quot;)} library(nlme) spf.lme.1&lt;-lme(Y~A*C,random = ~C | B, spf) spf.lme.2&lt;-lme(Y~A*C,random = ~1 | B, spf) anova(spf.lme.1) anova(spf.lme.2) summary(spf.lme.1) summary(spf.lme.2) GLMM Based on Zuur et al. (2009), chapter 13 DeerEcervi &lt;- read.table(&quot;17_Statistik5/data/DeerEcervi.txt&quot;, header=TRUE, dec=&quot;.&quot;) DeerEcervi$Ecervi.01 &lt;- DeerEcervi$Ecervi #Anzahl Larven hier in Presence/Absence uebersetzt DeerEcervi$Ecervi.01[DeerEcervi$Ecervi&gt;0]&lt;-1 DeerEcervi$fSex &lt;- factor(DeerEcervi$Sex) #Hirschlänge hier standardisiert, sonst würde der Achsenabschnitt im Modell für #einen Hirsch der Länge 0 modelliert, was schlecht interpretierbar ist, #jetzt ist der Achsenabschnitt für einen durschnittlich langen Hirsch DeerEcervi$CLength &lt;- DeerEcervi$Length - mean(DeerEcervi$Length) DeerEcervi$fFarm &lt;- factor(DeerEcervi$Farm) #Zunächst als GLM #Interaktionen mit fFarm nicht berücksichtigt, da zu viele Freiheitsgrade verbraucht würden DE.glm&lt;-glm(Ecervi.01 ~ CLength * fSex+fFarm, data = DeerEcervi, family = binomial) drop1(DE.glm, test = &quot;Chi&quot;) summary(DE.glm) anova(DE.glm) GLMM if(!require(MASS)){install.packages(&quot;MASS&quot;)} library(MASS) DE.PQL&lt;-glmmPQL(Ecervi.01 ~ CLength * fSex, random = ~ 1 | fFarm, family = binomial, data = DeerEcervi) summary(DE.PQL) g &lt;- 0.8883697 + 0.0378608 * DeerEcervi$CLength p.averageFarm1&lt;-exp(g)/(1+exp(g)) I&lt;-order(DeerEcervi$CLength) #Avoid spaghetti plot plot(DeerEcervi$CLength,DeerEcervi$Ecervi.01,xlab=&quot;Length&quot;, ylab=&quot;Probability of presence of E. cervi L1&quot;) lines(DeerEcervi$CLength[I],p.averageFarm1[I],lwd=3) p.Upp&lt;-exp(g+1.96*1.462108)/(1+exp(g+1.96*1.462108)) p.Low&lt;-exp(g-1.96*1.462108)/(1+exp(g-1.96*1.462108)) lines(DeerEcervi$CLength[I],p.Upp[I]) lines(DeerEcervi$CLength[I],p.Low[I]) if(!require(lme4)){install.packages(&quot;lme4&quot;)} library(lme4) DE.lme4&lt;-lmer(Ecervi.01 ~ CLength * fSex +(1|fFarm), family = binomial, data = DeerEcervi) summary(DE.lme4) if(!require(glmmML)){install.packages(&quot;glmmML&quot;)} library(glmmML) DE.glmmML&lt;-glmmML(Ecervi.01 ~ CLength * fSex, cluster = fFarm,family=binomial, data = DeerEcervi) summary(DE.glmmML) "],
["10-2-ubungen-5.html", "10.2 Übungen 5", " 10.2 Übungen 5 Übung 5.1: Split-plot ANOVA Datensatz splityield.csv Versuch zum Ernteertrag (yield) einer Kulturpflanze in Abhängigkeit der drei Faktoren Bewässerung (irrigated vs. control), Düngung (N, NP, P) und Aussaatdichten (low, medium, high). Es gab vier ganze Felder (block), die zwei Hälften mit den beiden Bewässerungstreatments, diese wiederum drei Drittel für die drei Saatdichten und diese schliesslich je drei Drittel für die drei Düngertreatments hatten. Aufgaben Bestimmt das minimal adäquate Modell Stellt die Ergebnisse dar Übung 5.2: GLMM Führt mit dem Datensatz novanimal.csv eine logistische Regression durch, wobei ihr die einzelnen Käufer (single campus_card holder) als weitere randomisierte Variable mitberücksichtigt. Kann der Fleischkonsum durch das Geschlecht, die Hochschulzugehörigkeit und das Alter erklärt werden? Vergleich die Ergebnisse mit der eurem multiplen logistische Modell von 4.2 Kann der Fleischkonsum nun besser durch das Geschlecht, die Hochschulzugehörigkeit und das Alter erklärt werden? Aufgaben Bestimmt das minimal adäquate Modell Stellt die Ergebnisse dar Ähnliches Vorgehen wie bei der Übung 4.2S: Generiert eine neue Variable “Fleisch” (0 = kein Fleisch, 1 = Fleisch) Entfernt fehlende Werte aus der Variable “Fleisch” Lasst für die Analyse den Menü-Inhalt “Buffet” weg "],
["10-3-musterlosung-aufgabe-5-1-split-plot-anova.html", "10.3 Musterlösung Aufgabe 5.1: Split-plot ANOVA", " 10.3 Musterlösung Aufgabe 5.1: Split-plot ANOVA Übungsaufgabe (hier so ausführlich formuliert, wie dies auch in der Klausur der Fall sein wird) Ladet den Datensatz splityield.csv. Dieser enthält Versuchsergebnisse eines Experiments zum Ernteertrag (yield) einer Kulturpflanze in Abhängigkeit der drei Faktoren Bewässerung (irrigated vs. control), Düngung (N, NP, P) und Aussaatdichten (low, medium, high). Es gab vier ganze Felder (block), die zwei Hälften mit den beiden Bewässerungstreatments (irrigation), diese wiederum drei Drittel für die drei Saatdichten (density) und diese schliesslich je drei Drittel für die drei Düngertreatments (fertilizer) hatten. Ermittelt das minimal adäquate statistische Modell, das den Ernteertrag in Abhängigkeit von den angegebenen Faktoren beschreibt. Bitte erklärt und begründet die einzelnen Schritte, die ihr unternehmt, um zu diesem Ergebnis zu kommen. Dazu erstellt bitte ein Word-Dokument, in das ihr Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, eure Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere Vorgehen dokumentieren. Dieser Ablauf sollte insbesondere beinhalten: Überprüfen der Datenstruktur nach dem Einlesen, welches sind die abhängige(n) und welches die unabängige(n) Variablen Explorative Datenanalyse, um zu sehen, ob evtl. Dateneingabefehler vorliegen oder Datentransformationen vorgenommen werden sollten Auswahl und Begründung eines statistischen Verfahrens Bestimmung des vollständigen/maximalen Models Selektion des/der besten Models/Modelle Generieren aller Zahlen, Statistiken und Tabellen, die für eine wiss (Ergebnisdarstellung benötigt werden) Formuliert abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden. Abzugeben sind am Ende (a) Ein lauffähiges R-Skript; (b) begründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation) und (c) ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit). R-Skript als Download kommentierter Lösungsweg splityield &lt;-read.delim(&quot;17_Statistik5/data/splityield.csv&quot;, sep=&quot;,&quot;) #Checken der eingelesenen Daten splityield Man sieht, dass das Design vollkommen balanciert ist, d.h. jede Kombination irrigation density fertilizer kommt genau 4x vor (in jedem der vier Blöcke A-D einmal). str(splityield) summary(splityield) splityield$density&lt;-factor(splityield$density, levels=c(&quot;low&quot;,&quot;medium&quot;,&quot;high&quot;)) Man sieht, dass die Variable yield metrisch ist, während die vier anderen Variablen schon korrekt als kategoriale Variablen (factors) kodiert sind #Explorative Datenanalyse (auf Normalverteilung, Varianzhomogenität) boxplot(splityield$yield) boxplot(yield~fertilizer, data=splityield) boxplot(yield~irrigation, data=splityield) boxplot(yield~density, data=splityield) boxplot(yield~irrigation*density*fertilizer,data=splityield) Die Boxplots sind generell hinreichend symmetrisch, so dass man davon ausgehen kann, dass keine problematische Abweichung von der Normalverteilung vorliegt. Die Varianzhomogenität sieht für den Gesamtboxplot sowie für fertilizer und density bestens aus, für irrigation und für die 3-fach-Interaktion deuten sich aber gewisse Varianzheterogenitäten an, d. h. die Boxen (Interquartil-Spannen) sind deutlich unterschiedlich lang. Da das Design aber vollkommen „balanciert“ war, wie wir von oben wissen, sind selbst relativ stark divergierende Varianzen nicht besonders problematisch. Der Boxplot der Dreifachinteraktion zeigt zudem, dass grössere Varianzen (~Boxen) mal bei kleinen, mal bei grossen Mittelwerten vorkommen, womit wir bedenkenlos weitermachen können (Wenn die grossen Varianzen immer bei grossen Mittelwerten aufgetreten wären, hätten wir eine log- oder Wurzeltransformation von yield in Betracht ziehen müssen). boxplot(log10(yield)~irrigation*density*fertilizer,data=splityield) #bringt keine Verbesserung aov.1&lt;-aov(yield~irrigation*density*fertilizer+Error(block/irrigation/density),data=splityield) Das schwierigste an der Analyse ist hier die Definition des Splitt-Plot ANOVA-Modells. Hier machen wir es mit der einfachsten Möglichkeit, dem aov-Befehl. Um diesen richtig zu spezifieren, muss man verstanden haben, welches der „random“-Faktor war und wie die „fixed“ factors ineinander geschachtelt waren. In diesem Fall ist block der random Faktor, in den zunächst irrigation und dann density geschachtelt sind (die unterste Ebene fertilizer muss man nicht mehr angeben, da diese in der nächsthöheren nicht repliziert ist). (Übrigens: das simple 3-faktorielle ANOVA-Modell aov(yield~irrigationdensityfertilizer,data=splityield) würde unterstellen, dass alle 72 subplot unabhängig von allen anderen angeordnet sind, also nicht in Blöcken. Man kann ausprobieren, wie sich das Ergebnis mit dieser Einstellung unterscheidet) summary(aov.1) Wir bekommen p-Werte für die drei Einzeltreatments, die drei 2-fach-Interaktionen und die 3- fach Interaktion. Keinen p-Wert gibt es dagegen für block, da dieser als „random“ Faktor spezifiziert wurde. Signifikant sind für sich genommen irrigation und fertilizer sowie die Interaktionen irrigation:density und irrigation:fertilizer. # Modelvereinfachung aov.2&lt;-aov(yield~irrigation+density+fertilizer+irrigation:density+irrigation:fertilizer+ Error(block/irrigation/density),data=splityield) summary(aov.2) Jetzt muss man nur noch herausfinden, wie irrigation und fertilizer wirken und wie die Interaktionen aussehen. Bei multiplen ANOVAs macht man das am besten visuell: #Visualisierung der Ergebnisse boxplot(yield~fertilizer,data=splityield) boxplot(yield~irrigation,data=splityield) interaction.plot(splityield$fertilizer,splityield$irrigation,splityield$yield) interaction.plot(splityield$density,splityield$irrigation,splityield$yield) "],
["10-4-musterlosung-aufgabe-5-2s-glmm.html", "10.4 Musterlösung Aufgabe 5.2S: GLMM", " 10.4 Musterlösung Aufgabe 5.2S: GLMM Meine Leseempfehlung Kapitel 4.3.1 von Christopher Molnar Für Interessierte hier oder hier kommentierter Lösungsweg # Genereiert eine Dummyvariable: Fleisch 1, kein Fleisch 0 df &lt;- nova # kopiert originaler Datensatz df$meat &lt;- ifelse(nova$label_content == &quot;Fleisch&quot;, 1, 0) df_ &lt;- df[df$label_content != &quot;Buffet&quot;, ] # entfernt Personen die sich ein Buffet Teller gekauft haben und speichert es in eine neuen Datensatz # Löscht alle Missings bei der Variable &quot;Fleisch&quot; df_ &lt;- df_[!is.na(df_$meat), ] # setzt andere Reihenfolge für die Hochschulzugehörigkeit df_$member &lt;- factor(df_$member, levels = c(&quot;Studierende&quot;, &quot;Mitarbeitende&quot;)) # sieht euch die Verteilung zwischen Fleisch und kein Fleisch an # table(df_$meat) # definiert das logistische Modell mit card_num als random intercept und wendet es auf den Datensatz an (vgl. Statistik 5, Folien 17-23, achtung Beispiel dort ist mit Package nlme ) library(lme4) mod0 &lt;- glmer(meat ~ gender + member + age + (1|card_num), data = df_, binomial(&quot;logit&quot;)) # könnte Alter und Geschlecht weglassen (da aber soziodemografische Variablen, lasse ich sie drin) summary(mod0) # Pseudo R^2 library(MuMIn) r.squaredGLMM(mod0) # das marginale R^2 gibt uns die erklärte Varianz der fixen Effekte: hier 5% # das conditionale R^2 gibt uns die erklärte Varianz für das ganze Modell (mit fixen und variablen Effekten): hier 26% # für weitere Informationen: https://rdrr.io/cran/MuMIn/man/r.squaredGLMM.html # zusätzliche Informationen, welche für die Interpretation gut sein kann # berechnet den Standardfehler (mehr infos: https://www.youtube.com/watch?v=r-txC-dpI-E oder hier: https://mgimond.github.io/Stats-in-R/CI.html) # weitere info: https://stats.stackexchange.com/questions/26650/how-do-i-reference-a-regression-models-coefficients-standard-errors se &lt;- sqrt(diag(vcov(mod0))) # zeigt eine Tabelle der Schätzer mit 95% Konfidenzintervall =&gt; falls 0 enthalten dann ist der Unterschied statistisch nicht signifikant tab1 &lt;- cbind(Est = fixef(mod0), LL = fixef(mod0) - 1.96 * se, UL = fixef(mod0) + 1.96 * se) # erzeugt die Odds Ratios tab2 &lt;- exp(tab1) "],
["11-konsolidierung-statistik-11-11-19-11-2019.html", "Kapitel 11 Konsolidierung Statistik (11.11. - 19.11.2019)", " Kapitel 11 Konsolidierung Statistik (11.11. - 19.11.2019) In den vier Blöcken “Konsolidierung Statistik” repetieren die Studierenden wichtige Verfahren der Inferenz-Statistik. Beginnend mit den beiden einfachsten Fällen, dem Chi-Quadrat-Test für die Assoziation zwischen zwei kategorialen Variablen und dem t-Test auf Unterschiede in Mittelwerten zwischen zwei Gruppen, wiederholen die Studierende auch die Varianzanalyse (ANOVA) als Verallgemeinerung des t-Tests, einschliesslich post-hoc-Tests und mehrfaktorieller ANOVA. Weiter geht es mit der Repetition von komplexere Versionen linearer Regressionen und generalized linear models (GLMs), die einige wesentliche Limitierungen von linearen Modellen überwinden. Abschliessend bekommen die Studierenden eine Einführung in die Welt der Ordinationen z.B. PCA. Der Ablauf der nächsten vier Blöcke: . Hier geht es zum Download "],
["11-1-ubung-konsolidierung-statistik.html", "11.1 Übung Konsolidierung Statistik", " 11.1 Übung Konsolidierung Statistik Für die nächsten drei gemeinsamen Sitzungen (11./18./19. November) könnt ihr auf eure eigene Art die statistischen Methoden nochmals anwenden: a. Ihr sucht euch einen eigenen Datensatz b. Ihr nehmt einen vorgeschlagenen Datensatz c. Ihr nehmt euren Datensatz aus der Fallstudie Wichtig: Beachtet, dass ihr einen Datensatz sucht, mit dem folgende Analyse möglich sind: Assoziationstests Varianzanalysen linerare Modelle Für eine gute Prüfungsvorbereitung erfasst ihr (Einzel oder als Gruppe) zu jeder obigen Analysemethode einen kurzen Bericht (max. 1/2 bis 1 Seite). Dieser Bericht beinhaltet folgende Punkte: a. lauffähiges R-Skript b. begründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation) c. ausformulierter Methoden- und Ergebnisteil (für eine wiss.Arbeit) * Methodenteil: Begründung wieso ihr euch für diese/n Test/Methode entschieden habt * Ergebnisteil: Interpretation der Ergebnisse (ggf. mit Formel) und einer passenden Abbildung/Tabelle Das Ziel dieser Übung ist es, dass ihr euch überlegt was für eine Variablenstruktur der Datensatz aufweisen muss, um die obigen Analysen zu durchzuführen dass ihr einen eigenen Datensatz findet, welches euer Interesse weckt dass ihr verschiedene Quellen zu Open Data gesehen und kennen gelernt habt dass ihr für die Prüfung gut vorbereitet sind Open Datasets R data sets In R gibt es vordefinierte Datensätze, welche gut abrufbar sind. Beispiele sind: sleep USAccDeaths USArrests … data() # erzeugt eine Liste mit den Datensätzen, welche in R verfügbaren sind head(chickwts) str(chickwts) Kaggle Auf Kaggle findet ihr öffentlich zugängliche Datensätze. Einzig was ihr tun müsst, ist euch registrieren. Beispiele sind: 911 foodPreferences S.F. salaries … # Load packages and data data_911 &lt;- read_delim(&quot;17_Statistik_Konsolidierung1/911.csv&quot;, delim = &quot;,&quot;) str(data_911) Tidytuesday Tidytuesday ist eine Plattform, in der wöchentlich - jeden Dienstag - einen öffentlich zugänglichen Datensatz publiziert. Dieses Projekt ist aus der R4DS Online Learning Community und dem R for Data Science Lehrbuch hervorgegangen. Beispiele sind: Women in the Workplace Dairy production &amp; Consumption Star Wars Survey Global Coffee Chains Malaria Deaths … Download via Github - 1. Möglichkeit 1. Geht zum File, welches ihr herunterladen wollt 2. Klickt auf das File (.csv, .xlsx etc.), um den Inhalt innerhalb der GitHub Benutzeroberfläche anzuzeigen 3. Klickt mit der rechten Maustaste auf den Knopf “raw” 4. (Ziel) Speichern unter… Download via Github - 2. Möglichkeit # Beachtet dabei, dass ihr die URL zum originalen (raw) Datensatz habt star_wars &lt;- read_delim(&quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-05-14/week7_starwars.csv&quot;, delim = &quot;,&quot;, col_names = T) Download via Github - 3. Möglichkeit # mehr Infos dazu findet ihr hier: https://rpubs.com/lokraj/github_csv urlfile = &quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-05-14/week7_starwars.csv&quot; # definiere zuerst die URL auf der R zugreifen soll star_wars &lt;- read_delim(url(urlfile), delim = &quot;,&quot;, col_names = T) # lade die Datei von der obigen URL # head(star_wars) opendata.swiss Auf opendata.swiss sind offene, frei verfügbare Daten der Schweizerischen Behörden zu finden. opendata.swiss ist ein gemeinsames Projekt von Bund, Kantonen, Gemeinden und weiteren Organisationen mit einem staatlichen Auftrag. Beispiele sind: Statistik der Schweizer Städte Verpflegungsbetriebe nach Jahr und Stadtquartier Altpapiermengen … Open Data Katalog Stadt Zürich Auf der Seite der Stadt Zürich Open Data findet ihr verschiedene Datensätze der Stadt Zürich. Spannend daran ist, dass die veröffentlichten Daten kostenlos und zur freien - auch kommerziellen - Weiterverwendung zur Verfügung. Beispiele sind: Bevölkerung nach Bildungsstand, Jahr, Alter und Geschlecht seit 1970 Luftqualitätsmessungen Häufigste Hauptsprachen … # lade die Datei &quot;Häufigste Sprachen&quot; urlfile = &quot;https://data.stadt-zuerich.ch/dataset/14f0e561-78a5-4f47-8ccf-9ae93d37e990/resource/91aa31e3-3272-4f8e-8763-c08964f8b16c/download/bev301od3011.csv&quot; dat_lang &lt;- read_delim(url(urlfile), delim = &quot;,&quot;, col_names = T) head(dat_lang) Given Datasets R data sets chickwts iris mtcars Titanic (achtung hat das table Datenformat) starwars (dplyr package) Research Methods data sets NOVANIMAL Ukraine (Demoskript 3) Ipomopsis (Demoskript 3) "],
["12-statistik-6-12-11-2019.html", "Kapitel 12 Statistik 6 (12.11.2019)", " Kapitel 12 Statistik 6 (12.11.2019) Statistik 6 führt in multivariat-deskriptive Methoden ein, die dazu dienen Datensätze mit multiplen abhängigen und multiplen unabhängigen Variablen effektiv zu analysieren. Dabei betonen Ordinationen kontinuierliche Gradienten und fokussieren auf zusammengehörende Variablen, während Cluster-Analysen Diskontinuitäten betonen und auf zusammengehörende Beobachtungen fokussieren. Es folgt eine konzeptionelle Einführung in die Idee von Ordinationen als einer Technik der deskriptiven Statistik, die Strukturen in multivariaten Datensätzen via Dimensionsreduktion visualisiert. Das Prinzip und die praktische Implementierung wird detailliert am Beispiel der Hauptkomponentenanalyse (PCA) erklärt. Danach folgen kurze Einführungen in weitere Ordinationstechniken für besondere Fälle, welche bestimmte Limitierungen der PCA überwinden, namentlich CA, DCA und NMDS. "],
["12-1-statistik-6-demoskript.html", "12.1 Statistik 6 - Demoskript", " 12.1 Statistik 6 - Demoskript Ordinationen I (c) Juergen Dengler Demoscript als Download PCA if(!require(labdsv)){install.packages(&quot;labdsv&quot;)} library(labdsv) #Für Ordinationen benötigen wir Matrizen, nicht Data.frames #Generieren von Daten raw&lt;-matrix(c(1,2,2.5,2.5,1,0.5,0,1,2,4,3,1),nrow=6) colnames(raw)&lt;-c(&quot;spec.1&quot;,&quot;spec.2&quot;) rownames(raw)&lt;-c(&quot;r1&quot;,&quot;r2&quot;,&quot;r3&quot;,&quot;r4&quot;,&quot;r5&quot;,&quot;r6&quot;) raw #originale Daten im zweidimensionalen Raum x1&lt;-raw[,1] y1&lt;-raw[,2] z&lt;-c(rep(1:6)) #Plot Abhängigkeit der Arten vom Umweltgradienten plot(c(x1,y1)~c(z,z),type=&quot;n&quot;,axes=T,bty=&quot;l&quot;,las=1,xlim=c(1,6),ylim=c(0,5),xlab=&quot;Umweltgradient&quot;,ylab=&quot;Deckung der Arten&quot;) points(x1~z,pch=21,type=&quot;b&quot;) points(y1~z,pch=16,type=&quot;b&quot;) #zentrierte Daten cent&lt;-scale(raw,scale=F) x2&lt;-cent[,1] y2&lt;-cent[,2] #rotierte Daten o.pca&lt;-pca(raw) x3 &lt;- o.pca$scores[,1] y3 &lt;- o.pca$scores[,2] #Visualisierung der Schritte im Ordinationsraum plot(c(y1,y2,y3)~c(x1,x2,x3),type=&quot;n&quot;,axes=T,bty=&quot;l&quot;,las=1,xlim=c(-4,4),ylim=c(-4,4),xlab=&quot;Art 1&quot;,ylab=&quot;Art 2&quot;) points(y1~x1,pch=21,type=&quot;b&quot;,col=&quot;green&quot;,lwd=2) points(y2~x2,pch=16,type=&quot;b&quot;,col=&quot;red&quot;, lwd=2) points(y3~x3,pch=17,type=&quot;b&quot;,col=&quot;blue&quot;, lwd=2) #Durchführung der PCA o.pca&lt;-pca(raw) #Koordinaten im Ordinationsraum o.pca$scores #Korrelationen der Variablen mit den Ordinationsachsen o.pca$loadings #Erklärte Varianz der Achsen E&lt;-o.pca$sdev^2/o.pca$totdev*100 E #mit prcomp pca.2&lt;-prcomp(raw,scale=F) summary(pca.2) plot(pca.2) biplot(pca.2) #mit vegan if(!require(vegan)){install.packages(&quot;vegan&quot;)} library(&quot;vegan&quot;) pca.3 &lt;- rda(raw, scale=FALSE) #Die Funktion rda führt ein PCA aus an wenn nicht Umwelt und Artdaten definiert werden summary(pca.3,axes=0) biplot(pca.3, scaling=2) #Mit Beispieldaten aus Wildi (2013) if(!require(dave)){install.packages(&quot;dave&quot;)} library(dave) data(sveg) str(sveg) summary(sveg) names(sveg) #PCA: Deckungen Wurzeltransformiert, cor=T erzwingt Nutzung der Korrelationsmatrix pca.5&lt;-pca(sveg^0.25,cor=T) #Koordinaten im Ordinationsraum pca.5$scores #Korrelationen der Variablen mit den Ordinationsachsen pca.5$loadings #Erklärte Varianz der Achsen in Prozent (sdev ist die Wurzel daraus) E&lt;-pca.5$sdev^2/pca.5$totdev*100 E E[1:5] #PCA-Plot der Lage der Beobachtungen im Ordinationsraum plot(pca.5$scores[,1],pca.5$scores[,2],type=&quot;n&quot;, asp=1, xlab=&quot;PC1&quot;, ylab=&quot;PC2&quot;) points(pca.5$scores[,1],pca.5$scores[,2],pch=18) #Subjektive Auswahl von Arten zur Darstellung sel.sp &lt;- c(3,11,23,39,46,72,77,96) snames &lt;- names(sveg[,sel.sp]) snames #PCA-Plot der Korrelationen der Variablen (hier Arten) mit den Achsen (h) x &lt;- pca.5$loadings[,1] y&lt;-pca.5$loadings[,2] plot(x,y,type=&quot;n&quot;,asp=1) arrows(0,0,x[sel.sp],y[sel.sp],length=0.08) text(x[sel.sp],y[sel.sp],snames,pos=1,cex=0.6) # Mit vegan pca.6&lt;-rda(sveg^0.25, scale=TRUE) #Erklärte Varianz der Achsen summary(pca.6,axes=0) #PCA-Plot der Lage der Beobachtungen im Ordinationsraum biplot(pca.6, scaling=1,display = c(&quot;sites&quot;),type = c(&quot;points&quot;)) #Subjektive Auswahl von Arten zur Darstellung sel.sp &lt;- c(3,11,23,39,46,72,77,96) snames &lt;- names(sveg[,sel.sp]) snames #PCA-Plot der Korrelationen der Variablen (hier Arten) mit den Achsen (h) scores&lt;-scores(pca.6,display=c(&quot;species&quot;)) x&lt;-scores[,1] y&lt;-scores[,2] plot(x,y,type=&quot;n&quot;,asp=1) arrows(0,0,x[sel.sp],y[sel.sp],length=0.08) text(x[sel.sp],y[sel.sp],snames,pos=1,cex=0.6) CA ca&lt;-cca(sveg^0.5) #Arten (o) und Communities (+) plotten plot(ca) #Nur Arten plotten x&lt;-ca$CA$u[,1];y&lt;-ca$CA$u[,2] plot(x,y) plot(ca, display = c(&quot;species&quot;),type = c(&quot;points&quot;))#alternative #Anteilige Varianz, die durch die ersten beiden Achsen erklärt wird ca$CA$eig[1:2]/sum(ca$CA$eig) DCA dca &lt;- decorana(sveg,mk=10) plot(dca$rproj, asp=1) dca2 &lt;- decorana(sveg,mk=100) plot(dca2$rproj, asp=1) NMDS #Distanzmatrix als Start erzeugen mde &lt;-vegdist(sveg,method=&quot;euclidean&quot;) mde mde &lt;-vegdist(sveg,method=&quot;bray&quot;)#Alternative mit einem für Vegetationsdaten üblichen Dissimilarity index mde #Zwei verschiedene NMDS-Methoden if(!require(MASS)){install.packages(&quot;MASS&quot;)} library(MASS) set.seed(1) #macht man, wenn man bei einer Wiederholung exakt die gleichen Ergebnisse will imds&lt;-isoMDS(mde,k=2) set.seed(1) mmds&lt;-metaMDS(mde,k=2) plot(imds$points) plot(mmds$points) #Stress = S² = Abweichung der zweidimensionalen NMDS-Lösung von der originalen Distanzmatrix stressplot(imds,mde) stressplot(mmds,mde) "],
["12-2-statistik-6-ubungen.html", "12.2 Statistik 6: Übungen", " 12.2 Statistik 6: Übungen Übung 6.1: PCA (naturwissenschaftlich) Datensatz Doubs.RData Lädt den Datensatz Doubs.RData mit dem folgenden Befehl ins R: load(“Doubs.RData”) Die Umweltvariablen findet ihr im data.frame env die Abundanzen im data.frame spe. Im data.frame fishtrait findet ihr die Vollständigen Namen der Fische Der Datensatz enthält Daten zum Vorkommen von Fischarten und den zugehörigen Umweltvariablen im Fluss Doubs (Jura). Es gibt 30 Probestellen (sites), an denen jeweils die Abundanzen von 27 Fischarten (auf einer Skalen von 0 bis 5) sowie 11 Umweltvariablen erhoben wurden: dfs = Distance from source (km) ele = Elevation (m a.s.l.) slo = Slope (‰) dis = Mean annual discharge (m3 s-1) pH = pH of water har = Hardness (Ca concentration) (mg L-1) pho = Phosphate concentration (mg L-1) nit = Nitrate concentration (mg L-1) amm = Ammonium concentration (mg L-1) oxy = Dissolved oxygen (mg L-1) bod = Biological oxygen demand (mg L-1) Eure Aufgabe ist nun, in einem ersten Schritt eine PCA für die 11 Umweltvariablen zu rechnen. Da die einzelnen Variablen auf ganz unterschiedlichen Skalen gemessen wurden, ist dazu eine Standardisierung nötig (pca mit der Funktion rda, scale=TRUE). Überlegt, wie viele Achsen wichtig sind und für was sie jeweils stehen. In einem zweiten Schritt sollen dann die vollständig unkorrelierten PCA-Achsen als Prädiktoren einer multiplen Regression zur Erklärung der Fischartenzahl (Anzahl kann z.B. kann mit dem Befehl specnumber(spe) ermittel werden) verwendet werden (wahlweise lm oder glm). Gebt das minimal adäquate Modell an und interpretiert dieses (wahlweise im frequentist oder information theoretician approach). (Wer noch mehr probieren möchte, kann zum Vergleich noch eine multiple Regression mit den Originaldaten rechnen). "],
["12-3-musterloseung-aufgabe-6-1-pca.html", "12.3 Musterlöseung Aufgabe 6.1: PCA", " 12.3 Musterlöseung Aufgabe 6.1: PCA R-Skript als Download load(&quot;18_Statistik6/Doubs.RData&quot;) summary(env) summary(spe) #Die Dataframes env und spe enthalten die Umwelt- respective die Artdaten if(!require(vegan)){install.packages(&quot;vegan&quot;)} library(&quot;vegan&quot;) Die PCA wird im Package vegan mit dem Befehl rda ausgeführt, wobei in diesem scale = TRUE gesetzt warden muss, da die Umweltdaten mit ganz unterschiedlichen Einheiten und Wertebereichen daherkommen env.pca &lt;- rda(env, scale=TRUE) env.pca #In env.pca sieht man, dass es bei 11 Umweltvariablen logischerweise 11 orthogonale Principle Components gibt summary(env.pca,axes=0) #Hier sieht man auch die Übersetzung der Eigenvalues in erklärte Varianzen der einzelnen Principle Components summary(env.pca) #Hier das ausführliche Summary mit den Art- und Umweltkorrelationen auf den ersten sechs Achsen screeplot(env.pca, bstick=TRUE, npcs=length(env.pca$CA$eig)) #Visualisierung der Anteile erklärter Varianz, auch im Vergleich zu einem Broken-Stick-Modell Die Anteile fallen steil ab. Nur die ersten vier Achsen erklären jeweils mehr als 5 % (und zusammen über 90 %) Das Broken-stick-Modell würde sogar nur die ersten beiden Achsen als relevant vorschlagen Da die Relevanz für das Datenmuster in den Umweltdaten nicht notwendig die Relevanz für die Erklärung der Artenzahlen ist, nehmen wir ins globale Modell grosszügig die ersten vier Achsen rein (PC1-PC4) Die Bedeutung der Achsen (benötigt man später für die Interpretation!) findet man in den “species scores” (da so, wie wir die PCA hier gerechnet haben, die Umweltdaten die Arten sind. Zusätzlich oder alternative kann man sich die ersten vier Achsen auch visualisieren, indem man PC2 vs. PC1 (ohne choices), PC3 vs. PC1 oder PC4 vs. PC1 plottet. par(mfrow=c(2,2)) biplot(env.pca, scaling=1) biplot(env.pca, choices=c(1,3),scaling=1) biplot(env.pca, choices=c(1,4),scaling=1) PC1 steht v.a. für Nitrat (positiv), Sauerstoff (negativ) PC2 steht v.a. für pH (positiv) PC3 steht v.a. für pH (positiv) und slo (negativ) PC4 steht v.a. für pH (negativ) und slo (negativ) #Wir extrahieren nun die ersten vier PC-Scores aller Aufnahmeflächen scores&lt;-scores(env.pca,choices=c(1:4),display=c(&quot;sites&quot;)) scores #Berechnung der Artenzahl mittels specnumber; Artenzahl und Scores werden zum Dataframe für die Regressionsanalyse hinzugefügt doubs &lt;- data.frame(env, scores, species_richness=specnumber(spe)) doubs str(doubs) ##Lösung mit lm (alternativ ginge Poisson-glm) und frequentist approach (alternativ ginge Multimodelinference mit AICc) lm.pc.0 &lt;- lm(species_richness ~ PC1+PC2+PC3+PC4, data = doubs) summary(lm.pc.0) #Modellvereinfachung: PC4 ist nicht signifikant und wird entfernt lm.pc.1 &lt;- lm(species_richness ~ PC1+PC2+PC3, data = doubs) summary(lm.pc.1) #jetzt sind alle Achsen signifikant und werden in das minimal adäquate Modell aufgenommen #Modelldiagnostik/Modellvalidierung par(mfrow=c(2,2)) plot(lm.pc.1) Nicht besonders toll, ginge aber gerade noch. Da wir aber ohnehin Zähldaten haben, können wir es mit einem Poisson-GLM versuchen #Alternativ mit glm glm.pc.0 &lt;- glm(species_richness ~ PC1+PC2+PC3+PC4, family = &quot;poisson&quot;, data = doubs) summary(glm.pc.0) glm.pc.1 &lt;- glm(species_richness ~ PC1+PC2+PC3, family = &quot;poisson&quot;, data = doubs) summary(glm.pc.1) plot(glm.pc.1) #sieht nicht besser aus als LM, die Normalverteilung ist sogar schlechter LM oder GLM sind für die Analyse möglich, Modellwahl nach Gusto. Man muss jetzt noch die Ergebnisse adäquat aus all den erzielten Outputs zusammenstellen (siehe Ergebnistext). In dieser Aufgabe haben wir ja die PC-Achsen als Alternative zur direkten Modellierung mit den originalen Umweltvariablen ausprobiert. Deshalb (war nicht Teil der Aufgabe), kommt hier noch eine Lösung, wie wir es bisher gemacht hätten. Zum Vergleich die Modellierung mit den Originaldaten #Korrelationen zwischen Prädiktoren cor &lt;- cor(doubs[,1:11]) cor[abs(cor)&lt;.7] &lt;-0 cor #Die Korrelationsmatrix betrachtet man am besten in Excel. #Es zeigt sich, dass es zwei grosse Gruppen von untereinander hochkorrelierten Variablen gibt: zum einen dfs-ele-dis-har-nit, zum anderen pho-nit-amm-oxy-bod, während slo und pH mit jeweils keiner anderen Variablen hochkorreliert sind. Insofern behalten wir eine aus der ersten Gruppe (ele), eine aus der zweiten Gruppe (pho) und die beiden «unabhängigen». #Globalmodell (als hinreichend unabhängige Variablen werden ele, slo, pH und pho aufgenommen) lm.orig.0 &lt;- lm(species_richness ~ ele+slo+pH+pho, data =doubs) summary(lm.orig.0) #Modellvereinfachung: slo als am wenigsten signifikante Variable gestrichen lm.orig.1 &lt;- lm(species_richness ~ ele+pH+pho, data =doubs) summary(lm.orig.1) #Modellvereinfachung: pH ist immer noch nicht signifikant und wird gestrichen lm.orig.2 &lt;- lm(species_richness ~ ele+pho, data =doubs) summary(lm.orig.2) #Modelldiagnostik par(mfrow=c(2,2)) plot(lm.orig.2) #nicht so gut, besonders die Bananenform in der linken obereren Abbildung #Nach Modellvereinfachung bleiben zwei signifikante Variablen, ele und pho. #Da das nicht so gut aussieht, versuchen wir es mit dem theoretisch angemesseneren Modell, einem Poisson-GLM. #Versuch mit glm glm.orig.0 &lt;- glm(species_richness ~ ele+pho+pH+slo, family = &quot;poisson&quot;, data =doubs) summary(glm.orig.0) glm.orig.1 &lt;- glm(species_richness ~ ele+pho+slo, family = &quot;poisson&quot;, data =doubs) summary(glm.orig.1) glm.orig.2 &lt;- glm(species_richness ~ ele+pho, family = &quot;poisson&quot;, data =doubs) summary(glm.orig.2) plot(glm.orig.2) #Das sieht deutlich besser aus als beim LM. Wir müssen aber noch prüfen, ob evtl. Overdispersion vorliegt. if(!require(AER)){install.packages(&quot;AER&quot;)} library(AER) dispersiontest(glm.orig.2) #signifikante Überdispersion #Ja, es gibt signifikante Overdispersion (obwohl der Dispersionparameter sogar unter 2 ist, also nicht extrem). Wir können nun entweder quasipoisson oder negativebinomial nehmen. glmq.orig.2 &lt;- glm(species_richness ~ ele+pho, family = &quot;quasipoisson&quot;, data =doubs) summary(glmq.orig.2) #Parameterschätzung bleiben gleich, aber Signifikanzen sind niedriger als beim GLM ohne Overdispersion. plot(glmq.orig.2) Sieht gut aus, wir hätten hier also unser finales Modell. Im Vergleich der beiden Vorgehensweisen (PC-Achsen vs. Umweltdaten direkt) scheint in diesem Fall die direkte Modellierung der Umweltachsen informativer: Man kommt mit zwei Prädiktoren aus, die jeweils direkt für eine der Hauptvariablen stehen – Meereshöhe und Phosphor – zugleich aber jeweils eine grössere Gruppe von Variablen mit hohen Korrelationen inkludieren, im ersten Fall Variablen, die sich im Flusslauf von oben nach unten systematisch ändern, im zweiten Masse der Nährstoffbelastung des Gewässers. Bei der PCA-Lösung kamen drei signifikante Komponenten heraus, die allerdings nicht so leicht zu interpretieren sind. Dies insbesondere, weil in diesem Fall auf der Ebene PC2 vs. PC1 die Mehrzahl der Umweltparameter ungefähr in 45-Grad-Winkeln angeordnet sind. Im allgemeinen Fall kann aber die Nutzung von PC-Achsen durchaus eine gute Lösung sein. "],
["13-statistik-7-18-11-2019.html", "Kapitel 13 Statistik 7 (18.11.2019)", " Kapitel 13 Statistik 7 (18.11.2019) In Statistik 7 beschäftigen wir uns zunächst damit, wie wir Ordinationsdiagramme informativer gestalten können, etwa durch die Beschriftung der Beobachtunge, post-hoc-Projektion der Prädiktorvariablen oder Response surfaces. Während wir bislang mit «unconstrained» Ordinationen gearbeitet haben, welche die Gesamtvariabilität in den Beobachtungen visualisieren, beschränken die jeweiligen «constrained»-Varianten derselben Ordinationsmethoden die Betrachtung auf den Teil der Variabilität, welcher durch eine Linearkombination der berücksichtigen Prädiktoren erklärt werden kann. Wir beschäftigen uns im Detail mit der Redundanz-Analyse (RDA), der «constrained»-Variante der PCA und gehen einen kompletten analytischen Ablauf mit Aufbereitung, Interpretation und Visualisierung der Ergebnisse am Beispiel eines gemeinschaftsökologischen Datensatzes (Fischgesellschaften und Umweltfaktoren im Jura-Fluss Doubs) durch "],
["13-1-statistik-7-demoskript.html", "13.1 Statistik 7 - Demoskript", " 13.1 Statistik 7 - Demoskript Ordinationen II (c) Juergen Dengler Demoscript als Download Datensatz Doubs.RData Funktion triplot.rda.R Interpretation von Ordinationen (Wildi pp. 96 et seq.) Wildi pp. 96 et seq.) #Plot Arten if(!require(dave)){install.packages(&quot;dave&quot;)} library(dave) ca&lt;-cca(sveg^0.5) #Plot mit ausgewählten Arten sel.spec&lt;-c(3,11,23,31,39,46,72,77,96) snames&lt;-names(sveg[,sel.spec]) snames sx &lt;- ca$CA$v[sel.spec,1] sy &lt;- ca$CA$v[sel.spec,2] plot(ca$CA$u, asp=1) points(sx,sy,pch=16) snames &lt;- make.cepnames(snames) text(sx,sy,snames,pos=c(1,2,1,1,3,2,4,3,1),cex=0.8) # Plotte post-hoc gefittete Umweltvariablen sel.sites &lt;- c(&quot;pH.peat&quot;, &quot;Acidity.peat&quot;, &quot;CEC.peat&quot;, &quot;P.peat&quot;, &quot;Waterlev.max&quot;) ev &lt;-envfit(ca,ssit[,sel.sites]) plot(ev,add=T,cex=0.8) #Plot &quot;response surfaces&quot; in der CA plot(ca$CA$u, asp=1) ordisurf(ca,ssit$pH.peat,add=T) plot(ca$CA$u, asp=1) ordisurf(ca,ssit$Waterlev.av,add=T,col=&quot;blue&quot;) #Das gleiche für die DCA (geht nicht, da das &quot;detrending&quot; die Distanzmatrix zerstört) dca &lt;- decorana(sveg,mk=10) plot(dca$rproj, asp=1) ordisurf(dca,ssit$pH.peat,add=T) ordisurf(dca,ssit$Waterlev.av,add=T,col=&quot;blue&quot;) #Das gleiche mit NMDS mde &lt;-vegdist(sveg,method=&quot;euclidean&quot;) mmds&lt;-metaMDS(mde,k=2) if(!require(MASS)){install.packages(&quot;MASS&quot;)} library(MASS) imds&lt;-isoMDS(mde,k=2) plot(mmds$points) ordisurf(mmds,ssit$pH.peat,add=T) ordisurf(mmds,ssit$Waterlev.av,add=T,col=&quot;blue&quot;) plot(imds$points) ordisurf(imds,ssit$pH.peat,add=T) ordisurf(imds,ssit$Waterlev.av,add=T,col=&quot;blue&quot;) Constrained ordination #5 Umweltvariablen gewählt, durch die die Ordination constrained werden soll ssit summary(ssit) s5&lt;-c(&quot;pH.peat&quot;,&quot;P.peat&quot;,&quot;Waterlev.av&quot;,&quot;CEC.peat&quot;,&quot;Acidity.peat&quot;) ssit5&lt;-ssit[s5] data(sveg) summary(sveg) #RDA = constrained PCA rda &lt;-rda(sveg,ssit5) plot(rda) #CCA = constrained CA cca &lt;-cca(sveg,ssit5) plot(cca) #Unconstrained and constrained variance tot &lt;- cca$tot.chi constr &lt;- cca$CCA$tot.chi constr/tot Mehr Details zu RDA aus Borcard et al. (Numerical ecology with R) # Doubs Datensatz in den workspace laden load(&quot;Doubs.RData&quot;) spe env spa summary(spe) summary(env) summary(spa) # Entfernen der Untersuchungsfläche ohne Arten spe &lt;- spe[-8, ] env &lt;- env[-8, ] spa &lt;- spa[-8, ] # Karten für 4 Fischarten dev.new(title = &quot;Four fish species&quot;, noRStudioGD = TRUE) par(mfrow = c(2, 2)) plot(spa, asp = 1, col = &quot;brown&quot;, cex = spe$Satr, xlab = &quot;x (km)&quot;, ylab = &quot;y (km)&quot;, main = &quot;Brown trout&quot;) lines(spa, col = &quot;light blue&quot;) plot(spa, asp = 1, col = &quot;brown&quot;, cex = spe$Thth, xlab = &quot;x (km)&quot;, ylab = &quot;y (km)&quot;, main = &quot;Grayling&quot;) lines(spa, col = &quot;light blue&quot;) plot(spa, asp = 1, col = &quot;brown&quot;, cex = spe$Alal, xlab = &quot;x (km)&quot;, ylab = &quot;y (km)&quot;, main = &quot;Bleak&quot;) lines(spa, col = &quot;light blue&quot;) plot(spa, asp = 1, col = &quot;brown&quot;, cex = spe$Titi, xlab = &quot;x (km)&quot;, ylab = &quot;y (km)&quot;, main = &quot;Tench&quot;) lines(spa, col = &quot;light blue&quot;) # Set aside the variable &#39;dfs&#39; (distance from the source) for # later use dfs &lt;- env[, 1] # Remove the &#39;dfs&#39; variable from the env data frame env2 &lt;- env[, -1] # Recode the slope variable (slo) into a factor (qualitative) # variable to show how these are handled in the ordinations slo2 &lt;- rep(&quot;.very_steep&quot;, nrow(env)) slo2[env$slo &lt;= quantile(env$slo)[4]] &lt;- &quot;.steep&quot; slo2[env$slo &lt;= quantile(env$slo)[3]] &lt;- &quot;.moderate&quot; slo2[env$slo &lt;= quantile(env$slo)[2]] &lt;- &quot;.low&quot; slo2 &lt;- factor(slo2, levels = c(&quot;.low&quot;, &quot;.moderate&quot;, &quot;.steep&quot;, &quot;.very_steep&quot;)) table(slo2) # Create an env3 data frame with slope as a qualitative variable env3 &lt;- env2 env3$slo &lt;- slo2 # Create two subsets of explanatory variables # Physiography (upstream-downstream gradient) envtopo &lt;- env2[, c(1 : 3)] names(envtopo) # Water quality envchem &lt;- env2[, c(4 : 10)] names(envchem) # Hellinger-transform the species dataset library(vegan) spe.hel &lt;- decostand(spe, &quot;hellinger&quot;) spe.hel # Redundancy analysis (RDA) ## RDA of the Hellinger-transformed fish species data, constrained ## by all the environmental variables contained in env3 (spe.rda &lt;- rda(spe.hel ~ ., env3)) # Observe the shortcut formula summary(spe.rda) # Scaling 2 (default) # Canonical coefficients from the rda object coef(spe.rda) # Unadjusted R^2 retrieved from the rda object (R2 &lt;- RsquareAdj(spe.rda)$r.squared) # Adjusted R^2 retrieved from the rda object (R2adj &lt;- RsquareAdj(spe.rda)$adj.r.squared) ## Triplots of the rda results (lc scores) ## Site scores as linear combinations of the environmental variables dev.new(title = &quot;RDA scaling 1 and 2 + lc&quot;, width = 12, height = 6, noRStudioGD = TRUE) par(mfrow = c(1, 2)) # Scaling 1 plot(spe.rda,scaling = 1,display = c(&quot;sp&quot;, &quot;lc&quot;, &quot;cn&quot;),main = &quot;Triplot RDA spe.hel ~ env3 - scaling 1 - lc scores&quot;) spe.sc1 &lt;- scores(spe.rda, choices = 1:2, scaling = 1, display = &quot;sp&quot;) arrows(0, 0, spe.sc1[, 1] * 0.92,spe.sc1[, 2] * 0.92,length = 0, lty = 1, col = &quot;red&quot;) text(-0.75, 0.7, &quot;a&quot;, cex = 1.5) # Scaling 2 plot(spe.rda, display = c(&quot;sp&quot;, &quot;lc&quot;, &quot;cn&quot;), main = &quot;Triplot RDA spe.hel ~ env3 - scaling 2 - lc scores&quot;) spe.sc2 &lt;- scores(spe.rda, choices = 1:2, display = &quot;sp&quot;) arrows(0, 0, spe.sc2[, 1] * 0.92, spe.sc2[, 2] * 0.92,length = 0,lty = 1,col = &quot;red&quot;) text(-0.82, 0.55, &quot;b&quot;, cex = 1.5) ## Triplots of the rda results (wa scores) ## Site scores as weighted averages (vegan&#39;s default) # Scaling 1 : distance triplot dev.new(title = &quot;RDA plot&quot;, width = 12, height = 6, noRStudioGD = TRUE) par(mfrow = c(1,2)) plot(spe.rda, scaling = 1, main = &quot;Triplot RDA spe.hel ~ env3 - scaling 1 - wa scores&quot;) arrows(0, 0, spe.sc1[, 1] * 0.92, spe.sc1[, 2] * 0.92, length = 0, lty = 1, col = &quot;red&quot;) # Scaling 2 (default) : correlation triplot plot(spe.rda, main = &quot;Triplot RDA spe.hel ~ env3 - scaling 2 - wa scores&quot;) arrows(0, 0, spe.sc2[, 1] * 0.92, spe.sc2[, 2] * 0.92, length = 0, lty = 1, col = &quot;red&quot;) # Select species with goodness-of-fit at least 0.6 in the # ordination plane formed by axes 1 and 2 spe.good &lt;- goodness(spe.rda) sel.sp &lt;- which(spe.good[, 2] &gt;= 0.6) sel.sp # Triplots with homemade function triplot.rda(), scalings 1 and 2 source(&quot;triplot.rda.R&quot;) dev.new(title = &quot;RDA plot with triplot.rda&quot;, width = 12, height = 6, noRStudioGD = TRUE) par(mfrow = c(1,2)) triplot.rda(spe.rda, site.sc = &quot;lc&quot;, scaling = 1, cex.char2 = 0.7, pos.env = 3, pos.centr = 1, mult.arrow = 1.1, mar.percent = 0.05, select.spe = sel.sp) text(-0.92, 0.72, &quot;a&quot;, cex = 2) triplot.rda(spe.rda, site.sc = &quot;lc&quot;, scaling = 2, cex.char2 = 0.7, pos.env = 3, pos.centr = 1, mult.arrow = 1.1, mar.percent = 0.05, select.spe = sel.sp) text(-2.82, 2, &quot;b&quot;, cex = 2) # Global test of the RDA result anova(spe.rda, permutations = how(nperm = 999)) # Tests of all canonical axes anova(spe.rda, by = &quot;axis&quot;, permutations = how(nperm = 999)) ## Partial RDA: effect of water chemistry, holding physiography ## constant # Simple syntax; X and W may be in separate tables of quantitative # variables (spechem.physio &lt;- rda(spe.hel, envchem, envtopo)) summary(spechem.physio) # Formula interface; X and W variables must be in the same # data frame (spechem.physio2 &lt;- rda(spe.hel ~ pH + har + pho + nit + amm + oxy + bod + Condition(ele + slo + dis), data = env2)) # Test of the partial RDA, using the results with the formula # interface to allow the tests of the axes to be run anova(spechem.physio2, permutations = how(nperm = 999)) anova(spechem.physio2, permutations = how(nperm = 999), by = &quot;axis&quot;) # Partial RDA triplots (with fitted site scores) # with function triplot.rda # Scaling 1 dev.new(title = &quot;Partial RDA&quot;,width = 12, height = 6, noRStudioGD = TRUE) par(mfrow = c(1, 2)) triplot.rda(spechem.physio, site.sc = &quot;lc&quot;, scaling = 1, cex.char2 = 0.8, pos.env = 3, mar.percent = 0) text(-0.58, 0.64, &quot;a&quot;, cex = 2) # Scaling 2 triplot.rda(spechem.physio, site.sc = &quot;lc&quot;, scaling = 2, cex.char2 = 0.8, pos.env = 3, mult.spe = 1.1, mar.percent = 0.04) text(-3.34, 3.64, &quot;b&quot;, cex = 2) Variation partioning ## Variation partitioning with two sets of explanatory variables # Explanation of fraction labels (two, three and four explanatory # matrices) with optional colours dev.new(title = &quot;Symbols of variation partitioning fractions&quot;, width = 6, height = 2.3, noRStudioGD = TRUE) par(mfrow = c(1, 3), mar = c(1, 1, 1, 1)) showvarparts(2, bg = c(&quot;red&quot;, &quot;blue&quot;)) showvarparts(3, bg = c(&quot;red&quot;, &quot;blue&quot;, &quot;yellow&quot;)) showvarparts(4, bg = c(&quot;red&quot;, &quot;blue&quot;, &quot;yellow&quot;, &quot;green&quot;)) ## 1. Variation partitioning with all explanatory variables ## (except dfs) (spe.part.all &lt;- varpart(spe.hel, envchem, envtopo)) # Plot of the partitioning results par(mfrow = c(1, 1)) dev.new(title = &quot;Variation partitioning - all variables&quot;, noRStudioGD = TRUE) plot(spe.part.all, digits = 2, bg = c(&quot;red&quot;, &quot;blue&quot;), Xnames = c(&quot;Chemistry&quot;, &quot;Physiography&quot;), id.size = 0.7) "],
["13-2-statistik-7-demoskript-extended.html", "13.2 Statistik 7 - Demoskript extended", " 13.2 Statistik 7 - Demoskript extended Ordinationen II (c) Juergen Dengler Demoscript als Download Datensatz Doubs.RData Funktion triplot.rda.R #Interpretation von Ordinationen (Wildi pp. 96 et seq.) #Plot Arten if(!require(dave)){install.packages(&quot;dave&quot;)} library(dave) ca&lt;-cca(sveg^0.5) #Plot mit ausgewählten Arten sel.spec&lt;-c(3,11,23,31,39,46,72,77,96) snames&lt;-names(sveg[,sel.spec]) snames sx &lt;- ca$CA$v[sel.spec,1] sy &lt;- ca$CA$v[sel.spec,2] plot(ca$CA$u, asp=1) points(sx,sy,pch=16) snames &lt;- make.cepnames(snames) text(sx,sy,snames,pos=c(1,2,1,1,3,2,4,3,1),cex=0.8) # Plotte post-hoc gefittete Umweltvariablen sel.sites &lt;- c(&quot;pH.peat&quot;, &quot;Acidity.peat&quot;, &quot;CEC.peat&quot;, &quot;P.peat&quot;, &quot;Waterlev.max&quot;) ev &lt;-envfit(ca,ssit[,sel.sites]) plot(ev,add=T,cex=0.8) #Plot &quot;response surfaces&quot; in der CA plot(ca$CA$u, asp=1) ordisurf(ca,ssit$pH.peat,add=T) plot(ca$CA$u, asp=1) ordisurf(ca,ssit$Waterlev.av,add=T,col=&quot;blue&quot;) #Das gleiche für die DCA (geht nicht, da das &quot;detrending&quot; die Distanzmatrix zerstört) dca &lt;- decorana(sveg,mk=10) plot(dca$rproj, asp=1) ordisurf(dca,ssit$pH.peat,add=T) ordisurf(dca,ssit$Waterlev.av,add=T,col=&quot;blue&quot;) #Das gleiche mit NMDS mde &lt;-vegdist(sveg,method=&quot;euclidean&quot;) mmds&lt;-metaMDS(mde,k=2) if(!require(MASS)){install.packages(&quot;MASS&quot;)} library(MASS) imds&lt;-isoMDS(mde,k=2) plot(mmds$points) ordisurf(mmds,ssit$pH.peat,add=T) ordisurf(mmds,ssit$Waterlev.av,add=T,col=&quot;blue&quot;) plot(imds$points) ordisurf(imds,ssit$pH.peat,add=T) ordisurf(imds,ssit$Waterlev.av,add=T,col=&quot;blue&quot;) # Constrained ordination -------------------------------------------------- #5 Umweltvariablen gewählt, durch die die Ordination constrained werden soll ssit summary(ssit) s5&lt;-c(&quot;pH.peat&quot;,&quot;P.peat&quot;,&quot;Waterlev.av&quot;,&quot;CEC.peat&quot;,&quot;Acidity.peat&quot;) ssit5&lt;-ssit[s5] data(sveg) summary(sveg) #RDA = constrained PCA rda &lt;-rda(sveg,ssit5) plot(rda) #CCA = constrained CA cca &lt;-cca(sveg,ssit5) plot(cca) #Unconstrained and constrained variance tot &lt;- cca$tot.chi constr &lt;- cca$CCA$tot.chi constr/tot ## Mehr Details zu RDA aus Borcard et al. (Numerical ecology with R) # Doubs Datensatz in den workspace laden load(&quot;Doubs.RData&quot;) spe env spa summary(spe) summary(env) summary(spa) # Entfernen der Untersuchungsfläche ohne Arten spe &lt;- spe[-8, ] env &lt;- env[-8, ] spa &lt;- spa[-8, ] # Karten für 4 Fischarten dev.new(title = &quot;Four fish species&quot;, noRStudioGD = TRUE) par(mfrow = c(2, 2)) plot(spa, asp = 1, col = &quot;brown&quot;, cex = spe$Satr, xlab = &quot;x (km)&quot;, ylab = &quot;y (km)&quot;, main = &quot;Brown trout&quot;) lines(spa, col = &quot;light blue&quot;) plot(spa, asp = 1, col = &quot;brown&quot;, cex = spe$Thth, xlab = &quot;x (km)&quot;, ylab = &quot;y (km)&quot;, main = &quot;Grayling&quot;) lines(spa, col = &quot;light blue&quot;) plot(spa, asp = 1, col = &quot;brown&quot;, cex = spe$Alal, xlab = &quot;x (km)&quot;, ylab = &quot;y (km)&quot;, main = &quot;Bleak&quot;) lines(spa, col = &quot;light blue&quot;) plot(spa, asp = 1, col = &quot;brown&quot;, cex = spe$Titi, xlab = &quot;x (km)&quot;, ylab = &quot;y (km)&quot;, main = &quot;Tench&quot;) lines(spa, col = &quot;light blue&quot;) # Set aside the variable &#39;dfs&#39; (distance from the source) for # later use dfs &lt;- env[, 1] # Remove the &#39;dfs&#39; variable from the env data frame env2 &lt;- env[, -1] # Recode the slope variable (slo) into a factor (qualitative) # variable to show how these are handled in the ordinations slo2 &lt;- rep(&quot;.very_steep&quot;, nrow(env)) slo2[env$slo &lt;= quantile(env$slo)[4]] &lt;- &quot;.steep&quot; slo2[env$slo &lt;= quantile(env$slo)[3]] &lt;- &quot;.moderate&quot; slo2[env$slo &lt;= quantile(env$slo)[2]] &lt;- &quot;.low&quot; slo2 &lt;- factor(slo2, levels = c(&quot;.low&quot;, &quot;.moderate&quot;, &quot;.steep&quot;, &quot;.very_steep&quot;)) table(slo2) # Create an env3 data frame with slope as a qualitative variable env3 &lt;- env2 env3$slo &lt;- slo2 # Create two subsets of explanatory variables # Physiography (upstream-downstream gradient) envtopo &lt;- env2[, c(1 : 3)] names(envtopo) # Water quality envchem &lt;- env2[, c(4 : 10)] names(envchem) # Hellinger-transform the species dataset library(vegan) spe.hel &lt;- decostand(spe, &quot;hellinger&quot;) spe.hel # Redundancy analysis (RDA) ======================================= ## RDA of the Hellinger-transformed fish species data, constrained ## by all the environmental variables contained in env3 (spe.rda &lt;- rda(spe.hel ~ ., env3)) # Observe the shortcut formula summary(spe.rda) # Scaling 2 (default) # Canonical coefficients from the rda object coef(spe.rda) # Unadjusted R^2 retrieved from the rda object (R2 &lt;- RsquareAdj(spe.rda)$r.squared) # Adjusted R^2 retrieved from the rda object (R2adj &lt;- RsquareAdj(spe.rda)$adj.r.squared) ## Triplots of the rda results (lc scores) ## Site scores as linear combinations of the environmental variables dev.new(title = &quot;RDA scaling 1 and 2 + lc&quot;, width = 12, height = 6, noRStudioGD = TRUE) par(mfrow = c(1, 2)) # Scaling 1 plot(spe.rda,scaling = 1,display = c(&quot;sp&quot;, &quot;lc&quot;, &quot;cn&quot;),main = &quot;Triplot RDA spe.hel ~ env3 - scaling 1 - lc scores&quot;) spe.sc1 &lt;- scores(spe.rda, choices = 1:2, scaling = 1, display = &quot;sp&quot;) arrows(0, 0, spe.sc1[, 1] * 0.92,spe.sc1[, 2] * 0.92,length = 0, lty = 1, col = &quot;red&quot;) text(-0.75, 0.7, &quot;a&quot;, cex = 1.5) # Scaling 2 plot(spe.rda, display = c(&quot;sp&quot;, &quot;lc&quot;, &quot;cn&quot;), main = &quot;Triplot RDA spe.hel ~ env3 - scaling 2 - lc scores&quot;) spe.sc2 &lt;- scores(spe.rda, choices = 1:2, display = &quot;sp&quot;) arrows(0, 0, spe.sc2[, 1] * 0.92, spe.sc2[, 2] * 0.92,length = 0,lty = 1,col = &quot;red&quot;) text(-0.82, 0.55, &quot;b&quot;, cex = 1.5) ## Triplots of the rda results (wa scores) ## Site scores as weighted averages (vegan&#39;s default) # Scaling 1 : distance triplot dev.new(title = &quot;RDA plot&quot;, width = 12, height = 6, noRStudioGD = TRUE) par(mfrow = c(1,2)) plot(spe.rda, scaling = 1, main = &quot;Triplot RDA spe.hel ~ env3 - scaling 1 - wa scores&quot;) arrows(0, 0, spe.sc1[, 1] * 0.92, spe.sc1[, 2] * 0.92, length = 0, lty = 1, col = &quot;red&quot;) # Scaling 2 (default) : correlation triplot plot(spe.rda, main = &quot;Triplot RDA spe.hel ~ env3 - scaling 2 - wa scores&quot;) arrows(0, 0, spe.sc2[, 1] * 0.92, spe.sc2[, 2] * 0.92, length = 0, lty = 1, col = &quot;red&quot;) # Select species with goodness-of-fit at least 0.6 in the # ordination plane formed by axes 1 and 2 spe.good &lt;- goodness(spe.rda) sel.sp &lt;- which(spe.good[, 2] &gt;= 0.6) sel.sp # Triplots with homemade function triplot.rda(), scalings 1 and 2 source(&quot;triplot.rda.R&quot;) dev.new(title = &quot;RDA plot with triplot.rda&quot;, width = 12, height = 6, noRStudioGD = TRUE) par(mfrow = c(1,2)) triplot.rda(spe.rda, site.sc = &quot;lc&quot;, scaling = 1, cex.char2 = 0.7, pos.env = 3, pos.centr = 1, mult.arrow = 1.1, mar.percent = 0.05, select.spe = sel.sp) text(-0.92, 0.72, &quot;a&quot;, cex = 2) triplot.rda(spe.rda, site.sc = &quot;lc&quot;, scaling = 2, cex.char2 = 0.7, pos.env = 3, pos.centr = 1, mult.arrow = 1.1, mar.percent = 0.05, select.spe = sel.sp) text(-2.82, 2, &quot;b&quot;, cex = 2) # Global test of the RDA result anova(spe.rda, permutations = how(nperm = 999)) # Tests of all canonical axes anova(spe.rda, by = &quot;axis&quot;, permutations = how(nperm = 999)) ## Partial RDA: effect of water chemistry, holding physiography ## constant # Simple syntax; X and W may be in separate tables of quantitative # variables (spechem.physio &lt;- rda(spe.hel, envchem, envtopo)) summary(spechem.physio) # Formula interface; X and W variables must be in the same # data frame (spechem.physio2 &lt;- rda(spe.hel ~ pH + har + pho + nit + amm + oxy + bod + Condition(ele + slo + dis), data = env2)) # Test of the partial RDA, using the results with the formula # interface to allow the tests of the axes to be run anova(spechem.physio2, permutations = how(nperm = 999)) anova(spechem.physio2, permutations = how(nperm = 999), by = &quot;axis&quot;) # Partial RDA triplots (with fitted site scores) # with function triplot.rda # Scaling 1 dev.new(title = &quot;Partial RDA&quot;,width = 12, height = 6, noRStudioGD = TRUE) par(mfrow = c(1, 2)) triplot.rda(spechem.physio, site.sc = &quot;lc&quot;, scaling = 1, cex.char2 = 0.8, pos.env = 3, mar.percent = 0) text(-0.58, 0.64, &quot;a&quot;, cex = 2) # Scaling 2 triplot.rda(spechem.physio, site.sc = &quot;lc&quot;, scaling = 2, cex.char2 = 0.8, pos.env = 3, mult.spe = 1.1, mar.percent = 0.04) text(-3.34, 3.64, &quot;b&quot;, cex = 2) ######################################### ###---ab hier im Kurs nicht gezeigt---### # Variance inflation factors (VIF) in two RDAs # First RDA of this Chapter: all environmental variables # except dfs vif.cca(spe.rda) # Partial RDA physiographic variables only spechem.physio&lt;-rda(spe.hel, envchem, envtopo) vif.cca(spechem.physio) ## Forward selection of explanatory variables # RDA with all explanatory variables except dfs spe.rda.all &lt;- rda(spe.hel ~ ., data = env2) # Global adjusted R^2 (R2a.all &lt;- RsquareAdj(spe.rda.all)$adj.r.squared) # Forward selection using forward.sel() if(!require(adespatial)){install.packages(&quot;adespatial&quot;)} library(adespatial) forward.sel(spe.hel, env2, adjR2thresh = R2a.all) # Forward selection using vegan&#39;s ordistep() # This function allows the use of factors. mod0 &lt;- rda(spe.hel ~ 1, data = env2) step.forward &lt;- ordistep(mod0, scope = formula(spe.rda.all), direction = &quot;forward&quot;, permutations = how(nperm = 499) ) RsquareAdj(step.forward) # Backward elimination using vegan&#39;s ordistep() step.backward &lt;- ordistep(spe.rda.all, permutations = how(nperm = 499)) # With redundant argument direction = &quot;backward&quot;: # step.backward &lt;- # ordistep(spe.rda.all, # direction = &quot;backward&quot;, # permutations = how(nperm = 499) RsquareAdj(step.backward) # Forward selection using vegan&#39;s ordiR2step() # using a double stopping criterion (Blanchet et al. 2008a) # and object env containing only quantitative variables. step2.forward &lt;- ordiR2step(mod0, scope = formula(spe.rda.all), direction = &quot;forward&quot;, R2scope = TRUE, permutations = how(nperm = 199) ) RsquareAdj(step2.forward) # Forward selection using vegan&#39;s ordiR2step() # using a double stopping criterion (Blanchet et al. 2008a) # and object env3 containing a factor. mod00 &lt;- rda(spe.hel ~ 1, data = env3) spe.rda2.all &lt;- rda(spe.hel ~ ., data = env3) step3.forward &lt;- ordiR2step(mod00, scope = formula(spe.rda2.all), direction = &quot;forward&quot;, permutations = how(nperm = 199) ) RsquareAdj(step3.forward) # Note that the adjusted R^2 of the complete model is smaller # than that of the complete RDA with only quantitative # variables. # Some information has been lost when transforming the # quantitative slo variable into a factor with 4 levels. # Partial forward selection with variable slo held constant mod0p &lt;- rda(spe.hel ~ Condition(slo), data = env2) mod1p &lt;- rda(spe.hel ~ . + Condition(slo), data = env2) step.p.forward &lt;- ordiR2step(mod0p, scope = formula(mod1p), direction = &quot;forward&quot;, permutations = how(nperm = 199) ) ## Parsimonious RDA (spe.rda.pars &lt;- rda(spe.hel ~ ele + oxy + bod, data = env2)) anova(spe.rda.pars, permutations = how(nperm = 999)) anova(spe.rda.pars, permutations = how(nperm = 999), by = &quot;axis&quot;) (R2a.pars &lt;- RsquareAdj(spe.rda.pars)$adj.r.squared) # Compare the variance inflation factors vif.cca(spe.rda.all) vif.cca(spe.rda.pars) # Triplots of the parsimonious RDA (with fitted site scores) dev.new(title = &quot;Parsimonious RDA scaling 1&quot;, width = 7, height = 12, noRStudioGD = TRUE) par(mfrow = c(2, 1)) # Scaling 1 triplot.rda(spe.rda.pars, site.sc = &quot;lc&quot;, scaling = 1, cex.char2 = 0.8, pos.env = 2, mult.spe = 0.9, mult.arrow = 0.92, mar.percent = 0.01) # Scaling 2 triplot.rda(spe.rda.pars, site.sc = &quot;lc&quot;, scaling = 2, cex.char2 = 0.8, pos.env = 2, mult.spe = 1.1, mar.percent = -0.02) ## Environmental reconstruction (calibration) with RDA # New (fictitious) objects with fish abundances # Variables(species) must match those in the original data set in # name, number and order # New site 1 is made from rounded means of species in sites 1 to 15 site1.new &lt;- round(apply(spe[1:15, ], 2, mean)) # New site 2 is made from rounded means of species in sites 16 - 29 site2.new &lt;- round(apply(spe[16:29, ], 2, mean)) obj.new &lt;- t(cbind(site1.new, site2.new)) # Hellinger transformation of the new sites obj.new.hel &lt;- decostand(obj.new, &quot;hel&quot;) # Calibration calibrate(spe.rda.pars, obj.new.hel) # Compare with real values at sites 7 to 9 and 22 to 24: env2[7:9, c(1, 9, 10)] env2[22:24, c(1, 9, 10)] ###---bis hier im Kurs nicht gezeigt---### ########################################## # Explanation of fraction labels (two, three and four explanatory # matrices) with optional colours dev.new(title = &quot;Symbols of variation partitioning fractions&quot;, width = 6, height = 2.3, noRStudioGD = TRUE) par(mfrow = c(1, 3), mar = c(1, 1, 1, 1)) showvarparts(2, bg = c(&quot;red&quot;, &quot;blue&quot;)) showvarparts(3, bg = c(&quot;red&quot;, &quot;blue&quot;, &quot;yellow&quot;)) showvarparts(4, bg = c(&quot;red&quot;, &quot;blue&quot;, &quot;yellow&quot;, &quot;green&quot;)) ## 1. Variation partitioning with all explanatory variables ## (except dfs) (spe.part.all &lt;- varpart(spe.hel, envchem, envtopo)) # Plot of the partitioning results par(mfrow = c(1, 1)) dev.new(title = &quot;Variation partitioning - all variables&quot;, noRStudioGD = TRUE) plot(spe.part.all, digits = 2, bg = c(&quot;red&quot;, &quot;blue&quot;), Xnames = c(&quot;Chemistry&quot;, &quot;Physiography&quot;), id.size = 0.7) ######################################### ###---ab hier im Kurs nicht gezeigt---### ## 2. Variation partitioning after forward selection of explanatory ## variables # Separate forward selection in each subset of environmental # variables spe.chem &lt;- rda(spe.hel, envchem) R2a.all.chem &lt;- RsquareAdj(spe.chem)$adj.r.squared forward.sel(spe.hel, envchem, adjR2thresh = R2a.all.chem, nperm = 9999 ) spe.topo &lt;- rda(spe.hel, envtopo) R2a.all.topo &lt;- RsquareAdj(spe.topo)$adj.r.squared forward.sel(spe.hel, envtopo, adjR2thresh = R2a.all.topo, nperm = 9999 ) # Parsimonious subsets of explanatory variables, based on forward # selections names(envchem) envchem.pars &lt;- envchem[, c(4, 6, 7)] names(envtopo) envtopo.pars &lt;- envtopo[, c(1, 2)] # Variation partitioning (spe.part &lt;- varpart(spe.hel, envchem.pars, envtopo.pars)) dev.new(title = &quot;Variation partitioning - parsimonious subsets&quot;, noRStudioGD = TRUE) plot(spe.part, digits = 2, bg = c(&quot;red&quot;, &quot;blue&quot;), Xnames = c(&quot;Chemistry&quot;, &quot;Physiography&quot;), id.size = 0.7) # Tests of all testable fractions # Test of fraction [a+b] anova(rda(spe.hel, envchem.pars), permutations = how(nperm = 999)) # Test of fraction [b+c] anova(rda(spe.hel, envtopo.pars), permutations = how(nperm = 999)) # Test of fraction [a+b+c] env.pars &lt;- cbind(envchem.pars, envtopo.pars) anova(rda(spe.hel, env.pars), permutations = how(nperm = 999)) # Test of fraction [a] anova(rda(spe.hel, envchem.pars, envtopo.pars), permutations = how(nperm = 999) ) # Test of fraction [c] anova(rda(spe.hel, envtopo.pars, envchem.pars), permutations = how(nperm = 999)) ## 3. Variation partitioning without the &#39;nit&#39; variable envchem.pars2 &lt;- envchem[, c(6, 7)] (spe.part2 &lt;- varpart(spe.hel, envchem.pars2, envtopo.pars)) dev.new(title = &quot;Variation partitioning - parsimonious subset 2&quot;, noRStudioGD = TRUE) plot(spe.part2, digits = 2) ## Two-way MANOVA by RDA # Creation of a factor &#39;elevation&#39; (3 levels, 9 sites each) ele.fac &lt;- gl(3, 9, labels = c(&quot;high&quot;, &quot;mid&quot;, &quot;low&quot;)) # Creation of a factor mimicking &#39;pH&#39; pH.fac &lt;- as.factor(c(1, 2, 3, 2, 3, 1, 3, 2, 1, 2, 1, 3, 3, 2, 1, 1, 2, 3, 2, 1, 2, 3, 2, 1, 1, 3, 3)) # Is the two-way factorial design balanced? table(ele.fac, pH.fac) # Creation of Helmert contrasts for the factors and the interaction ele.pH.helm &lt;- model.matrix(~ ele.fac * pH.fac, contrasts = list(ele.fac = &quot;contr.helmert&quot;, pH.fac = &quot;contr.helmert&quot;))[, -1] ele.pH.helm ele.pH.helm2 &lt;- model.matrix(~ ele.fac + pH.fac, contrasts = list(ele.fac = &quot;contr.helmert&quot;, pH.fac = &quot;contr.helmert&quot;))[, -1] colnames(ele.pH.helm2) # Check property 1 of Helmert contrasts : all variables sum to 0 apply(ele.pH.helm, 2, sum) # Check property 2 of Helmert contrasts: their crossproducts # must be 0 within and between groups (factors and interaction) crossprod(ele.pH.helm) # Verify multivariate homogeneity of within-group covariance # matrices using the betadisper() function (vegan package) # implementing Marti Anderson&#39;s testing method (Anderson 2006) # To avoid the rist of heterogeneity of variances with respect to # one factor because of the dispersion in the other (in case of # interaction), creation of a factor crossing the two factors, i.e. # defining the cell-by-cell attribution of the data cell.fac &lt;- gl(9, 3) spe.hel.d1 &lt;- dist(spe.hel[1:27, ]) # Test of homogeneity of within-cell dispersions (spe.hel.cell.MHV &lt;- betadisper(spe.hel.d1, cell.fac)) anova(spe.hel.cell.MHV) # Parametric test (not recommended here) permutest(spe.hel.cell.MHV) # Alternatively, test homogeneity of dispersions within each # factor. # These tests ore more robust with this small example because # there are now 9 observations per group instead of 3. # Factor &quot;elevation&quot; (spe.hel.ele.MHV &lt;- betadisper(spe.hel.d1, ele.fac)) anova(spe.hel.ele.MHV) # Parametric test (not recommended here) permutest(spe.hel.ele.MHV) # Permutation test # Factor &quot;pH&quot; (spe.hel.pH.MHV &lt;- betadisper(spe.hel.d1, pH.fac)) anova(spe.hel.pH.MHV) permutest(spe.hel.pH.MHV) # Permutation test ## Step-by-step procedure using function rda() # Test the interaction first. Factors ele and pH (columns 1-4) # are assembled to form the matrix of covariables for the test. interaction.rda &lt;- rda(spe.hel[1:27, ], ele.pH.helm[, 5:8], ele.pH.helm[, 1:4]) anova(interaction.rda, permutations = how(nperm = 999)) # Test the main factor ele. The factor pH and the interaction # are assembled to form the matrix of covariables. factor.ele.rda &lt;- rda(spe.hel[1:27, ], ele.pH.helm[, 1:2], ele.pH.helm[, 3:8]) anova(factor.ele.rda, permutations = how(nperm = 999), strata = pH.fac ) # Test the main factor pH. The factor ele and the interaction # are assembled to form the matrix of covariables. factor.pH.rda &lt;- rda(spe.hel[1:27, ], ele.pH.helm[, 3:4], ele.pH.helm[, c(1:2, 5:8)]) anova(factor.pH.rda, permutations = how(nperm = 999), strata = ele.fac) # RDA with the significant factor ele ele.rda.out &lt;- rda(spe.hel[1:27, ]~ ., as.data.frame(ele.fac)) # Triplot with &quot;wa&quot; sites related to factor centroids, and species # arrows dev.new(title = &quot;Multivariate ANOVA - elevation&quot;, noRStudioGD = TRUE) plot(ele.rda.out, scaling = 1, display = &quot;wa&quot;, main = &quot;Multivariate ANOVA, factor elevation - scaling 1 - wa scores&quot;) ordispider(ele.rda.out, ele.fac, scaling = 1, label = TRUE, col = &quot;blue&quot; ) spe.sc1 &lt;- scores(ele.rda.out, scaling = 1, display = &quot;species&quot;) arrows(0, 0, spe.sc1[, 1] * 0.3, spe.sc1[, 2] * 0.3, length = 0.1, angle = 10, col = &quot;red&quot; ) text( spe.sc1[, 1] * 0.3, spe.sc1[, 2] * 0.3, labels = rownames(spe.sc1), pos = 4, cex = 0.8, col = &quot;red&quot; ) ## Permutational MANOVA using adonis2() adonis2(spe.hel[1:27, ] ~ ele.fac * pH.fac, method = &quot;euc&quot;, by = &quot;term&quot; ) ## RDA with a single second degree explanatory variable # Create a matrix of dfs and its orthogonal second degree term # using function poly() dfs.df &lt;- poly(dfs, 2) colnames(dfs.df) &lt;- c(&quot;dfs&quot;, &quot;dfs2&quot;) # Verify that the polynomial terms are orthogonal cor(dfs.df) # Find out if both variables are significant forward.sel(spe.hel, dfs.df) # RDA and test spe.dfs.rda &lt;- rda(spe.hel ~ ., as.data.frame(dfs.df)) anova(spe.dfs.rda) # Triplot using &quot;lc&quot; (model) site scores and scaling 2 dev.new(title = &quot;RDA w. 2nd-degree variable - scaling 2&quot;, noRStudioGD = TRUE) triplot.rda(spe.dfs.rda, site.sc = &quot;lc&quot;, scaling = 2, plot.sites = FALSE, pos.env = 1, mult.arrow = 0.9, move.origin = c(-0.25, 0), mar.percent = 0) ## Polynomial RDA, second degree, with forward selection source (&#39;https://raw.githubusercontent.com/zdealveindy/anadat-r/master/scripts/NumEcolR2/polyvars.R&#39;) ## among all environmental variables env.square &lt;- polyvars(env2, degr = 2) names(env.square) spe.envsq.rda &lt;- rda(spe.hel ~ ., env.square) R2ad &lt;- RsquareAdj(spe.envsq.rda)$adj.r.squared spe.envsq.fwd &lt;- forward.sel(spe.hel, env.square, adjR2thresh = R2ad) spe.envsq.fwd envsquare.red &lt;- env.square[, sort(spe.envsq.fwd$order)] (spe.envsq.fwd.rda &lt;- rda(spe.hel ~., envsquare.red)) RsquareAdj(spe.envsq.fwd.rda) summary(spe.envsq.fwd.rda) # Triplot using lc (model) site scores and scaling 2 dev.new(title = &quot;RDA w. 2nd-order variables - scaling 2&quot;, noRStudioGD = TRUE) triplot.rda(spe.envsq.fwd.rda, site.sc = &quot;lc&quot;, scaling = 2, plot.sites = FALSE, pos.env = 1, mult.arrow = 0.9, mult.spe = 0.9, mar.percent = 0) ## Distance-based redundancy analysis (db-RDA) : square-rooted ## percentage difference response matrix and &#39;ele&#39; factor with ## factor pH and interaction as covariables # Rename columns of matrix of Helmert contrasts (for convenience) colnames(ele.pH.helm) &lt;- c(&quot;ele1&quot;, &quot;ele2&quot;, &quot;pH1&quot;, &quot;pH2&quot;, &quot;ele1pH1&quot;, &quot;ele1pH2&quot;, &quot;ele2pH1&quot;, &quot;ele2pH2&quot; ) # Create the matrix of covariables. MUST be of class matrix, # NOT data.frame covariables &lt;- ele.pH.helm[, 3:8] # Compute the dissimilarity response matrix with vegans vegdist() spe.bray27 &lt;- vegdist(spe[1:27, ], &quot;bray&quot;) # or with function dist.ldc() of adespatial spe.bray27 &lt;- dist.ldc(spe[1:27, ], &quot;percentdiff&quot;) # 1. dbrda() on the square-rooted dissimilarity matrix bray.ele.dbrda &lt;- dbrda( sqrt(spe.bray27) ~ ele.pH.helm[, 1:2] + Condition(covariables)) anova(bray.ele.dbrda, permutations = how(nperm = 999)) # 2. capscale() with raw (site by species) data # Rename factor (cosmetic for plot) ele.fac. &lt;- ele.fac bray.env.cap &lt;- capscale(spe[1:27, ] ~ ele.fac. + Condition(covariables), data = as.data.frame(ele.pH.helm), distance = &quot;bray&quot;, add = &quot;lingoes&quot;, comm = spe[1:27, ]) anova(bray.env.cap, permutations = how(nperm = 999)) # Plot with &quot;wa&quot; scores to see dispersion of sites around the # factor levels triplot.rda(bray.env.cap, site.sc = &quot;wa&quot;, scaling = 1) # The results of the two analyses are slightly different because # (1) the test is not performed in the same manner and (2) the # correction to make the response matrix Euclidean is not the same. ### NOT IN THE BOOK ### ## Alternative ways of computing db-RDA # 1. capscale() with raw (site by species) data # Alternate coding with explicit covariables coming from same # object as the constraining variables : bray.env.capscale &lt;- capscale(spe[1:27, ] ~ ele1 + ele2 + Condition(pH1 + pH2 + ele1pH1 + ele1pH2 + ele2pH1 + ele2pH2), data = as.data.frame(ele.pH.helm), distance = &quot;bray&quot;, add = &quot;cailliez&quot;, comm = spe[1:27, ]) anova(bray.env.capscale, permutations = how(nperm = 999)) # 2. PCoA with Lingoes (1971) correction # Explicit steps if(!require(ape)){install.packages(&quot;ape&quot;)} library(ape) spe.bray27.lin &lt;- pcoa(spe.bray27, correction = &quot;lingoes&quot;) spe.bray27.lingoes &lt;- spe.bray27.lin$vectors.cor # Test of the factor ele. Factor pH and interaction, Helmert-coded, # form the matrix of covariables spe.L.ele.dbrda &lt;- rda(spe.bray27.lingoes, ele.pH.helm[, 1:2], covariables) anova(spe.L.ele.dbrda, permutations = how(nperm = 999)) # Same by staying in {vegan} and using wcmdscale() : spe.lingoes2 &lt;- wcmdscale(spe.bray27, add = &quot;lingoes&quot;) anova(rda(spe.lingoes2 ~ ele.pH.helm[, 1:2] + Condition(covariables))) ### END NOT IN THE BOOK ### # ----------------------------------------------------------------- # The Code It Yourself corner #3 myRDA &lt;- function(Y, X) { ## 1. Preparation of the data Y.mat &lt;- as.matrix(Y) Yc &lt;- scale(Y.mat, scale = FALSE) X.mat &lt;- as.matrix(X) Xcr &lt;- scale(X.mat) # Dimensions n &lt;- nrow(Y) p &lt;- ncol(Y) m &lt;- ncol(X) ## 2. Computation of the multivariate linear regression # Matrix of regression coefficients (eq. 11.9) B &lt;- solve(t(Xcr) %*% Xcr) %*% t(Xcr) %*% Yc # Matrix of fitted values (eq. 11.10) Yhat &lt;- Xcr %*% B # Matrix of residuals Yres &lt;- Yc - Yhat ## 3. PCA on fitted values # Covariance matrix (eq. 11.12) S &lt;- cov(Yhat) # Eigenvalue decomposition eigenS &lt;- eigen(S) # How many canonical axes? kc &lt;- length(which(eigenS$values &gt; 0.00000001)) # Eigenvalues of canonical axes ev &lt;- eigenS$values[1 : kc] # Total variance (inertia) of the centred matrix Yc trace = sum(diag(cov(Yc))) # Orthonormal eigenvectors (contributions of response # variables, scaling 1) U &lt;- eigenS$vectors[, 1 : kc] row.names(U) &lt;- colnames(Y) # Site scores (vegan&#39;s wa scores, scaling 1; eq.11.17) F &lt;- Yc %*% U row.names(F) &lt;- row.names(Y) # Site constraints (vegan&#39;s &#39;lc&#39; scores, scaling 1; # eq. 11.18) Z &lt;- Yhat %*% U row.names(Z) &lt;- row.names(Y) # Canonical coefficients (eq. 11.19) CC &lt;- B %*% U row.names(CC) &lt;- colnames(X) # Explanatory variables # Species-environment correlations corXZ &lt;- cor(X, Z) # Diagonal matrix of weights D &lt;- diag(sqrt(ev / trace)) # Biplot scores of explanatory variables coordX &lt;- corXZ %*% D # Scaling 1 coordX2 &lt;- corXZ # Scaling 2 row.names(coordX) &lt;- colnames(X) row.names(coordX2) &lt;- colnames(X) # Scaling to sqrt of the relative eigenvalue # (for scaling 2) U2 &lt;- U %*% diag(sqrt(ev)) row.names(U2) &lt;- colnames(Y) F2 &lt;- F %*% diag(1/sqrt(ev)) row.names(F2) &lt;- row.names(Y) Z2 &lt;- Z %*% diag(1/sqrt(ev)) row.names(Z2) &lt;- row.names(Y) # Unadjusted R2 R2 &lt;- sum(ev/trace) # Adjusted R2 R2a &lt;- 1 - ((n - 1)/(n - m - 1)) * (1 - R2) # 4. PCA on residuals # Write your own code as in Chapter 5. It could begin # with : # eigenSres &lt;- eigen(cov(Yres)) # evr &lt;- eigenSres$values # 5. Output result &lt;- list(trace, R2, R2a, ev, CC, U, F, Z, coordX, U2, F2, Z2, coordX2) names(result) &lt;- c(&quot;Total_variance&quot;, &quot;R2&quot;, &quot;R2adj&quot;, &quot;Can_ev&quot;, &quot;Can_coeff&quot;, &quot;Species_sc1&quot;, &quot;wa_sc1&quot;, &quot;lc_sc1&quot;, &quot;Biplot_sc1&quot;, &quot;Species_sc2&quot;, &quot;wa_sc2&quot;, &quot;lc_sc2&quot;, &quot;Biplot_sc2&quot;) result } doubs.myRDA &lt;- myRDA(spe.hel, env2) summary(doubs.myRDA) # Retrieve adjusted R-square doubs.myRDA$R2adj ################################################################### # Canonical correspondence analysis (CCA) ========================= ## CCA of the raw fish species data, constrained by all the ## environmental variables in env3 (spe.cca &lt;- cca(spe ~ ., env3)) summary(spe.cca) # Scaling 2 (default) # Unadjusted and adjusted R^2 - like statistics RsquareAdj(spe.cca) ## CCA triplots (using lc site scores) dev.new(title = &quot;CCA triplot - lc scores&quot;, width = 6, height = 12, noRStudioGD = TRUE) par(mfrow = c(2, 1)) # Scaling 1: species scores scaled to the relative eigenvalues, # sites are weighted averages of the species plot(spe.cca, scaling = 1, display = c(&quot;sp&quot;, &quot;lc&quot;, &quot;cn&quot;), main = &quot;Triplot CCA spe ~ env3 - scaling 1&quot; ) text(-2.3, 4.1, &quot;a&quot;, cex = 1.5) # Default scaling 2: site scores scaled to the relative # eigenvalues, species are weighted averages of the sites plot(spe.cca, display = c(&quot;sp&quot;, &quot;lc&quot;, &quot;cn&quot;), main = &quot;Triplot CCA spe ~ env3 - scaling 2&quot;) text(-2.3, 2.6, &quot;b&quot;, cex = 1.5) dev.new(title = &quot;CCA biplot - without species&quot;, width = 9, height = 5, noRStudioGD = TRUE) par(mfrow = c(1, 2)) # CCA scaling 1 biplot without species (using lc site scores) plot(spe.cca, scaling = 1, display = c(&quot;lc&quot;, &quot;cn&quot;), main = &quot;Biplot CCA spe ~ env3 - scaling 1&quot; ) # CCA scaling 2 biplot with species but without sites plot(spe.cca, scaling = 2, display = c(&quot;sp&quot;, &quot;cn&quot;), main = &quot;Biplot CCA spe ~ env3 - scaling 2&quot; ) # Permutation test of the overall analysis anova(spe.cca, permutations = how(nperm = 999)) # Permutation test of each axis anova(spe.cca, by = &quot;axis&quot;, permutations = how(nperm = 999)) ## CCA-based forward selection using vegan&#39;s ordistep() # This function allows the use of factors like &#39;slo&#39; in env3 cca.step.forward &lt;- ordistep(cca(spe ~ 1, data = env3), scope = formula(spe.cca), direction = &quot;forward&quot;, permutations = how(nperm = 199)) ## Parsimonious CCA using ele, oxy and bod spe.cca.pars &lt;- cca(spe ~ ele + oxy + bod, data = env3) anova(spe.cca.pars, permutations = how(nperm = 999)) anova(spe.cca.pars, permutations = how(nperm = 999), by = &quot;axis&quot;) # R-square like statistics RsquareAdj(spe.cca.pars) # Compare variance inflation factors vif.cca(spe.cca) vif.cca(spe.cca.pars) # ----------------------------------------------------------------- ## A simple function to compute CCA-based variation partitioning ## with bootstrap adjusted R-square # This function is limited to two explanatory matrices varpart.cca &lt;- function(Y, X, W) { # Computation of the three CCA yx.cca &lt;- cca(Y, X) yw.cca &lt;- cca(Y, W) yxw.cca &lt;- cca(Y, cbind(X, W)) # Computation of adjusted inertia fract.ab &lt;- RsquareAdj(yx.cca)$adj.r.squared fract.bc &lt;- RsquareAdj(yw.cca)$adj.r.squared fract.abc &lt;- RsquareAdj(yxw.cca)$adj.r.squared fract.a &lt;- fract.abc-fract.bc fract.b &lt;- fract.ab-fract.a fract.c &lt;- fract.abc-fract.ab fract.d &lt;- 1-fract.abc # Output of results res &lt;- matrix(0, 7, 1) rownames(res) &lt;- c(&quot;[ab]&quot;, &quot;[bc]&quot;, &quot;[abc]&quot;, &quot;[a]&quot;, &quot;[b]&quot;, &quot;[c]&quot;, &quot;[d]&quot;) colnames(res) &lt;- &quot;Value&quot; res[1, 1] &lt;- round(fract.ab, 4) res[2, 1] &lt;- round(fract.bc, 4) res[3, 1] &lt;- round(fract.abc, 4) res[4, 1] &lt;- round(fract.a, 4) res[5, 1] &lt;- round(fract.b, 4) res[6, 1] &lt;- round(fract.c, 4) res[7, 1] &lt;- round(fract.d, 4) res } # ----------------------------------------------------------------- ## Three-dimensional interactive ordination plots ## (requires vegan3d package) if(!require(vegan3d)){install.packages(&quot;vegan3d&quot;)} library(vegan3d) # Plot of the sites only (wa scores) ordirgl(spe.cca.pars, type = &quot;t&quot;, scaling = 1) # Connect weighted average scores to linear combination scores orglspider(spe.cca.pars, scaling = 1, col = &quot;purple&quot;) # Plot the sites (wa scores) with a clustering result # Colour sites according to cluster membership gr &lt;- cutree(hclust(vegdist(spe.hel, &quot;euc&quot;), &quot;ward.D2&quot;), 4) ordirgl(spe.cca.pars, type = &quot;t&quot;, scaling = 1, ax.col = &quot;black&quot;, col = gr + 1 ) # Connect sites to cluster centroids orglspider(spe.cca.pars, gr, scaling = 1) # Complete CCA 3D triplot ordirgl(spe.cca.pars, type = &quot;t&quot;, scaling = 2) orgltext(spe.cca.pars, display = &quot;species&quot;, type = &quot;t&quot;, scaling = 2, col = &quot;cyan&quot; ) # Plot species groups (Jaccard dissimilarity, useable in R mode) gs &lt;- cutree( hclust(vegdist(t(spe), method = &quot;jaccard&quot;), &quot;ward.D2&quot;), k = 4) ordirgl(spe.cca.pars, display = &quot;species&quot;, type = &quot;t&quot;, col = gs + 1) ## Linear discriminant analysis (LDA) # Ward clustering result of Hellinger-transformed species data, # cut into 4 groups gr &lt;- cutree(hclust(vegdist(spe.hel, &quot;euc&quot;), &quot;ward.D2&quot;), k = 4) # Environmental matrix with only 3 variables (ele, oxy and bod) env.pars2 &lt;- as.matrix(env2[, c(1, 9, 10)]) # Verify multivariate homogeneity of within-group covariance # matrices using the betadisper() function {vegan} env.pars2.d1 &lt;- dist(env.pars2) (env.MHV &lt;- betadisper(env.pars2.d1, gr)) anova(env.MHV) permutest(env.MHV) # Permutational test # Log transform ele and bod env.pars3 &lt;- cbind(log(env2$ele), env2$oxy, log(env2$bod)) colnames(env.pars3) &lt;- c(&quot;ele.ln&quot;, &quot;oxy&quot;, &quot;bod.ln&quot;) rownames(env.pars3) &lt;- rownames(env2) env.pars3.d1 &lt;- dist(env.pars3) (env.MHV2 &lt;- betadisper(env.pars3.d1, gr)) permutest(env.MHV2) # Preliminary test : do the means of the explanatory variable # differ among groups? # Compute Wilk&#39;S lambda test # First way: with function Wilks.test() of package rrcov, Ï2 test Wilks.test(env.pars3, gr) # Second way: with function manova() of stats, which uses # an F-test approximation lw &lt;- manova(env.pars3 ~ as.factor(gr)) summary(lw, test = &quot;Wilks&quot;) ## Computation of LDA - identification functions (on unstandardized ## variables) env.pars3.df &lt;- as.data.frame(env.pars3) (spe.lda &lt;- lda(gr ~ ele.ln + oxy + bod.ln, data = env.pars3.df)) # Alternate coding without formula interface : # spe.lda &lt;- lda(env.pars3.df, gr) # The result object contains the information necessary to interpret # the LDA summary(spe.lda) # Display the group means for the 3 variables spe.lda$means # Extract the unstandardized identification functions (matrix C, # eq. 11.33 in Legendre and Legendre 2012) (C &lt;- spe.lda$scaling) # Classification of two new objects (identification) # A new object is created with two sites: # (1) ln(ele) = 6.8, oxygen = 9 and ln(bod) = 0.8 # and (2) ln(ele) = 5.5, oxygen = 10 and ln(bod) = 1.0 newo &lt;- data.frame(c(6.8, 5.5), c(9, 10), c(0.8, 1)) colnames(newo) &lt;- colnames(env.pars3) newo (predict.new &lt;- predict(spe.lda, newdata = newo)) ## Computation of LDA - discrimination functions (on standardized ## variables) env.pars3.sc &lt;- as.data.frame(scale(env.pars3.df)) spe.lda2 &lt;- lda(gr ~ ., data = env.pars3.sc) # Display the group means for the 3 variables spe.lda2$means # Extract the classification functions (C2 &lt;- spe.lda2$scaling) # Compute the canonical eigenvalues spe.lda2$svd^2 # Position the objects in the space of the canonical variates (Fp2 &lt;- predict(spe.lda2)$x) # alternative way : Fp2 &lt;- as.matrix(env.pars3.sc) %*% C2 # Classification of the objects (spe.class2 &lt;- predict(spe.lda2)$class) # Posterior probabilities of the objects to belong to the groups # (rounded for easier interpretation) (spe.post2 &lt;- round(predict(spe.lda2)$posterior, 2)) # Contingency table of prior versus predicted classifications (spe.table2 &lt;- table(gr, spe.class2)) # Proportion of correct classification (classification success) diag(prop.table(spe.table2, 1)) # Plot the LDA results using the homemade function plot.lda() source(&#39;https://raw.githubusercontent.com/zdealveindy/anadat-r/master/scripts/NumEcolR2/plot.lda.R&#39;) if(!require(ellipse)){install.packages(&quot;ellipse&quot;)} library(ellipse) dev.new(title = &quot;Discriminant analysis&quot;, noRStudioGD = TRUE) plot.lda(lda.out = spe.lda2, groups = gr, plot.sites = 2, plot.centroids = 1, mul.coef = 2.35) # LDA with jackknife-based classification (i.e., leave-one-out # cross-validation) (spe.lda.jac &lt;- lda(gr ~ ele.ln + oxy + bod.ln, data = env.pars3.sc, CV = TRUE)) summary(spe.lda.jac) # Numbers and proportions of correct classification spe.jac.class &lt;- spe.lda.jac$class spe.jac.table &lt;- table(gr, spe.jac.class) # Classification success diag(prop.table(spe.jac.table, 1)) # NOT IN THE BOOK : ============================================== # Example of Legendre and Legendre (2012, p. 683) grY &lt;- c(1, 1, 1, 2, 2, 3, 3) x1 &lt;- c(1, 2, 2, 8, 8, 8, 9) x2 &lt;- c(2, 2, 1, 7, 6, 3, 3) X &lt;- as.data.frame(cbind(x1, x2)) # Computation of unstandardized identification functions unstand.lda &lt;- lda(grY ~ ., data = X) # Computation of standardized discriminant functions X.sc &lt;- as.data.frame(scale(X)) stand.lda &lt;- lda(grY ~ ., data = X.sc) # END NOT IN THE BOOK ============================================= ## Principal response curves (PRC) # Code from the prc() help file, with additional comments # Chlorpyrifos experiment and experimental design : Pesticide # treatment in ditches (replicated) and followed over from 4 weeks # before to 24 weeks after exposure # Extract the data (available in vegan) data(pyrifos) # Create factors for time (week) and treatment (dose). Create an # additional factor &quot;ditch&quot; representing the mesocosm, for testing # purposes week &lt;- gl(11, 12, labels = c(-4, -1, 0.1, 1, 2, 4, 8, 12, 15, 19, 24)) dose &lt;- factor(rep(c(0.1, 0, 0, 0.9, 0, 44, 6, 0.1, 44, 0.9, 0, 6), 11)) ditch &lt;- gl(12, 1, length = 132) # PRC mod &lt;- prc(pyrifos, dose, week) mod # Modified RDA summary(mod) # Results formatted as PRC # PRC plot; at the right of it, only species with large total # (log-transformed) abundances are reported logabu &lt;- colSums(pyrifos) dev.new(title = &quot;PRC&quot;, noRStudioGD = TRUE) plot(mod, select = logabu &gt; 200,main = &quot;PRC&quot;) # Statistical test # Ditches are randomized, we have a time series, and are only # interested in the first axis ctrl &lt;- how(plots = Plots(strata = ditch, type = &quot;free&quot;), within = Within(type = &quot;series&quot;), nperm = 999) anova(mod, permutations = ctrl, first = TRUE) ## Predictive co-correspondence analysis (CoCA) if(!require(cocorresp)){install.packages(&quot;cocorresp&quot;)} library(cocorresp) data(bryophyte) data(vascular) # Co-correspondence analysis is computed using the function coca() # The default option is method = &quot;predictive&quot; (carp.pred &lt;- coca(bryophyte ~ ., data = vascular)) # Leave-one-out cross-validation crossval(bryophyte, vascular) # Permutation test (carp.perm &lt;- permutest(carp.pred, permutations = 99)) # Only two significant axes: refit (carp.pred &lt;- coca(bryophyte ~ ., data = vascular, n.axes = 2)) # Extract the site scores and the species loadings used in # the biplots carp.scores &lt;- scores(carp.pred) load.bryo &lt;- carp.pred$loadings$Y load.plant &lt;- carp.pred$loadings$X # We have generated two plots. As in ter Braak and Schaffers # (2004, Fig. 3), in both plots the site scores are derived # from the vascular plants (carp.scores$sites$X) and the # species scores are the &quot;loadings with respect to # normalized site scores&quot; # Printing options: ?plot.predcoca dev.new(title = &quot;Predictive co-correspondence analysis (CoCA)&quot;, width = 12, height = 6, noRStudioGD = TRUE) par(mfrow = c(1, 2)) plot(carp.pred, type = &quot;none&quot;, main = &quot;Bryophytes&quot;, xlim = c(-2, 3), ylim = c(-3, 2)) points(carp.scores$sites$X, pch = 16, cex = 0.5) text(load.bryo, labels = rownames(load.bryo), cex = 0.7, col = &quot;red&quot; ) plot(carp.pred, type = &quot;none&quot;, main = &quot;Vascular plants&quot;, xlim = c(-2, 3), ylim = c(-3, 2) ) points(carp.scores$sites$X, pch = 16, cex = 0.5) text(load.plant, labels = rownames(load.plant), cex = 0.7, col = &quot;blue&quot; ) # Detach package cocorresp to avoid conflicts with ade4: detach(&quot;package:cocorresp&quot;, unload = TRUE) # If not sufficient: unloadNamespace(&quot;cocorresp&quot;) ## Canonical correlation analysis (CCorA) # Preparation of data (transformations to make variable # distributions approximately symmetrical) envchem2 &lt;- envchem envchem2$pho &lt;- log(envchem$pho) envchem2$nit &lt;- sqrt(envchem$nit) envchem2$amm &lt;- log1p(envchem$amm) envchem2$bod &lt;- log(envchem$bod) envtopo2 &lt;- envtopo envtopo2$ele &lt;- log(envtopo$ele) envtopo2$slo &lt;- log(envtopo$slo) envtopo2$dis &lt;- sqrt(envtopo$dis) # CCorA (on standardized variables) chem.topo.ccora &lt;- CCorA(envchem2, envtopo2, stand.Y = TRUE, stand.X = TRUE, permutations = how(nperm = 999)) chem.topo.ccora dev.new(title = &quot;Canonical correlation analysis (CCorA)&quot;, width = 9, height = 6, noRStudioGD = TRUE) biplot(chem.topo.ccora, plot.type = &quot;biplot&quot;) ## Co-inertia analysis # PCA on both matrices using ade4 functions if(!require(ade4)){install.packages(&quot;ade4&quot;)} library(ade4) dudi.chem &lt;- dudi.pca(envchem2, scale = TRUE, scannf = FALSE) dudi.topo &lt;- dudi.pca(envtopo2, scale = TRUE, scannf = FALSE) # Cumulated relative variation of eigenvalues cumsum(dudi.chem$eig / sum(dudi.chem$eig)) # Cumulated relative variation of eigenvalues cumsum(dudi.topo$eig / sum(dudi.topo$eig)) # Are the row weights equal in the 2 analyses? all.equal(dudi.chem$lw, dudi.topo$lw) # Co-inertia analysis coia.chem.topo &lt;- coinertia(dudi.chem, dudi.topo, scannf = FALSE, nf = 2) summary(coia.chem.topo) # Relative variation on first eigenvalue coia.chem.topo$eig[1] / sum(coia.chem.topo$eig) # Permutation test randtest(coia.chem.topo, nrepet = 999) # Plot results dev.new(title = &quot;Co-inertia analysis&quot;, noRStudioGD = TRUE) plot(coia.chem.topo, main = &quot;Co-inertia analysis&quot;) # Multiple factor analysis (MFA) ================================== # MFA on 3 groups of variables : # Regroup the 3 tables (Hellinger-transformed species, # physiographic variables, chemical variables) tab3 &lt;- data.frame(spe.hel, envtopo, envchem) dim(tab3) # Number of variables in each group (grn &lt;- c(ncol(spe), ncol(envtopo), ncol(envchem))) # Close the previous graphic windows graphics.off() # Compute the MFA without multiple plots if(!require(FactoMineR)){install.packages(&quot;FactoMineR&quot;)} library(FactoMineR) t3.mfa &lt;- MFA( tab3, group = grn, type = c(&quot;c&quot;, &quot;s&quot;, &quot;s&quot;), ncp = 2, name.group = c(&quot;Fish community&quot;, &quot;Physiography&quot;, &quot;Water quality&quot;), graph = FALSE ) t3.mfa summary(t3.mfa) t3.mfa$ind # Plot the results dev.new(title = &quot;Partial axes&quot;, noRStudioGD = TRUE) plot(t3.mfa, choix = &quot;axes&quot;, habillage = &quot;group&quot;, shadowtext = TRUE) dev.new(title = &quot;Quantitative variables&quot;, noRStudioGD = TRUE) plot( t3.mfa, choix = &quot;ind&quot;, partial = &quot;all&quot;, habillage = &quot;group&quot;) dev.new(title = &quot;Sites&quot;, noRStudioGD = TRUE) plot(t3.mfa, choix = &quot;var&quot;, habillage = &quot;group&quot;, shadowtext = TRUE) dev.new(title = &quot;Groups&quot;, noRStudioGD = TRUE) plot(t3.mfa, choix = &quot;group&quot;) # RV coefficients with tests (p-values above the diagonal of # the matrix) rvp &lt;- t3.mfa$group$RV rvp[1, 2] &lt;- coeffRV(spe.hel, scale(envtopo))$p.value rvp[1, 3] &lt;- coeffRV(spe.hel, scale(envchem))$p.value rvp[2, 3] &lt;- coeffRV(scale(envtopo), scale(envchem))$p.value round(rvp[-4, -4], 6) # Eigenvalues, scree plot and broken stick model ev &lt;- t3.mfa$eig[, 1] names(ev) &lt;- paste(&quot;MFA&quot;, 1 : length(ev)) source(&#39;https://raw.githubusercontent.com/zdealveindy/anadat-r/master/scripts/NumEcolR2/screestick.R&#39;) dev.new(title = &quot;MFA eigenvalues and broken stick model&quot;, noRStudioGD = TRUE) screestick(ev, las = 2) # Alternative to the standard, automatic MFA plots : # Plot only the significant variables (correlations) # Select the most characteristic variables aa &lt;- dimdesc(t3.mfa, axes = 1:2, proba = 0.0001) # Plot dev.new(title = &quot;MFA : correlations among significant variables&quot;, noRStudioGD = TRUE) varsig &lt;- t3.mfa$quanti.var$cor[unique(c(rownames(aa$Dim.1$quanti), rownames(aa$Dim.2$quanti))), ] plot(varsig[, 1:2], asp = 1, type = &quot;n&quot;, xlim = c(-1, 1), ylim = c(-1, 1) ) abline(h = 0, lty = 3) abline(v = 0, lty = 3) symbols(0, 0, circles = 1, inches = FALSE, add = TRUE) arrows(0, 0, varsig[, 1], varsig[, 2], length = 0.08, angle = 20) for (v in 1 : nrow(varsig)) { if (abs(varsig[v, 1]) &gt; abs(varsig[v, 2])) { if (varsig[v, 1] &gt;= 0) pos &lt;- 4 else pos &lt;- 2 } else { if (varsig[v, 2] &gt;= 0) pos &lt;- 3 else pos &lt;- 1 } text(varsig[v, 1], varsig[v, 2], labels = rownames(varsig)[v], pos = pos) } ## RLQ and fourth-corner analyses data(aravo) dim(aravo$spe) dim(aravo$traits) dim(aravo$env) # Preliminary analyses: CA, Hill-Smith and PCA afcL.aravo &lt;- dudi.coa(aravo$spe, scannf = FALSE) acpR.aravo &lt;- dudi.hillsmith(aravo$env, row.w = afcL.aravo$lw, scannf = FALSE) acpQ.aravo &lt;- dudi.pca(aravo$traits, row.w = afcL.aravo$cw, scannf = FALSE) # RLQ analysis rlq.aravo &lt;- rlq( dudiR = acpR.aravo, dudiL = afcL.aravo, dudiQ = acpQ.aravo, scannf = FALSE) dev.new(title = &quot;RLQ&quot;, noRStudioGD = TRUE) plot(rlq.aravo) # Traits by environment crossed table rlq.aravo$tab # Since the plots are crowded, one can plot them one by one # in large graphical windows. dev.new(title = &quot;RLQ - site (L) scores&quot;, noRStudioGD = TRUE) s.label(rlq.aravo$lR, plabels.boxes.draw = FALSE, ppoints.alpha = 0, psub.text = &quot;a&quot;, psub.cex = 2, psub.position = &quot;topleft&quot;) dev.new(title = &quot;RLQ - species abundances&quot;, noRStudioGD = TRUE) s.label(rlq.aravo$lQ, plabels.boxes.draw = FALSE, ppoints.alpha = 0, psub.text = &quot;b&quot;, psub.cex = 2, psub.position = &quot;topleft&quot;) dev.new(title = &quot;RLQ - environmental variables&quot;, noRStudioGD = TRUE) s.arrow(rlq.aravo$l1, psub.text = &quot;c&quot;, psub.cex = 2, psub.position = &quot;topleft&quot;) dev.new(title = &quot;RLQ - species traits&quot;, noRStudioGD = TRUE) s.arrow(rlq.aravo$c1,psub.text = &quot;d&quot;,psub.cex = 2, psub.position = &quot;topleft&quot;) # Global test randtest(rlq.aravo, nrepet = 999, modeltype = 6) ## Fourth-corner analysis (takes time with 49999 permutations!) fourth.aravo &lt;- fourthcorner( tabR = aravo$env, tabL = aravo$spe, tabQ = aravo$traits, modeltype = 6, p.adjust.method.G = &quot;none&quot;, p.adjust.method.D = &quot;none&quot;, nrepet = 49999) # Correction for multiple testing, here using FDR if(!require(ade4)){install.packages(&quot;ade4&quot;)} library(ade4) fourth.aravo.adj &lt;- p.adjust.4thcorner( fourth.aravo, p.adjust.method.G = &quot;fdr&quot;, p.adjust.method.D = &quot;fdr&quot;, p.adjust.D = &quot;global&quot;) # Plot significant associations dev.new(title = &quot;Fourth-corner analysis&quot;, noRStudioGD = TRUE) plot(fourth.aravo.adj, alpha = 0.05, stat = &quot;D2&quot;) # Biplot combining RLQ and fourth-corner results dev.new(title = &quot;Combining RLQ and Fourth-corner results&quot;, noRStudioGD = TRUE) plot(fourth.aravo.adj, x.rlq = rlq.aravo, alpha = 0.05, stat = &quot;D2&quot;, type = &quot;biplot&quot;) ################################################################### # RLQ and fourth-corner analyses (Doubs data) ###### summary(fishtraits) rownames(fishtraits) names(spe) names(fishtraits) tra &lt;- fishtraits[ , 6:15] tra # Preliminary analyses: CA, Hill-Smith and PCA afcL.doubs &lt;- dudi.coa(spe, scannf = FALSE) acpR.doubs &lt;- dudi.hillsmith(env3, row.w = afcL.doubs$lw, scannf = FALSE) acpQ.doubs &lt;- dudi.pca(tra, row.w = afcL.doubs$cw, scannf = FALSE) # RLQ analysis rlq.doubs &lt;- rlq( dudiR = acpR.doubs, dudiL = afcL.doubs, dudiQ = acpQ.doubs, scannf = FALSE) dev.new(title = &quot;RLQ&quot;,width = 12,height = 7,noRStudioGD = TRUE) plot(rlq.doubs) # Traits by environment crossed table rlq.doubs$tab # Since the plots are crowded, one can plot them one by one # in large graphical windows: dev.new(title = &quot;RLQ - site (L) scores&quot;, noRStudioGD = TRUE) s.label(rlq.doubs$lR, plabels.boxes.draw = FALSE, ppoints.alpha = 0, psub.text = &quot;a&quot;, psub.cex = 2, psub.position = &quot;topleft&quot;) dev.new(title = &quot;RLQ - species abundances&quot;, noRStudioGD = TRUE) s.label(rlq.doubs$lQ, plabels.boxes.draw = FALSE, ppoints.alpha = 0, psub.text = &quot;b&quot;, psub.cex = 2, psub.position = &quot;topleft&quot;) dev.new(title = &quot;RLQ - environmental variables&quot;, noRStudioGD = TRUE) s.arrow(rlq.doubs$l1, psub.text = &quot;c&quot;, psub.cex = 2, psub.position = &quot;topleft&quot;) dev.new(title = &quot;RLQ - species traits&quot;, noRStudioGD = TRUE) s.arrow(rlq.doubs$c1, psub.text = &quot;d&quot;, psub.cex = 2, psub.position = &quot;topleft&quot;) # Global test randtest(rlq.doubs, nrepet = 999, modeltype = 6) # Fourth-corner analysis (takes time with 49999 permutations!) ###### ?fourthcorner fourth.doubs2 &lt;- fourthcorner( tabR = env3, tabL = spe, tabQ = tra, modeltype = 2, p.adjust.method.G = &quot;fdr&quot;, p.adjust.method.D = &quot;fdr&quot;, nrepet = 49999 ) fourth.doubs2 summary(fourth.doubs2) fourth.doubs &lt;- fourthcorner( tabR = env2, tabL = spe, tabQ = tra, modeltype = 6, p.adjust.method.G = &quot;none&quot;, p.adjust.method.D = &quot;none&quot;, nrepet = 49999) # Correction for multiple testing, here using FDR fourth.doubs.adj &lt;- p.adjust.4thcorner( fourth.doubs, p.adjust.method.G = &quot;fdr&quot;, p.adjust.method.D = &quot;fdr&quot;, p.adjust.D = &quot;global&quot;) fourth.doubs.adj summary(fourth.doubs.adj) # Plot dev.new(title = &quot;Fourth-corner analysis&quot;, noRStudioGD = TRUE) plot(fourth.doubs.adj, alpha = 0.05, stat = &quot;D2&quot;) plot(fourth.doubs2, stat = &quot;D2&quot;) plot(fourth.doubs2, stat = &quot;G&quot;) # Biplot combining RLQ and fourth-corner results dev.new(title = &quot;Combining RLQ and Fourth-corner results&quot;, noRStudioGD = TRUE) plot(fourth.doubs.adj, x.rlq = rlq.doubs, alpha = 0.05, stat = &quot;D2&quot;, type = &quot;biplot&quot;) plot(fourth.doubs2, x.rlq = rlq.doubs, alpha = 0.05, stat = &quot;D2&quot;, type = &quot;biplot&quot;) "],
["13-3-statistik-7-ubungen.html", "13.3 Statistik 7: Übungen", " 13.3 Statistik 7: Übungen 13.3.1 Übung 7.1: RDA (naturwissenschaftlich) Moordatensatz in library(dave) : sveg (Vegetationsdaten) ssit (Umweltdaten) Führt eine RDA mit allen in der Vorlesung gezeigten Schritten durch und interpretiert die Ergebnisse. Von den Umweltvariablen entfallen x.axis &amp; y.axis Für die partielle RDA und die Varianzpartitionierung bildet zwei Gruppen: Physiographie (Waterlev.max, Waterlev.av, Waterlev.min, log.peat.lev, log slope.deg) Chemie (alle übrigen) "],
["13-4-musterlosung-aufgabe-7-1-rda.html", "13.4 Musterlösung Aufgabe 7.1: RDA", " 13.4 Musterlösung Aufgabe 7.1: RDA R-Skript als Download Übungsaufgabe (hier so ausführlich formuliert, wie dies auch in der Klausur der Fall sein wird) Ladet die library dave, welche den Moordatensatz enthält. sveg beinhaltet presenceabsence-Daten aller untersuchten Arten in den Plots; ssit beinhaltet 18 metrische Umweltdaten sowie Koordinaten der Plots Führt eine RDA und eine Varianzpartizionierung in die Variablengruppen Physiographie (Waterlev.max, Waterlev.av, Waterlev.min, log.peat.lev, log slope.deg) und Chemie (alle übrigen) durch. Formuliert abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden. Während im Text normalerweise die Variablen ausgeschrieben werden solltet, genügt es hier (da ihr die entsprechenden Infos nicht bekommen habt und nur raten könntet), wenn ihr die Abkürzungen aus dem dataframe nehmt. 13.4.1 Übung 7.1 - RDA – Lösung # Load the Moordatensatz data. __Moordatensatz laden__ if(!require(dave)){install.packages(&quot;dave&quot;)} library(dave) data(sveg) data(ssit) summary(sveg) summary(ssit) str(ssit) # x.axis and y.axis vom data frame data frame ssit entfernen env2 &lt;- ssit[, -c(19,20)] Betrachtung der Daten zeigt, dass die Koordinaten in Spalten 19 und 20 sind, die daraufhin entfernt werden. # Generiere zwei subset der erklärenden Variablen # Physiografie (upstream-downstream-Gradient) envtopo &lt;- env2[, c(11 : 15)] names(envtopo) # Chemie envchem &lt;- env2[, c(1:10,16:18)] names(envchem) # Hellinger-transform the species dataset library(vegan) spe.hel &lt;- decostand(sveg, &quot;hellinger&quot;) spe.hel Vorstehend wurden die Variablen in die zwei Gruppen Chemistry und Physiography aufgteilt. Die Hellilnger-Transformation wird gemeinhin empfohlen (wobei dahingestellt sei, ob sie auch bei presence-absence-Daten nötig ist). Die weiteren Analysen führen wir mit der default-Einstellung „Scaling 2“ durch. (Je nach Bedarf bzw. persönlichen Vorlieben könnte auch Scaling 1 genommen werden). ## RDA of the Hellinger-transformed mire species data, constrained ## by all the environmental variables contained in env2 __Redundancy analysis (RDA)__ ## RDA der Hellinger-transformireten Moorarten-Daten, constrained ## mit allen Umweltvarialben die in env2 enthalten sind (spe.rda &lt;- rda(spe.hel ~ ., env2)) # Observe the shortcut formula summary(spe.rda) # Skalierung 2 (default) # Canonical coefficients from the rda object coef(spe.rda) # Unadjusted R^2 retrieved from the rda object (R2 &lt;- RsquareAdj(spe.rda)$r.squared) # Adjusted R^2 retrieved from the rda object (R2adj &lt;- RsquareAdj(spe.rda)$adj.r.squared) Man erhält R²adj. = 0.376 Jetzt kann man den Triplot erstellen ## Triplots of the rda results (lc scores) ## Site scores as linear combinations of the environmental variables dev.new(title = &quot;RDA scaling 1 and 2 + lc&quot;,width = 15,height = 6,noRStudioGD = TRUE) par(mfrow = c(1, 2)) # 1 und 2 Achse plot(spe.rda, display = c(&quot;sp&quot;, &quot;lc&quot;, &quot;cn&quot;), main = &quot;Triplot RDA spe.hel ~ env2 - scaling 2 - lc scores&quot;) spe.sc2 &lt;- scores(spe.rda, choices = 1:2, display = &quot;sp&quot;) arrows(0, 0, spe.sc2[, 1] * 0.92, spe.sc2[, 2] * 0.92,length = 0, lty = 1,col = &quot;red&quot;) text(-0.82, 0.55, &quot;b&quot;, cex = 1.5) #1 und 3 Achse plot(spe.rda, display = c(&quot;sp&quot;, &quot;lc&quot;, &quot;cn&quot;), choices = c(1,3), main = &quot;Triplot RDA spe.hel ~ env2 - scaling 2 - lc scores&quot;) spe.sc2 &lt;- scores(spe.rda, choices =c(1,3), display = &quot;sp&quot;) arrows(0, 0, spe.sc2[, 1] * 0.92, spe.sc2[, 2] * 0.92,length = 0, lty = 1,col = &quot;red&quot;) text(-0.82, 0.55, &quot;b&quot;, cex = 1.5) ## Triplots of the rda results (wa scores) ## Site scores as weighted averages (vegan&#39;s default) # Scaling 1 : distance triplot dev.new(title = &quot;RDA scaling 2 + wa&quot;,width = 7,height = 6,noRStudioGD = TRUE) # Scaling 2 (default) : correlation triplot plot(spe.rda, main = &quot;Triplot RDA spe.hel ~ env3 - scaling 2 - wa scores&quot;) arrows(0, 0, spe.sc2[, 1] * 0.92, spe.sc2[, 2] * 0.92, length = 0, lty = 1, col Auswahl der höchstkorrelierten Arten (Grenzwert kann subjektiv nach Bedarf gesetzt werden, hier 0.5). # Select species with goodness-of-fit at least 0.6 in the # ordination plane formed by axes 1 and 2 spe.good &lt;- goodness(spe.rda) sel.sp &lt;- which(spe.good[, 2] &gt;= 0.6) sel.sp # Triplots with homemade function triplot.rda() source(&quot;19_Statistik7/triplot.rda.R&quot;) # Triplots with homemade function triplot.rda(), scaling 2 setwd(&quot;S:/pools/n/N-zen_naturmanag_lsfm/FS_Vegetationsanalyse/Lehre (Module)/MSc. Research Methods/Statistik Dengler 2019/DataSets&quot;) source(&quot;triplot.rda.R&quot;) dev.new(title = &quot;RDA plot with triplot.rda&quot;,width = 7,height = 6, noRStudioGD = TRUE) triplot.rda(spe.rda, site.sc = &quot;lc&quot;, cex.char2 = 0.7, pos.env = 3, pos.centr = 1, mult.arrow = 1.1, mar.percent = 0.05, select.spe = sel.sp) # Global test of the RDA result anova(spe.rda, permutations = how(nperm = 999)) # Tests of all canonical axes anova(spe.rda, permutations = how(nperm = 999)) anova(spe.rda, by = &quot;axis&quot;, permutations = how(nperm = 999)) Die ersten drei RDA-Achsen sind also signifikant. Man könnte also auch noch eine Visualisierung von RDA 3 vs. RDA 1 machen. Partielle RDA Simple syntax; X and W may be in separate tables of quantitative variables spechem.physio &lt;- rda(spe.hel, envchem, envtopo) summary(spechem.physio) # Formula interface; X and W variables must be in the same # data frame (spechem.physio2 &lt;- rda(spe.hel ~ pH.peat + log.ash.perc + Ca_peat +Mg_peat + Na_peat + K_peat + Acidity.peat + CEC.peat + Base.sat.perc + P.peat + pH.water + log.cond.water + log.Ca.water + Condition(Waterlev.max + Waterlev.av + Waterlev.min + log.peat.lev + log.slope.deg), data = env2)) # Test of the partial RDA, using the results with the formula # interface to allow the tests of the axes to be run anova(spechem.physio2, permutations = how(nperm = 999)) anova(spechem.physio2, permutations = how(nperm = 999), by = &quot;axis&quot;) # Partial RDA triplots (with fitted site scores) # with function triplot.rda dev.new(title = &quot;Partial RDA&quot;,width = 7,height = 6,noRStudioGD = TRUE) triplot.rda(spechem.physio, site.sc = &quot;lc&quot;, scaling = 2, cex.char2 = 0.8, pos.env = 3, mult.spe = 1.1, mar.percent = 0.04) text(-3.34, 3.64, &quot;b&quot;, cex = 2) Varianzpartitionierung ## 1. Variation partitioning with all explanatory variables (spe.part.all &lt;- varpart(spe.hel, envchem, envtopo)) # Plot of the partitioning results dev.new(title = &quot;Variation partitioning&quot;,width = 7,height = 7,noRStudioGD = TRUE) plot(spe.part.all, digits = 2, bg = c(&quot;red&quot;, &quot;blue&quot;), Xnames = c(&quot;Chemistry&quot;, &quot;Physiography&quot;), id.size = 0.7) Die durch die erhobenen Umweltvariablen insgesamt erklärte Varianz (37.6%, s.o.) entfällt zu 19.4% auf chemische Variablen, 3.6% auf physiographische Variablen und zu 14.6% auf gemeinsame Erklärung. "],
["14-statistik-8-19-11-2019.html", "Kapitel 14 Statistik 8 (19.11.2019)", " Kapitel 14 Statistik 8 (19.11.2019) In Statistik 8 lernen die Studierenden Clusteranalysen/Klassifikationen als eine den Ordinationen komplementäre Technik der deskriptiven Statistik multivariater Datensätze kennen. Es gibt Partitionierungen (ohne Hierarchie), divisive und agglomerative Clusteranalysen (die jeweils eine Hierarchie produzieren). Etwas genauer gehen wir auf die k-means Clusteranalyse (eine Partitionierung) und eine Reihe von agglomerativen Clusterverfahren ein. Hierbei hat das gewählte Distanzmass und der Modus für die sukzessive Fusion von Clustern einen grossen Einfluss auf das Endergebnis. Wir besprechen ferner, wie man die Ergebnisse von Clusteranalysen adäquat visualisieren und mit anderen statistischen Prozeduren kombinieren kann. Im Abschluss von Statistik 8 werden wir dann die an den acht Statistiktagen behandelten Verfahren noch einmal rückblickend betrachten und thematisieren, welches Verfahren wann gewählt werden sollte. Ebenfalls ist Platz, um den adäquaten Ablauf statistischer Analysen vom Einlesen der Daten bis zur Verschriftlichung der Ergebnisse, einschliesslich der verschiedenen zu treffenden Entscheidungen, zu thematisieren. "],
["14-1-statistik-8-demoskript.html", "14.1 Statistik 8 - Demoskript", " 14.1 Statistik 8 - Demoskript Cluster-Analysen (c) Juergen Dengler Demoscript als Download Datensatz Doubs.RData Funktion drawmap.R drawmap.R Funktion hcoplot.R hcoplot.R k-means clustering # das Moordatenset aus Wildi... if(!require(dave)){install.packages(&quot;dave&quot;)} library(dave) pca&lt;-pca(sveg^0.25,cor=T) ca&lt;-cca(sveg^0.5) kmeans.1 &lt;- kmeans(sveg,4) kmeans.1 plot(ca$CA$u, asp=1,col=kmeans.1[[1]]) kmeans.2 &lt;- kmeans(sveg,3) plot(pca$scores[,1],pca$scores[,2],type=&quot;n&quot;, asp=1, xlab=&quot;PC1&quot;, ylab=&quot;PC2&quot;) points(pca$scores[,1],pca$scores[,2],pch=18,col=kmeans.2[[1]]) plot(pca$scores[,1],pca$scores[,3],type=&quot;n&quot;, asp=1, xlab=&quot;PC1&quot;, ylab=&quot;PC3&quot;) points(pca$scores[,1],pca$scores[,3],pch=18,col=kmeans.2[[1]]) # k-means partitioning, 2 to 10 groups KM.cascade &lt;-cascadeKM(sveg, inf.gr = 2, sup.gr = 10, iter = 100,criterion = &quot;ssi&quot;) summary(KM.cascade) KM.cascade$results KM.cascade$partition # k-means visualisation plot(KM.cascade, sortg = TRUE) Agglomarative Clusteranalyse mit Daten und Skripten aus Borcard et al. (2018) load(&quot;Doubs.Rdata&quot;) # Remove empty site 8 spe &lt;- spe[-8, ] env &lt;- env[-8, ] spa &lt;- spa[-8, ] latlong &lt;- latlong[-8, ] Dendogramme berechnen und ploten ## Hierarchical agglomerative clustering of the species abundance # Compute matrix of chord distance among sites spe.norm &lt;- decostand(spe, &quot;normalize&quot;) spe.ch &lt;- vegdist(spe.norm, &quot;euc&quot;) # Attach site names to object of class &#39;dist&#39; attr(spe.ch, &quot;Labels&quot;) &lt;- rownames(spe) par(mfrow = c(1, 1)) # Compute single linkage agglomerative clustering spe.ch.single &lt;- hclust(spe.ch, method = &quot;single&quot;) # Plot a dendrogram using the default options plot(spe.ch.single, labels = rownames(spe), main = &quot;Chord - Single linkage&quot;) # Compute complete-linkage agglomerative clustering spe.ch.complete &lt;- hclust(spe.ch, method = &quot;complete&quot;) plot(spe.ch.complete, labels = rownames(spe), main = &quot;Chord - Complete linkage&quot;) # Compute UPGMA agglomerative clustering spe.ch.UPGMA &lt;- hclust(spe.ch, method = &quot;average&quot;) plot(spe.ch.UPGMA, labels = rownames(spe), main = &quot;Chord - UPGMA&quot;) # Compute centroid clustering spe.ch.centroid &lt;- hclust(spe.ch, method = &quot;centroid&quot;) plot(spe.ch.centroid, labels = rownames(spe), main = &quot;Chord - Centroid&quot;) # Compute Ward&#39;s minimum variance clustering spe.ch.ward &lt;-hclust(spe.ch, method = &quot;ward.D2&quot;) plot(spe.ch.ward, labels = rownames(spe), main = &quot;Chord - Ward&quot;) # Compute beta-flexible clustering using cluster::agnes() # beta = -0.1 spe.ch.beta1 &lt;- agnes(spe.ch, method = &quot;flexible&quot;,par.method = 0.55) # beta = -0.25 spe.ch.beta2 &lt;- agnes(spe.ch, method = &quot;flexible&quot;, par.method = 0.625) # beta = -0.5 spe.ch.beta3 &lt;- agnes(spe.ch, method = &quot;flexible&quot;,par.method = 0.75) # Change the class of agnes objects class(spe.ch.beta1) spe.ch.beta1 &lt;- as.hclust(spe.ch.beta1) class(spe.ch.beta1) spe.ch.beta2 &lt;- as.hclust(spe.ch.beta2) spe.ch.beta3 &lt;- as.hclust(spe.ch.beta3) par(mfrow = c(2, 2)) plot(spe.ch.beta1, labels = rownames(spe), main = &quot;Chord - Beta-flexible (beta=-0.1)&quot;) plot(spe.ch.beta2, labels = rownames(spe), main = &quot;Chord - Beta-flexible (beta=-0.25)&quot;) plot(spe.ch.beta3, labels = rownames(spe), main = &quot;Chord - Beta-flexible (beta=-0.5)&quot;) # Compute Ward&#39;s minimum variance clustering spe.ch.ward &lt;- hclust(spe.ch, method = &quot;ward.D2&quot;) plot(spe.ch.ward, labels = rownames(spe), main = &quot;Chord - Ward&quot;) Cophenetic correlations # Single linkage clustering spe.ch.single.coph &lt;- cophenetic(spe.ch.single) cor(spe.ch, spe.ch.single.coph) # Complete linkage clustering spe.ch.comp.coph &lt;- cophenetic(spe.ch.complete) cor(spe.ch, spe.ch.comp.coph) # Average clustering spe.ch.UPGMA.coph &lt;- cophenetic(spe.ch.UPGMA) cor(spe.ch, spe.ch.UPGMA.coph) # Ward clustering spe.ch.ward.coph &lt;- cophenetic(spe.ch.ward) cor(spe.ch, spe.ch.ward.coph) # Shepard-like diagrams par(mfrow = c(2, 2)) plot( spe.ch, spe.ch.single.coph, xlab = &quot;Chord distance&quot;, ylab = &quot;Cophenetic distance&quot;, asp = 1, xlim = c(0, sqrt(2)), ylim = c(0, sqrt(2)), main = c(&quot;Single linkage&quot;, paste(&quot;Cophenetic correlation =&quot;, round( cor(spe.ch, spe.ch.single.coph), 3 ))) ) abline(0, 1) lines(lowess(spe.ch, spe.ch.single.coph), col = &quot;red&quot;) plot( spe.ch, spe.ch.comp.coph, xlab = &quot;Chord distance&quot;, ylab = &quot;Cophenetic distance&quot;, asp = 1, xlim = c(0, sqrt(2)), ylim = c(0, sqrt(2)), main = c(&quot;Complete linkage&quot;, paste(&quot;Cophenetic correlation =&quot;, round( cor(spe.ch, spe.ch.comp.coph), 3 ))) ) abline(0, 1) lines(lowess(spe.ch, spe.ch.comp.coph), col = &quot;red&quot;) plot( spe.ch, spe.ch.UPGMA.coph, xlab = &quot;Chord distance&quot;, ylab = &quot;Cophenetic distance&quot;, asp = 1, xlim = c(0, sqrt(2)), ylim = c(0, sqrt(2)), main = c(&quot;UPGMA&quot;, paste(&quot;Cophenetic correlation =&quot;, round( cor(spe.ch, spe.ch.UPGMA.coph), 3 ))) ) abline(0, 1) lines(lowess(spe.ch, spe.ch.UPGMA.coph), col = &quot;red&quot;) plot( spe.ch, spe.ch.ward.coph, xlab = &quot;Chord distance&quot;, ylab = &quot;Cophenetic distance&quot;, asp = 1, xlim = c(0, sqrt(2)), ylim = c(0, max(spe.ch.ward$height)), main = c(&quot;Ward&quot;, paste(&quot;Cophenetic correlation =&quot;, round( cor(spe.ch, spe.ch.ward.coph), 3 ))) ) abline(0, 1) lines(lowess(spe.ch, spe.ch.ward.coph), col = &quot;red&quot;) Optimale Anzahl Cluster ## Select a dendrogram (Ward/chord) and apply three criteria ## to choose the optimal number of clusters # Choose and rename the dendrogram (&quot;hclust&quot; object) hc &lt;- spe.ch.ward # hc &lt;- spe.ch.beta2 # hc &lt;- spe.ch.complete par(mfrow = c(1, 2)) # Average silhouette widths (Rousseeuw quality index) Si &lt;- numeric(nrow(spe)) for (k in 2:(nrow(spe) - 1)) { sil &lt;- silhouette(cutree(hc, k = k), spe.ch) Si[k] &lt;- summary(sil)$avg.width } k.best &lt;- which.max(Si) plot(1:nrow(spe),Si,type = &quot;h&quot;, main = &quot;Silhouette-optimal number of clusters&quot;, xlab = &quot;k (number of clusters)&quot;, ylab = &quot;Average silhouette width&quot;) axis(1,k.best,paste(&quot;optimum&quot;, k.best, sep = &quot;\\n&quot;), col = &quot;red&quot;, font = 2, col.axis = &quot;red&quot;) points(k.best,max(Si),pch = 16,col = &quot;red&quot;,cex = 1.5) # Optimal number of clusters according to matrix correlation # statistic (Pearson) # Homemade function grpdist from Borcard et al. (2018) grpdist &lt;- function(X) { require(cluster) veg &lt;- as.data.frame(as.factor(X)) distgr &lt;- daisy(veg,&quot;gower&quot;) distgr } kt &lt;- data.frame(k = 1:nrow(spe), r = 0) for (i in 2:(nrow(spe) - 1)) { gr &lt;- cutree(hc, i) distgr &lt;- grpdist(gr) mt &lt;- cor(spe.ch, distgr, method = &quot;pearson&quot;) kt[i, 2] &lt;- mt } k.best &lt;- which.max(kt$r) plot(kt$k,kt$r,type = &quot;h&quot;, main = &quot;Matrix correlation-optimal number of clusters&quot;, xlab = &quot;k (number of clusters)&quot;, ylab = &quot;Pearson&#39;s correlation&quot;) axis(1,k.best,paste(&quot;optimum&quot;, k.best, sep = &quot;\\n&quot;), col = &quot;red&quot;,font = 2,col.axis = &quot;red&quot;) points(k.best,max(kt$r),pch = 16,col = &quot;red&quot;,cex = 1.5) # Optimal number of clusters according as per indicator species # analysis (IndVal, Dufrene-Legendre; package: labdsv) IndVal &lt;- numeric(nrow(spe)) ng &lt;- numeric(nrow(spe)) for (k in 2:(nrow(spe) - 1)) { iva &lt;- indval(spe, cutree(hc, k = k), numitr = 1000) gr &lt;- factor(iva$maxcls[iva$pval &lt;= 0.05]) ng[k] &lt;- length(levels(gr)) / k iv &lt;- iva$indcls[iva$pval &lt;= 0.05] IndVal[k] &lt;- sum(iv) } k.best &lt;- which.max(IndVal[ng == 1]) + 1 col3 &lt;- rep(1, nrow(spe)) col3[ng == 1] &lt;- 3 par(mfrow = c(1, 2)) plot(1:nrow(spe),IndVal, type = &quot;h&quot;, main = &quot;IndVal-optimal number of clusters&quot;, xlab = &quot;k (number of clusters)&quot;, ylab = &quot;IndVal sum&quot;, col = col3) axis(1,k.best,paste(&quot;optimum&quot;, k.best, sep = &quot;\\n&quot;), col = &quot;red&quot;,font = 2,col.axis = &quot;red&quot;) points(which.max(IndVal),max(IndVal),pch = 16,col = &quot;red&quot;,cex = 1.5) text(28, 15.7, &quot;a&quot;, cex = 1.8) plot(1:nrow(spe),ng, type = &quot;h&quot;, xlab = &quot;k (number of clusters)&quot;, ylab = &quot;Ratio&quot;, main = &quot;Proportion of clusters with significant indicator species&quot;, col = col3) axis(1,k.best,paste(&quot;optimum&quot;, k.best, sep = &quot;\\n&quot;), col = &quot;red&quot;,font = 2,col.axis = &quot;red&quot;) points(k.best,max(ng),pch = 16,col = &quot;red&quot;,cex = 1.5) text(28, 0.98, &quot;b&quot;, cex = 1.8) Final dendrogram with the selected clusters # Choose the number of clusters k &lt;- 4 # Silhouette plot of the final partition spech.ward.g &lt;- cutree(spe.ch.ward, k = k) sil &lt;- silhouette(spech.ward.g, spe.ch) rownames(sil) &lt;- row.names(spe) plot(sil,main = &quot;Silhouette plot - Chord - Ward&quot;, cex.names = 0.8,col = 2:(k + 1),nmax = 100) # Reorder clusters if(!require(gclus)){install.packages(&quot;gclus&quot;)} library(&quot;gclus&quot;) spe.chwo &lt;- reorder.hclust(spe.ch.ward, spe.ch) # Plot reordered dendrogram with group labels par(mfrow = c(1, 1)) plot(spe.chwo,hang = -1, xlab = &quot;4 groups&quot;, sub = &quot;&quot;, ylab = &quot;Height&quot;, main = &quot;Chord - Ward (reordered)&quot;, labels = cutree(spe.chwo, k = k)) rect.hclust(spe.chwo, k = k) # Plot the final dendrogram with group colors (RGBCMY...) # Fast method using the additional hcoplot() function: # Usage: # hcoplot(tree = hclust.object, # diss = dissimilarity.matrix, # lab = object labels (default NULL), # k = nb.clusters, # title = paste(&quot;Reordered dendrogram from&quot;,deparse(tree$call), # sep=&quot;\\n&quot;)) source(&quot;20_Statistik8/hcoplot.R&quot;) hcoplot(spe.ch.ward, spe.ch, lab = rownames(spe), k = 4) # Plot the Ward clusters on a map of the Doubs River # (see Chapter 2) source(&quot;20_Statistik8/drawmap.R&quot;) drawmap(xy = spa,clusters = spech.ward.g, main = &quot;Four Ward clusters along the Doubs River&quot;) Miscellaneous graphical outputs # Heat map of the dissimilarity matrix ordered with the dendrogram heatmap(as.matrix(spe.ch),Rowv =NULL , symm = TRUE,margin = c(3, 3)) # Ordered community table # Species are ordered by their weighted averages on site scores. # Dots represent absences. library(vegan) or &lt;- vegemite(spe, spe.chwo) "],
["14-2-statistik-8-ubungen.html", "14.2 Statistik 8: Übungen", " 14.2 Statistik 8: Übungen 14.2.1 Übung 8.1: Clusteranalyse (sozioökonomisch) Datensatz crime2.csv Raten von 7 Kriminalitätsformen pro 100000 Einwohner und Jahr für die Bundesstaaten der USA (a) Führt eine k-means- und eine agglomerative Clusteranalyse eurer Wahl durch. (b) Überlegt in beiden Fällen, wie viele Cluster sinnvoll sind (k-means z. B.visuelle Betrachtung einer PCA, agglomerative Clusteranalyse z. B. SilhouettePlot). (c) Abschliessend entscheidet euch für eine Clusterung und vergleicht die erhaltenen Cluster bezüglich der Kriminalitätsformen mittels ANOVA und interpretiert die Cluster entsprechend. Hinweis: Wegen der sehr ungleichen Varianzen muss auf jeden Fall eine Standardisierung stattfinden, damit Distanzen zwischen den verschiedenen Kriminalitätsraten sinnvoll berechnet werden können "],
["14-3-musterlosung-aufgabe-8-1-clusteranalysen.html", "14.3 Musterlösung Aufgabe 8.1: Clusteranalysen", " 14.3 Musterlösung Aufgabe 8.1: Clusteranalysen R-Skript als Download Übungsaufgabe (hier so ausführlich formuliert, wie dies auch in der Klausur der Fall sein wird) Ladet den Datensatz crime2.csv. Dieser enthält Raten von 7 Kriminatlitätsformen pro 100000 Einwohner und Jahr für die Bundesstaaten der USA. Führt eine k-means- und eine agglomerative Clusteranalyse eurer Wahl durch. Bitte beachet, dass wegen der sehr ungleichen Varianzen in jedem Fall eine Standardisierung stattfinden muss, damit die Distanzen zwischen den verschiedenen Kriminalitätsraten sinnvoll berechnet werden können. Überlegt in beiden Fällen, wie viele Cluster sinnvoll sind (k-means: z. B. visuelle Betrachtung einer PCA, agglomertive Clusteranalyse: z. B. Silhoutte-Plot). Entscheidet euch dann für eine der beiden Clusterungen und vergleicht dann die erhaltenen Cluster bezüglich der Kriminalitätsformen und interpretiert die Cluster entsprechend. Bitte erklärt und begründet die einzelnen Schritte, die ihr unternehmt, um zu diesem Ergebnis zu kommen. Dazu erstellt bitte ein Word-Dokument, in das ihr Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, eure Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere Vorgehen dokumentieren. Formuliert abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden. Abzugeben sind am Ende (a) Ein lauffähiges R-Skript; (b) begründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation) und (c) ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit). 14.3.1 Übung 8.1 - Clusteranalysen – Lösung crime &lt;-read.csv(&quot;20_Statistik8/crime2.csv&quot;,sep=&quot;;&quot;) crime Im mitgelieferten R-Skript habe ich die folgenden Analysen zunächst mit untransformierten, dann mit standardisierten Kriminalitätsraten berechnet. Ihr könnt die Ergebnisse vergleichen und seht, dass sie sehr unterschiedlich ausfallen. crimez &lt;- crime crimez[,c(2:8)] &lt;- lapply(crime[, c(2:8)], scale) crimez „scale“ führt eine Standardisierung (z-Transformation) durch, so dass alle Variablen anschiessen einen Mittelwert von 0 und eine SD von 1 haben, ausgenommen natürlich die 1. Spalte mit den Kürzeln der Bundesstaaten. Anschliessend wird das SSI-Kriterium getestet und zwar für Partitionierungen von 2 bis 6 Gruppen (wie viele Gruppen man maximal haben will, muss man pragmatisch nach der jeweiligen Fragestelltung entscheiden). library(vegan) crimez.KM.cascade &lt;- cascadeKM( crimez[,c(2:8)], inf.gr = 2, sup.gr = 6, iter = 100, criterion = &quot;ssi&quot; ) summary(crimez.KM.cascade) crimez.KM.cascade$results crimez.KM.cascade$partition # k-means visualisation library(cclust) plot(crimez.KM.cascade, sortg = TRUE) Nach SSI ist die 4-Gruppenlösung die beste, mit dieser wird also weitergerechnet. # 4 Kategorien sind nach SSI offensichtlich besonders gut modelz &lt;- kmeans(crimez[,c(2:8)],4) modelz #File für ANOVA (Originaldaten der Vorfälle, nicht die ztransformierten) crime.KM4 &lt;- data.frame(crime,modelz[1]) crime.KM4$cluster &lt;-as.factor(crime.KM4$cluster) crime.KM4 str(crime.KM4) Von den agglomerativen Clusterverfahren habe ich mich für Ward’s minimum variance clustering entschieden, da dieses allgemein als besonders geeignet gilt. Vor der Berechnung von crime.norm und crime.ch muss man die Spalte mit den Bundesstaatenkürzeln entfern. #Agglomerative Clusteranalyse crime2 &lt;- crime[,-1] crime.norm &lt;- decostand(crime2, &quot;normalize&quot;) crime.ch &lt;- vegdist(crime.norm, &quot;euc&quot;) # Attach site names to object of class &#39;dist&#39; attr(crime.ch, &quot;Labels&quot;) &lt;- crime[,1] #Ward&#39;s minimum variance clustering crime.ch.ward &lt;- hclust(crime.ch, method = &quot;ward.D2&quot;) par(mfrow = c(1, 1)) plot(crime.ch.ward, labels = crime[,1], main = &quot;Chord - Ward&quot;) # Choose and rename the dendrogram (&quot;hclust&quot; object) hc &lt;- crime.ch.ward # hc &lt;- spe.ch.beta2 # hc &lt;- spe.ch.complete dev.new( title = &quot;Optimal number of clusters&quot;, width = 12, height = 8, noRStudioGD = TRUE ) dev.off() par(mfrow = c(1, 2)) # Average silhouette widths (Rousseeuw quality index) library(cluster) Si &lt;- numeric(nrow(crime)) for (k in 2:(nrow(crime) - 1)) { sil &lt;- silhouette(cutree(hc, k = k), crime.ch) Si[k] &lt;- summary(sil)$avg.width } k.best &lt;- which.max(Si) plot( 1:nrow(crime), Si, type = &quot;h&quot;, main = &quot;Silhouette-optimal number of clusters&quot;, xlab = &quot;k (number of clusters)&quot;, ylab = &quot;Average silhouette width&quot; ) axis( 1, k.best, paste(&quot;optimum&quot;, k.best, sep = &quot;\\n&quot;), col = &quot;red&quot;, font = 2, col.axis = &quot;red&quot; ) points(k.best, max(Si), pch = 16, col = &quot;red&quot;, cex = 1.5 ) Demnach wären beim Ward’s-Clustering nur zwei Gruppen die optimale Lösung. Für die Vergleiche der Bundesstaatengruppen habe ich mich im Folgenden für die k-meansClusterung mit 4 Gruppen entschieden. Damit die Boxplots und die ANOVA direkt interpretierbar sind, werden für diese, anders als für die Clusterung, die untransformierten Incidenz-Werte verwendet (also crime statt crimez). Die Spalte mit der Clusterzugehörigkeit im Fall von k-means mit 4 Clustern hängt man als Spalte an (Achtung: muss als Faktor definiert werden!). Anschliessend kann man die 7 ANOVAs rechnen, die Posthoc-Vergleiche durchführen und die zugehörigen Boxplots mit Buchstaben für die homogenen Gruppen erzeugen. Sinnvollerweise gruppiert man die Abbildungen gleich, z. B. je 2 x 2. Das Skript ist hier simple für jede Verbrechensart wiederholt (nur die erste und letzte gezeigt). Erfahrenere R-Nutzer können das Ganze hier natürlich durch eine Schleife abkürzen. library(multcomp) par(mfrow=c(2,2)) ANOVA.Murder &lt;- aov(Murder~cluster,data=crime.KM4) summary (ANOVA.Murder) letters &lt;- cld(glht(ANOVA.Murder, linfct=mcp(cluster=&quot;Tukey&quot;))) boxplot(Murder~cluster, data=crime.KM4, xlab=&quot;Cluster&quot;, ylab=&quot;Murder&quot;) mtext(letters$mcletters$Letters, at=1:4) ANOVA.Vehicle &lt;- aov(Vehicle~cluster,data=crime.KM4) summary (ANOVA.Vehicle) letters &lt;- cld(glht(ANOVA.Vehicle, linfct=mcp(cluster=&quot;Tukey&quot;))) boxplot(Vehicle~cluster, data=crime.KM4, xlab=&quot;Cluster&quot;, ylab=&quot;Vehicle&quot;) mtext(letters$mcletters$Letters, at=1:4) Die Boxplots erlauben jetzt auch eine Beurteilung der Modelldiagnostik: sind die Residuen hinreichen normalverteilt (symmetrisch) und sind die Varianzen zwischen den Kategorien einigermassen ähnlich. Mit der Symmetrie/Normalverteilung sieht es OK aus. Die Varianzhomogenität ist nicht optimal – meist deutlich grössere Varianz bei höheren Mittelwerten. Eine log-Transformation hätte das verbessert und könnte hier gut begründet werden. Da die p-Werte sehr niedrig waren und die Varianzheterogenität noch nicht extrem war, habe ich aber von einer Transformation abgesehen, da jede Transformation die Interpretation der Ergebnisse erschwert. Jetzt muss man nur noch herausfinden, welche Bundesstaaten überhaupt zu welchem der vier Cluster gehören, sonst ist das ganze schöne Ergebnis nutzlos. Z. B. kann man in R auf den Dataframe clicken und ihn nach cluster sortieren. "],
["15-raumanalyse-1-25-11-2019.html", "Kapitel 15 Raumanalyse 1 (25.11.2019)", " Kapitel 15 Raumanalyse 1 (25.11.2019) Die erste Übung zur Raumanalyse illustriert das einfache Laden und Anzeigen von Geodaten im Vektor- und Raster-Datenformat. Zusätzlich veranschaulicht die Übung den Umgang mit Koordinatensystemen sowie die Vektor-Raster-Konvertierung. Einfach erste Analysen umfassen den Spatial Join (Annotieren von Punkten mit Attributen von die Punkte einbettenden Vektordaten) sowie Puffer-Operationen. Zum Abschluss thematisiert die Übung die Aggregationsabhängigkeit räumlicher Daten durch die Illustration des Modifiable Areal Unit Problem (MAUP). Inhaltlich orientiert sich die Übung an Bodeneigenschaften für den Untersuchungsraum Schweiz. "],
["15-1-ubung-spatial-join-puffer-und-maup.html", "15.1 Übung: Spatial Join, Puffer und MAUP", " 15.1 Übung: Spatial Join, Puffer und MAUP 15.1.1 Thematische Einbettung Mit dieser Übung wirst Du Schritt fürfür Schritt an einfache Methoden der Raumanalyse herangeführt. Dies umfasst das Laden und Plotten von Vektor- und Rasterdaten sowie den wichtigen Umgang mit Koordinatensystemen. Darauf folgen einige Übungen zur räumlichen Anreicherung von Punktdaten unter Verwendung des Konzepts des “Spatial Join”. Dazu arbeiten wir mit einem Datensatz aus Punktstichproben zur Wasserverfügbarkeit in Schweizer Böden. Es soll untersucht werden, ob ein Zusammenhang besteht zwischen der an den Punkten gemessenen Wasserverfügbarkeit und dem Join-Layer Bodeneignung/Skelettanteil. Im zweiten Teil der Übung wirst Du die Messwerte zur Wasserverfügbarkeit auf unterschiedlichen räumlichen Skalen aggregieren (Kantone und Bezirke) und prüfen, ob ggf. das Modifiable Areal Unit Problem auftritt. 15.1.2 Vorbereitung Es gibt bereits eine Vielzahl von Packages um in R mit räumlichen Daten zu arbeiten, die ihrerseits wiederum auf weiteren Packages basieren (Stichwort dependencies). Für Vektordaten dominierte lange das Package sp, welches nun aber schrittweise durch sf abgelöst wird. Wir werden wenn immer möglich mit sf arbeiten und nur in Ausnahmefällen auf andere Packages zurück greifen. Für Rasterdaten exisitert das Package raster Für die Integration von Vektor und Rasterdaten existiert das Package stars: Spatiotemporal Arrays for Raster and Vector Datacubes. Diese Tools sind teilweise sehr gut dem Tidyverse-Workflow (group_by, mutate, summarise, %&gt;%) integriert. Lade zu beginn die folgenden notwenigen Packages (installiere die fehlenden Packages mit install.package(&quot;packagename&quot;)). library(sf) library(tidyverse) library(stars) library(raster) 15.1.3 Aufgabe 1: Daten runterladen und importieren Lade zunächst die Datensätze unter folgenden Links herunter und importiere sie mit dem Befehl read_sf() in R: bodeneignung_skelett.gpkg: Datensatz des Bundesamt für Landwirtschaft, modifiziert (weitere Informationen) kantone.gpkg: Ein Datensatz der Swisstopo, modifiziert (weitere Informationen) bezirke.gpkg: Ein Datensatz der Swisstopo, modifiziert (weitere Informationen) wasserverfuegbarkeit_boden.gpkg: Ein Datensatz der WSL, modifiziert (weiteren Informationen) Es handelt sich um Geodatensätze im Format Geopackage (“*.gpkg”), eine alternatives Datenformat zum bekannteren Format “Shapefiles”. Lade nun die Datensätze wie folgt ein: # Pfad muss natürlich angepasst werden wasser &lt;- read_sf(&quot;21_RaumAn1/data/wasserverfuegbarkeit_boden.gpkg&quot;) kantone &lt;- read_sf(&quot;21_RaumAn1/data/kantone.gpkg&quot;) bezirke &lt;- read_sf(&quot;21_RaumAn1/data/bezirke.gpkg&quot;) skelettgehalt &lt;- read_sf(&quot;21_RaumAn1/data/bodeneignung_skelett.gpkg&quot;) Schau Dir die importierten Datensätze an, nutzt dafür View(), str(), class(). Studiere ausserdem die weiteren Informationen zu den Datensätzen. 15.1.4 Aufgabe 2: Daten Visualisieren Wir hatten anfangs erwähnt, dass Geodaten mit sf und raster sich teilweise sehr schön in die bekannten Tidyverse workflows integrieren lassen. Das merkt man schnell, wenn man die Daten visualisieren möchte. In InfoVis 1 &amp; 2 haben wir intensiv mit ggplot2 gearbeitet und dort die Layers geom_point() und geom_line() kennen gelernt. Zusätzlich beinhaltet ggplot die Möglichkeit, mit geom_sf() Vektordaten direkt und sehr einfach zu plotten. Führe die angegebenen R-Befehle aus und studiere die entstehenden Plots. Welche Unterschiede findest Du? Wie erklärst Du diese Unterschiede? ggplot(bezirke) + geom_sf() ggplot(wasser) + geom_sf() 15.1.5 Aufgabe 3 Koordinatensysteme zuweisen In den obigen beiden sehr einfachen Kartogrammen fallen verschiedene Dinge auf: die X/Y Achsen weisen zwei ganz unterschiedlichen Zahlenbereiche auf (vergleiche die Achsenbeschriftungen), und der Umriss der Schweiz scheint im zweiten Plot “gestaucht” zu sein. Dies hat natürlich damit zu tun, dass die beiden Datensätze in unterschiedlichen Koordinatensystemen erfasst wurden. Koordinatensysteme werden mit CRS (Coordinate Reference System) abgekürzt. Mit st_crs() könnnen die zugewiesenen Koordinatensysteme abgefragt werden. st_crs(wasser) ## Coordinate Reference System: NA st_crs(bezirke) ## Coordinate Reference System: NA Leider sind in unserem Fall keine Koordinatensysteme zugewiesen. Mit etwas Erfahrung kann man das Koordinatensystem aber erraten, so viele kommen nämlich gar nicht in Frage. Am häufigsten trifft man hierzulande eines der drei folgenden Koordinatensysteme an: CH1903 LV03: das alte Koordinatensystem der Schweiz CH1903+ LV95: das neue Koordinatensystem der Schweiz WGS84: ein häufig genutztes weltumspannendes geodätisches Koordinatensystem, sprich die Koordinaten werden in Länge und Breite angegeben (Lat/Lon). Nun gilt es, anhand der Koordinaten die in der Spalte geometry ersichtlich sind das korrekte Koordinatensystem festzustellen. Wenn man sich auf epsg.io/map die Schweiz anschaut, kann man die Koordinaten in verschiedenen Koordinatensystem betrachten. Bedienungshinweise: Koordinanten (des Fadenkreuzes) werden im ausgewählten Koordinatensystem dargestellt Das Koordinatensystem, in welchem die Koordinaten dargestellt werden sollen, kann mit “Change” angepasst werden Für Enthusiasten: Schau Dir die Schweiz in verschiedenen Koordinatensystemen an, in dem Du auf “Reproject Map” klickst Wenn man diese Koordinaten mit den Koordinaten unserer Datensätze vergleicht, dann ist schnell klar, dass es sich beim Datensatz wasser um das Koordinatensystem WGS84 handelt und bei bezirke das Koordinatensystem CH1903+ LV95. Diese Koordinatensyteme weisen wir nun mit st_set_crs() und dem entsprechenden EPSG-Code (siehe die jeweiligen Links) zu. wasser &lt;- st_set_crs(wasser, 4326) bezirke &lt;- st_set_crs(bezirke, 2056) # zuweisen mit st_set_crs(), abfragen mit st_crs() st_crs(wasser) ## Coordinate Reference System: ## EPSG: 4326 ## proj4string: &quot;+proj=longlat +datum=WGS84 +no_defs&quot; Weise auch für die anderen Datensätze (kantone und skelettgehalt) das korrekte Koordinatensytem zu. Jetzt wo das CRS der Datensätze bekannt ist, können diese in einem gemeinsamen Plot visualisiert werden, ggplot kümmert sich darum die unterschiedlichen Koordinatensysteme zu vereinheitlichen. Probier das aus, indem du kantone und wasser in einem ggplot kombinierst. Die Achsen werden dann immer in WGS84 beschriftet. Wenn das stört, kann man coord_sf(datum = 2056) in einem weiteren Layer spezifizieren. Oder aber man blendet die Achsenbeschriftung mit theme_void() komplett aus. Versuche beide Varianten. 15.1.6 Aufgabe 4: Koordinatensyteme transformieren In der vorherigen Übung haben wir das bestehende Koordinatensystem zugewiesen. Dabei haben wir die bestehenden Koordinaten (in der Spalte geom) nicht manipuliert. Ganz anders ist eine Transformation der Daten von einem Koordinatensystem in das andere. Bei einer Transformation werden die Koordinaten in das neue Koordinatensystem umgerechnet und somit manipuliert. Aus praktischen Gründen wollen wir all unsere Daten ins neue Schweizer Koordinatensystem CH1903+ LV95 transfomieren. Transformiere den Datensatz wasser mit st_transform()in CH1903+ LV95, nutze dafür den korrekten EPSG-Code. Vor der Transformation (betrachte die Spalte geom sowie die Attribute epsg (SRID) und proj4string): wasser ## Simple feature collection with 993 features and 1 field ## geometry type: POINT ## dimension: XY ## bbox: xmin: 6.006132 ymin: 45.84509 xmax: 10.46693 ymax: 47.76851 ## epsg (SRID): 4326 ## proj4string: +proj=longlat +datum=WGS84 +no_defs ## # A tibble: 993 x 2 ## wasserverfuegbarkeit geom ## &lt;dbl&gt; &lt;POINT [°]&gt; ## 1 -167 (8.997155 45.84509) ## 2 0 (7.072459 45.91802) ## 3 0 (7.153695 45.90636) ## 4 0 (7.251192 45.91059) ## 5 0 (7.380257 45.92894) ## 6 0 (7.423909 45.92544) ## 7 0 (9.013727 45.91338) ## 8 -91 (9.051913 45.89967) ## 9 0 (7.069596 45.9689) ## 10 -102 (7.175481 45.9782) ## # … with 983 more rows Nach der Transformation (betrachte die Spalte geom): wasser ## Simple feature collection with 993 features and 1 field ## geometry type: POINT ## dimension: XY ## bbox: xmin: 2489453 ymin: 1078252 xmax: 2830887 ymax: 1291537 ## epsg (SRID): 2056 ## proj4string: +proj=somerc +lat_0=46.95240555555556 +lon_0=7.439583333333333 +k_0=1 +x_0=2600000 +y_0=1200000 +ellps=bessel +towgs84=674.374,15.056,405.346,0,0,0,0 +units=m +no_defs ## # A tibble: 993 x 2 ## wasserverfuegbarkeit geom ## &lt;dbl&gt; &lt;POINT [m]&gt; ## 1 -167 (2721079 1078252) ## 2 0 (2571587 1085224) ## 3 0 (2577885 1083902) ## 4 0 (2585453 1084350) ## 5 0 (2595470 1086373) ## 6 0 (2598856 1085983) ## 7 0 (2722214 1085868) ## 8 -91 (2725207 1084404) ## 9 0 (2571392 1090881) ## 10 -102 (2579603 1091882) ## # … with 983 more rows 15.1.7 Aufgabe 5: Wir wollen nun wissen, ob die Wasserverfügbarkeit im Boden mit dem Skelettgehalt zusammen hängt. Dazu nutzen wir die GIS-Technik Spatial Join, die in der Vorlesung beschrieben wurde. In sf können wir Spatial Joins mit der Funktion st_join durchführen, dabei gibt es nur left sowie inner-Joins (vgl. PrePro 1 &amp; 2). So müssen die Punkte “Links”, also an erste Stelle aufgeführt werden, da wir ja Attribute an die Punkte anheften wollen. ## Simple feature collection with 993 features and 2 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: 2489453 ymin: 1078252 xmax: 2830887 ymax: 1291537 ## epsg (SRID): 2056 ## proj4string: +proj=somerc +lat_0=46.95240555555556 +lon_0=7.439583333333333 +k_0=1 +x_0=2600000 +y_0=1200000 +ellps=bessel +towgs84=674.374,15.056,405.346,0,0,0,0 +units=m +no_defs ## # A tibble: 993 x 3 ## wasserverfuegbarkeit geom SKELETT ## &lt;dbl&gt; &lt;POINT [m]&gt; &lt;dbl&gt; ## 1 -167 (2721079 1078252) 3 ## 2 0 (2571587 1085224) NA ## 3 0 (2577885 1083902) NA ## 4 0 (2585453 1084350) NA ## 5 0 (2595470 1086373) NA ## 6 0 (2598856 1085983) NA ## 7 0 (2722214 1085868) 4 ## 8 -91 (2725207 1084404) 4 ## 9 0 (2571392 1090881) NA ## 10 -102 (2579603 1091882) 4 ## # … with 983 more rows Führe den obigen Spatial Join aus und erstelle anschliessend einen Boxplot pro Skelett-Kategorie. Für ggplot boxplots ist es sinnvoll, den Skelettgehalt vorgängig von numeric in factor zu konvertieren (falls Du nicht mehr weisst weshalb, schau nochmals nach in PrePro und InfoVis). Nun haben wir das Ziel der Aufgabe erreicht und die Messpunkte durch räumliche Zusatzinformation aufgewertet. Wir werden das Resultat an dieser Stelle aber nicht weiter interpretieren, dass wäre Teil einer Bodenkunde Vorlesung. 15.1.8 Aufgabe 6: Spatial Join mit Flächen In der letzten Aufgabe haben wir für jede Probe aus wasser den Skelettgehalt des darunterliegenden Polygons ermittelt. Für Proben, die gerade an der Grenze zu einem Polygon mit einem anderen Skelettgehalt liegen ist dieser Wert aber nicht sehr aussagekräftig. So könnte es zum Beispiel wichtiger sein zu wissen, was der dominierende Skelettgehalt innerhalb eines 2 km Radius um die Probe ist. In den kommenden Teilaufgaben lösen wir diese Herausforderung. 15.1.8.1 Teilaufgabe A: Punkte mit Puffer versehen Dafür müssen wir die Punkte mit einem Puffer versehen. Dies erreichen wir mit st_buffer(). Erstelle einen Datensatz wasser_2km, in dem jeder Punkt mit 2’000 m gepuffert wurde. Visualisiere dann diesen Datensatz. Beachte, dass es sich nun nicht mehr um Punkte, sondern um Flächen handelt (POLYGON). 15.1.8.2 Teilaufgabe B: Vektordatensatz in Raster konvertieren Um Flächen miteinander zu verrechnen (“Was ist der dominierende Skelettgehalt im Umkreis von 2km?”) ist es einfacher, wenn der Skelettgehalt-Datensatz im Raster-Datenformat daher kommt. Dazu wandeln wir den Vektordatensatz skelettgehalt mit einer Vektor-nach-Raster-Konvertierung in den Rasterdatensatz skelett_raster. Hierzu brauchen wir die Funktion fasterize() (fast rasterize) aus der gleichnamigen Library. Installiere diese Library (wenn nötig) und importiere sie in die aktuelle Session mit library(fasterize). In einem ersten Schritt müssen wir eine Raster-Vorlage erstellen, welche dazu dient, die räumliche Ausdehnung und die Auflösung (Zellengrösse) des resultierenden Datensatzes festzulegen. library(fasterize) raster_template &lt;- raster(extent(skelettgehalt), resolution = 1000) Danach wird mit fasterize der Polygon-Datensatz in ein Raster konvertiert. Mit field = kann festgelegt werden, aus welcher Spalte die Werte des Output Datensatzes entnommen werden sollten. GIS-Experten werden sich erinnern, dass im Gegensatz zu Flächen in einem Vektordatensatz, welche viele verschiedene Attributen haben können, ein Raster nur noch ein Attribut (hier SKELETT). skelett_raster &lt;- fasterize(skelettgehalt,raster_template,field = &quot;SKELETT&quot;) ggplot() + geom_stars(data = st_as_stars(skelett_raster)) + coord_equal() # geom_stars() ist (noch) nicht so clever wie geom_sf() das CRS wird nicht berücksichtigt 15.1.8.3 Teilaufgabe C: Rasterwerte extrahieren Mit raster::extract() könnnen nun die Rasterwerte aus dem Rasterdatensatz extrahiert werden. In fun = kann festgelegt werden, ob und mit welcher Funktion die vielen Rasterzellen pro Polygon aggregiert werden sollen. Wir möchten nur den häufigsten Wert zurück erhalten, sprich den Modus dafür gibt es in R leider keine eingebaute Funktion, weshalb wir unsere eigene basteln müssen: mode &lt;- function(x,na.rm = FALSE) { ux &lt;- unique(x) ux[which.max(tabulate(match(x, ux)))] } Nun können wir mit raster::extract() den Modus in jedem Puffer berechnen. wasser_skelett$skelett_mode &lt;- extract(skelett_raster,wasser_2km,fun = mode)[,1] Jetzt könnnen wir prüfen, wie oft die beiden Join-Varianten übereinstimmen. Erstelle dazu einen Facet-Plot, indem für jede Skelett-Kategorie die Modus-Kategorie (sprich die häufigste Kategorie) im den Punkt umgebenden Puffer darstellt. 15.1.9 Aufgabe 7: Spatial Join mit Kantone, Bezirke Zum Abschluss der Übung wenden wir uns nun noch der Aggregationsabhängigkeit von Geodaten zu. Dazu wollen wir die Daten zur Wasserverfügbarkeit auf verschiedenen Massstäben aggregieren. Als Aggregationseinheiten verwenden wir zwei politische Gliederungen der Schweit - Kantone und Bezirke. Die Frage stellt sich, ob die Daten ev. das MAUP illustrieren. Hier könnten wir wir nochmals st_join() verwenden, aber da wir diesmal Polygone im Fokus haben (Bezirke, Kantone) und mehrere Punkte in einem Polygon vorkommen können, ist dieser Weg etwas umständlich. Wir nutzen deshalb die Funktion aggregate(), und spezifizieren x = wasser, by = kantone und FUN = mean. Hinweis: Das Beschriften der Kantone ist fakultativ und nicht ganz trivial. 15.1.10 Aufgabe 8 (für Ambitionierte) Politische Grenzen sind für die meisten natürlichen Phänomene irrelevant. Wir könnten deshalb auch eine regelmässige Kachelung (sog. Tesselierung) des Untersuchungsgebietes vornehmen. Dafür könnten wir mit st_make_grid() eine Kachelung für das gewählte Untersuchungsgebiet (x =) in einer bestimmten Grösse (cellsize =) als Quadrate (square = TRUE) oder sogar mit hübschen Hexagonen (square = FALSE) durchführen. Probier’s aus! Eine weitere, zusätzliche (und sehr anspruchsvolle) Herausforderung ist das Zeichnen und Beschriften der Kantosgrenzen. Hierzu geben wir für Wagemutige gerne Tips. "],
["15-2-ubung-losung.html", "15.2 Übung – Lösung", " 15.2 Übung – Lösung R-Script als Download library(sf) library(tidyverse) library(stars) library(raster) ## -- Aufgabe 1: Daten runterladen und importieren -- ## # Pfad muss natürlich angepasst werden wasser &lt;- read_sf(&quot;21_RaumAn1/data/wasserverfuegbarkeit_boden.gpkg&quot;) kantone &lt;- read_sf(&quot;21_RaumAn1/data/kantone.gpkg&quot;) bezirke &lt;- read_sf(&quot;21_RaumAn1/data/bezirke.gpkg&quot;) skelettgehalt &lt;- read_sf(&quot;21_RaumAn1/data/bodeneignung_skelett.gpkg&quot;) ## -- Aufgabe 2: Daten Visualisieren -- ## ggplot(bezirke) + geom_sf() ggplot(wasser) + geom_sf() ## -- Aufgabe 3 Koordinatensysteme zuweisen -- ## st_crs(wasser) st_crs(bezirke) wasser &lt;- st_set_crs(wasser, 4326) bezirke &lt;- st_set_crs(bezirke, 2056) # zuweisen mit st_set_crs(), abfragen mit st_crs() st_crs(wasser) kantone &lt;- st_set_crs(kantone, 2056) skelettgehalt &lt;- st_set_crs(skelettgehalt, 2056) ggplot() + geom_sf(data = kantone) + geom_sf(data = wasser) ## -- Aufgabe 4: Koordinatensyteme transformieren -- ## wasser wasser &lt;- st_transform(wasser, 2056) wasser ## -- Aufgabe 5 -- ## wasser_skelett &lt;- st_join(wasser,skelettgehalt) wasser_skelett wasser_skelett %&gt;% mutate(SKELETT = factor(SKELETT)) %&gt;% ggplot() + geom_boxplot(aes(SKELETT,wasserverfuegbarkeit)) ## -- Aufgabe 6: Spatial Join mit Flächen -- ## wasser_2km &lt;- st_buffer(wasser,2000) ggplot(wasser_2km) + geom_sf(fill = &quot;blue&quot;) library(fasterize) raster_template &lt;- raster(extent(skelettgehalt), resolution = 1000) skelett_raster &lt;- fasterize(skelettgehalt,raster_template,field = &quot;SKELETT&quot;) ggplot() + geom_stars(data = st_as_stars(skelett_raster)) + coord_equal() # geom_stars() ist (noch) nicht so clever wie geom_sf() das CRS wird nicht berücksichtigt mode &lt;- function(x,na.rm = FALSE) { ux &lt;- unique(x) ux[which.max(tabulate(match(x, ux)))] } wasser_skelett$skelett_mode &lt;- extract(skelett_raster,wasser_2km,fun = mode)[,1] ggplot(wasser_skelett) + geom_bar(aes(skelett_mode)) + labs(x = &quot;Skelettgehalt gem. Variante 1&quot;, y = &quot;Anzahl&quot;, title = &quot;Vergleich zwischen den beiden Join Varianten&quot;) + facet_wrap(~SKELETT,labeller = label_both) ## -- Aufgabe 7: Spatial Join mit Kantone, Bezirke -- ## library(ggrepel) aggregate(x = wasser, by = kantone, FUN = mean) %&gt;% ggplot() + geom_sf(aes(fill = wasserverfuegbarkeit)) + geom_sf(data = wasser,size = 0.1) + geom_text_repel( data = summarise(group_by(kantone,NAME)), aes(label = NAME, geometry = geom), stat = &quot;sf_coordinates&quot; ) + labs(title = &quot;Mittlere Wasserverfügbarkeit im Boden&quot;, subtitle = &quot;pro Kanton&quot;, fill = &quot;&quot;) + theme_void() + theme(legend.position = &quot;bottom&quot;,legend.direction = &quot;horizontal&quot;) aggregate(wasser, bezirke, mean) %&gt;% ggplot() + geom_sf(aes(fill = wasserverfuegbarkeit)) + geom_sf(data = wasser,size = 0.1) + geom_sf(data = kantone, fill = NA, colour = &quot;grey&quot;) + labs(title = &quot;Mittlere Wasserverfügbarkeit im Boden&quot;, subtitle = &quot;pro Bezirk&quot;, fill = &quot;&quot;) + theme_void() + theme(legend.position = &quot;bottom&quot;,legend.direction = &quot;horizontal&quot;) ## -- Aufgabe 8 (für Ambitionierte) -- ## hex &lt;- st_make_grid(kantone,cellsize = 20000, square = FALSE) %&gt;% st_sf() %&gt;% aggregate(x = wasser, by = .,mean) %&gt;% st_intersection(st_union(kantone)) hex2 &lt;- st_join(hex,kantone,largest = TRUE) %&gt;% st_set_precision(100) %&gt;% group_by(Abk) %&gt;% summarise() ggplot() + geom_sf(data = hex, aes(fill = wasserverfuegbarkeit), colour = NA) + scale_fill_continuous(na.value = NA) + labs(title = &quot;Mittlere Wasserverfügbarkeit im Boden&quot;, subtitle = &quot;20km Hexagon&quot;, fill = &quot;&quot;) + geom_sf(data = hex2, colour = &quot;grey&quot;, fill = NA) + geom_sf_text(data = hex2, aes(label = Abk), size = 3, colour = &quot;grey&quot;) + theme_void() + theme(legend.position = &quot;bottom&quot;,legend.direction = &quot;horizontal&quot;) "],
["16-raumanalyse-2-26-11-2019.html", "Kapitel 16 Raumanalyse 2 (26.11.2019)", " Kapitel 16 Raumanalyse 2 (26.11.2019) "],
["16-1-thematische-einbettung-und-vorbereitung.html", "16.1 Thematische Einbettung und Vorbereitung", " 16.1 Thematische Einbettung und Vorbereitung In dieser zweiten Übung wirst Du wiederum Geodatensätze verarbeiten und darstellen. Wir starten mit einem Punktdatensatz zu einem Messnetz zur Erhebung der Luftqualität in der Schweiz (Stickstoffdioxid NO2 um genau zu sein). Im Gegensatz zum Punktdatensatz zur Wasserverfügbarkeit aus der vorherigen Übung, sind die Messstellen des Messnetzes zur Luftqualität sehr unregelmässig im Raum verteilt. Trotzdem möchten wir versuchen ein kontinuierliches Raster von Luftqualitätswerten für die ganze Schweiz zu interpolieren. Wir starten mit der einfachen Interpolations-Methode Inverse Distance Weighting IDW. Danach wollen wir für den gleichen Datensatz nach dem Ansatz der nächsten Nachbarn die Thiessen Polygone konstruieren. Im zweiten Teil der Übung wollen wir Dichteverteilung untersuchen. Dabei untersuchen wir einen Datensatz mit Bewegungsdaten eines Rotmilans in der Schweiz. Mittels einer Kernel Density Estimation (KDE) berechnen wir eine kontinuierliche Dichteverteilung, über die wir eine Annäherung an das Habitat des untersuchten Greifvogels berechnen können. Bevor wir aber starten, schauen wir uns die Punktdatensätze genauer an indem wir die G-Function berechnen und plotten. Importiere zunächst die Daten. Weise dann die korrekten Koordinatensysteme zu (CRS zuweisen). luftqualitaet.gpkg: Luftqualitätsmessungen NO2, (weitere Informationen) kantone.gpkg: Ein Datensatz der Swisstopo (weitere Informationen) rotmilan.gpkg: Der Datensatz rotmilan.gpkg stammt aus einem grösseren Forschungsprojekt der Vogelwarte Sempach Mechanismen der Populationsdynamik beim Rotmilan. Der Datensatz wurde über die Plattform movebank zur Verfügung gestellt. Es handelt sich dabei um ein einzelnes Individuum, welches seit 2017 mit einem Sender versehen ist und über ganz Mitteleuropa zieht. Wir arbeiten in dieser Übung nur mit denjenigen Datenpunkten, die in der Schweiz erfasst wurden. Wer den ganzen Datensatz analysieren möchte, kann sich diesen über den Movebank-Link runterladen. library(gstat) library(sf) library(tidyverse) library(lubridate) library(stars) luftqualitaet &lt;- read_sf(&quot;22_RaumAn2/data/luftqualitaet.gpkg&quot;) kantone &lt;- read_sf(&quot;21_RaumAn1/data/kantone.gpkg&quot;) rotmilan &lt;- read_sf(&quot;22_RaumAn2/data/rotmilan.gpkg&quot;) luftqualitaet &lt;- st_set_crs(luftqualitaet,2056) kantone &lt;- st_set_crs(kantone, 2056) rotmilan &lt;- st_set_crs(rotmilan, 2056) "],
["16-2-ubung-a-analyse-von-punktverteilungen.html", "16.2 Übung A: Analyse von Punktverteilungen", " 16.2 Übung A: Analyse von Punktverteilungen 16.2.1 Aufgabe 1: G-Function Als erstes berechnen wir die G-Function für die Rotmilanpositionen wie auch für die Luftqualitätsmessungen NO2: Schritt 1: Da es sich beim Rotmilan um relativ viele Datenpunkte handelt, nehmen wir mit st_sample() ein subset aus dem Datensatz. Dies verkürzt die Rechenzeit substantiell und verändert die Resultate kaum. rotmilan_sample &lt;- sample_n(rotmilan,1000) Schritt 2: Mit st_distance() können Distanzen zwischen zwei sf Datensätze berechnet werden. Wird nur ein Datensatz angegeben, wird eine Kreuzmatrix erstellt wo die Distanzen zwischen allen Features zu allen anderen Features dargestellt werden. Wir nützen diese Funktion zur Berechnung der nächsten Nachbarn. rotmilan_distanzmatrix &lt;- st_distance(rotmilan_sample) # zeige die ersten 6 Zeilen und Spalten der Matrix # jeder Distanz wurde 2x Gemessen (vergleiche Wert [2,1] mit [1,2]) # die Diagonale ist die Distanz zu sich selber (gleich 0) rotmilan_distanzmatrix[1:6,1:6] ## Units: [m] ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.00 3763.38 79723.66 36276.805 57242.66 37669.282 ## [2,] 3763.38 0.00 79608.46 34412.186 54845.86 35432.223 ## [3,] 79723.66 79608.46 0.00 51217.035 49319.89 53827.648 ## [4,] 36276.80 34412.19 51217.04 0.000 22078.58 4412.119 ## [5,] 57242.66 54845.86 49319.89 22078.580 0.00 19655.703 ## [6,] 37669.28 35432.22 53827.65 4412.119 19655.70 0.000 Schritt 3: Nun wollen wir wissen, wie gross die kürzeste Distanz von jedem Punkt zu seinem nächsten Nachbarn beträgt, also die kürzeste Distanz pro Zeile. Zuerst aber müssen wir die diagonalen Werte noch entfernen, denn diese stellen ja jeweils die Distanz zu sich selber dar und sind immer 0. Danach kann mit apply() eine Funktion (FUN = min) über die Zeilen (MARGIN = 1) einer Matrix (X = rotmilan_distanzmatrix) gerechnet werden. Zusätzlich müssen wir noch na.rm = TRUE setzen, damit NA Werte von der Berechnung ausgeschlossen werden. Das Resultat ist ein Vektor mit gleich vielen Werten wie Zeilen in der Matrix. diag(rotmilan_distanzmatrix) &lt;- NA # entfernt alle diagonalen Werte rotmilan_mindist &lt;- apply(rotmilan_distanzmatrix,1,min, na.rm = TRUE) Nun wollen wir die kumulierte Häufigkeit der Werte in einer Verteilungsfunktion (engl: Empirical Cumulative Distribution Function, ECDF) darstellen. Dafür müssen wir den Vektor zuerst noch in einen Dataframe packen, damit ggplot damit klar kommt: rotmilan_mindist_df &lt;- data.frame(distanzen = rotmilan_mindist) ggplot() + geom_step(data = rotmilan_mindist_df, aes(distanzen),stat = &quot;ecdf&quot;) Führe nun die gleichen Schritte mit luftqualitaet durch und vergleiche die ECDF-Plots. Hinweis: st_sample() ist bei luftqualitaet nicht nötig, da es sich hier um einen kleineren Datensatz handelt. "],
["16-3-ubung-a-losung-3.html", "16.3 Übung A – Lösung", " 16.3 Übung A – Lösung R-Script als Download library(gstat) library(sf) library(tidyverse) library(lubridate) library(stars) luftqualitaet &lt;- read_sf(&quot;22_RaumAn2/data/luftqualitaet.gpkg&quot;) kantone &lt;- read_sf(&quot;21_RaumAn1/data/kantone.gpkg&quot;) rotmilan &lt;- read_sf(&quot;22_RaumAn2/data/rotmilan.gpkg&quot;) luftqualitaet &lt;- st_set_crs(luftqualitaet,2056) kantone &lt;- st_set_crs(kantone, 2056) rotmilan &lt;- st_set_crs(rotmilan, 2056) ################################################### ## -- Übung A - Analyse von Punktverteilungen -- ## ################################################### ## -- Aufgabe 1: G-Function-- ## rotmilan_sample &lt;- sample_n(rotmilan,1000) rotmilan_distanzmatrix &lt;- st_distance(rotmilan_sample) # zeige die ersten 6 Zeilen und Spalten der Matrix # jeder Distanz wurde 2x Gemessen (vergleiche Wert [2,1] mit [1,2]) # die Diagonale ist die Distanz zu sich selber (gleich 0) rotmilan_distanzmatrix[1:6,1:6] diag(rotmilan_distanzmatrix) &lt;- NA # entfernt alle diagonalen Werte rotmilan_mindist &lt;- apply(rotmilan_distanzmatrix,1,min, na.rm = TRUE) rotmilan_mindist_df &lt;- data.frame(distanzen = rotmilan_mindist) ggplot() + geom_step(data = rotmilan_mindist_df, aes(distanzen),stat = &quot;ecdf&quot;) luftqualitaet_distanzmatrix &lt;- st_distance(luftqualitaet) diag(luftqualitaet_distanzmatrix) &lt;- NA luftqualitaet_mindist &lt;- apply(luftqualitaet_distanzmatrix,1,min,na.rm = TRUE) luftqualitaet_mindist_df &lt;- data.frame(distanzen = luftqualitaet_mindist, data = &quot;Luftqualität&quot;) rotmilan_mindist_df$data &lt;- &quot;Rotmilan&quot; mindist_df &lt;- rbind(luftqualitaet_mindist_df,rotmilan_mindist_df) ggplot() + geom_step(data = mindist_df, aes(distanzen, colour = data),stat = &quot;ecdf&quot;) + labs(colour = &quot;Datensatz&quot;) "],
["16-4-ubung-b-raumliche-interpolation.html", "16.4 Übung B: Räumliche Interpolation", " 16.4 Übung B: Räumliche Interpolation library(gstat) library(sf) library(tidyverse) library(lubridate) library(stars) luftqualitaet &lt;- read_sf(&quot;22_RaumAn2/data/luftqualitaet.gpkg&quot;) kantone &lt;- read_sf(&quot;21_RaumAn1/data/kantone.gpkg&quot;) rotmilan &lt;- read_sf(&quot;22_RaumAn2/data/rotmilan.gpkg&quot;) luftqualitaet &lt;- st_set_crs(luftqualitaet,2056) kantone &lt;- st_set_crs(kantone, 2056) rotmilan &lt;- st_set_crs(rotmilan, 2056) 16.4.1 Aufgabe 2: Räumliche Interpolation mit IDW Die Library gstat bietet verschiedene Möglichkeiten, Datenpunkte zu interpolieren, unter anderem auch den IDW. Leider ist das Package noch nicht so benutzerfreundlich wie sf: Das Package wird aber aktuell überarbeitet und in mittlerer Zukunft sollte es ebenso einfach zugänglich sein. Damit Ihr Euch nicht mit den Eigenheiten dieser Library umschlagen müsst, haben wir eine Function vorbereitet, die Euch die Verwendung der IDW-Interpolation erleichtern soll. Wir nehmen Euch damit etwas Komplexität weg und liefern Euch ein pfannenfertiges Werkzeug. Das hat auch Nachteile und wir ermutigen alle, die dafür Kapazität haben, unsere Function eingehend zu studieren und allenfalls ganz auf die Function zu verzichten und stattdessen direkt gstat zu verwenden. Egal für welche Variante Ihr Euch entscheidet, installiert vorgängig die Library gstat. Liest anschliessend die Funktion my_idw ein damit ihr sie nutzen könnt. my_idw &lt;- function(groundtruth,column,cellsize, nmax = Inf, maxdist = Inf, idp = 2, extent = NULL){ require(gstat) require(sf) require(raster) if(is.null(extent)){ extent &lt;- groundtruth } samples &lt;- st_make_grid(extent,cellsize,what = &quot;centers&quot;) %&gt;% st_as_sf() my_formula &lt;- formula(paste(column,&quot;~1&quot;)) idw_sf &lt;- gstat::idw(formula = my_formula,groundtruth,newdata = samples,nmin = 1, maxdist = maxdist, idp = idp) idw_matrix &lt;- cbind(st_coordinates(idw_sf),idw_sf$var1.pred) ras &lt;- raster::rasterFromXYZ(idw_matrix) if(all(grepl(&quot;polygon&quot;,st_geometry_type(extent),ignore.case = TRUE))){ ras &lt;- raster::mask(ras,st_as_sf(st_zm(extent))) } ras } Nun könnt Ihr mit my_idw() den Datensatz luftqualitaet folgendermassen interpolieren. my_idw(groundtruth = luftqualitaet,column = &quot;value&quot;,cellsize = 1000) Folgende Parameter stehen Euch zur Verfügung: Notwendige Parameter: groundtruth: Punktdatensatz mit den Messwerten (sf-Objekt) column: Name der Spalte mit den Messwerten (in Anführungs- und Schlusszeichen) cellsize: Zellgrösse des output Rasters Optionale Parameter nmax: Maximale Anzahl Punkte, die für die Interpolation berücksichtigt werden sollen. Default: Inf (alle Werte im gegebenen Suchradius) maxdist: Suchradius, welcher für die Interpolation verwendet werden soll. Default Inf (alle Werte bis nmax) idp: Inverse Distance Power: die Potenz, mit der der Nenner gesteigert werden soll. Default: 2. Werte werden im Kehrwert des Quadrates gewichtet: \\(\\frac{1}{dist^{idp}}\\). extent: Gebiet, für welches die Interpolation durchgeführt werden soll. Default NULL (die Ausdehnung von groundtruth). Wenn extent ein Polygon ist, wird die Interpolation für dieses Gebiet “geclipped” Rechnet so den IDW für die Luftqualitätsmessungen mit verschiedenen Parametern und visualisiert jeweils die Resultate (die Beschriftung der Werte ist fakultativ). ## [inverse distance weighted interpolation] ## [inverse distance weighted interpolation] ## [inverse distance weighted interpolation] ## [inverse distance weighted interpolation] 16.4.2 Aufgabe 3: Interpolation mit Nearest Neighbour / Thiessen Polygone Eine weitere einfache Möglichkeit zur Interpolation bietet die Erstellung eines Voronoi-Diagrammes, auch als Thiessen-Polygone oder Dirichlet-Zerlegung bekannt. sf liefert dazu die Funktion st_voronoi(), die einen Punktdatensatz annimmt und eben um die Punkte die Thiessenpolygone konstruiert. Dazu braucht es lediglich einen kleinen Vorverarbeitungsschritt: sf möchte für jedes Feature, also für jede Zeile in unserem Datensatz, ein Voronoidiagramm. Das macht bei uns wenig Sinn, weil jede Zeile nur aus einem Punkt besteht. Deshalb müssen wir vorher luftqualitaet mit st_union() von einem POINT in ein MULTIPOINT Objekt konvertieren, in welchem alle Punkte in einer Zeile zusammengefasst sind. luftqualitaet_union &lt;- st_union(luftqualitaet) thiessenpolygone &lt;- st_voronoi(luftqualitaet_union) thiessenpolygone &lt;- st_set_crs(thiessenpolygone, 2056) ggplot() + geom_sf(data = kantone) + geom_sf(data = thiessenpolygone, fill = NA) st_voronoi hat die Thiessenpolygone etwas weiter gezogen als wir sie wollen. Dies ist allerdings eine schöne Illustration der Randeffekte von Thiessenpolygonen, die zum Rand hin (wo es immer weniger Punkte hat) sehr gross werden können. Wir können die Polygone auf die Ausdehnung der Schweiz mit st_intersection() clippen. Auch hier braucht es zwei kleine Vorverarbeitungsschritte: wie vorher müssen wir die einzelnen Kantons-Polygone miteinander verschmelzen. Dies erreichen wir mit st_union(). Wir speichern den Output als schweiz, was als Resultat ein einzelnes Polygon der Schweizergrenze retourniert. für die Thiessen-Polygone machen wir genau das Umgekehrte: st_voronoi() liefert ein einzelnes Feature mit allen Polygonen, welches sich nicht gerne clippen lässt. Mit st_cast() wird die GEOMETRYCOLLECTION in Einzelpolygone aufgeteilt. schweiz &lt;- st_union(kantone) thiessenpolygone &lt;- st_cast(thiessenpolygone) thiessenpolygone_clip &lt;- st_intersection(thiessenpolygone,schweiz) ggplot() + geom_sf(data = kantone) + geom_sf(data = thiessenpolygone_clip, fill = NA) Jetzt müssen wir nur noch den jeweiligen Wert für jedes Polygon ermitteln. Dies erreichen wir wieder durch st_join. Auch hier ist noch ein kleiner Vorverarbeitungsschritt nötig: Wir konvertieren das sfc Objekt (nur Geometrien) in ein sf Objekt (Geometrien mit Attributtabelle). thiessenpolygone_clip &lt;- st_as_sf(thiessenpolygone_clip) thiessenpolygone_clip &lt;- st_join(thiessenpolygone_clip,luftqualitaet) ggplot() + geom_sf(data = kantone) + geom_sf(data = thiessenpolygone_clip, aes(fill = value)) + scale_fill_viridis_c() + theme_void() "],
["16-5-ubung-b-losung-3.html", "16.5 Übung B – Lösung", " 16.5 Übung B – Lösung R-Script als Download library(gstat) library(sf) library(tidyverse) library(lubridate) library(stars) luftqualitaet &lt;- read_sf(&quot;22_RaumAn2/data/luftqualitaet.gpkg&quot;) kantone &lt;- read_sf(&quot;21_RaumAn1/data/kantone.gpkg&quot;) rotmilan &lt;- read_sf(&quot;22_RaumAn2/data/rotmilan.gpkg&quot;) luftqualitaet &lt;- st_set_crs(luftqualitaet,2056) kantone &lt;- st_set_crs(kantone, 2056) rotmilan &lt;- st_set_crs(rotmilan, 2056) ############################################# ## -- Übung B - Räumliche Interpolation -- ## ############################################# ## -- Aufgabe 2: Räumliche Interpolation mit IDW -- ## my_idw &lt;- function(groundtruth,column,cellsize, nmax = Inf, maxdist = Inf, idp = 2, extent = NULL){ require(gstat) require(sf) require(raster) if(is.null(extent)){ extent &lt;- groundtruth } samples &lt;- st_make_grid(extent,cellsize,what = &quot;centers&quot;) %&gt;% st_as_sf() my_formula &lt;- formula(paste(column,&quot;~1&quot;)) idw_sf &lt;- gstat::idw(formula = my_formula,groundtruth,newdata = samples,nmin = 1, maxdist = maxdist, idp = idp) idw_matrix &lt;- cbind(st_coordinates(idw_sf),idw_sf$var1.pred) ras &lt;- raster::rasterFromXYZ(idw_matrix) if(all(grepl(&quot;polygon&quot;,st_geometry_type(extent),ignore.case = TRUE))){ ras &lt;- raster::mask(ras,st_as_sf(st_zm(extent))) } ras } ## my_idw(groundtruth = luftqualitaet,column = &quot;value&quot;,cellsize = 1000) ## nmax = Inf maxdist = 40000 idp = 2 idw &lt;- my_idw(luftqualitaet,&quot;value&quot;,1000,nmax = nmax,maxdist = maxdist,idp = idp, extent = kantone) luftqualitaet_extreme &lt;- luftqualitaet %&gt;% arrange(value) %&gt;% slice(c(1:5,(n()-4):n())) ggplot() + geom_stars(data = st_as_stars(idw)) + ggrepel::geom_text_repel( data = luftqualitaet_extreme, aes(label = value, geometry = geom), stat = &quot;sf_coordinates&quot;, min.segment.length = 0 ) + scale_fill_viridis_c(na.value = NA) + labs(title = &quot;Luftqualitätswerte Schweiz NO2, Interpoliert mit IDW&quot;, fill = &quot;μg/m3&quot;, subtitle = paste(&quot;nmax: &quot;,nmax,&quot;\\nmaxdist: &quot;,maxdist,&quot;\\nidp: &quot;,idp,sep = &quot; &quot;) ) + theme_void() + coord_equal() nmax = 50 maxdist = Inf idp = 1 idw &lt;- my_idw(luftqualitaet,&quot;value&quot;,1000,nmax = nmax,maxdist = maxdist,idp = idp, extent = kantone) ggplot() + geom_stars(data = st_as_stars(idw)) + ggrepel::geom_text_repel( data = luftqualitaet_extreme, aes(label = value, geometry = geom), stat = &quot;sf_coordinates&quot;, min.segment.length = 0 ) + scale_fill_viridis_c(na.value = NA) + labs(title = &quot;Luftqualitätswerte Schweiz NO2, Interpoliert mit IDW&quot;, fill = &quot;μg/m3&quot;, subtitle = paste(&quot;nmax: &quot;,nmax,&quot;\\nmaxdist: &quot;,maxdist,&quot;\\nidp: &quot;,idp,sep = &quot; &quot;) ) + theme_void()+ coord_equal() nmax = 50 maxdist = Inf idp = 2 idw &lt;- my_idw(luftqualitaet,&quot;value&quot;,1000,nmax = nmax,maxdist = maxdist,idp = idp, extent = kantone) ggplot() + geom_stars(data = st_as_stars(idw)) + ggrepel::geom_text_repel( data = luftqualitaet_extreme, aes(label = value, geometry = geom), stat = &quot;sf_coordinates&quot;, min.segment.length = 0 ) + scale_fill_viridis_c(na.value = NA) + labs(title = &quot;Luftqualitätswerte Schweiz NO2, Interpoliert mit IDW&quot;, fill = &quot;μg/m3&quot;, subtitle = paste(&quot;nmax: &quot;,nmax,&quot;\\nmaxdist: &quot;,maxdist,&quot;\\nidp: &quot;,idp,sep = &quot; &quot;) ) + theme_void()+ coord_equal() nmax = 50 maxdist = Inf idp = 3 idw &lt;- my_idw(luftqualitaet,&quot;value&quot;,1000,nmax = nmax,maxdist = maxdist,idp = idp, extent = kantone) ggplot() + geom_stars(data = st_as_stars(idw)) + ggrepel::geom_text_repel( data = luftqualitaet_extreme, aes(label = value, geometry = geom), stat = &quot;sf_coordinates&quot;, min.segment.length = 0 ) + scale_fill_viridis_c(na.value = NA) + labs(title = &quot;Luftqualitätswerte Schweiz NO2, Interpoliert mit IDW&quot;, fill = &quot;μg/m3&quot;, subtitle = paste(&quot;nmax: &quot;,nmax,&quot;\\nmaxdist: &quot;,maxdist,&quot;\\nidp: &quot;,idp,sep = &quot; &quot;) ) + theme_void() ## -- Aufgabe 3: Interpolation mit Nearest Neighbour / Thiessen Polygone -- ## luftqualitaet_union &lt;- st_union(luftqualitaet) thiessenpolygone &lt;- st_voronoi(luftqualitaet_union) thiessenpolygone &lt;- st_set_crs(thiessenpolygone, 2056) ggplot() + geom_sf(data = kantone) + geom_sf(data = thiessenpolygone, fill = NA) schweiz &lt;- st_union(kantone) thiessenpolygone &lt;- st_cast(thiessenpolygone) thiessenpolygone_clip &lt;- st_intersection(thiessenpolygone,schweiz) ggplot() + geom_sf(data = kantone) + geom_sf(data = thiessenpolygone_clip, fill = NA) thiessenpolygone_clip &lt;- st_as_sf(thiessenpolygone_clip) thiessenpolygone_clip &lt;- st_join(thiessenpolygone_clip,luftqualitaet) ggplot() + geom_sf(data = kantone) + geom_sf(data = thiessenpolygone_clip, aes(fill = value)) + scale_fill_viridis_c() + theme_void() "],
["16-6-ubung-c-dichteverteilungen.html", "16.6 Übung C: Dichteverteilungen", " 16.6 Übung C: Dichteverteilungen library(gstat) library(sf) library(tidyverse) library(lubridate) library(stars) luftqualitaet &lt;- read_sf(&quot;22_RaumAn2/data/luftqualitaet.gpkg&quot;) kantone &lt;- read_sf(&quot;21_RaumAn1/data/kantone.gpkg&quot;) rotmilan &lt;- read_sf(&quot;22_RaumAn2/data/rotmilan.gpkg&quot;) luftqualitaet &lt;- st_set_crs(luftqualitaet,2056) kantone &lt;- st_set_crs(kantone, 2056) rotmilan &lt;- st_set_crs(rotmilan, 2056) 16.6.1 Aufgabe 4: Rotmilan Bewegungsdaten visualisieren Die erste Frage, die bei solchen Bewegungsstudien typischerweise gestellt wird, lautet: Wo hält sich das Tier hauptsächlich auf? Um diese Frage zu beantworten, kann man als erstes einfach die Datenpunkte in einer einfachen Karte visualisieren. 16.6.2 Aufgabe 5: Kernel Density Estimation berechnen In einer ersten Annäherung funktioniert dies, doch wir sehen hier ein klassisches Problem des “Overplotting”. Das heisst, dass wir durch die Überlagerung vieler Punkte in den dichten Regionen nicht abschätzen können, wie viele Punkte dort effektiv liegen und ggf. übereinander liegen. Es gibt hier verschiedene Möglichkeiten, die Punktdichte klarer zu visualisieren. Eine unter Biologen sehr beliebte Methode ist die Dichteverteilung mit einer Kernel Density Estimation (KDE). Dies v.a. darum, weil mit KDE das Habitat (Streifgebiet) eines Tieres abgeschätzt werden kann. Homeranges werden oft mit KDE95 und Core Areas mit KDE50 definiert (Fleming C., Calabrese J., 2016). Ähnlich wie beim IDW sind auch die verfügbaren KDE-Funktionen in R etwas kompliziert in der Handhabung. Damit wir dieses Verfahren aber dennoch auf unsere Rotmilan-Daten anwenden können, haben wir eine eigene KDE-Funktion erstellt, die wir Euch zur Verfügung stellen. Die Funktion beruht auf den Libraries MASS, raster, sf und stars. Die letzen drei solltet Ihr bereits installiert haben. Überprüft nun, ob MASS ebenfalls schon installiert ist und holt dies bei Bedarf nach. Im Anschluss könnt Ihr die nachstehende Funktion einlesen: my_kde &lt;- function(points,cellsize, bandwith, extent = NULL){ require(MASS) require(raster) require(sf) require(stars) if(is.null(extent)){ extent_vec &lt;- st_bbox(points)[c(1,3,2,4)] } else{ extent_vec &lt;- st_bbox(extent)[c(1,3,2,4)] } n_y &lt;- ceiling((extent_vec[4]-extent_vec[3])/cellsize) n_x &lt;- ceiling((extent_vec[2]-extent_vec[1])/cellsize) extent_vec[2] &lt;- extent_vec[1]+(n_x*cellsize)-cellsize extent_vec[4] &lt;- extent_vec[3]+(n_y*cellsize)-cellsize coords &lt;- st_coordinates(points) matrix &lt;- kde2d(coords[,1],coords[,2],h = bandwith,n = c(n_x,n_y),lims = extent_vec) raster(matrix) } Die Parameter der Funktion sollten relativ klar sein: points: Ein Punktdatensatz aus der Class sf cellsize: Die Zellgrösse des output-Rasters bandwith: Der Suchradius für die Dichteberechnung extent (optional): Der Perimeter, in dem die Dichteverteilung berechnet werden soll. Wenn kein Perimeter angegeben wird, wird die “bounding box” von points genutzt. Wenn wir nun mit my_kde() die Dichteverteilung berechnen, erhalten wir einen Raseterdatensatz zurück, der sich mit base::plot() schnell visualisieren lässt. schweiz &lt;- st_union(kantone) rotmilan_kde &lt;- my_kde(rotmilan,cellsize = 1000,bandwith = 10000, extent = schweiz) rotmilan_kde ## class : RasterLayer ## dimensions : 221, 349, 77129 (nrow, ncol, ncell) ## resolution : 1000, 1000 (x, y) ## extent : 2484924, 2833924, 1074768, 1295768 (xmin, xmax, ymin, ymax) ## crs : NA ## source : memory ## names : layer ## values : 0, 1.125546e-08 (min, max) plot(rotmilan_kde) Um den Raster-Datensatz in ggplot() zu integrieren, müssen wir ihn zu einem stars-Objekt konvertieren und können danach geom_stars() verwenden. ggplot() + geom_stars(data = st_as_stars(rotmilan_kde)) + geom_sf(data = kantone, fill = NA) + scale_fill_viridis_c() + theme_void() + theme(legend.position = &quot;none&quot;) Die Kernel Density Estimation ist nun sehr stark von den tiefen Werten dominiert, da die Dichte in den meisten Zellen unseres Untersuchungsgebiets nahe bei Null liegt. Wir können die tiefen Werte ausblenden, indem wir nur die höchsten 5% der Werte darstellen. Dafür berechnen wir mit raster::quantile die 95. Perzentile aller Rasterzellen und nutzen dies um die Farbskala in ggplot zu limitieren (KDE95). Zusätzlich hilft eine logarithmische Transformation der KDE-Werte, die Farbskala etwas sichtbarer zu machen. q95 &lt;- raster::quantile(rotmilan_kde,probs = 0.95) ggplot() + geom_sf(data = kantone, fill = NA) + geom_stars(data = st_as_stars(rotmilan_kde),alpha = 0.8) + scale_fill_viridis_c(trans = &quot;log10&quot;,limits = c(q95,NA),na.value = NA) + theme_void() + labs(fill = &quot;KDE&quot;,title = &quot;Dichteverteilung von Bewegungsdaten eines Rotmilans&quot;,subtitle = &quot;Jahre 2017-2019&quot;) + theme(legend.position = &quot;top&quot;, legend.direction = &quot;horizontal&quot;) 16.6.3 Aufgabe 6: Dichteverteilung mit Thiessen Polygonen Thiessen Polygone bieten eine spannende Alternative um Unterschiede in der Dichteverteilung von Punktdatensätzen zu visualisieren. Wir wollen dies nun ausprobieren und konstruieren zum Schluss die Thiessenpolygone für die Rotmilan-Daten für das Untersuchungsgebiet Schweiz. thiessenpolygone &lt;- rotmilan %&gt;% st_union() %&gt;% st_voronoi() ggplot() + geom_sf(data = kantone) + geom_sf(data = thiessenpolygone, fill = NA) + geom_sf(data = rotmilan) + theme_void() schweiz &lt;- st_union(kantone) thiessenpolygone ## Geometry set for 1 feature ## geometry type: GEOMETRYCOLLECTION ## dimension: XY ## bbox: xmin: 2285390 ymin: 920063.6 xmax: 2958708 ymax: 1505862 ## epsg (SRID): 2056 ## proj4string: +proj=somerc +lat_0=46.95240555555556 +lon_0=7.439583333333333 +k_0=1 +x_0=2600000 +y_0=1200000 +ellps=bessel +towgs84=674.374,15.056,405.346,0,0,0,0 +units=m +no_defs thiessenpolygone &lt;- st_cast(thiessenpolygone) thiessenpolygone ## Geometry set for 4365 features ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: 2285390 ymin: 920063.6 xmax: 2958708 ymax: 1505862 ## epsg (SRID): 2056 ## proj4string: +proj=somerc +lat_0=46.95240555555556 +lon_0=7.439583333333333 +k_0=1 +x_0=2600000 +y_0=1200000 +ellps=bessel +towgs84=674.374,15.056,405.346,0,0,0,0 +units=m +no_defs ## First 5 geometries: # Dieser Schritt kann eine Weile dauern thiessenpolygone_clip &lt;- st_intersection(thiessenpolygone,schweiz) Wenn wir jetzt die Thiessenpolygone (ohne Punkte) darstellen, wird deutlicher, wie die Dichteverteilung im Innern des Clusters aussieht. "],
["16-7-ubung-c-losung.html", "16.7 Übung C – Lösung", " 16.7 Übung C – Lösung R-Script als Download library(gstat) library(sf) library(tidyverse) library(lubridate) library(stars) luftqualitaet &lt;- read_sf(&quot;22_RaumAn2/data/luftqualitaet.gpkg&quot;) kantone &lt;- read_sf(&quot;21_RaumAn1/data/kantone.gpkg&quot;) rotmilan &lt;- read_sf(&quot;22_RaumAn2/data/rotmilan.gpkg&quot;) luftqualitaet &lt;- st_set_crs(luftqualitaet,2056) kantone &lt;- st_set_crs(kantone, 2056) rotmilan &lt;- st_set_crs(rotmilan, 2056) ######################################## ## -- Übung C - Dichteverteilungen -- ## ######################################## ## -- Aufgabe 4: Rotmilan Bewegungsdaten visualisieren -- ## ggplot(kantone) + geom_sf() + geom_sf(data = rotmilan) + theme_void() ## -- Aufgabe 5: Kernel Density Estimation berechnen -- ## my_kde &lt;- function(points,cellsize, bandwith, extent = NULL){ require(MASS) require(raster) require(sf) require(stars) if(is.null(extent)){ extent_vec &lt;- st_bbox(points)[c(1,3,2,4)] } else{ extent_vec &lt;- st_bbox(extent)[c(1,3,2,4)] } n_y &lt;- ceiling((extent_vec[4]-extent_vec[3])/cellsize) n_x &lt;- ceiling((extent_vec[2]-extent_vec[1])/cellsize) extent_vec[2] &lt;- extent_vec[1]+(n_x*cellsize)-cellsize extent_vec[4] &lt;- extent_vec[3]+(n_y*cellsize)-cellsize coords &lt;- st_coordinates(points) matrix &lt;- kde2d(coords[,1],coords[,2],h = bandwith,n = c(n_x,n_y),lims = extent_vec) raster(matrix) } schweiz &lt;- st_union(kantone) rotmilan_kde &lt;- my_kde(rotmilan,cellsize = 1000,bandwith = 10000, extent = schweiz) rotmilan_kde plot(rotmilan_kde) ggplot() + geom_stars(data = st_as_stars(rotmilan_kde)) + geom_sf(data = kantone, fill = NA) + scale_fill_viridis_c() + theme_void() + theme(legend.position = &quot;none&quot;) q95 &lt;- raster::quantile(rotmilan_kde,probs = 0.95) ggplot() + geom_sf(data = kantone, fill = NA) + geom_stars(data = st_as_stars(rotmilan_kde),alpha = 0.8) + scale_fill_viridis_c(trans = &quot;log10&quot;,limits = c(q95,NA),na.value = NA) + theme_void() + labs(fill = &quot;KDE&quot;,title = &quot;Dichteverteilung von Bewegungsdaten eines Rotmilans&quot;,subtitle = &quot;Jahre 2017-2019&quot;) + theme(legend.position = &quot;top&quot;, legend.direction = &quot;horizontal&quot;) ## -- Aufgabe 6: Dichteverteilung mit Thiessen Polygonen -- ## thiessenpolygone &lt;- rotmilan %&gt;% st_union() %&gt;% st_voronoi() ggplot() + geom_sf(data = kantone) + geom_sf(data = thiessenpolygone, fill = NA) + geom_sf(data = rotmilan) + theme_void() schweiz &lt;- st_union(kantone) thiessenpolygone thiessenpolygone &lt;- st_cast(thiessenpolygone) thiessenpolygone # Dieser Schritt kann eine Weile dauern thiessenpolygone_clip &lt;- st_intersection(thiessenpolygone,schweiz) ggplot() + geom_sf(data = schweiz) + geom_sf(data = thiessenpolygone_clip) + theme_void() "],
["17-raumanalyse-3-03-12-2019.html", "Kapitel 17 Raumanalyse 3 (03.12.2019)", " Kapitel 17 Raumanalyse 3 (03.12.2019) "],
["17-1-thematische-einbettung-und-vorbereitung-1.html", "17.1 Thematische Einbettung und Vorbereitung", " 17.1 Thematische Einbettung und Vorbereitung Heute berechnen wir Morans \\(I\\) auf verschiedenen Datensätzen und vergleichen die Resultate. Dafür nutzen wir keine Funktion aus einem Package, sondern erarbeiten uns alles selber, basierend auf der Formel von Morans \\(I\\): \\[I = \\frac{n}{\\sum_{i=1}^n (y_i - \\bar{y})^2} \\times \\frac{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}(y_i - \\bar{y})(y_j - \\bar{y})}{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}}\\] Diese sieht sehr beeindruckend aus, aber wenn wir die Formel in ihre Einzelbestandteile aufteilen sehen wir, dass diese in sich gar nicht so komplex sind. Als erster Schritt müssen wir die notwendigen Libraries und Geodaten Laden: bezirke.gpkg wasserverfuegbarkeit_bode.gpkg library(tidyverse) library(sf) bezirke &lt;- read_sf(&quot;21_RaumAn1/data/bezirke.gpkg&quot;) wasser &lt;- read_sf(&quot;21_RaumAn1/data/wasserverfuegbarkeit_boden.gpkg&quot;) wasser &lt;- wasser %&gt;% st_set_crs(4326) %&gt;% st_transform(2056) bezirke &lt;- st_set_crs(bezirke, 2056) Nun können wir die mittlere Wasserverfügbarkeit pro Bezirk berechnen. Visualisiert dies in einem ersten Schritt und notiert euch, was für einen Wert ihr von Morans \\(I\\) erwartet (eher -1 oder eher 1 oder nahe 0?). Der Plot muss nicht genau so aussehen wie der unsere. wasser_bezirke &lt;- aggregate(wasser,bezirke, mean) "],
["17-2-ubung-a-morans-i.html", "17.2 Übung A: Morans I", " 17.2 Übung A: Morans I 17.2.1 Aufgabe 1: Herleitung der Formel Wir widmen uns der Berechnung von Morans \\(I\\) und packen dafür die Formel Schritt für Schritt aus und zerteilen so ein komplexes Problem in relativ simple Einzelteile. Dabei beginnen wir mit dem ersten Bruch und berechnen uns zuerst den Zähler, dann dem Nenner. So können wir den Bruch auflösen und uns dem zweiten Bruch zuwenden: \\[I = \\frac{n}{\\sum_{i=1}^n (y_i - \\bar{y})^2} \\times \\frac{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}(y_i - \\bar{y})(y_j - \\bar{y})}{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}}\\] 17.2.1.1 Bruch 1 Widmen wir uns dem erste Bruch: \\[\\frac{n}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\\] 17.2.1.1.1 Zähler (von Bruch 1) Beginnen wir mit dem Zähler, \\(n\\). Dies ist lediglich die Anzahl Messwerte in unserem Datensatz, also die Anzahl Bezirke. n &lt;- nrow(wasser_bezirke) n ## [1] 192 17.2.1.1.2 Nenner (von Bruch 1) Der Nenner des ersten Bruches (\\({\\sum_{i=1}^n (y_i - \\bar{y})^2}\\)) ist sehr ähnlich wie die Berechnung der Varianz: Summiere bei allen Messwerten (von \\(i = 1\\) bis \\(n\\)) folgendes: Das Quadrat von der Differenz aus jedem Messwert und dem Durchschnitt aller Messwerte. Also berechnen wir zuerst diese Differenzwerte (Messwert minus Mittelwert): # Die Werte aller Bezirke: y &lt;- wasser_bezirke$wasserverfuegbarkeit # Der Durchschnittswert aller Bezirke ybar &lt;- mean(y, na.rm = TRUE) # von jedem Wert den Durchschnittswert abziehen: dy &lt;- y - ybar Nun quadrieren wir diese Werte: dy_2 &lt;- dy^2 und ziehen daraus die Summe: dy_sum &lt;- sum(dy_2, na.rm = TRUE) 17.2.1.1.3 Zähler durch Nenner dividieren (Bruch 1) Um den ersten Bruch zu Lösen müssen wir nun noch die Anzahl werte durch dy_sum teilen. vr &lt;- n/dy_sum 17.2.1.2 Bruch 2 Nun zum zweiten Teil des Bruches \\[\\frac{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}(y_i - \\bar{y})(y_j - \\bar{y})}{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}}\\] Hier hat es jeweils zwei Summenzeichen nacheinander mit unterschiedlichen Laufvariablen (\\(i\\) und \\(j\\)). Dies bedeutet dass wir mit Kreuzmatrizen arbeiten, da wir alle Messwerte mit allen anderen Messwerten vergleichen (\\(w_{ij}\\) ist die erste Kreuzmatrix, \\((y_i - \\bar{y})(y_j - \\bar{y})\\) ist die zweite Kreuzmatrix). 17.2.1.2.1 Zähler (Bruch 2) Der erste Begriff, \\(w_{ij}\\), sind die räumlichen Gewichte aller Bezirke. Sind die Bezirke benachbart, dann gilt ein Gewicht von 1. Sind sie nicht benachbart, gilt ein Gewicht von 0. Wie wir “benachbart” definieren ist noch offen. Sie müssen sich berühren (dürfen sich aber nicht überlappen): st_touches() Sie müssen innerhalb einer bestimmten Distanz zueinander liegen: st_is_within_distance() Sie müssen sich überlappen: st_overlaps() Egal für welche Variante ihr euch entscheidet, setzt sparse = FASE damit eine Kreuzmatrix erstellt wird. w &lt;- st_touches(wasser_bezirke, sparse = FALSE) w[1:6, 1:6] ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] FALSE FALSE TRUE TRUE FALSE FALSE ## [2,] FALSE FALSE TRUE TRUE FALSE FALSE ## [3,] TRUE TRUE FALSE TRUE FALSE FALSE ## [4,] TRUE TRUE TRUE FALSE FALSE FALSE ## [5,] FALSE FALSE FALSE FALSE FALSE FALSE ## [6,] FALSE FALSE FALSE FALSE FALSE FALSE (Lasst euch nicht davon beirren, dass wir nun TRUE und FALSE statt 1 und 0 haben. In R sind TRUE und 1 äquivalent, sowie auch FALSE und 0) Wir sehen hier, dass der erste Bezirk im Datensatz (Zeile 1), den 3. und 4. Bezirk im Datensatz berührt. Er berührt aber den 2., 5. und 6. Bezirk nicht. Ob dies stimmt können wir einfach nachprüfen: Der nächste Teil, \\((y_i - \\bar{y})\\) kennen wir schon vom ersten Bruch und haben wir auch bereits gelöst. Wie eingangs erwähnt müssen wir durch das \\((y_j - \\bar{y})\\) diesmal das Produkt aller Wertekombinationen berechnen. Dies erreichen wir mit der Funktion tcrossprod(): pm &lt;- tcrossprod(dy) pm[1:6,1:6] ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 7156.025 7797.938 8933.269 8662.261 5339.446 8728.465 ## [2,] 7797.938 8497.434 9734.607 9439.288 5818.408 9511.430 ## [3,] 8933.269 9734.607 11151.904 10813.589 6665.532 10896.235 ## [4,] 8662.261 9439.288 10813.589 10485.538 6463.320 10565.676 ## [5,] 5339.446 5818.408 6665.532 6463.320 3984.011 6512.717 ## [6,] 8728.465 9511.430 10896.235 10565.676 6512.717 10646.427 Nun multiplizieren wir die Werte mit den Gewichten \\(w\\), damit wir nur noch die Werte von den Bezirken haben, die benachbart sind (und eliminieren nicht-benachbarte Werte). pmw &lt;- pm * w w[1:6,1:6] ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] FALSE FALSE TRUE TRUE FALSE FALSE ## [2,] FALSE FALSE TRUE TRUE FALSE FALSE ## [3,] TRUE TRUE FALSE TRUE FALSE FALSE ## [4,] TRUE TRUE TRUE FALSE FALSE FALSE ## [5,] FALSE FALSE FALSE FALSE FALSE FALSE ## [6,] FALSE FALSE FALSE FALSE FALSE FALSE pmw[1:6,1:6] ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.000 0.000 8933.269 8662.261 0 0 ## [2,] 0.000 0.000 9734.607 9439.288 0 0 ## [3,] 8933.269 9734.607 0.000 10813.589 0 0 ## [4,] 8662.261 9439.288 10813.589 0.000 0 0 ## [5,] 0.000 0.000 0.000 0.000 0 0 ## [6,] 0.000 0.000 0.000 0.000 0 0 Den Zähler des ersten Bruches haben wir errechnet, nachdem wir die Summe der gewichten Werten gebildet haben: spmw &lt;- sum(pmw, na.rm = TRUE) spmw ## [1] 5178725 17.2.1.2.2 Nenner (Bruch 2) Für den Nenner des zweiten Bruches müssen wir nur noch die die Gewichte summieren. Sprich: wie viele Bezirke sind benachbart? smw &lt;- sum(w, na.rm = TRUE) 17.2.1.2.3 Auflösung (Bruch 2) So können wir den zweiten Bruch auflösen: sw &lt;- spmw / smw 17.2.1.3 Brüche Multiplizieren Der allerletzte Schritt besteht darin, die Werte aus den Brüchen miteinander zu multiplizieren. MI &lt;- vr * sw MI ## [1] 0.5454743 17.2.2 Aufgabe 2: Morans I für Kantone berechnen Nun wollen wir die Wasserverfügbarkeitswerte auf der Ebene der Kantone aggregieren und untersuchen, ob und wie sich Morans \\(I\\) verändert. Importiere dafür den Datensatz “kantone.gpkg” aus RaumAn1 (s.u.), allenfalls musst du das CRS noch setzen. Aggregiere Wasserqualitätswerte auf ebene der Kantone, visualisiere diese anschliessend und berechne danach Morans \\(I\\). kantone.gpkg "],
["17-3-ubung-b-weitere-mapping-libraries.html", "17.3 Übung B: Weitere “mapping” Libraries", " 17.3 Übung B: Weitere “mapping” Libraries 17.3.1 Aufgabe 1: Interaktive Karten mit Plotly Was bei ggplot oftmals fehlt ist die Möglichkeit, mit der Karte zu interagieren: Zum Beispiel rein- und raus Zoomen sowie Werte einzelner Messpunkte anzeigen. Eine sehr einfache Möglichkeit dies zu erreichen ist mit der Library plotly. Die Funktion ggplotly() verwandelt einen ggplot-Plot automatisch in einen interaktiven Plot um. Speichere dazu den letzten Plot in einer Variabel (z.B: p_kantone &lt;- ggplot(wasser_kantone) + geom_sf()...). Installiere plotly und lade die Library. Wende nun ggplotly auf p_kantone an. library(sf) library(tidyverse) library(ggplot2) library(plotly) kantone &lt;- read_sf(&quot;21_RaumAn1/data/kantone.gpkg&quot;) wasser &lt;- read_sf(&quot;21_RaumAn1/data/wasserverfuegbarkeit_boden.gpkg&quot;) kantone &lt;- st_set_crs(kantone, 2056) wasser &lt;- wasser %&gt;% st_set_crs(4326) %&gt;% st_transform(2056) wasser_kantone &lt;- aggregate(wasser,kantone, mean) p_kantone &lt;- ggplot(wasser_kantone) + geom_sf(aes(fill = wasserverfuegbarkeit), colour = &quot;white&quot;,lwd = 0.2) + scale_fill_viridis_c() + labs(title = &quot;Mittlere Wasserverfügbarkeit nach Kantone&quot;,fill = &quot;&quot;) + theme_void() ggplotly(p_kantone) 17.3.2 Aufgabe 2: Hintergrundkarten mit tmap Was bei sowohl bei ggplot wie auch bei plolty fehlt, ist eine Hintergrundkarte. Diese Funktionalität wird von tmap zur Verfügung gestellt. Auf den erste Blick erscheint tmap nicht viel anders zu sein als ggplot: Der Syntax ist sehr ähnlich und auch output ist vergleichbar. library(tmap) tm_shape(wasser_kantone) + tm_polygons(col = &quot;wasserverfuegbarkeit&quot;, alpha = 0.3,palette = &quot;viridis&quot;,midpoint = NA) Setzt man jedoch tmap_mode() zu view, wird statt einer statischen Karte eine interaktive Karte mit Basemap generiert. tmap_mode(&quot;view&quot;) tm_shape(wasser_kantone) + tm_polygons(col = &quot;wasserverfuegbarkeit&quot;, alpha = 0.3,palette = &quot;viridis&quot;,midpoint = NA) "],
["17-4-ubung-c-optional-abstimmungsdaten.html", "17.4 Übung C (Optional): Abstimmungsdaten", " 17.4 Übung C (Optional): Abstimmungsdaten Datensatz importieren und libraries laden: zweitwohnungen_gemeinden.gpkg Morans \\(I\\) können wir auch als Funktion aufbereiten: morans_i &lt;- function(sf_object,col) { require(sf) n &lt;- nrow(sf_object) y &lt;- unlist(st_set_geometry(sf_object,NULL)[,col],use.names = FALSE) ybar &lt;- mean(y, na.rm = TRUE) dy &lt;- y - ybar dy_sum &lt;- sum(dy^2, na.rm = TRUE) vr &lt;- n/dy_sum w &lt;- st_touches(sf_object,sparse = FALSE) pm &lt;- tcrossprod(dy) pmw &lt;- pm * w spmw &lt;- sum(pmw, na.rm = TRUE) smw &lt;- sum(w, na.rm = TRUE) sw &lt;- spmw / smw MI &lt;- vr * sw MI } 17.4.1 Aufgabe 1: Morans I auf Gemeindeebene MI_zweitw_gemeinde &lt;- morans_i(gemeinden_zweitwohnung,&quot;ja_in_percent&quot;) ggplot() + geom_sf(data = gemeinden_zweitwohnung, aes(fill = ja_in_percent), colour = &quot;grey&quot;,lwd = 0.1) + labs(fill = &quot;Ja Anteil (%)&quot;, title = &quot;Zweitwohnungsinitiative, Gemeindeebene&quot;, subtitle = paste(&quot;Morans I&quot;,formatC(MI_zweitw_gemeinde,2,flag = &quot;+&quot;))) + scale_fill_gradient2(low = &quot;#2c7bb6&quot;,mid = &quot;#ffffbf&quot;,high = &quot;#d7191c&quot;,midpoint = 50,breaks = c(0,25,50,75,100),limits = c(0,100)) + theme_void() 17.4.2 Aufgabe 2: Morans I auf Kantonsebene Mit der Spalte kanton können wir die Gemeinde Resultate auf Kantonsebene aggregieren (mit group_by() und summarise()). Tipp: Nutze weighted.mean(): 17.4.3 Aufgabe 3: Bivariate Plot mit biscale Allenfalls ist es noch wichtig zu wissen, welche Bedeutung der Abstimmung in den einzelnen Gemeinden zugesprochen wurde. Also wie “laut” war das Nein bzw. das Ja. Dies können wir mit der Stimmbeteiligung ausdrücken. Um zwei Variablen gleichzeitig zu Visualisieren (z.B. Ja-Anteil und Stimmbeteiligung) gibt es die Möglichkeit einer bivariaten Farbskala. Diese wurde mit der Library biscale in ggplot implementiert. library(biscale) library(cowplot) data &lt;- gemeinden_zweitwohnung %&gt;% filter(!is.na(ja_in_percent), !is.na(beteiligung_in_percent)) %&gt;% bi_class(x = ja_in_percent, y = beteiligung_in_percent, style = &quot;equal&quot;, dim = 3) map &lt;- ggplot() + geom_sf(data = data, mapping = aes(fill = bi_class), color = &quot;white&quot;, size = 0.1, show.legend = FALSE) + bi_scale_fill(pal = &quot;DkBlue&quot;, dim = 3) + labs( title = &quot;&quot;, caption = &quot;Ja Anteil und Stimmbeteiligung (jeweils in %) auf der Gemeindeebene&quot; ) + bi_theme() legend &lt;- bi_legend(pal = &quot;DkBlue&quot;, dim = 3, xlab = &quot;%-Ja Anteil &quot;, ylab = &quot;%-Beteiligung &quot;, size = 8) finalPlot &lt;- ggdraw() + draw_plot(map, 0, 0, 1, 1) + draw_plot(legend, 0.1, .65, 0.2, 0.2) finalPlot "]
]
